<!DOCTYPE html>
<!-- saved from url=(0075)https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&page=3&format=pubmed&size=200 -->
<html lang="en"><head itemscope="" itemtype="http://schema.org/WebPage" prefix="og: http://ogp.me/ns#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Mobile properties -->
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov/">
  <link rel="preconnect" href="https://www.ncbi.nlm.nih.gov/">
  <link rel="preconnect" href="https://www.google-analytics.com/">

  
  
    <link rel="stylesheet" href="./page3_PubMed_files/output.5ecf62baa0fa.css" type="text/css">
  

  <link rel="stylesheet" href="./page3_PubMed_files/output.452c70ce66f7.css" type="text/css">

  
    
  

  
    <link rel="stylesheet" href="./page3_PubMed_files/output.97c300a159d1.css" type="text/css">
  

  


    <title>chatGPT - Search Results - PubMed</title>

  
  
  <!-- Favicons -->
  <link rel="shortcut icon" type="image/ico" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico">
  <link rel="icon" type="image/png" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.png">

  <!-- 192x192, as recommended for Android
  http://updates.html5rocks.com/2014/11/Support-for-theme-color-in-Chrome-39-for-Android
  -->
  <link rel="icon" type="image/png" sizes="192x192" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-192.png">

  <!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
  <link rel="apple-touch-icon-precomposed" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-57.png">
  <!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-72.png">
  <!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-114.png">
  <!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-144.png">


  <!-- For Pinger + Google Optimize integration (NS-820) -->
  <meta name="ncbi_sg_optimize_id" content="">

  <!-- Mobile browser address bar color -->
  <meta name="theme-color" content="#20558a">

  <!-- Preserve the Referrer when going from HTTPS to HTTP -->
  <meta name="referrer" content="origin-when-cross-origin">

  <meta name="ncbi_pinger_gtm_track" content="true">
<!-- Logging params: Pinger defaults -->

  
    <meta name="ncbi_app" content="pubmed">
  

  
    <meta name="ncbi_db" content="pubmed">
  

  
    <meta name="ncbi_phid" content="658B00010FEE1FF500003C893F9FF69C.1.m_7">
  

  
    <meta name="ncbi_pinger_stat_url" content="https://www.ncbi.nlm.nih.gov/stat">
  

  
    <meta name="log_category" content="literature">
  

  
    <meta name="ncbi_cost_center" content="pubmed">
  



  <!-- Logging params: Pinger custom -->
  
    <meta name="log_op" content="search">
  
    <meta name="log_query" content="chatGPT">
  
    <meta name="ncbi_pdid" content="searchresult">
  
    <meta name="ncbi_pageno" content="3">
  
    <meta name="log_resultcount" content="2844">
  
    <meta name="log_userterm" content="chatGPT">
  
    <meta name="log_processedquery" content="&quot;chatGPT&quot;[All Fields]">
  
    <meta name="log_filtersactive" content="False">
  
    <meta name="log_filters" content="">
  
    <meta name="ncbi_log_query" content="chatGPT">
  
    <meta name="log_proximity_search_active" content="False">
  
    <meta name="log_format" content="pubmed">
  
    <meta name="log_sortorder" content="relevance">
  
    <meta name="log_pagesize" content="200">
  
    <meta name="log_displayeduids" content="37530687,37252334,37337531,37387067,38421689,38246831,37722370,38133923,38170274,38284363,38348835,38534908,37202644,38533173,37982677,38315648,37392000,37410674,37757713,37839567,37987870,37182055,37529789,37648157,37968109,38347665,37917165,38516983,38314324,38314011,38365990,37692649,38180108,37676187,38545309,38201398,38007922,38180782,37903939,37626010,38420484,38440047,38163001,38263214,37844413,37212584,38457373,38237878,38465158,37123797,38215455,38126878,38245622,38340312,37936506,38032062,38270461,37389659,37855397,38113774,38428889,38448201,38430368,38248805,38361368,38149617,37423349,38354285,38486402,37962570,38249792,38148925,38076813,37385548,38507989,37701430,37529688,37502981,38485486,37822477,38037955,37923639,38124357,38238871,38435597,37713147,38309720,37296802,37330210,37930057,37779351,38246839,38135548,38069953,38243411,38366690,38350517,38331591,38533615,38197996,36869927,38010917,37908959,37553556,38540647,38327910,37183438,37040823,37693092,38144348,37821602,37246194,37368124,38478661,37058235,38490655,37989755,36981544,36946005,38234125,37939643,37855948,37499282,36924907,37158147,38435177,38313827,36834073,37991499,37725885,37794249,37560946,37352529,38049285,37547515,37926642,38020160,37999815,37699647,37632558,37625267,38016664,37678271,37948100,38371717,37462242,36753318,38182023,37707707,38016014,37809155,36757192,38206515,38458774,38310063,37840252,37714915,38206257,37709536,38152714,37229893,36701446,38409178,38283995,38000671,37833847,37634667,36798998,37984563,38194819,38015597,37462773,37519406,37210281,37792149,38279999,38334288,38088393,38443499,38523565,38268099,37659658,38056130,38058223,36972383,38108178,37457604,38394625,37493985,38353558,38501898,38313589,38357084,37368125,38379960,38247132,38167988,37705958,37193434,38059514">
  
    <meta name="ncbi_search_id" content="R3Cs2GC7H0x0KE8U7UASSA:deef7cbfe7e81e7152f8e0e261e5ab1a">
  
    <meta name="ncbi_adj_nav_search_id" content="Chfzx1mAtUpuMk-lqZpFcw:fbd847f89c96214ac23b7c62d7cb721a">
  



  <!-- Social meta tags for unfurling urls -->
  
<meta name="description" content="chatGPT - Search Results - PubMed"><meta name="robots" content="noindex,follow,noarchive"><meta property="og:title" content="chatGPT - Search Results - PubMed"><meta property="og:url" content="https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&amp;page=3&amp;format=pubmed&amp;size=200"><meta property="og:description" content="chatGPT - Search Results - PubMed"><meta property="og:image" content="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/pubmed-meta-image-v2.jpg"><meta property="og:image:secure_url" content="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/pubmed-meta-image-v2.jpg"><meta property="og:type" content="website"><meta property="og:site_name" content="PubMed"><meta name="twitter:domain" content="pubmed.ncbi.nlm.nih.gov"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="chatGPT - Search Results - PubMed"><meta name="twitter:url" content="https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&amp;page=3&amp;format=pubmed&amp;size=200"><meta name="twitter:description" content="chatGPT - Search Results - PubMed"><meta name="twitter:image" content="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/pubmed-meta-image-v2.jpg">


  <!-- OpenSearch XML -->
  <link rel="search" type="application/opensearchdescription+xml" href="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/opensearch.xml" title="PubMed search">

  <!-- Disables severely broken elements when no JS -->
  <noscript>
    <link rel="stylesheet" type="text/css" href="https://cdn.ncbi.nlm.nih.gov/pubmed/09ad9aad-98d9-47ec-b2ea-fb4dba3d550d/core/no-script.css">
  </noscript>

  
    <link rel="canonical" href="https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&amp;page=3&amp;format=pubmed&amp;size=200">
  


</head>
<body>

  
  <main class="search-page" id="search-page">
    <div class="search-results" id="search-results">
      
        <pre class="search-results-chunk">PMID- 37530687
OWN - NLM
STAT- Publisher
LR  - 20231004
IS  - 1531-6564 (Electronic)
IS  - 0363-5023 (Linking)
VI  - 48
IP  - 10
DP  - 2023 Oct
TI  - Exploring the Role of a Large Language Model on Carpal Tunnel Syndrome 
      Management: An&nbsp;Observation Study of ChatGPT.
PG  - 1025-1033
LID - S0363-5023(23)00360-X [pii]
LID - 10.1016/j.jhsa.2023.07.003 [doi]
AB  - PURPOSE: Recently, large language models, such as ChatGPT, have emerged as 
      promising tools to facilitate scientific research and health care management. The 
      present study aimed to explore the extent of knowledge possessed by ChatGPT 
      concerning carpal tunnel syndrome (CTS), a compressive neuropathy that may lead 
      to impaired hand function and that is frequently encountered in the field of hand 
      surgery. METHODS: Six questions pertaining to diagnosis and management of CTS 
      were posed to ChatGPT. The responses were subsequently analyzed and evaluated 
      based on their accuracy, coherence, and comprehensiveness. In addition, ChatGPT 
      was requested to provide five high-level evidence references in support of its 
      answers. A simulated doctor-patient consultation was also conducted to assess 
      whether ChatGPT could offer safe medical advice. RESULTS: ChatGPT supplied 
      clinically relevant information regarding CTS, although at a relatively 
      superficial level. In the context of doctor-patient interaction, ChatGPT 
      suggested a diagnostic pathway that deviated from the widely accepted clinical 
      consensus on CTS diagnosis. Nevertheless, it incorporated differential diagnoses 
      and valuable management options for CTS. Although ChatGPT demonstrated the 
      ability to retain and recall information from previous patient conversations, it 
      infrequently produced pertinent references, many of which were either nonexistent 
      or incorrect. CONCLUSIONS: ChatGPT displayed the capability to deliver validated 
      medical information on CTS to nonmedical individuals. However, the generation of 
      nonexistent and inaccurate references by ChatGPT presents a challenge to academic 
      integrity. CLINICAL RELEVANCE: To increase their utility in medicine and 
      academia, large language models must go through specialized reputable data set 
      training and validation from experts. It is essential to note that at present, 
      large language models cannot replace the expertise of health care professionals 
      and may act as a supportive tool.
CI  - Copyright © 2023. Published by Elsevier Inc.
FAU - Seth, Ishith
AU  - Seth I
AD  - Faculty of Medicine, Monash University, Melbourne, Victoria, Australia; 
      Department of Plastic Surgery, Peninsula Health, Melbourne, Victoria, Australia; 
      Faculty of Medicine, The University of Melbourne, Melbourne, Victoria, Australia. 
      Electronic address: ishithseth1@gmail.com.
FAU - Xie, Yi
AU  - Xie Y
AD  - Department of Plastic Surgery, Peninsula Health, Melbourne, Victoria, Australia.
FAU - Rodwell, Aaron
AU  - Rodwell A
AD  - Department of Surgery, The Wollongong Hospital, Wollongong, New South Wales, 
      Australia.
FAU - Gracias, Dylan
AU  - Gracias D
AD  - Department of Surgery, Townsville Hospital, Townsville, Queensland, Australia.
FAU - Bulloch, Gabriella
AU  - Bulloch G
AD  - Faculty of Medicine, The University of Melbourne, Melbourne, Victoria, Australia.
FAU - Hunter-Smith, David J
AU  - Hunter-Smith DJ
AD  - Faculty of Medicine, Monash University, Melbourne, Victoria, Australia.
FAU - Rozen, Warren M
AU  - Rozen WM
AD  - Faculty of Medicine, Monash University, Melbourne, Victoria, Australia; 
      Department of Plastic Surgery, Peninsula Health, Melbourne, Victoria, Australia.
LA  - eng
PT  - Journal Article
DEP - 20230801
PL  - United States
TA  - J Hand Surg Am
JT  - The Journal of hand surgery
JID - 7609631
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - CTS
OT  - ChatGPT
OT  - carpal tunnel syndrome
OT  - chatbot
EDAT- 2023/08/02 13:08
MHDA- 2023/08/02 13:08
CRDT- 2023/08/02 10:28
PHST- 2023/03/17 00:00 [received]
PHST- 2023/06/09 00:00 [revised]
PHST- 2023/07/10 00:00 [accepted]
PHST- 2023/08/02 13:08 [pubmed]
PHST- 2023/08/02 13:08 [medline]
PHST- 2023/08/02 10:28 [entrez]
AID - S0363-5023(23)00360-X [pii]
AID - 10.1016/j.jhsa.2023.07.003 [doi]
PST - ppublish
SO  - J Hand Surg Am. 2023 Oct;48(10):1025-1033. doi: 10.1016/j.jhsa.2023.07.003. Epub 
      2023 Aug 1.

PMID- 37252334
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230605
IS  - 2052-2975 (Print)
IS  - 2052-2975 (Electronic)
IS  - 2052-2975 (Linking)
VI  - 53
DP  - 2023 Jun
TI  - Enhancing infectious disease response: A demonstrative dialogue with ChatGPT and 
      ChatGPT-4 for future outbreak preparedness.
PG  - 101153
LID - 10.1016/j.nmni.2023.101153 [doi]
LID - 101153
FAU - Al-Tawfiq, Jaffar A
AU  - Al-Tawfiq JA
AD  - Specialty Internal Medicine and Quality Department, Johns Hopkins Aramco 
      Healthcare, Dhahran, Saudi Arabia.
AD  - Infectious Diseases Division, Department of Medicine, Indiana University School 
      of Medicine, Indianapolis, IN, USA.
AD  - Infectious Diseases Division, Department of Medicine, Johns Hopkins University 
      School of Medicine, Baltimore, MD, USA.
FAU - Jamal, Amr
AU  - Jamal A
AD  - Department of Family and Community Medicine, King Saud University, Riyadh, Saudi 
      Arabia.
FAU - Rodriguez-Morales, Alfonso J
AU  - Rodriguez-Morales AJ
AD  - Clinical Epidemiology and Biostatistics, Universidad Científica del Sur, Lima, 
      4861, Peru.
AD  - Gilbert and Rose-Marie Chagoury School of Medicine, Lebanese American University, 
      Beirut, 1102, Lebanon.
FAU - Temsah, Mohamad-Hani
AU  - Temsah MH
AD  - Department of Pediatrics, King Saud University Medical City, Riyadh, Saudi 
      Arabia.
LA  - eng
PT  - Journal Article
DEP - 20230519
PL  - England
TA  - New Microbes New Infect
JT  - New microbes and new infections
JID - 101624750
PMC - PMC10195765
OTO - NOTNLM
OT  - COVID-19
OT  - ChatGPT
OT  - ChatGPT-4
OT  - pandemics
COIS- The authors declare that they have no known competing financial interests or 
      personal relationships that could have appeared to influence the work reported in 
      this paper.
EDAT- 2023/05/30 13:07
MHDA- 2023/05/30 13:08
PMCR- 2023/05/19
CRDT- 2023/05/30 11:53
PHST- 2023/04/28 00:00 [received]
PHST- 2023/05/09 00:00 [revised]
PHST- 2023/05/12 00:00 [accepted]
PHST- 2023/05/30 13:08 [medline]
PHST- 2023/05/30 13:07 [pubmed]
PHST- 2023/05/30 11:53 [entrez]
PHST- 2023/05/19 00:00 [pmc-release]
AID - S2052-2975(23)00072-0 [pii]
AID - 101153 [pii]
AID - 10.1016/j.nmni.2023.101153 [doi]
PST - ppublish
SO  - New Microbes New Infect. 2023 Jun;53:101153. doi: 10.1016/j.nmni.2023.101153. 
      Epub 2023 May 19.

PMID- 37337531
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230621
IS  - 2632-6140 (Electronic)
IS  - 2632-6140 (Linking)
VI  - 5
IP  - 1
DP  - 2023
TI  - Evaluating the limits of AI in medical specialisation: ChatGPT's performance on 
      the UK Neurology Specialty Certificate Examination.
PG  - e000451
LID - 10.1136/bmjno-2023-000451 [doi]
LID - e000451
AB  - BACKGROUND: Large language models such as ChatGPT have demonstrated potential as 
      innovative tools for medical education and practice, with studies showing their 
      ability to perform at or near the passing threshold in general medical 
      examinations and standardised admission tests. However, no studies have assessed 
      their performance in the UK medical education context, particularly at a 
      specialty level, and specifically in the field of neurology and neuroscience. 
      METHODS: We evaluated the performance of ChatGPT in higher specialty training for 
      neurology and neuroscience using 69 questions from the Pool-Specialty Certificate 
      Examination (SCE) Neurology Web Questions bank. The dataset primarily focused on 
      neurology (80%). The questions spanned subtopics such as symptoms and signs, 
      diagnosis, interpretation and management with some questions addressing specific 
      patient populations. The performance of ChatGPT 3.5 Legacy, ChatGPT 3.5 Default 
      and ChatGPT-4 models was evaluated and compared. RESULTS: ChatGPT 3.5 Legacy and 
      ChatGPT 3.5 Default displayed overall accuracies of 42% and 57%, respectively, 
      falling short of the passing threshold of 58% for the 2022 SCE neurology 
      examination. ChatGPT-4, on the other hand, achieved the highest accuracy of 64%, 
      surpassing the passing threshold and outperforming its predecessors across 
      disciplines and subtopics. CONCLUSIONS: The advancements in ChatGPT-4's 
      performance compared with its predecessors demonstrate the potential for 
      artificial intelligence (AI) models in specialised medical education and 
      practice. However, our findings also highlight the need for ongoing development 
      and collaboration between AI developers and medical experts to ensure the models' 
      relevance and reliability in the rapidly evolving field of medicine.
CI  - © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No 
      commercial re-use. See rights and permissions. Published by BMJ.
FAU - Giannos, Panagiotis
AU  - Giannos P
AUID- ORCID: 0000-0003-1037-1983
AD  - Department of Life Sciences, Imperial College London, London, UK.
AD  - Society of Meta-Research and Biomedical Innovation, London, UK.
AD  - Promotion of Emerging and Evaluative Research Society, London, UK.
LA  - eng
PT  - Journal Article
DEP - 20230615
PL  - England
TA  - BMJ Neurol Open
JT  - BMJ neurology open
JID - 101775450
PMC - PMC10277081
OTO - NOTNLM
OT  - clinical neurology
OT  - health policy &amp; practice
OT  - medicine
COIS- Competing interests: None declared.
EDAT- 2023/06/20 06:42
MHDA- 2023/06/20 06:43
PMCR- 2023/06/15
CRDT- 2023/06/20 02:26
PHST- 2023/06/06 00:00 [accepted]
PHST- 2023/06/20 06:43 [medline]
PHST- 2023/06/20 06:42 [pubmed]
PHST- 2023/06/20 02:26 [entrez]
PHST- 2023/06/15 00:00 [pmc-release]
AID - bmjno-2023-000451 [pii]
AID - 10.1136/bmjno-2023-000451 [doi]
PST - epublish
SO  - BMJ Neurol Open. 2023 Jun 15;5(1):e000451. doi: 10.1136/bmjno-2023-000451. 
      eCollection 2023.

PMID- 37387067
OWN - NLM
STAT- MEDLINE
DCOM- 20231217
LR  - 20231217
IS  - 1879-8365 (Electronic)
IS  - 0926-9630 (Linking)
VI  - 305
DP  - 2023 Jun 29
TI  - Revolutionizing Healthcare with Foundation AI Models.
PG  - 469-470
LID - 10.3233/SHTI230533 [doi]
AB  - ChatGPT is a foundation Artificial Intelligence (AI) model that has opened up new 
      opportunities in digital healthcare. Particularly, it can serve as a co-pilot 
      tool for doctors in the interpretation, summarization, and completion of reports. 
      Furthermore, it can build upon the ability to access the large literature and 
      knowledge on the internet. So, chatGPT could generate acceptable responses for 
      the medical examination. Hence. It offers the possibility of enhancing healthcare 
      accessibility, expandability, and effectiveness. Nonetheless, chatGPT is 
      vulnerable to inaccuracies, false information, and bias. This paper briefly 
      describes the potential of Foundation AI models to transform future healthcare by 
      presenting ChatGPT as an example tool.
FAU - Ali, Hazrat
AU  - Ali H
AD  - College of Science and Engineering, Hamad Bin Khalifa University, Qatar 
      Foundation, Doha, Qatar.
FAU - Qadir, Junaid
AU  - Qadir J
AD  - Department of Computer Engineering, Qatar University, Doha, Qatar.
FAU - Alam, Tanvir
AU  - Alam T
AD  - College of Science and Engineering, Hamad Bin Khalifa University, Qatar 
      Foundation, Doha, Qatar.
FAU - Househ, Mowafa
AU  - Househ M
AD  - College of Science and Engineering, Hamad Bin Khalifa University, Qatar 
      Foundation, Doha, Qatar.
FAU - Shah, Zubair
AU  - Shah Z
AD  - College of Science and Engineering, Hamad Bin Khalifa University, Qatar 
      Foundation, Doha, Qatar.
LA  - eng
PT  - Journal Article
PL  - Netherlands
TA  - Stud Health Technol Inform
JT  - Studies in health technology and informatics
JID - 9214582
MH  - Humans
MH  - *Delivery of Health Care/trends
MH  - *Artificial Intelligence
MH  - Internet
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - ChatGPT
OT  - Foundation AI models
OT  - Healthcare
OT  - Medical AI
EDAT- 2023/06/30 06:42
MHDA- 2023/07/03 06:41
CRDT- 2023/06/30 04:23
PHST- 2023/07/03 06:41 [medline]
PHST- 2023/06/30 06:42 [pubmed]
PHST- 2023/06/30 04:23 [entrez]
AID - SHTI230533 [pii]
AID - 10.3233/SHTI230533 [doi]
PST - ppublish
SO  - Stud Health Technol Inform. 2023 Jun 29;305:469-470. doi: 10.3233/SHTI230533.

PMID- 38421689
OWN - NLM
STAT- MEDLINE
DCOM- 20240301
LR  - 20240317
IS  - 2369-3762 (Electronic)
IS  - 2369-3762 (Linking)
VI  - 10
DP  - 2024 Feb 29
TI  - Exploring the Feasibility of Using ChatGPT to Create Just-in-Time Adaptive 
      Physical Activity mHealth Intervention Content: Case Study.
PG  - e51426
LID - 10.2196/51426 [doi]
LID - e51426
AB  - BACKGROUND: Achieving physical activity (PA) guidelines' recommendation of 150 
      minutes of moderate-to-vigorous PA per week has been shown to reduce the risk of 
      many chronic conditions. Despite the overwhelming evidence in this field, PA 
      levels remain low globally. By creating engaging mobile health (mHealth) 
      interventions through strategies such as just-in-time adaptive interventions 
      (JITAIs) that are tailored to an individual's dynamic state, there is potential 
      to increase PA levels. However, generating personalized content can take a long 
      time due to various versions of content required for the personalization 
      algorithms. ChatGPT presents an incredible opportunity to rapidly produce 
      tailored content; however, there is a lack of studies exploring its feasibility. 
      OBJECTIVE: This study aimed to (1) explore the feasibility of using ChatGPT to 
      create content for a PA JITAI mobile app and (2) describe lessons learned and 
      future recommendations for using ChatGPT in the development of mHealth JITAI 
      content. METHODS: During phase 1, we used Pathverse, a no-code app builder, and 
      ChatGPT to develop a JITAI app to help parents support their child's PA levels. 
      The intervention was developed based on the Multi-Process Action Control (M-PAC) 
      framework, and the necessary behavior change techniques targeting the M-PAC 
      constructs were implemented in the app design to help parents support their 
      child's PA. The acceptability of using ChatGPT for this purpose was discussed to 
      determine its feasibility. In phase 2, we summarized the lessons we learned 
      during the JITAI content development process using ChatGPT and generated 
      recommendations to inform future similar use cases. RESULTS: In phase 1, by using 
      specific prompts, we efficiently generated content for 13 lessons relating to 
      increasing parental support for their child's PA following the M-PAC framework. 
      It was determined that using ChatGPT for this case study to develop PA content 
      for a JITAI was acceptable. In phase 2, we summarized our recommendations into 
      the following six steps when using ChatGPT to create content for mHealth behavior 
      interventions: (1) determine target behavior, (2) ground the intervention in 
      behavior change theory, (3) design the intervention structure, (4) input 
      intervention structure and behavior change constructs into ChatGPT, (5) revise 
      the ChatGPT response, and (6) customize the response to be used in the 
      intervention. CONCLUSIONS: ChatGPT offers a remarkable opportunity for rapid 
      content creation in the context of an mHealth JITAI. Although our case study 
      demonstrated that ChatGPT was acceptable, it is essential to approach its use, 
      along with other language models, with caution. Before delivering content to 
      population groups, expert review is crucial to ensure accuracy and relevancy. 
      Future research and application of these guidelines are imperative as we deepen 
      our understanding of ChatGPT and its interactions with human input.
CI  - ©Amanda Willms, Sam Liu. Originally published in JMIR Medical Education 
      (https://mededu.jmir.org), 29.02.2024.
FAU - Willms, Amanda
AU  - Willms A
AUID- ORCID: 0000-0002-2644-5804
AD  - School of Exercise Science, Physical and Health Education, University of 
      Victoria, Victoria, BC, Canada.
FAU - Liu, Sam
AU  - Liu S
AUID- ORCID: 0000-0003-2364-7774
AD  - School of Exercise Science, Physical and Health Education, University of 
      Victoria, Victoria, BC, Canada.
LA  - eng
PT  - Journal Article
DEP - 20240229
PL  - Canada
TA  - JMIR Med Educ
JT  - JMIR medical education
JID - 101684518
SB  - IM
MH  - Child
MH  - Humans
MH  - Feasibility Studies
MH  - *Exercise
MH  - Health Behavior
MH  - Algorithms
MH  - *Telemedicine
PMC - PMC10940976
OTO - NOTNLM
OT  - ChatGPT
OT  - app design
OT  - application
OT  - behavior change
OT  - content creation
OT  - digital health
OT  - mHealth
OT  - mobile app
OT  - mobile apps
OT  - mobile health
OT  - physical activity
COIS- Conflicts of Interest: None declared.
EDAT- 2024/02/29 12:44
MHDA- 2024/03/01 06:44
PMCR- 2024/02/29
CRDT- 2024/02/29 11:53
PHST- 2023/07/31 00:00 [received]
PHST- 2023/12/27 00:00 [accepted]
PHST- 2023/12/15 00:00 [revised]
PHST- 2024/03/01 06:44 [medline]
PHST- 2024/02/29 12:44 [pubmed]
PHST- 2024/02/29 11:53 [entrez]
PHST- 2024/02/29 00:00 [pmc-release]
AID - v10i1e51426 [pii]
AID - 10.2196/51426 [doi]
PST - epublish
SO  - JMIR Med Educ. 2024 Feb 29;10:e51426. doi: 10.2196/51426.

PMID- 38246831
OWN - NLM
STAT- MEDLINE
DCOM- 20240311
LR  - 20240311
IS  - 1938-0682 (Electronic)
IS  - 1558-7673 (Linking)
VI  - 22
IP  - 2
DP  - 2024 Apr
TI  - Urological Cancers and ChatGPT: Assessing the Quality of Information and Possible 
      Risks for Patients.
PG  - 454-457.e4
LID - S1558-7673(23)00283-5 [pii]
LID - 10.1016/j.clgc.2023.12.017 [doi]
AB  - INTRODUCTION: OpenAI has created ChatGPT, an artificial intelligence language 
      model that has gained considerable recognition for its capacity to produce text 
      responses resembling human language. Consequently, this study seeks to evaluate 
      the effectiveness of ChatGPT's responses in addressing publicly accessible 
      queries related to prostate, kidney, bladder, and testicular cancers. MATERIAL 
      AND METHODS: A comprehensive compilation of frequently asked questions (FAQs) 
      pertaining to prostate, bladder, kidney, and testicular cancers was gathered from 
      diverse sources. Additionally, the recommendations outlined in the European 
      Association of Urology (EAU) 2023 Guideline Oncology were consulted. The chosen 
      questions for evaluation were presented to the ChatGPT 4.0 premium version. The 
      quality of ChatGPT responses was appraised using the global quality score (GQS). 
      Each ChatGPT response was independently reviewed by a panel of physicians, who 
      assigned a GQS score to assess its overall quality. RESULTS: For prostate cancer, 
      64.6% of the questions had a GQS score of 5, compared to 62.9 % for bladder, 
      68.1% for kidney, and 63.9% for testicular cancers, whereas none of the responses 
      had a GQS score of 1. Meanwhile, the category with the lowest proportion of 
      responses, with a GQS score of 5 for each disease, was prognosis and follow-up. 
      The mean GQS score of the answers given to EAU guideline questions was 
      statistically significantly lower than the average score of the answers given to 
      FAQs. CONCLUSION: ChatGPT is a valuable tool for addressing general inquiries 
      regarding urological cancers, boasting commendable accuracy rates. Nonetheless, 
      its performance in responding to questions aligned with the EAU guideline was 
      deemed unsatisfactory.
CI  - Copyright © 2024 Elsevier Inc. All rights reserved.
FAU - Ozgor, Faruk
AU  - Ozgor F
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey. 
      Electronic address: md.farukozgor@gmail.com.
FAU - Caglar, Ufuk
AU  - Caglar U
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
FAU - Halis, Ahmet
AU  - Halis A
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
FAU - Cakir, Hakan
AU  - Cakir H
AD  - Department of Urology, Fulya Acibadem Hospital, Istanbul, Turkey.
FAU - Aksu, Ufuk Can
AU  - Aksu UC
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
FAU - Ayranci, Ali
AU  - Ayranci A
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
FAU - Sarilar, Omer
AU  - Sarilar O
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20240105
PL  - United States
TA  - Clin Genitourin Cancer
JT  - Clinical genitourinary cancer
JID - 101260955
RN  - Testicular Germ Cell Tumor
SB  - IM
MH  - Male
MH  - Humans
MH  - Artificial Intelligence
MH  - *Urologic Neoplasms
MH  - *Testicular Neoplasms
MH  - *Urology
MH  - *Neoplasms, Germ Cell and Embryonal
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Global quality score
OT  - Information sources
OT  - Urooncology
COIS- Disclosure The authors have no conflicts of interest to declare. All co-authors 
      have seen and agree with the contents of the manuscript and there is no financial 
      interest to report. We certify that the submission is original work and is not 
      under review at any other publication.
EDAT- 2024/01/22 00:42
MHDA- 2024/03/11 06:44
CRDT- 2024/01/21 21:54
PHST- 2023/11/20 00:00 [received]
PHST- 2023/12/23 00:00 [revised]
PHST- 2023/12/31 00:00 [accepted]
PHST- 2024/03/11 06:44 [medline]
PHST- 2024/01/22 00:42 [pubmed]
PHST- 2024/01/21 21:54 [entrez]
AID - S1558-7673(23)00283-5 [pii]
AID - 10.1016/j.clgc.2023.12.017 [doi]
PST - ppublish
SO  - Clin Genitourin Cancer. 2024 Apr;22(2):454-457.e4. doi: 
      10.1016/j.clgc.2023.12.017. Epub 2024 Jan 5.

PMID- 37722370
OWN - NLM
STAT- MEDLINE
DCOM- 20231207
LR  - 20231207
IS  - 1421-9832 (Electronic)
IS  - 1018-8665 (Linking)
VI  - 239
IP  - 6
DP  - 2023
TI  - Trends in Accuracy and Appropriateness of Alopecia Areata Information Obtained 
      from a Popular Online Large Language Model, ChatGPT.
PG  - 952-957
LID - 10.1159/000534005 [doi]
AB  - BACKGROUND: Patients with alopecia areata (AA) may access a wide range of sources 
      for information about AA, including the recently developed ChatGPT. Assessing the 
      quality of health information provided by these sources is crucial, as patients 
      are utilizing them in increasing numbers. OBJECTIVES: The aim of the study was to 
      evaluate appropriateness and accuracy of responses to common patient questions 
      about AA generated by ChatGPT. METHODS: Responses generated by ChatGPT 3.5 and 
      ChatGPT 4.0 to 25 questions addressing common patient concerns were assessed by 
      multiple attending dermatologists in an academic center for appropriateness and 
      accuracy. Appropriateness of responses by both models for use in two hypothetical 
      contexts as follows: (1) for patient-facing general information websites, and (2) 
      for electronic health record (EHR) message drafts. RESULTS: The accuracy across 
      all responses was 4.41 out of 5. Accuracy scores of responses ChatGPT 3.5 
      responses had a mean of 4.29, whereas those generated by ChatGPT 4.0 had mean 
      accuracy score of 4.53. Assessments ranged from 100% of responses rated as 
      appropriate for the general question category to 79% questions about management 
      for an EHR message draft. Raters largely preferred responses generated by ChatGPT 
      4.0 versus ChatGPT 3.5. Reviewer agreement was found to be moderate across all 
      questions, with a 53.7% agreement and Fleiss' κ co-efficient of 0.522 (p value 
      &amp;lt;0.001). CONCLUSIONS: The large language model ChatGPT outputted mostly 
      appropriate information for common patient concerns. While not all responses were 
      accurate, the trend toward improvement with newer iterations suggests potential 
      future utility for patients and dermatologists.
CI  - © 2023 S. Karger AG, Basel.
FAU - O'Hagan, Ross
AU  - O'Hagan R
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York, USA.
FAU - Kim, Randie H
AU  - Kim RH
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York, USA.
FAU - Abittan, Brian J
AU  - Abittan BJ
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York, USA.
FAU - Caldas, Stella
AU  - Caldas S
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York, USA.
FAU - Ungar, Jonathan
AU  - Ungar J
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York, USA.
FAU - Ungar, Benjamin
AU  - Ungar B
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York, USA.
LA  - eng
PT  - Journal Article
DEP - 20230918
PL  - Switzerland
TA  - Dermatology
JT  - Dermatology (Basel, Switzerland)
JID - 9203244
RN  - Diffuse alopecia
SB  - IM
MH  - Humans
MH  - *Alopecia Areata/drug therapy
MH  - Head
MH  - Language
MH  - Research Personnel
OTO - NOTNLM
OT  - Alopecia areata
OT  - Artificial intelligence
OT  - ChatGPT
EDAT- 2023/09/19 00:43
MHDA- 2023/12/07 12:42
CRDT- 2023/09/18 18:23
PHST- 2023/04/24 00:00 [received]
PHST- 2023/09/04 00:00 [accepted]
PHST- 2023/12/07 12:42 [medline]
PHST- 2023/09/19 00:43 [pubmed]
PHST- 2023/09/18 18:23 [entrez]
AID - 000534005 [pii]
AID - 10.1159/000534005 [doi]
PST - ppublish
SO  - Dermatology. 2023;239(6):952-957. doi: 10.1159/000534005. Epub 2023 Sep 18.

PMID- 38133923
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240108
IS  - 2561-326X (Electronic)
IS  - 2561-326X (Linking)
VI  - 7
DP  - 2023 Dec 22
TI  - College Students' Employability, Cognition, and Demands for ChatGPT in the AI Era 
      Among Chinese Nursing Students: Web-Based Survey.
PG  - e50413
LID - 10.2196/50413 [doi]
LID - e50413
AB  - BACKGROUND: With the rapid development of artificial intelligence (AI) and the 
      widespread use of ChatGPT, nursing students' artificial intelligence quotient 
      (AIQ), employability, cognition, and demand for ChatGPT are worthy of attention. 
      OBJECTIVE: We aimed to investigate Chinese nursing students' AIQ and 
      employability status as well as their cognition and demand for the latest AI 
      tool-ChatGPT. This study was conducted to guide future initiatives in nursing 
      intelligence education and to improve the employability of nursing students. 
      METHODS: We used a cross-sectional survey to understand nursing college students' 
      AIQ, employability, cognition, and demand for ChatGPT. Using correlation analysis 
      and multiple hierarchical regression analysis, we explored the relevant factors 
      in the employability of nursing college students. RESULTS: In this study, out of 
      1788 students, 1453 (81.30%) had not used ChatGPT, and 1170 (65.40%) had never 
      heard of ChatGPT before this survey. College students' employability scores were 
      positively correlated with AIQ, self-regulation ability, and their home location 
      and negatively correlated with school level. Additionally, men scored higher on 
      college students' employability compared to women. Furthermore, 76.5% of the 
      variance was explained by the multiple hierarchical regression model for 
      predicting college students' employability scores. CONCLUSIONS: Chinese nursing 
      students have limited familiarity and experience with ChatGPT, while their AIQ 
      remains intermediate. Thus, educators should pay more attention to cultivating 
      nursing students' AIQ and self-regulation ability to enhance their employability. 
      Employability, especially for female students, those from rural backgrounds, and 
      students in key colleges, deserves more attention in future educational efforts.
CI  - ©Yuanyuan Luo, Huiting Weng, Li Yang, Ziwei Ding, Qin Wang. Originally published 
      in JMIR Formative Research (https://formative.jmir.org), 22.12.2023.
FAU - Luo, Yuanyuan
AU  - Luo Y
AUID- ORCID: 0009-0007-0889-5098
AD  - Clinical Nursing Teaching and Research Section, The Second Xiangya Hospital of 
      Central South University, Changsha, China.
AD  - Xiangya School of Nursing, Central South University, Changsha, Hunan, China.
FAU - Weng, Huiting
AU  - Weng H
AUID- ORCID: 0000-0002-6071-4369
AD  - Clinical Nursing Teaching and Research Section, The Second Xiangya Hospital of 
      Central South University, Changsha, China.
FAU - Yang, Li
AU  - Yang L
AUID- ORCID: 0000-0001-5766-2978
AD  - Clinical Nursing Teaching and Research Section, The Second Xiangya Hospital of 
      Central South University, Changsha, China.
FAU - Ding, Ziwei
AU  - Ding Z
AUID- ORCID: 0009-0004-4151-0266
AD  - Clinical Nursing Teaching and Research Section, The Second Xiangya Hospital of 
      Central South University, Changsha, China.
FAU - Wang, Qin
AU  - Wang Q
AUID- ORCID: 0000-0003-1640-8337
AD  - Clinical Nursing Teaching and Research Section, The Second Xiangya Hospital of 
      Central South University, Changsha, China.
LA  - eng
PT  - Journal Article
DEP - 20231222
PL  - Canada
TA  - JMIR Form Res
JT  - JMIR formative research
JID - 101726394
PMC - PMC10770778
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - China
OT  - artificial intelligence
OT  - artificial intelligence quotient
OT  - college student
OT  - college students’ employability
OT  - nursing students
COIS- Conflicts of Interest: None declared.
EDAT- 2023/12/22 12:43
MHDA- 2023/12/22 12:44
PMCR- 2023/12/22
CRDT- 2023/12/22 11:54
PHST- 2023/06/29 00:00 [received]
PHST- 2023/11/22 00:00 [accepted]
PHST- 2023/08/31 00:00 [revised]
PHST- 2023/12/22 12:44 [medline]
PHST- 2023/12/22 12:43 [pubmed]
PHST- 2023/12/22 11:54 [entrez]
PHST- 2023/12/22 00:00 [pmc-release]
AID - v7i1e50413 [pii]
AID - 10.2196/50413 [doi]
PST - epublish
SO  - JMIR Form Res. 2023 Dec 22;7:e50413. doi: 10.2196/50413.

PMID- 38170274
OWN - NLM
STAT- MEDLINE
DCOM- 20240129
LR  - 20240206
IS  - 1432-1971 (Electronic)
IS  - 0172-0643 (Linking)
VI  - 45
IP  - 2
DP  - 2024 Feb
TI  - Progression of an Artificial Intelligence Chatbot (ChatGPT) for Pediatric 
      Cardiology Educational Knowledge Assessment.
PG  - 309-313
LID - 10.1007/s00246-023-03385-6 [doi]
AB  - Artificial intelligence chatbots, like ChatGPT, have become powerful tools that 
      are disrupting how humans interact with technology. The potential uses within 
      medicine are vast. In medical education, these chatbots have shown improvements, 
      in a short time span, in generalized medical examinations. We evaluated the 
      overall performance and improvement between ChatGPT 3.5 and 4.0 in a test of 
      pediatric cardiology knowledge. ChatGPT 3.5 and ChatGPT 4.0 were used to answer 
      text-based multiple-choice questions derived from a Pediatric Cardiology Board 
      Review textbook. Each chatbot was given an 88 question test, subcategorized into 
      11 topics. We excluded questions with modalities other than text (sound clips or 
      images). Statistical analysis was done using an unpaired two-tailed t-test. Of 
      the same 88 questions, ChatGPT 4.0 answered 66% of the questions correctly 
      (n = 58/88) which was significantly greater (p &lt; 0.0001) than ChatGPT 3.5, which 
      only answered 38% (33/88). The ChatGPT 4.0 version also did better on each 
      subspeciality topic as compared to ChatGPT 3.5. While acknowledging that ChatGPT 
      does not yet offer subspecialty level knowledge in pediatric cardiology, the 
      performance in pediatric cardiology educational assessments showed a considerable 
      improvement in a short period of time between ChatGPT 3.5 and 4.0.
CI  - © 2024. Crown.
FAU - Gritti, Michael N
AU  - Gritti MN
AUID- ORCID: 0000-0002-3666-4015
AD  - Division of Cardiology, The Labatt Family Heart Centre, The Hospital for Sick 
      Children, 555 University Ave, Toronto, ON, M5G 1X8, Canada. 
      michael.gritti@sickkids.ca.
AD  - Department of Pediatrics, University of Toronto, Toronto, ON, Canada. 
      michael.gritti@sickkids.ca.
FAU - AlTurki, Hussain
AU  - AlTurki H
AD  - Department of Pediatrics, University of Toronto, Toronto, ON, Canada.
AD  - Department of Pediatrics, The Hospital for Sick Children, Toronto, ON, Canada.
FAU - Farid, Pedrom
AU  - Farid P
AD  - Division of Cardiology, The Labatt Family Heart Centre, The Hospital for Sick 
      Children, 555 University Ave, Toronto, ON, M5G 1X8, Canada.
AD  - Schulich School of Medicine and Dentistry, University of Western Ontario, London, 
      ON, Canada.
FAU - Morgan, Conall T
AU  - Morgan CT
AD  - Division of Cardiology, The Labatt Family Heart Centre, The Hospital for Sick 
      Children, 555 University Ave, Toronto, ON, M5G 1X8, Canada.
AD  - Department of Pediatrics, University of Toronto, Toronto, ON, Canada.
LA  - eng
PT  - Journal Article
DEP - 20240103
PL  - United States
TA  - Pediatr Cardiol
JT  - Pediatric cardiology
JID - 8003849
SB  - IM
MH  - Child
MH  - Humans
MH  - Artificial Intelligence
MH  - Software
MH  - *Medicine
MH  - *Cardiology
MH  - Educational Measurement
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Education
OT  - Natural language processing
OT  - Pediatric Cardiology
EDAT- 2024/01/04 01:18
MHDA- 2024/01/29 06:43
CRDT- 2024/01/03 12:47
PHST- 2023/09/16 00:00 [received]
PHST- 2023/12/13 00:00 [accepted]
PHST- 2024/01/29 06:43 [medline]
PHST- 2024/01/04 01:18 [pubmed]
PHST- 2024/01/03 12:47 [entrez]
AID - 10.1007/s00246-023-03385-6 [pii]
AID - 10.1007/s00246-023-03385-6 [doi]
PST - ppublish
SO  - Pediatr Cardiol. 2024 Feb;45(2):309-313. doi: 10.1007/s00246-023-03385-6. Epub 
      2024 Jan 3.

PMID- 38284363
OWN - NLM
STAT- MEDLINE
DCOM- 20240301
LR  - 20240301
IS  - 1938-1344 (Electronic)
IS  - 0190-6011 (Linking)
VI  - 54
IP  - 3
DP  - 2024 Mar
TI  - Performance of ChatGPT Compared to Clinical Practice Guidelines in Making 
      Informed Decisions for Lumbosacral Radicular Pain: A Cross-sectional Study.
PG  - 1-7
LID - 10.2519/jospt.2024.12151 [doi]
AB  - OBJECTIVE: To compare the accuracy of an artificial intelligence chatbot to 
      clinical practice guidelines (CPGs) recommendations for providing answers to 
      complex clinical questions on lumbosacral radicular pain. DESIGN: Cross-sectional 
      study. METHODS: We extracted recommendations from recent CPGs for diagnosing and 
      treating lumbosacral radicular pain. Relative clinical questions were developed 
      and queried to OpenAI's ChatGPT (GPT-3.5). We compared ChatGPT answers to CPGs 
      recommendations by assessing the (1) internal consistency of ChatGPT answers by 
      measuring the percentage of text wording similarity when a clinical question was 
      posed 3 times, (2) reliability between 2 independent reviewers in grading ChatGPT 
      answers, and (3) accuracy of ChatGPT answers compared to CPGs recommendations. 
      Reliability was estimated using Fleiss' kappa (κ) coefficients, and accuracy by 
      interobserver agreement as the frequency of the agreements among all judgments. 
      RESULTS: We tested 9 clinical questions. The internal consistency of text ChatGPT 
      answers was unacceptable across all 3 trials in all clinical questions (mean 
      percentage of 49%, standard deviation of 15). Intrareliability (reviewer 1: κ = 
      0.90, standard error [SE] = 0.09; reviewer 2: κ = 0.90, SE = 0.10) and 
      interreliability (κ = 0.85, SE = 0.15) between the 2 reviewers was "almost 
      perfect." Accuracy between ChatGPT answers and CPGs recommendations was slight, 
      demonstrating agreement in 33% of recommendations. CONCLUSION: ChatGPT performed 
      poorly in internal consistency and accuracy of the indications generated compared 
      to clinical practice guideline recommendations for lumbosacral radicular pain. J 
      Orthop Sports Phys Ther 2024;54(3):1-7. Epub 29 January 2024. 
      doi:10.2519/jospt.2024.12151.
FAU - Gianola, Silvia
AU  - Gianola S
FAU - Bargeri, Silvia
AU  - Bargeri S
FAU - Castellini, Greta
AU  - Castellini G
FAU - Cook, Chad
AU  - Cook C
FAU - Palese, Alvisa
AU  - Palese A
FAU - Pillastrini, Paolo
AU  - Pillastrini P
FAU - Salvalaggio, Silvia
AU  - Salvalaggio S
FAU - Turolla, Andrea
AU  - Turolla A
FAU - Rossettini, Giacomo
AU  - Rossettini G
LA  - eng
PT  - Journal Article
PL  - United States
TA  - J Orthop Sports Phys Ther
JT  - The Journal of orthopaedic and sports physical therapy
JID - 7908150
SB  - IM
MH  - Humans
MH  - Cross-Sectional Studies
MH  - *Artificial Intelligence
MH  - Reproducibility of Results
MH  - *Back Pain
MH  - Decision Making
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - machine learning
OT  - musculoskeletal
OT  - natural language processing
OT  - orthopaedics
EDAT- 2024/01/29 12:42
MHDA- 2024/03/01 06:45
CRDT- 2024/01/29 07:56
PHST- 2024/03/01 06:45 [medline]
PHST- 2024/01/29 12:42 [pubmed]
PHST- 2024/01/29 07:56 [entrez]
AID - 10.2519/jospt.2024.12151 [doi]
PST - ppublish
SO  - J Orthop Sports Phys Ther. 2024 Mar;54(3):1-7. doi: 10.2519/jospt.2024.12151.

PMID- 38348835
OWN - NLM
STAT- Publisher
LR  - 20240213
IS  - 1784-973X (Electronic)
IS  - 0001-5385 (Linking)
DP  - 2024 Feb 13
TI  - Performance of ChatGPT as an AI-assisted decision support tool in medicine: a 
      proof-of-concept study for interpreting symptoms and management of common cardiac 
      conditions (AMSTELHEART-2).
PG  - 1-9
LID - 10.1080/00015385.2024.2303528 [doi]
AB  - BACKGROUND: It is thought that ChatGPT, an advanced language model developed by 
      OpenAI, may in the future serve as an AI-assisted decision support tool in 
      medicine. OBJECTIVE: To evaluate the accuracy of ChatGPT's recommendations on 
      medical questions related to common cardiac symptoms or conditions. METHODS: We 
      tested ChatGPT's ability to address medical questions in two ways. First, we 
      assessed its accuracy in correctly answering cardiovascular trivia questions 
      (n = 50), based on quizzes for medical professionals. Second, we entered 20 
      clinical case vignettes on the ChatGPT platform and evaluated its accuracy 
      compared to expert opinion and clinical course. Lastly, we compared the latest 
      research version (v3.5; 27 September 2023) with a prior version (v3.5; 30 January 
      2023) to evaluate improvement over time. RESULTS: We found that ChatGPT latest 
      version correctly answered 92% of the trivia questions, with slight variation in 
      accuracy in the domains coronary artery disease (100%), pulmonary and venous 
      thrombotic embolism (100%), atrial fibrillation (90%), heart failure (90%) and 
      cardiovascular risk management (80%). In the 20 case vignettes, ChatGPT's 
      response matched in 17 (85%) of the cases with the actual advice given. 
      Straightforward patient-to-physician questions were all answered correctly 
      (10/10). In more complex cases, where physicians (general practitioners) asked 
      other physicians (cardiologists) for assistance or decision support, ChatGPT was 
      correct in 70% of cases, and otherwise provided incomplete, inconclusive, or 
      inappropriate recommendations when compared with expert consultation. ChatGPT 
      showed significant improvement over time; as the January version correctly 
      answered 74% (vs 92%) of trivia questions (p = 0.031), and correctly answered a 
      mere 50% of complex cases. CONCLUSIONS: Our study suggests that ChatGPT has 
      potential as an AI-assisted decision support tool in medicine, particularly for 
      straightforward, low-complex medical questions, but further research is needed to 
      fully evaluate its potential.
FAU - Harskamp, Ralf E
AU  - Harskamp RE
AUID- ORCID: 0000-0001-9041-0350
AD  - Department of General Practice, Amsterdam UMC location University of Amsterdam, 
      Amsterdam, The Netherlands.
AD  - Amsterdam Public Health, Personalized Medicine, Amsterdam, The Netherlands.
FAU - De Clercq, Lukas
AU  - De Clercq L
AUID- ORCID: 0000-0002-6175-9518
AD  - Department of General Practice, Amsterdam UMC location University of Amsterdam, 
      Amsterdam, The Netherlands.
AD  - Amsterdam Public Health, Personalized Medicine, Amsterdam, The Netherlands.
LA  - eng
PT  - Journal Article
DEP - 20240213
PL  - England
TA  - Acta Cardiol
JT  - Acta cardiologica
JID - 0370570
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - cardiovascular medicine
OT  - chatbot
OT  - ehealth
EDAT- 2024/02/13 12:48
MHDA- 2024/02/13 12:48
CRDT- 2024/02/13 08:16
PHST- 2024/02/13 12:48 [medline]
PHST- 2024/02/13 12:48 [pubmed]
PHST- 2024/02/13 08:16 [entrez]
AID - 10.1080/00015385.2024.2303528 [doi]
PST - aheadofprint
SO  - Acta Cardiol. 2024 Feb 13:1-9. doi: 10.1080/00015385.2024.2303528.

PMID- 38534908
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240329
IS  - 2254-9625 (Electronic)
IS  - 2174-8144 (Print)
IS  - 2174-8144 (Linking)
VI  - 14
IP  - 3
DP  - 2024 Mar 17
TI  - Examining Students' Acceptance and Use of ChatGPT in Saudi Arabian Higher 
      Education.
PG  - 709-721
LID - 10.3390/ejihpe14030047 [doi]
AB  - This study examines students' acceptance and use of ChatGPT in Saudi Arabian (SA) 
      higher education, where there is growing interest in the use of this tool since 
      its inauguration in 2022. Quantitative research data, through a self-reporting 
      survey drawing on the "Unified Theory of Acceptance and Use of Technology" 
      (UTAUT2), were collected from 520 students in one of the public universities in 
      SA at the start of the first semester of the study year 2023-2024. The findings 
      of structural equation modeling partially supported the UTAUT and previous 
      research in relation to the significant direct effect of performance expectancy 
      (PE), social influence (SI), and effort expectancy (EE) on behavioral intention 
      (BI) on the use of ChatGPT and the significant direct effect of PE, SI, and BI on 
      actual use of ChatGPT. Nonetheless, the results did not support earlier research 
      in relation to the direct relationship between facilitating conditions (FCs) and 
      both BI and actual use of ChatGPT, which was found to be negative in the first 
      relationship and insignificant in the second one. These findings were because of 
      the absence of resources, support, and aid from external sources in relation to 
      the use of ChatGPT. The results showed partial mediation of BI in the link 
      between PE, SI, and FC and actual use of ChatGPT in education and a full 
      mediation in the link of BI between EE and actual use of ChatGPT in education. 
      The findings provide numerous implications for scholars and higher education 
      institutions in SA, which are also of interest to other institutions in similar 
      contexts.
FAU - Sobaih, Abu Elnasr E
AU  - Sobaih AEE
AUID- ORCID: 0000-0002-2730-689X
AD  - Management Department, College of Business Administration, King Faisal 
      University, Al-Hassa 31982, Saudi Arabia.
AD  - Faculty of Tourism and Hotel Management, Helwan University, Cairo 12612, Egypt.
FAU - Elshaer, Ibrahim A
AU  - Elshaer IA
AUID- ORCID: 0000-0002-7800-0428
AD  - Management Department, College of Business Administration, King Faisal 
      University, Al-Hassa 31982, Saudi Arabia.
AD  - Faculty of Tourism and Hotel Management, Suez Canal University, Ismailia 41522, 
      Egypt.
FAU - Hasanein, Ahmed M
AU  - Hasanein AM
AUID- ORCID: 0000-0002-0664-9017
AD  - Management Department, College of Business Administration, King Faisal 
      University, Al-Hassa 31982, Saudi Arabia.
AD  - Faculty of Tourism and Hotel Management, Helwan University, Cairo 12612, Egypt.
LA  - eng
GR  - GRANT5451/This research was funded by the Deanship of Scientific Research, Vice 
      Presidency for Graduate Studies and Scientific Research, King Faisal University, 
      Saudi Arabia,/
PT  - Journal Article
DEP - 20240317
PL  - Switzerland
TA  - Eur J Investig Health Psychol Educ
JT  - European journal of investigation in health, psychology and education
JID - 101751466
PMC - PMC10969089
OTO - NOTNLM
OT  - ChatGPT
OT  - Chatbot
OT  - Saudi Arabian education
OT  - artificial intelligence
OT  - technology acceptance
OT  - technology use
COIS- The authors declare no conflicts of interest.
EDAT- 2024/03/27 12:50
MHDA- 2024/03/27 12:51
PMCR- 2024/03/17
CRDT- 2024/03/27 09:28
PHST- 2024/01/03 00:00 [received]
PHST- 2024/03/10 00:00 [revised]
PHST- 2024/03/15 00:00 [accepted]
PHST- 2024/03/27 12:51 [medline]
PHST- 2024/03/27 12:50 [pubmed]
PHST- 2024/03/27 09:28 [entrez]
PHST- 2024/03/17 00:00 [pmc-release]
AID - ejihpe14030047 [pii]
AID - ejihpe-14-00047 [pii]
AID - 10.3390/ejihpe14030047 [doi]
PST - epublish
SO  - Eur J Investig Health Psychol Educ. 2024 Mar 17;14(3):709-721. doi: 
      10.3390/ejihpe14030047.

PMID- 37202644
OWN - NLM
STAT- MEDLINE
DCOM- 20230809
LR  - 20230809
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 51
IP  - 9
DP  - 2023 Sep
TI  - The Double-Edged Sword of AI in Biomedical Engineering: ChatGPT's Controversial 
      Impact on Research and Collaboration Paradigms.
PG  - 1904-1905
LID - 10.1007/s10439-023-03237-7 [doi]
AB  - As artificial intelligence (AI) permeates various fields, ChatGPT emerges as a 
      contentious force in biomedical engineering, stirring both excitement and 
      concern. This letter explores the disruptive potential and controversial 
      implications of ChatGPT on research, collaboration, and the future of the field. 
      By presenting thought-provoking questions and confronting divisive issues, we aim 
      to spark a lively debate on the responsible integration of AI technology in 
      biomedical engineering while safeguarding the essence of human expertise.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Ray, Partha Pratim
AU  - Ray PP
AD  - Sikkim University, Gangtok, India. ppray@cus.ac.in.
FAU - Majumder, Poulami
AU  - Majumder P
AD  - Maulana Abul Kalam Azad University of Technology, Kolkata, India.
LA  - eng
PT  - Letter
DEP - 20230518
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - Humans
MH  - *Biomedical Engineering
MH  - *Artificial Intelligence
MH  - Bioengineering
MH  - Technology
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Biomedical engineering
OT  - ChatGPT
OT  - Collaboration
OT  - Controversy
OT  - Research
EDAT- 2023/05/19 01:04
MHDA- 2023/08/09 06:43
CRDT- 2023/05/18 23:27
PHST- 2023/05/04 00:00 [received]
PHST- 2023/05/10 00:00 [accepted]
PHST- 2023/08/09 06:43 [medline]
PHST- 2023/05/19 01:04 [pubmed]
PHST- 2023/05/18 23:27 [entrez]
AID - 10.1007/s10439-023-03237-7 [pii]
AID - 10.1007/s10439-023-03237-7 [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2023 Sep;51(9):1904-1905. doi: 10.1007/s10439-023-03237-7. Epub 
      2023 May 18.

PMID- 38533173
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240328
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 16
IP  - 2
DP  - 2024 Feb
TI  - Pre-operative Planning of High Tibial Osteotomy With ChatGPT: Are We There Yet?
PG  - e54858
LID - 10.7759/cureus.54858 [doi]
LID - e54858
AB  - INTRODUCTION: ChatGPT (Chat Generative Pre-trained Transformer), developed by 
      OpenAI (San Francisco, CA, USA), has gained attention in the medical field. It 
      has the potential to enhance and simplify tasks, such as preoperative planning in 
      orthopedic surgery. We aimed to test ChatGPT's accuracy in measuring the angle of 
      correction for high tibial osteotomy for cases planned and performed at a 
      tertiary teaching hospital in Singapore. MATERIALS AND METHODS: Peri-operative 
      angular parameters from 114 consecutive patients who underwent medial opening 
      wedge high tibial osteotomy (MOWHTO) were used to query ChatGPT 3.0. First 
      ChatGPT 3.0 was queried on what information it required to plan a MOWHTO. Based 
      on its response, pre-operative medial proximal tibial angle (MPTA) and joint line 
      congruence angle (JLCA) were provided. ChatGPT 3.0 then responded with its 
      recommended angle of correction. This was compared against the manually planned 
      surgical correction by our fellowship-trained surgeon. A root mean square 
      analysis was then performed to compare ChatGPT 3.0 and manual planning. RESULTS: 
      The root mean square error (RMSE) of ChatGPT 3.0 in predicting correction angle 
      in MWHTO was 2.96, suggesting a very poor model fit. CONCLUSION: Although ChatGPT 
      3.0 represents a significant breakthrough in large language models with extensive 
      capabilities, it is not currently optimized to effectively perform complex 
      pre-operative planning in orthopedic surgery, specifically in the context of 
      MOWHTO. Further refinement and consideration of specific factors are necessary to 
      enhance its accuracy and suitability for such applications.
CI  - Copyright © 2024, Gengatharan et al.
FAU - Gengatharan, Dhivakaran
AU  - Gengatharan D
AD  - Orthopaedic Surgery, Sengkang General Hospital, Singapore, SGP.
FAU - Saggi, Sandip Singh
AU  - Saggi SS
AD  - Orthopaedic Surgery, Sengkang General Hospital, Singapore, SGP.
FAU - Bin Abd Razak, Hamid Rahmatullah
AU  - Bin Abd Razak HR
AD  - Musculoskeletal Sciences, Duke-Nus Medical School, Singapore, SGP.
AD  - Orthopaedic Surgery, Sengkang General Hospital, Singapore, SGP.
LA  - eng
PT  - Journal Article
DEP - 20240225
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10964394
OTO - NOTNLM
OT  - chatgpt
OT  - high tibial osteotomy
OT  - joint-preserving surgery
OT  - orthopedics
OT  - pre-operative planning
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/03/27 06:43
MHDA- 2024/03/27 06:44
PMCR- 2024/02/25
CRDT- 2024/03/27 03:51
PHST- 2024/02/23 00:00 [accepted]
PHST- 2024/03/27 06:44 [medline]
PHST- 2024/03/27 06:43 [pubmed]
PHST- 2024/03/27 03:51 [entrez]
PHST- 2024/02/25 00:00 [pmc-release]
AID - 10.7759/cureus.54858 [doi]
PST - epublish
SO  - Cureus. 2024 Feb 25;16(2):e54858. doi: 10.7759/cureus.54858. eCollection 2024 
      Feb.

PMID- 37982677
OWN - NLM
STAT- Publisher
LR  - 20231120
IS  - 2689-3622 (Electronic)
IS  - 2689-3614 (Linking)
DP  - 2023 Nov 20
TI  - Artificial Intelligence Versus Expert Plastic Surgeon: Comparative Study Shows 
      ChatGPT "Wins" Rhinoplasty Consultations: Should We Be Worried?
LID - 10.1089/fpsam.2023.0224 [doi]
AB  - Introduction: Large language models, such as ChatGPT, hold tremendous promise to 
      bridge gaps in patient education and enhance the decision-making resources 
      available online for patients seeking nasal surgery. Objective: To compare the 
      performance of ChatGPT in answering preoperative and postoperative patient 
      questions related to septorhinoplasty. Methods: Two sets of responses were 
      collected for the questions: one from an expert rhinoplasty surgeon with over two 
      decades of experience, and the other from ChatGPT-3.5. Seven expert rhinoplasty 
      surgeons, blinded to the source of responses, independently assessed the 
      responses using a 5-point Likert scale in four performance areas: empathy, 
      accuracy, completeness, and overall quality. Results: ChatGPT outperformed 
      physician responses in three of the four performance areas, earning significantly 
      higher ratings in accuracy, completeness, and overall quality (p &lt; 0.001). In 
      addition, ChatGPT was overwhelmingly preferred over physician responses 
      (p &lt; 0.001), with evaluators favoring ChatGPT in 80.95% of instances. 
      Conclusions: ChatGPT has demonstrated its remarkable ability to deliver accurate, 
      complete, and high-quality responses to preoperative and postoperative patient 
      questions. Although certain improvements are warranted, this artificial 
      intelligence tool has shown its potential to effectively counsel and educate 
      prospective septorhinoplasty patients at a level comparable with or exceeding 
      that of an expert surgeon.
FAU - Durairaj, K Kay
AU  - Durairaj KK
AUID- ORCID: 0009-0004-1287-8619
AD  - Department of Otolaryngology-Head and Neck Surgery, Huntington Hospital, 
      Pasadena, California, USA.
AD  - Kay Durairaj, MD, A Medical Corp, Pasadena, California, USA.
FAU - Baker, Omer
AU  - Baker O
AD  - Kay Durairaj, MD, A Medical Corp, Pasadena, California, USA.
FAU - Bertossi, Dario
AU  - Bertossi D
AUID- ORCID: 0000-0002-8635-9967
AD  - Department of Head and Neck Surgery, University of Verona, Verona, Italy.
FAU - Dayan, Steven
AU  - Dayan S
AD  - Denova Research, Chicago, Illinois, USA.
FAU - Karimi, Kian
AU  - Karimi K
AUID- ORCID: 0000-0002-3803-8560
AD  - Dr. Kian Nasal &amp; Facial Plastic Surgery, Los Angeles, California, USA.
FAU - Kim, Roy
AU  - Kim R
AD  - Private Practice, San Francisco/Beverly Hills, California, USA.
FAU - Most, Sam
AU  - Most S
AUID- ORCID: 0000-0002-7385-3149
AD  - Division of Facial Plastic &amp; Reconstructive Surgery, Stanford University, 
      Stanford, California, USA.
FAU - Robotti, Enrico
AU  - Robotti E
AD  - Private Practice, Bergamo, Italy.
FAU - Rosengaus, Frank
AU  - Rosengaus F
AD  - Private Practice, Mexico City, Mexico.
LA  - eng
PT  - Journal Article
DEP - 20231120
PL  - United States
TA  - Facial Plast Surg Aesthet Med
JT  - Facial plastic surgery &amp; aesthetic medicine
JID - 101757922
SB  - IM
EDAT- 2023/11/20 13:41
MHDA- 2023/11/20 13:41
CRDT- 2023/11/20 09:32
PHST- 2023/11/20 13:41 [medline]
PHST- 2023/11/20 13:41 [pubmed]
PHST- 2023/11/20 09:32 [entrez]
AID - 10.1089/fpsam.2023.0224 [doi]
PST - aheadofprint
SO  - Facial Plast Surg Aesthet Med. 2023 Nov 20. doi: 10.1089/fpsam.2023.0224.

PMID- 38315648
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240207
IS  - 2767-3170 (Electronic)
IS  - 2767-3170 (Linking)
VI  - 3
IP  - 2
DP  - 2024 Feb
TI  - Harnessing the open access version of ChatGPT for enhanced clinical opinions.
PG  - e0000355
LID - 10.1371/journal.pdig.0000355 [doi]
LID - e0000355
AB  - With the advent of Large Language Models (LLMs) like ChatGPT, the integration of 
      Generative Artificial Intelligence (GAI) into clinical medicine is becoming 
      increasingly feasible. This study aimed to evaluate the ability of the freely 
      available ChatGPT-3.5 to generate complex differential diagnoses, comparing its 
      output to case records of the Massachusetts General Hospital published in the New 
      England Journal of Medicine (NEJM). Forty case records were presented to 
      ChatGPT-3.5, prompting it to provide a differential diagnosis and then narrow it 
      down to the most likely diagnosis. The results indicated that the final diagnosis 
      was included in ChatGPT-3.5's original differential list in 42.5% of the cases. 
      After narrowing, ChatGPT correctly determined the final diagnosis in 27.5% of the 
      cases, demonstrating a decrease in accuracy compared to previous studies using 
      common chief complaints. These findings emphasize the necessity for further 
      investigation into the capabilities and limitations of LLMs in clinical scenarios 
      while highlighting the potential role of GAI as an augmented clinical opinion. 
      Anticipating the growth and enhancement of GAI tools like ChatGPT, physicians and 
      other healthcare workers will likely find increasing support in generating 
      differential diagnoses. However, continued exploration and regulation are 
      essential to ensure the safe and effective integration of GAI into healthcare 
      practice. Future studies may seek to compare newer versions of ChatGPT or 
      investigate patient outcomes with physicians integrating this GAI technology. 
      Understanding and expanding GAI's capabilities, particularly in differential 
      diagnosis, may foster innovation and provide additional resources, especially in 
      underserved areas in the medical field.
CI  - Copyright: © 2024 Tenner et al. This is an open access article distributed under 
      the terms of the Creative Commons Attribution License, which permits unrestricted 
      use, distribution, and reproduction in any medium, provided the original author 
      and source are credited.
FAU - Tenner, Zachary M
AU  - Tenner ZM
AUID- ORCID: 0009-0000-9843-0719
AD  - New York University Grossman Long Island School of Medicine, Mineola, New York, 
      United States of America.
FAU - Cottone, Michael C
AU  - Cottone MC
AD  - New York University Grossman Long Island School of Medicine, Mineola, New York, 
      United States of America.
FAU - Chavez, Martin R
AU  - Chavez MR
AD  - New York University Grossman Long Island School of Medicine, Mineola, New York, 
      United States of America.
AD  - Department of Obstetrics and Gynecology, New York University Langone Health-Long 
      Island, Mineola, New York, United States of America.
LA  - eng
PT  - Journal Article
DEP - 20240205
PL  - United States
TA  - PLOS Digit Health
JT  - PLOS digital health
JID - 9918335064206676
PMC - PMC10843476
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/02/05 14:44
MHDA- 2024/02/05 14:45
PMCR- 2024/02/05
CRDT- 2024/02/05 13:23
PHST- 2023/08/21 00:00 [received]
PHST- 2024/01/11 00:00 [accepted]
PHST- 2024/02/05 14:45 [medline]
PHST- 2024/02/05 14:44 [pubmed]
PHST- 2024/02/05 13:23 [entrez]
PHST- 2024/02/05 00:00 [pmc-release]
AID - PDIG-D-23-00320 [pii]
AID - 10.1371/journal.pdig.0000355 [doi]
PST - epublish
SO  - PLOS Digit Health. 2024 Feb 5;3(2):e0000355. doi: 10.1371/journal.pdig.0000355. 
      eCollection 2024 Feb.

PMID- 37392000
OWN - NLM
STAT- MEDLINE
DCOM- 20231129
LR  - 20231129
IS  - 1741-2854 (Electronic)
IS  - 0020-7640 (Linking)
VI  - 69
IP  - 8
DP  - 2023 Dec
TI  - Old dog, new tricks? Exploring the potential functionalities of ChatGPT in 
      supporting educational methods in social psychiatry.
PG  - 1882-1889
LID - 10.1177/00207640231178451 [doi]
AB  - BACKGROUND: Artificial Intelligence is ever-expanding and large-language models 
      are increasingly shaping teaching and learning experiences. ChatGPT is a 
      prominent recent example of this technology and has generated much debate around 
      the benefits and disadvantages of chatbots in educational domains. AIM: This 
      study seeks to demonstrate the possible use-cases of ChatGPT in supporting 
      educational methods specific to social psychiatry. METHODS: Through interactions 
      with ChatGPT 3.5, we asked this technology to list six ways in which it could aid 
      social psychiatry teaching. Subsequently, we requested that ChatGPT perform one 
      of the tasks it identified in its responses. FINDINGS: ChatGPT highlighted 
      several roles it could fulfil in educational settings, including as an 
      information provider, a tool for debates and discussions, a facilitator of 
      self-directed learning and a content-creator for course materials. For the latter 
      scenario, based on another prompt, ChatGPT generated a hypothetical case vignette 
      for a topic relevant to social psychiatry. CONCLUSIONS: Based on our experiences, 
      ChatGPT can be an effective teaching tool, offering opportunities for active and 
      case-based learning for students and instructors in social psychiatry. However, 
      in their current form, chatbots have several limitations that must be considered, 
      including misinformation and inherent biases, although these may only be 
      temporary in nature as these technologies continue to advance. Accordingly, we 
      argue that large-language models can support social psychiatry education with 
      appropriate caution and encourage educators to become attuned to their potential 
      through further detailed research in this area.
FAU - Smith, Alexander
AU  - Smith A
AUID- ORCID: 0000-0002-1769-5684
AD  - Department of Forensic Psychiatry, University of Bern, Switzerland.
FAU - Hachen, Stefanie
AU  - Hachen S
AD  - Department of Forensic Psychiatry, University of Bern, Switzerland.
FAU - Schleifer, Roman
AU  - Schleifer R
AD  - Department of Forensic Psychiatry, University of Bern, Switzerland.
FAU - Bhugra, Dinesh
AU  - Bhugra D
AD  - King's College London, UK.
FAU - Buadze, Anna
AU  - Buadze A
AUID- ORCID: 0000-0002-8683-6807
AD  - Department of Psychiatry, Psychotherapy and Psychosomatics, Psychiatric Hospital, 
      University of Zurich, Switzerland.
FAU - Liebrenz, Michael
AU  - Liebrenz M
AD  - Department of Forensic Psychiatry, University of Bern, Switzerland.
LA  - eng
PT  - Journal Article
DEP - 20230630
PL  - England
TA  - Int J Soc Psychiatry
JT  - The International journal of social psychiatry
JID - 0374726
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Community Psychiatry
MH  - Educational Status
MH  - Students
MH  - Language
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - active-learning
OT  - case-based learning
OT  - psychiatric education
OT  - teaching
COIS- Conflict of interestThe author(s) declared no potential conflicts of interest 
      with respect to the research, authorship, and/or publication of this article.
EDAT- 2023/07/01 11:42
MHDA- 2023/11/29 06:43
CRDT- 2023/07/01 02:32
PHST- 2023/11/29 06:43 [medline]
PHST- 2023/07/01 11:42 [pubmed]
PHST- 2023/07/01 02:32 [entrez]
AID - 10.1177/00207640231178451 [doi]
PST - ppublish
SO  - Int J Soc Psychiatry. 2023 Dec;69(8):1882-1889. doi: 10.1177/00207640231178451. 
      Epub 2023 Jun 30.

PMID- 37410674
OWN - NLM
STAT- MEDLINE
DCOM- 20230925
LR  - 20230925
IS  - 1469-0756 (Electronic)
IS  - 0032-5473 (Linking)
VI  - 99
IP  - 1176
DP  - 2023 Sep 21
TI  - Artificial intelligence in orthopaedics: can Chat Generative Pre-trained 
      Transformer (ChatGPT) pass Section 1 of the Fellowship of the Royal College of 
      Surgeons (Trauma &amp; Orthopaedics) examination?
PG  - 1110-1114
LID - 10.1093/postmj/qgad053 [doi]
AB  - PURPOSE: Chat Generative Pre-trained Transformer (ChatGPT) is a large language 
      artificial intelligence (AI) model which generates contextually relevant text in 
      response to questioning. After ChatGPT successfully passed the United States 
      Medical Licensing Examinations, proponents have argued it should play an 
      increasing role in medical service provision and education. AI in healthcare 
      remains in its infancy, and the reliability of AI systems must be scrutinized. 
      This study assessed whether ChatGPT could pass Section 1 of the Fellowship of the 
      Royal College of Surgeons (FRCS) examination in Trauma and Orthopaedic Surgery. 
      METHODS: The UK and Ireland In-Training Examination (UKITE) was used as a 
      surrogate for the FRCS. Papers 1 and 2 of UKITE 2022 were directly inputted into 
      ChatGPT. All questions were in a single-best-answer format without wording 
      alterations. Imaging was trialled to ensure ChatGPT utilized this information. 
      RESULTS: ChatGPT scored 35.8%: 30% lower than the FRCS pass rate and 8.2% lower 
      than the mean score achieved by human candidates of all training levels. 
      Subspecialty analysis demonstrated ChatGPT scored highest in basic science 
      (53.3%) and lowest in trauma (0%). In 87 questions answered incorrectly, ChatGPT 
      only stated it did not know the answer once and gave incorrect explanatory 
      answers for the remaining questions. CONCLUSION: ChatGPT is currently unable to 
      exert the higher-order judgement and multilogical thinking required to pass the 
      FRCS examination. Further, the current model fails to recognize its own 
      limitations. ChatGPT's deficiencies should be publicized equally as much as its 
      successes to ensure clinicians remain aware of its fallibility.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of 
      Postgraduate Medical Journal. All rights reserved. For permissions, please 
      e-mail: journals.permissions@oup.com.
FAU - Cuthbert, Rory
AU  - Cuthbert R
AUID- ORCID: 0000-0001-6211-3789
AD  - Guy's and St Thomas' Hospital National Health Service Foundation Trust, London, 
      SE1 9RT, United Kingdom.
FAU - Simpson, Ashley I
AU  - Simpson AI
AUID- ORCID: 0000-0001-6232-536X
AD  - Guy's and St Thomas' Hospital National Health Service Foundation Trust, London, 
      SE1 9RT, United Kingdom.
LA  - eng
PT  - Journal Article
PL  - England
TA  - Postgrad Med J
JT  - Postgraduate medical journal
JID - 0234135
SB  - IM
MH  - Humans
MH  - *Orthopedics
MH  - Artificial Intelligence
MH  - Fellowships and Scholarships
MH  - Reproducibility of Results
MH  - *Orthopedic Procedures
MH  - *Surgeons
OTO - NOTNLM
OT  - artificial intelligence
OT  - examination
OT  - orthopaedics
OT  - surgeons
OT  - trauma
EDAT- 2023/07/06 19:12
MHDA- 2023/09/25 06:41
CRDT- 2023/07/06 13:13
PHST- 2023/04/01 00:00 [received]
PHST- 2023/05/26 00:00 [revised]
PHST- 2023/06/01 00:00 [accepted]
PHST- 2023/09/25 06:41 [medline]
PHST- 2023/07/06 19:12 [pubmed]
PHST- 2023/07/06 13:13 [entrez]
AID - 7220358 [pii]
AID - 10.1093/postmj/qgad053 [doi]
PST - ppublish
SO  - Postgrad Med J. 2023 Sep 21;99(1176):1110-1114. doi: 10.1093/postmj/qgad053.

PMID- 37757713
OWN - NLM
STAT- MEDLINE
DCOM- 20231110
LR  - 20231110
IS  - 1532-2793 (Electronic)
IS  - 0260-6917 (Linking)
VI  - 131
DP  - 2023 Dec
TI  - A holistic approach to remote patient monitoring, fueled by ChatGPT and Metaverse 
      technology: The future of nursing education.
PG  - 105972
LID - S0260-6917(23)00266-6 [pii]
LID - 10.1016/j.nedt.2023.105972 [doi]
AB  - ChatGPT and Metaverse are contemporary artificial intelligence tools that are 
      increasingly being used in healthcare professional training, particularly for 
      remote patient monitoring. These technologies offer immersive and personalized 
      learning experiences for nurses, improving their skills and confidence in 
      managing remote patient care. ChatGPT can create simulated patient interactions 
      that mimic real-life scenarios, while the Metaverse can provide virtual reality 
      simulations and scenarios for nurses to practice and learn in a safe and 
      controlled environment. The unification of ChatGPT and Metaverse technology in 
      nursing education can enrich the learning experience and equip nurses with the 
      necessary skills for remote patient monitoring, ultimately leading to improved 
      patient outcomes and quality of care.
CI  - Copyright © 2023 Elsevier Ltd. All rights reserved.
FAU - Sharma, Manik
AU  - Sharma M
AD  - Department of CSA, DAV University Jalandhar, India. Electronic address: 
      manik10143@davuniversity.org.
FAU - Sharma, Samriti
AU  - Sharma S
AD  - Department of CS, GNDU Amritsar, India.
LA  - eng
PT  - Journal Article
DEP - 20230912
PL  - Scotland
TA  - Nurse Educ Today
JT  - Nurse education today
JID - 8511379
MH  - Humans
MH  - Artificial Intelligence
MH  - *Education, Nursing
MH  - Learning
MH  - *Virtual Reality
MH  - Delivery of Health Care
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Immersive learning
OT  - Metaverse
OT  - Nurse training
OT  - Remote patient monitoring
COIS- Declaration of competing interest No potential conflict of interest relevant to 
      this article was reported.
EDAT- 2023/09/28 00:42
MHDA- 2023/11/10 06:45
CRDT- 2023/09/27 18:09
PHST- 2023/03/25 00:00 [received]
PHST- 2023/08/18 00:00 [revised]
PHST- 2023/09/11 00:00 [accepted]
PHST- 2023/11/10 06:45 [medline]
PHST- 2023/09/28 00:42 [pubmed]
PHST- 2023/09/27 18:09 [entrez]
AID - S0260-6917(23)00266-6 [pii]
AID - 10.1016/j.nedt.2023.105972 [doi]
PST - ppublish
SO  - Nurse Educ Today. 2023 Dec;131:105972. doi: 10.1016/j.nedt.2023.105972. Epub 2023 
      Sep 12.

PMID- 37839567
OWN - NLM
STAT- MEDLINE
DCOM- 20231221
LR  - 20231221
IS  - 1878-8769 (Electronic)
IS  - 1878-8750 (Linking)
VI  - 180
DP  - 2023 Dec
TI  - Large Language Model-Based Neurosurgical Evaluation Matrix: A Novel Scoring 
      Criteria to Assess the Efficacy of ChatGPT as an Educational Tool for 
      Neurosurgery Board Preparation.
PG  - e765-e773
LID - S1878-8750(23)01448-1 [pii]
LID - 10.1016/j.wneu.2023.10.043 [doi]
AB  - INTRODUCTION: Technological advancements are reshaping medical education, with 
      digital tools becoming essential in all levels of training. Amidst this 
      transformation, the study explores the potential of ChatGPT, an artificial 
      intelligence model by OpenAI, in enhancing neurosurgical board education. The 
      focus extends beyond technology adoption to its effective utilization, with 
      ChatGPT's proficiency evaluated against practice questions from the Primary 
      Neurosurgery Written Board Exam. METHODS: Using the Congress of Neurologic 
      Surgeons (CNS) Self-Assessment Neurosurgery (SANS) Exam Board Review Prep 
      questions, we conducted 3 rounds of analysis with ChatGPT. We developed a novel 
      ChatGPT Neurosurgical Evaluation Matrix (CNEM) to assess the output quality, 
      accuracy, concordance, and clarity of ChatGPT's answers. RESULTS: ChatGPT 
      achieved spot-on accuracy for 66.7% of prompted questions, 59.4% of unprompted 
      questions, and 63.9% of unprompted questions with a leading phrase. Stratified by 
      topic, accuracy ranged from 50.0% (Vascular) to 78.8% (Neuropathology). In 
      comparison to SANS explanations, ChatGPT output was considered better in 19.1% of 
      questions, equal in 51.6%, and worse in 29.3%. Concordance analysis showed that 
      95.5% of unprompted ChatGPT outputs and 97.4% of unprompted outputs with a 
      leading phrase were aligned. CONCLUSIONS: Our study evaluated the performance of 
      ChatGPT in neurosurgical board education by assessing its accuracy, clarity, and 
      concordance. The findings highlight the potential and challenges of integrating 
      AI technologies like ChatGPT into medical and neurosurgical board education. 
      Further research is needed to refine these tools and optimize their performance 
      for enhanced medical education and patient care.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Mannam, Sneha Sai
AU  - Mannam SS
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Subtirelu, Robert
AU  - Subtirelu R
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Chauhan, Daksh
AU  - Chauhan D
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Ahmad, Hasan S
AU  - Ahmad HS
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Matache, Irina Mihaela
AU  - Matache IM
AD  - Department of Physiology, Faculty of Medicine, Carol Davila University of 
      Medicine and Pharmacy, Bucharest, Romania.
FAU - Bryan, Kevin
AU  - Bryan K
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Chitta, Siddharth V K
AU  - Chitta SVK
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Bathula, Shreya C
AU  - Bathula SC
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Turlip, Ryan
AU  - Turlip R
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Wathen, Connor
AU  - Wathen C
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Ghenbot, Yohannes
AU  - Ghenbot Y
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Ajmera, Sonia
AU  - Ajmera S
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Blue, Rachel
AU  - Blue R
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Chen, H Isaac
AU  - Chen HI
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Ali, Zarina S
AU  - Ali ZS
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Malhotra, Neil
AU  - Malhotra N
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Srinivasan, Visish
AU  - Srinivasan V
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Ozturk, Ali K
AU  - Ozturk AK
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA.
FAU - Yoon, Jang W
AU  - Yoon JW
AD  - Department of Neurosurgery, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania, USA. Electronic address: 
      jang.yoon@pennmedicine.upenn.edu.
LA  - eng
PT  - Journal Article
DEP - 20231014
PL  - United States
TA  - World Neurosurg
JT  - World neurosurgery
JID - 101528275
SB  - IM
MH  - Humans
MH  - *Neurosurgery
MH  - Artificial Intelligence
MH  - Educational Status
MH  - Neurosurgical Procedures
MH  - Language
OTO - NOTNLM
OT  - AI evaluation matrix
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Medical education technology
OT  - Neurosurgical education
EDAT- 2023/10/16 00:42
MHDA- 2023/12/21 06:43
CRDT- 2023/10/15 19:25
PHST- 2023/10/03 00:00 [received]
PHST- 2023/10/07 00:00 [accepted]
PHST- 2023/12/21 06:43 [medline]
PHST- 2023/10/16 00:42 [pubmed]
PHST- 2023/10/15 19:25 [entrez]
AID - S1878-8750(23)01448-1 [pii]
AID - 10.1016/j.wneu.2023.10.043 [doi]
PST - ppublish
SO  - World Neurosurg. 2023 Dec;180:e765-e773. doi: 10.1016/j.wneu.2023.10.043. Epub 
      2023 Oct 14.

PMID- 37987870
OWN - NLM
STAT- MEDLINE
DCOM- 20231127
LR  - 20231231
IS  - 1573-689X (Electronic)
IS  - 0148-5598 (Print)
IS  - 0148-5598 (Linking)
VI  - 47
IP  - 1
DP  - 2023 Nov 21
TI  - "ChatGPT, Can You Help Me Save My Child's Life?" - Diagnostic Accuracy and 
      Supportive Capabilities to Lay Rescuers by ChatGPT in Prehospital Basic Life 
      Support and Paediatric Advanced Life Support Cases - An In-silico Analysis.
PG  - 123
LID - 10.1007/s10916-023-02019-x [doi]
LID - 123
AB  - BACKGROUND: Paediatric emergencies are challenging for healthcare workers, first 
      aiders, and parents waiting for emergency medical services to arrive. With the 
      expected rise of virtual assistants, people will likely seek help from such 
      digital AI tools, especially in regions lacking emergency medical services. Large 
      Language Models like ChatGPT proved effective in providing health-related 
      information and are competent in medical exams but are questioned regarding 
      patient safety. Currently, there is no information on ChatGPT's performance in 
      supporting parents in paediatric emergencies requiring help from emergency 
      medical services. This study aimed to test 20 paediatric and two basic life 
      support case vignettes for ChatGPT and GPT-4 performance and safety in children. 
      METHODS: We provided the cases three times each to two models, ChatGPT and GPT-4, 
      and assessed the diagnostic accuracy, emergency call advice, and the validity of 
      advice given to parents. RESULTS: Both models recognized the emergency in the 
      cases, except for septic shock and pulmonary embolism, and identified the correct 
      diagnosis in 94%. However, ChatGPT/GPT-4 reliably advised to call emergency 
      services only in 12 of 22 cases (54%), gave correct first aid instructions in 9 
      cases (45%) and incorrectly advised advanced life support techniques to parents 
      in 3 of 22 cases (13.6%). CONCLUSION: Considering these results of the recent 
      ChatGPT versions, the validity, reliability and thus safety of ChatGPT/GPT-4 as 
      an emergency support tool is questionable. However, whether humans would perform 
      better in the same situation is uncertain. Moreover, other studies have shown 
      that human emergency call operators are also inaccurate, partly with worse 
      performance than ChatGPT/GPT-4 in our study. However, one of the main limitations 
      of the study is that we used prototypical cases, and the management may differ 
      from urban to rural areas and between different countries, indicating the need 
      for further evaluation of the context sensitivity and adaptability of the model. 
      Nevertheless, ChatGPT and the new versions under development may be promising 
      tools for assisting lay first responders, operators, and professionals in 
      diagnosing a paediatric emergency. TRIAL REGISTRATION: Not applicable.
CI  - © 2023. The Author(s).
FAU - Bushuven, Stefan
AU  - Bushuven S
AD  - Training Center for Emergency Medicine (NOTIS e.V), Breite Strasse 7, Engen, 
      78234, Germany. Stefan.Bushuven@notis-ev.de.
AD  - Department of Anesthesiology and Critical Care, Medical Center - University of 
      Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany. 
      Stefan.Bushuven@notis-ev.de.
AD  - Institute for Medical Education, University Hospital, LMU Munich, Munich, 
      Germany. Stefan.Bushuven@notis-ev.de.
FAU - Bentele, Michael
AU  - Bentele M
AD  - Training Center for Emergency Medicine (NOTIS e.V), Breite Strasse 7, Engen, 
      78234, Germany.
FAU - Bentele, Stefanie
AU  - Bentele S
AD  - Training Center for Emergency Medicine (NOTIS e.V), Breite Strasse 7, Engen, 
      78234, Germany.
FAU - Gerber, Bianka
AU  - Gerber B
AD  - Training Center for Emergency Medicine (NOTIS e.V), Breite Strasse 7, Engen, 
      78234, Germany.
FAU - Bansbach, Joachim
AU  - Bansbach J
AD  - Department of Anesthesiology and Critical Care, Medical Center - University of 
      Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany.
FAU - Ganter, Julian
AU  - Ganter J
AD  - Department of Anesthesiology and Critical Care, Medical Center - University of 
      Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany.
FAU - Trifunovic-Koenig, Milena
AU  - Trifunovic-Koenig M
AD  - Training Center for Emergency Medicine (NOTIS e.V), Breite Strasse 7, Engen, 
      78234, Germany.
FAU - Ranisch, Robert
AU  - Ranisch R
AD  - Faculty for Health Sciences Brandenburg, University of Potsdam, Potsdam, Germany.
LA  - eng
PT  - Journal Article
DEP - 20231121
PL  - United States
TA  - J Med Syst
JT  - Journal of medical systems
JID - 7806056
SB  - IM
MH  - Humans
MH  - Child
MH  - *Emergencies
MH  - Reproducibility of Results
MH  - *Emergency Medical Services
MH  - Health Personnel
MH  - Language
PMC - PMC10663183
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - First responder
OT  - GPT-4
OT  - Large language model
OT  - Medical didactics
OT  - Tele-medicine
COIS- The authors declare no competing interests.
EDAT- 2023/11/21 12:44
MHDA- 2023/11/27 12:42
PMCR- 2023/11/21
CRDT- 2023/11/21 11:10
PHST- 2023/05/09 00:00 [received]
PHST- 2023/11/13 00:00 [accepted]
PHST- 2023/11/27 12:42 [medline]
PHST- 2023/11/21 12:44 [pubmed]
PHST- 2023/11/21 11:10 [entrez]
PHST- 2023/11/21 00:00 [pmc-release]
AID - 10.1007/s10916-023-02019-x [pii]
AID - 2019 [pii]
AID - 10.1007/s10916-023-02019-x [doi]
PST - epublish
SO  - J Med Syst. 2023 Nov 21;47(1):123. doi: 10.1007/s10916-023-02019-x.

PMID- 37182055
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230516
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 4
DP  - 2023 Apr
TI  - Exploring the Boundaries of Reality: Investigating the Phenomenon of Artificial 
      Intelligence Hallucination in Scientific Writing Through ChatGPT References.
PG  - e37432
LID - 10.7759/cureus.37432 [doi]
LID - e37432
AB  - Background Chatbots are computer programs that use artificial intelligence (AI) 
      and natural language processing (NLP) to simulate conversations with humans. One 
      such chatbot is ChatGPT, which uses the third-generation generative pre-trained 
      transformer (GPT-3) developed by OpenAI. ChatGPT has been praised for its ability 
      to generate text, but concerns have been raised about its accuracy and precision 
      in generating data, as well as legal issues related to references. This study 
      aims to investigate the frequency of AI hallucination in research proposals 
      entirely drafted by ChatGPT. Methodology An analytical design was employed to 
      investigate AI hallucination by ChatGPT. A total of 178 references listed by 
      ChatGPT were verified for inclusion in the study. Statistical analysis was 
      performed by five researchers who entered their data into a Google Form, and the 
      final results were represented using pie charts and tables. Results Out of the 
      178 references analyzed, 69 references did not have a Digital Object Identifier 
      (DOI), and 28 references neither turned up on Google search nor had an existing 
      DOI. Three references were listed from books and not research articles. These 
      observations suggest that ChatGPT's ability to generate reliable references for 
      research topics may be limited by the availability of DOI and the accessibility 
      of online articles. Conclusions The study highlights the potential limitations of 
      ChatGPT's ability to generate reliable references for research proposals. AI 
      hallucination is a problem that may negatively impact decision-making and may 
      give rise to ethical and legal problems. Improving the training inputs by 
      including diverse, accurate, and contextually relevant data sets along with 
      frequent updates to the training models could potentially help address these 
      issues. However, until these issues are addressed, researchers using ChatGPT 
      should exercise caution in relying solely on the references generated by the AI 
      chatbot.
CI  - Copyright © 2023, Athaluri et al.
FAU - Athaluri, Sai Anirudh
AU  - Athaluri SA
AD  - Medicine, Rangaraya Medical College, Kakinada, IND.
FAU - Manthena, Sandeep Varma
AU  - Manthena SV
AD  - Medicine, Rangaraya Medical College, Kakinada, IND.
FAU - Kesapragada, V S R Krishna Manoj
AU  - Kesapragada VSRKM
AD  - Medicine, Rangaraya Medical College, Kakinada, IND.
FAU - Yarlagadda, Vineel
AU  - Yarlagadda V
AD  - Medicine, Rangaraya Medical College, Kakinada, IND.
FAU - Dave, Tirth
AU  - Dave T
AD  - Internal Medicine, Bukovinian State Medical University, Chernivtsi, UKR.
FAU - Duddumpudi, Rama Tulasi Siri
AU  - Duddumpudi RTS
AD  - Medicine, Rangaraya Medical College, Kakinada, IND.
LA  - eng
PT  - Journal Article
DEP - 20230411
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10173677
OTO - NOTNLM
OT  - ai hallucination
OT  - artificial intelligence
OT  - chatgpt
OT  - gpt-3
OT  - natural language processing
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/05/14 19:13
MHDA- 2023/05/14 19:14
PMCR- 2023/04/11
CRDT- 2023/05/14 12:26
PHST- 2023/04/11 00:00 [accepted]
PHST- 2023/05/14 19:14 [medline]
PHST- 2023/05/14 19:13 [pubmed]
PHST- 2023/05/14 12:26 [entrez]
PHST- 2023/04/11 00:00 [pmc-release]
AID - 10.7759/cureus.37432 [doi]
PST - epublish
SO  - Cureus. 2023 Apr 11;15(4):e37432. doi: 10.7759/cureus.37432. eCollection 2023 
      Apr.

PMID- 37529789
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230803
IS  - 2452-2473 (Print)
IS  - 2452-2473 (Electronic)
IS  - 2452-2473 (Linking)
VI  - 23
IP  - 3
DP  - 2023 Jul-Sep
TI  - Performance of emergency triage prediction of an open access natural language 
      processing based chatbot application (ChatGPT): A preliminary, scenario-based 
      cross-sectional study.
PG  - 156-161
LID - 10.4103/tjem.tjem_79_23 [doi]
AB  - OBJECTIVES: Artificial intelligence companies have been increasing their 
      initiatives recently to improve the results of chatbots, which are software 
      programs that can converse with a human in natural language. The role of chatbots 
      in health care is deemed worthy of research. OpenAI's ChatGPT is a supervised and 
      empowered machine learning-based chatbot. The aim of this study was to determine 
      the performance of ChatGPT in emergency medicine (EM) triage prediction. METHODS: 
      This was a preliminary, cross-sectional study conducted with case scenarios 
      generated by the researchers based on the emergency severity index (ESI) handbook 
      v4 cases. Two independent EM specialists who were experts in the ESI triage scale 
      determined the triage categories for each case. A third independent EM specialist 
      was consulted as arbiter, if necessary. Consensus results for each case scenario 
      were assumed as the reference triage category. Subsequently, each case scenario 
      was queried with ChatGPT and the answer was recorded as the index triage 
      category. Inconsistent classifications between the ChatGPT and reference category 
      were defined as over-triage (false positive) or under-triage (false negative). 
      RESULTS: Fifty case scenarios were assessed in the study. Reliability analysis 
      showed a fair agreement between EM specialists and ChatGPT (Cohen's Kappa: 
      0.341). Eleven cases (22%) were over triaged and 9 (18%) cases were under triaged 
      by ChatGPT. In 9 cases (18%), ChatGPT reported two consecutive triage categories, 
      one of which matched the expert consensus. It had an overall sensitivity of 57.1% 
      (95% confidence interval [CI]: 34-78.2), specificity of 34.5% (95% CI: 
      17.9-54.3), positive predictive value (PPV) of 38.7% (95% CI: 21.8-57.8), 
      negative predictive value (NPV) of 52.6 (95% CI: 28.9-75.6), and an F1 score of 
      0.461. In high acuity cases (ESI-1 and ESI-2), ChatGPT showed a sensitivity of 
      76.2% (95% CI: 52.8-91.8), specificity of 93.1% (95% CI: 77.2-99.2), PPV of 88.9% 
      (95% CI: 65.3-98.6), NPV of 84.4 (95% CI: 67.2-94.7), and an F1 score of 0.821. 
      The receiver operating characteristic curve showed an area under the curve of 
      0.846 (95% CI: 0.724-0.969, P &lt; 0.001) for high acuity cases. CONCLUSION: The 
      performance of ChatGPT was best when predicting high acuity cases (ESI-1 and 
      ESI-2). It may be useful when determining the cases requiring critical care. When 
      trained with more medical knowledge, ChatGPT may be more accurate for other 
      triage category predictions.
CI  - Copyright: © 2023 Turkish Journal of Emergency Medicine.
FAU - Sarbay, İbrahim
AU  - Sarbay İ
AUID- ORCID: 0000-0001-8804-2501
AD  - Department of Emergency Medicine, Keşan State Hospital, Edirne, Turkey.
FAU - Berikol, Göksu Bozdereli
AU  - Berikol GB
AUID- ORCID: 0000-0002-4529-3578
AD  - Department of Emergency Medicine, Bakırköy Dr. Sadi Konuk Training and Research 
      Hospital, İstanbul, Turkey.
FAU - Özturan, İbrahim Ulaş
AU  - Özturan İU
AUID- ORCID: 0000-0002-1364-5292
AD  - Department of Emergency Medicine, Kocaeli University, Faculty of Medicine, 
      Kocaeli, Turkey.
AD  - Department of Medical Education, Acibadem University, Institute of Health 
      Sciences, Istanbul, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20230626
PL  - India
TA  - Turk J Emerg Med
JT  - Turkish journal of emergency medicine
JID - 101681782
PMC - PMC10389099
OTO - NOTNLM
OT  - ChatGPT
OT  - Chatbot
OT  - emergency severity index
OT  - triage
COIS- None Declared.
EDAT- 2023/08/02 06:43
MHDA- 2023/08/02 06:44
PMCR- 2023/06/26
CRDT- 2023/08/02 04:03
PHST- 2023/03/18 00:00 [received]
PHST- 2023/04/13 00:00 [revised]
PHST- 2023/05/24 00:00 [accepted]
PHST- 2023/08/02 06:44 [medline]
PHST- 2023/08/02 06:43 [pubmed]
PHST- 2023/08/02 04:03 [entrez]
PHST- 2023/06/26 00:00 [pmc-release]
AID - TJEM-23-156 [pii]
AID - 10.4103/tjem.tjem_79_23 [doi]
PST - epublish
SO  - Turk J Emerg Med. 2023 Jun 26;23(3):156-161. doi: 10.4103/tjem.tjem_79_23. 
      eCollection 2023 Jul-Sep.

PMID- 37648157
OWN - NLM
STAT- MEDLINE
DCOM- 20231127
LR  - 20231127
IS  - 1544-3450 (Electronic)
IS  - 1086-5802 (Linking)
VI  - 63
IP  - 6
DP  - 2023 Nov-Dec
TI  - Evaluation of community pharmacists' perceptions and willingness to integrate 
      ChatGPT into their pharmacy practice: A study from Jordan.
PG  - 1761-1767.e2
LID - S1544-3191(23)00291-1 [pii]
LID - 10.1016/j.japh.2023.08.020 [doi]
AB  - OBJECTIVES: This study aimed to examine the extent of community pharmacists' 
      awareness of Chat Generative Pretraining Transformer (ChatGPT), their willingness 
      to embark on this new development of artificial intelligence (AI) development, 
      and barriers that face the incorporation of this nonconventional source of 
      information into pharmacy practice. METHODS: A cross-sectional study was 
      conducted among community pharmacists in Jordanian cities between April 26, 2023, 
      and May 10, 2023. Convenience and snowball sampling techniques were used to 
      select study participants owing to resource and time constraints. The 
      questionnaire was distributed by research assistants through popular social media 
      platforms. Logistic regression analysis was used to assess predictors affecting 
      their willingness to use this service in the future. RESULTS: A total of 221 
      community pharmacists participated in the current study (response rate was not 
      calculated because opt-in recruitment strategies were used). Remarkably, nearly 
      half of the pharmacists (n&nbsp;= 107, 48.4%) indicated a willingness to incorporate 
      the ChatGPT into their pharmacy practice. Nearly half of the pharmacists (n&nbsp;= 
      105, 47.5%) demonstrated a high perceived benefit score for ChatGPT, whereas 
      approximately 37% of pharmacists (n&nbsp;= 81) expressed a high concern score about 
      ChatGPT. More than 70% of pharmacists believed that ChatGPT lacked the ability to 
      use human judgment and make complicated ethical judgments in its responses (n&nbsp;= 
      168). Finally, logistics regression analysis showed that pharmacists who had 
      previous experience in using ChatGPT were more willing to integrate ChatGPT in 
      their pharmacy practice than those with no previous experience in using ChatGPT 
      (odds ratio 2.312, P&nbsp;= 0.035). CONCLUSION: Although pharmacists show a 
      willingness to incorporate ChatGPT into their practice, especially those with 
      previous experience, there are major concerns. These mainly revolve around the 
      tool's ability to make human-like judgments and ethical decisions. These findings 
      are crucial for the future development and integration of AI tools in pharmacy 
      practice.
CI  - Copyright © 2023 American Pharmacists Association®. Published by Elsevier Inc. 
      All rights reserved.
FAU - Abu-Farha, Rana
AU  - Abu-Farha R
FAU - Fino, Leen
AU  - Fino L
FAU - Al-Ashwal, Fahmi Y
AU  - Al-Ashwal FY
FAU - Zawiah, Mohammed
AU  - Zawiah M
FAU - Gharaibeh, Lobna
AU  - Gharaibeh L
FAU - Harahsheh, Mea'ad M
AU  - Harahsheh MM
FAU - Darwish Elhajji, Feras
AU  - Darwish Elhajji F
LA  - eng
PT  - Journal Article
DEP - 20230828
PL  - United States
TA  - J Am Pharm Assoc (2003)
JT  - Journal of the American Pharmacists Association : JAPhA
JID - 101176252
SB  - IM
MH  - Humans
MH  - Pharmacists
MH  - Jordan
MH  - Cross-Sectional Studies
MH  - Artificial Intelligence
MH  - *Community Pharmacy Services
MH  - Attitude of Health Personnel
MH  - Professional Role
MH  - *Pharmacy
COIS- Disclosure The authors declare no relevant conflicts of interest or financial 
      relationships.
EDAT- 2023/08/31 00:41
MHDA- 2023/11/27 12:44
CRDT- 2023/08/30 19:24
PHST- 2023/05/21 00:00 [received]
PHST- 2023/08/10 00:00 [revised]
PHST- 2023/08/22 00:00 [accepted]
PHST- 2023/11/27 12:44 [medline]
PHST- 2023/08/31 00:41 [pubmed]
PHST- 2023/08/30 19:24 [entrez]
AID - S1544-3191(23)00291-1 [pii]
AID - 10.1016/j.japh.2023.08.020 [doi]
PST - ppublish
SO  - J Am Pharm Assoc (2003). 2023 Nov-Dec;63(6):1761-1767.e2. doi: 
      10.1016/j.japh.2023.08.020. Epub 2023 Aug 28.

PMID- 37968109
OWN - NLM
STAT- MEDLINE
DCOM- 20240229
LR  - 20240229
IS  - 1741-3850 (Electronic)
IS  - 1741-3842 (Linking)
VI  - 46
IP  - 1
DP  - 2024 Feb 23
TI  - Artificial intelligence and allergic rhinitis: does ChatGPT increase or impair 
      the knowledge?
PG  - 123-126
LID - 10.1093/pubmed/fdad219 [doi]
AB  - BACKGROUND: Optimal management of allergic rhinitis requires patient education 
      with easy access to accurate information. However, previous online platforms have 
      provided misleading information. The demand for online medical information 
      continues to grow, especially with the introduction of advanced chatbots like 
      ChatGPT. METHODS: This study aimed to evaluate the quality of information 
      provided by ChatGPT regarding allergic rhinitis. A Likert scale was used to 
      assess the accuracy of responses, ranging from 1 to 5. Four authors independently 
      rated the responses from a healthcare professional's perspective. RESULTS: A 
      total of 20 questions covering various aspects of allergic rhinitis were asked. 
      Among the answers, eight received a score of 5 (no inaccuracies), five received a 
      score of 4 (minor non-harmful inaccuracies), six received a score of 3 
      (potentially misinterpretable inaccuracies) and one answer had a score of 2 
      (minor potentially harmful inaccuracies). CONCLUSIONS: The variability in 
      accuracy scores highlights the need for caution when relying solely on chatbots 
      like ChatGPT for medical advice. Patients should consult qualified healthcare 
      professionals and use online sources as a supplement. While ChatGPT has 
      advantages in medical information delivery, its use should be approached with 
      caution. ChatGPT can be useful for patient education but cannot replace 
      healthcare professionals.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of Faculty 
      of Public Health. All rights reserved. For permissions, please e-mail: 
      journals.permissions@oup.com.
FAU - Høj, Simon
AU  - Høj S
AUID- ORCID: 0000-0003-1576-3679
AD  - Steno Diabetes Center Copenhagen, Copenhagen University Hospital, Herlev 2730, 
      Denmark.
AD  - Department of Dermatology, Venereology, and Wound Healing Centre, Copenhagen 
      University Hospital-Bispebjerg, Copenhagen 2400, Denmark.
AD  - Department of Public Health, Environment, Occupation, and Health, Aarhus 
      University, Aarhus 8000, Denmark.
FAU - Thomsen, Simon F
AU  - Thomsen SF
AD  - Department of Dermatology, Venereology, and Wound Healing Centre, Copenhagen 
      University Hospital-Bispebjerg, Copenhagen 2400, Denmark.
AD  - Department of Biomedical Sciences, University of Copenhagen, Copenhagen 2200, 
      Denmark.
FAU - Meteran, Hanieh
AU  - Meteran H
AD  - Department of Internal Medicine, Section of Endocrinology, Copenhagen University 
      Hospital-Hvidovre, Hvidovre 2650, Denmark.
FAU - Sigsgaard, Torben
AU  - Sigsgaard T
AD  - Department of Public Health, Environment, Occupation, and Health, Aarhus 
      University, Aarhus 8000, Denmark.
FAU - Meteran, Howraman
AU  - Meteran H
AUID- ORCID: 0000-0001-5994-8064
AD  - Department of Public Health, Environment, Occupation, and Health, Aarhus 
      University, Aarhus 8000, Denmark.
AD  - Department of Internal Medicine, Respiratory Medicine Section, Copenhagen 
      University Hospital-Hvidovre, Hvidovre 2650, Denmark.
AD  - Department of Respiratory Medicine, Zealand University Hospital Roskilde-Næstved, 
      Næstved 4700, Denmark.
LA  - eng
PT  - Journal Article
PL  - England
TA  - J Public Health (Oxf)
JT  - Journal of public health (Oxford, England)
JID - 101188638
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Rhinitis, Allergic
MH  - Dietary Supplements
MH  - Health Facilities
MH  - Health Personnel
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - allergy
OT  - digital health
OT  - rhinitis
EDAT- 2023/11/16 00:43
MHDA- 2024/02/29 06:43
CRDT- 2023/11/15 21:23
PHST- 2023/05/22 00:00 [received]
PHST- 2023/09/14 00:00 [revised]
PHST- 2023/10/06 00:00 [accepted]
PHST- 2024/02/29 06:43 [medline]
PHST- 2023/11/16 00:43 [pubmed]
PHST- 2023/11/15 21:23 [entrez]
AID - 7423000 [pii]
AID - 10.1093/pubmed/fdad219 [doi]
PST - ppublish
SO  - J Public Health (Oxf). 2024 Feb 23;46(1):123-126. doi: 10.1093/pubmed/fdad219.

PMID- 38347665
OWN - NLM
STAT- MEDLINE
DCOM- 20240219
LR  - 20240219
IS  - 1536-7312 (Electronic)
IS  - 0196-206X (Linking)
VI  - 45
IP  - 1
DP  - 2024 Jan 1
TI  - Challenging the Chatbot: An Assessment of ChatGPT's Diagnoses and Recommendations 
      for DBP Case Studies.
PG  - e8-e13
LID - 10.1097/DBP.0000000000001255 [doi]
AB  - OBJECTIVE: Chat Generative Pretrained Transformer-3.5 (ChatGPT) is a publicly 
      available and free artificial intelligence chatbot that logs billions of visits 
      per day; parents may rely on such tools for developmental and behavioral medical 
      consultations. The objective of this study was to determine how ChatGPT evaluates 
      developmental and behavioral pediatrics (DBP) case studies and makes 
      recommendations and diagnoses. METHODS: ChatGPT was asked to list treatment 
      recommendations and a diagnosis for each of 97 DBP case studies. A panel of 3 DBP 
      physicians evaluated ChatGPT's diagnostic accuracy and scored treatment 
      recommendations on accuracy (5-point Likert scale) and completeness (3-point 
      Likert scale). Physicians also assessed whether ChatGPT's treatment plan 
      correctly addressed cultural and ethical issues for relevant cases. Scores were 
      analyzed using Python, and descriptive statistics were computed. RESULTS: The DBP 
      panel agreed with ChatGPT's diagnosis for 66.2% of the case reports. The mean 
      accuracy score of ChatGPT's treatment plan was deemed by physicians to be 4.6 
      (between entirely correct and more correct than incorrect), and the mean 
      completeness was 2.6 (between complete and adequate). Physicians agreed that 
      ChatGPT addressed relevant cultural issues in 10 out of the 11 appropriate cases 
      and the ethical issues in the single ethical case. CONCLUSION: While ChatGPT can 
      generate a comprehensive and adequate list of recommendations, the diagnosis 
      accuracy rate is still low. Physicians must advise caution to patients when using 
      such online sources.
CI  - Copyright © 2024 Wolters Kluwer Health, Inc. All rights reserved.
FAU - Kim, Rachel
AU  - Kim R
AD  - Division of Developmental and Behavioral Pediatrics, Steven and Alexandra Cohen's 
      Children Medical Center of New York, Lake Success, NY.
FAU - Margolis, Alex
AU  - Margolis A
FAU - Barile, Joe
AU  - Barile J
FAU - Han, Kyle
AU  - Han K
FAU - Kalash, Saia
AU  - Kalash S
FAU - Papaioannou, Helen
AU  - Papaioannou H
FAU - Krevskaya, Anna
AU  - Krevskaya A
FAU - Milanaik, Ruth
AU  - Milanaik R
LA  - eng
PT  - Journal Article
DEP - 20240209
PL  - United States
TA  - J Dev Behav Pediatr
JT  - Journal of developmental and behavioral pediatrics : JDBP
JID - 8006933
SB  - IM
MH  - Child
MH  - Humans
MH  - *Artificial Intelligence
MH  - Parents
MH  - *Physicians
COIS- The authors declare no conflict of interest.
EDAT- 2024/02/13 06:45
MHDA- 2024/02/19 06:43
CRDT- 2024/02/13 00:03
PHST- 2023/08/16 00:00 [received]
PHST- 2023/11/14 00:00 [accepted]
PHST- 2024/02/19 06:43 [medline]
PHST- 2024/02/13 06:45 [pubmed]
PHST- 2024/02/13 00:03 [entrez]
AID - 00004703-990000000-00154 [pii]
AID - 10.1097/DBP.0000000000001255 [doi]
PST - ppublish
SO  - J Dev Behav Pediatr. 2024 Jan 1;45(1):e8-e13. doi: 10.1097/DBP.0000000000001255. 
      Epub 2024 Feb 9.

PMID- 37917165
OWN - NLM
STAT- MEDLINE
DCOM- 20240119
LR  - 20240301
IS  - 1434-4726 (Electronic)
IS  - 0937-4477 (Linking)
VI  - 281
IP  - 2
DP  - 2024 Feb
TI  - Artificial intelligence chatbots as sources of patient education material for 
      obstructive sleep apnoea: ChatGPT versus Google Bard.
PG  - 985-993
LID - 10.1007/s00405-023-08319-9 [doi]
AB  - PURPOSE: To perform the first head-to-head comparative evaluation of patient 
      education material for obstructive sleep apnoea generated by two artificial 
      intelligence chatbots, ChatGPT and its primary rival Google Bard. METHODS: Fifty 
      frequently asked questions on obstructive sleep apnoea in English were extracted 
      from the patient information webpages of four major sleep organizations and 
      categorized as input prompts. ChatGPT and Google Bard responses were selected and 
      independently rated using the Patient Education Materials Assessment 
      Tool-Printable (PEMAT-P) Auto-Scoring Form by two otolaryngologists, with a 
      Fellowship of the Royal College of Surgeons (FRCS) and a special interest in 
      sleep medicine and surgery. Responses were subjectively screened for any 
      incorrect or dangerous information as a secondary outcome. The Flesch-Kincaid 
      Calculator was used to evaluate the readability of responses for both ChatGPT and 
      Google Bard. RESULTS: A total of 46 questions were curated and categorized into 
      three domains: condition (n = 14), investigation (n = 9) and treatment (n = 23). 
      Understandability scores for ChatGPT versus Google Bard on the various domains 
      were as follows: condition 90.86% vs.76.32% (p &lt; 0.001); investigation 89.94% vs. 
      71.67% (p &lt; 0.001); treatment 90.78% vs.73.74% (p &lt; 0.001). Actionability scores 
      for ChatGPT versus Google Bard on the various domains were as follows: condition 
      77.14% vs. 51.43% (p &lt; 0.001); investigation 72.22% vs. 54.44% (p = 0.05); 
      treatment 73.04% vs. 54.78% (p = 0.002). The mean Flesch-Kincaid Grade Level for 
      ChatGPT was 9.0 and Google Bard was 5.9. No incorrect or dangerous information 
      was identified in any of the generated responses from both ChatGPT and Google 
      Bard. CONCLUSION: Evaluation of ChatGPT and Google Bard patient education 
      material for OSA indicates the former to offer superior information across 
      several domains.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Cheong, Ryan Chin Taw
AU  - Cheong RCT
AUID- ORCID: 0000-0001-7846-8699
AD  - Otolaryngology-Head and Neck Surgery Department, The Royal Marsden NHS Foundation 
      Trust, Fulham Road, London, SW3 6JJ, UK. ryan.cheong@nhs.net.
FAU - Unadkat, Samit
AU  - Unadkat S
AD  - Otolaryngology-Head and Neck Surgery Department, The Royal National ENT and 
      Eastman Dental Hospitals, University College London Hospitals NHS Foundation 
      Trust, London, UK.
FAU - Mcneillis, Venkata
AU  - Mcneillis V
AD  - Otolaryngology-Head and Neck Surgery Department, The Royal National ENT and 
      Eastman Dental Hospitals, University College London Hospitals NHS Foundation 
      Trust, London, UK.
FAU - Williamson, Andrew
AU  - Williamson A
AD  - Otolaryngology-Head and Neck Surgery Department, The Royal Marsden NHS Foundation 
      Trust, Fulham Road, London, SW3 6JJ, UK.
FAU - Joseph, Jonathan
AU  - Joseph J
AD  - Otolaryngology-Head and Neck Surgery Department, The Royal National ENT and 
      Eastman Dental Hospitals, University College London Hospitals NHS Foundation 
      Trust, London, UK.
FAU - Randhawa, Premjit
AU  - Randhawa P
AD  - Otolaryngology-Head and Neck Surgery Department, The Royal National ENT and 
      Eastman Dental Hospitals, University College London Hospitals NHS Foundation 
      Trust, London, UK.
FAU - Andrews, Peter
AU  - Andrews P
AD  - Otolaryngology-Head and Neck Surgery Department, The Royal National ENT and 
      Eastman Dental Hospitals, University College London Hospitals NHS Foundation 
      Trust, London, UK.
FAU - Paleri, Vinidh
AU  - Paleri V
AD  - Otolaryngology-Head and Neck Surgery Department, The Royal Marsden NHS Foundation 
      Trust, Fulham Road, London, SW3 6JJ, UK.
LA  - eng
PT  - Journal Article
DEP - 20231102
PL  - Germany
TA  - Eur Arch Otorhinolaryngol
JT  - European archives of oto-rhino-laryngology : official journal of the European 
      Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the 
      German Society for Oto-Rhino-Laryngology - Head and Neck Surgery
JID - 9002937
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - Search Engine
MH  - Patient Education as Topic
MH  - *Sleep Apnea, Obstructive/therapy
MH  - *Surgeons
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Google Bard
OT  - Large language models
OT  - Obstructive sleep apnoea
OT  - Patient education material
EDAT- 2023/11/02 18:43
MHDA- 2024/01/19 06:42
CRDT- 2023/11/02 12:03
PHST- 2023/07/22 00:00 [received]
PHST- 2023/10/26 00:00 [accepted]
PHST- 2024/01/19 06:42 [medline]
PHST- 2023/11/02 18:43 [pubmed]
PHST- 2023/11/02 12:03 [entrez]
AID - 10.1007/s00405-023-08319-9 [pii]
AID - 10.1007/s00405-023-08319-9 [doi]
PST - ppublish
SO  - Eur Arch Otorhinolaryngol. 2024 Feb;281(2):985-993. doi: 
      10.1007/s00405-023-08319-9. Epub 2023 Nov 2.

PMID- 38516983
OWN - NLM
STAT- Publisher
LR  - 20240322
IS  - 1744-5205 (Electronic)
IS  - 0882-0538 (Linking)
DP  - 2024 Mar 22
TI  - Dr. Google vs. Dr. ChatGPT: Exploring the Use of Artificial Intelligence in 
      Ophthalmology by Comparing the Accuracy, Safety, and Readability of Responses to 
      Frequently Asked Patient Questions Regarding Cataracts and Cataract Surgery.
PG  - 1-8
LID - 10.1080/08820538.2024.2326058 [doi]
AB  - PURPOSE: Patients are using online search modalities to learn about their eye 
      health. While Google remains the most popular search engine, the use of large 
      language models (LLMs) like ChatGPT has increased. Cataract surgery is the most 
      common surgical procedure in the US, and there is limited data on the quality of 
      online information that populates after searches related to cataract surgery on 
      search engines such as Google and LLM platforms such as ChatGPT. We identified 
      the most common patient frequently asked questions (FAQs) about cataracts and 
      cataract surgery and evaluated the accuracy, safety, and readability of the 
      answers to these questions provided by both Google and ChatGPT. We demonstrated 
      the utility of ChatGPT in writing notes and creating patient education materials. 
      METHODS: The top 20 FAQs related to cataracts and cataract surgery were recorded 
      from Google. Responses to the questions provided by Google and ChatGPT were 
      evaluated by a panel of ophthalmologists for accuracy and safety. Evaluators were 
      also asked to distinguish between Google and LLM chatbot answers. Five validated 
      readability indices were used to assess the readability of responses. ChatGPT was 
      instructed to generate operative notes, post-operative instructions, and 
      customizable patient education materials according to specific readability 
      criteria. RESULTS: Responses to 20 patient FAQs generated by ChatGPT were 
      significantly longer and written at a higher reading level than responses 
      provided by Google (p &lt; .001), with an average grade level of 14.8 (college 
      level). Expert reviewers were correctly able to distinguish between a 
      human-reviewed and chatbot generated response an average of 31% of the time. 
      Google answers contained incorrect or inappropriate material 27% of the time, 
      compared with 6% of LLM generated answers (p &lt; .001). When expert reviewers were 
      asked to compare the responses directly, chatbot responses were favored (66%). 
      CONCLUSIONS: When comparing the responses to patients' cataract FAQs provided by 
      ChatGPT and Google, practicing ophthalmologists overwhelming preferred ChatGPT 
      responses. LLM chatbot responses were less likely to contain inaccurate 
      information. ChatGPT represents a viable information source for eye health for 
      patients with higher health literacy. ChatGPT may also be used by 
      ophthalmologists to create customizable patient education materials for patients 
      with varying health literacy.
FAU - Cohen, Samuel A
AU  - Cohen SA
AD  - Byers Eye Institute, Stanford University School of Medicine, Stanford, CA, USA.
FAU - Brant, Arthur
AU  - Brant A
AD  - Byers Eye Institute, Stanford University School of Medicine, Stanford, CA, USA.
FAU - Fisher, Ann Caroline
AU  - Fisher AC
AD  - Byers Eye Institute, Stanford University School of Medicine, Stanford, CA, USA.
FAU - Pershing, Suzann
AU  - Pershing S
AD  - Byers Eye Institute, Stanford University School of Medicine, Stanford, CA, USA.
FAU - Do, Diana
AU  - Do D
AD  - Byers Eye Institute, Stanford University School of Medicine, Stanford, CA, USA.
FAU - Pan, Carolyn
AU  - Pan C
AD  - Byers Eye Institute, Stanford University School of Medicine, Stanford, CA, USA.
LA  - eng
PT  - Journal Article
DEP - 20240322
PL  - England
TA  - Semin Ophthalmol
JT  - Seminars in ophthalmology
JID - 8610759
SB  - IM
OTO - NOTNLM
OT  - Cataract surgery
OT  - ChatGPT
OT  - Google
OT  - cataracts
OT  - patient education
OT  - readability
EDAT- 2024/03/22 12:45
MHDA- 2024/03/22 12:45
CRDT- 2024/03/22 06:53
PHST- 2024/03/22 12:45 [medline]
PHST- 2024/03/22 12:45 [pubmed]
PHST- 2024/03/22 06:53 [entrez]
AID - 10.1080/08820538.2024.2326058 [doi]
PST - aheadofprint
SO  - Semin Ophthalmol. 2024 Mar 22:1-8. doi: 10.1080/08820538.2024.2326058.

PMID- 38314324
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240206
IS  - 2212-1366 (Print)
IS  - 2212-1374 (Electronic)
IS  - 2212-1366 (Linking)
VI  - 44
DP  - 2024 Feb
TI  - Large-Scale assessment of ChatGPT's performance in benign and malignant bone 
      tumors imaging report diagnosis and its potential for clinical applications.
PG  - 100525
LID - 10.1016/j.jbo.2024.100525 [doi]
LID - 100525
AB  - OBJECTIVE: This study was designed to delve into the complexities involved in 
      diagnosing of benign and malignant bone tumors and to assess the potential of AI 
      technologies like ChatGPT in improving diagnostic accuracy and efficiency. The 
      study also explores the few-shot learning as a method to optimize ChatGPT's 
      performance in specialized medical domains such as benign and malignant bone 
      tumors diagnosis. METHODS: A total of 1366 benign and malignant bone 
      tumors-related imaging reports were collected and diagnosed by 25 experienced 
      physicians. The gold standard of diagnosis was established by combining clinical, 
      imaging and pathological principles.These reports were then input into the 
      ChatGPT model which underwent a few-shot learning method to generate diagnostic 
      results. The diagnostic results of the physicians and the AI model were compared 
      to evaluate the performance of ChatGPT. An experiment was conducted to assess the 
      influence of different radiologist's reporting styles on the model's diagnostic 
      performance. Furthermore, in-depth analysis of misdiagnosed cases was carried 
      out, categorizing diagnostic errors and exploring possible causes. RESULTS: The 
      diagnostic results generated by ChatGPT showed an accuracy of 0.73, sensitivity 
      of 0.95, and specificity of 0.58. After few-shot learning, ChatGPT demonstrated 
      significant improvement, achieving an accuracy of 0.87, sensitivity of 0.99, and 
      specificity of 0.73, bringing it much closer to the level of physician 
      diagnostics. In an experiment analyzing the influence of the radiologist's 
      reporting style, the model demonstrated higher sensitivity when interpreting 
      reports written by high-level radiologists. In 56 benign cases, ChatGPT 
      misdiagnosed them as malignant. Among these, 35 benign lesions- fibrous dysplasia 
      and osteofibrous dysplasia- were incorrectly identified as metastatic tumors or 
      osteosarcomas; 8 cases of myositis ossificans were wrongly diagnosed as 
      extraosseous osteosarcoma. 7 cases of giant cell tumor of bone at the end of long 
      bone were misdiagnosed as osteosarcoma by intermediate doctors. Chondroblastoma 
      was misdiagnosed as malignant tumor in 6 cases -2 osteosarcoma and 4 
      chondrosarcoma-In this study, 23 osteosarcoma cases were misdiagnosed by ChatGPT 
      as osteomyelitis; Chondrosarcoma was misdiagnosed as fibrous dysplasia or 
      aneurysmal bone cyst in 8 cases. Four cases of spinal chordoma were misdiagnosed 
      as spinal tuberculosis. CONCLUSION: Our findings highlight the potential of 
      ChatGPT in the diagnosis of benign and malignant bone tumors, offering advantages 
      like enhanced efficiency and a reduction in missed diagnoses. However, the 
      necessity of collaborative interactions between physicians and ChatGPT in 
      practical settings was underscored. With an examination into AI's capacity in 
      benign and malignant bone tumors diagnosis, this study lays the groundwork for 
      future AI advancements in medicine. Additionally, the benefits of few-shot 
      learning in fine-tuning ChatGPT applications in specialized fields were also 
      demonstrated.
CI  - © 2024 The Author(s).
FAU - Yang, Fan
AU  - Yang F
AD  - Department of Radiation, Beijing Jishuitan Hospital,Capital Medical University, 
      Beijing 100035, China.
FAU - Yan, Dong
AU  - Yan D
AD  - Department of Radiation, Beijing Jishuitan Hospital,Capital Medical University, 
      Beijing 100035, China.
FAU - Wang, Zhixiang
AU  - Wang Z
AD  - Department of Ultrasound, Beijing Friendship Hospital, Capital Medical 
      University, Beijing 100050, China.
LA  - eng
PT  - Journal Article
DEP - 20240122
PL  - Netherlands
TA  - J Bone Oncol
JT  - Journal of bone oncology
JID - 101610292
PMC - PMC10834989
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Bone tumors
OT  - ChatGPT
OT  - Diagnosis
COIS- The authors declare that they have no known competing financial interests or 
      personal relationships that could have appeared to influence the work reported in 
      this paper.
EDAT- 2024/02/05 06:44
MHDA- 2024/02/05 06:45
PMCR- 2024/01/22
CRDT- 2024/02/05 05:03
PHST- 2023/10/26 00:00 [received]
PHST- 2024/01/03 00:00 [revised]
PHST- 2024/01/07 00:00 [accepted]
PHST- 2024/02/05 06:45 [medline]
PHST- 2024/02/05 06:44 [pubmed]
PHST- 2024/02/05 05:03 [entrez]
PHST- 2024/01/22 00:00 [pmc-release]
AID - S2212-1374(24)00005-8 [pii]
AID - 100525 [pii]
AID - 10.1016/j.jbo.2024.100525 [doi]
PST - epublish
SO  - J Bone Oncol. 2024 Jan 22;44:100525. doi: 10.1016/j.jbo.2024.100525. eCollection 
      2024 Feb.

PMID- 38314011
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240206
IS  - 1178-2390 (Print)
IS  - 1178-2390 (Electronic)
IS  - 1178-2390 (Linking)
VI  - 17
DP  - 2024
TI  - Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A 
      Qualitative Study.
PG  - 461-471
LID - 10.2147/JMDH.S447368 [doi]
AB  - BACKGROUND: Artificial Intelligence (AI) applications are widely researched for 
      their potential in effectively improving the healthcare operations and disease 
      management. However, the research trend shows that these applications also have 
      significant negative implications on the service delivery. PURPOSE: To assess the 
      use of ChatGPT for mental health support. METHODS: Due to the novelty and 
      unfamiliarity of the ChatGPT technology, a quasi-experimental design was chosen 
      for this study. Outpatients from a public hospital were included in the sample. A 
      two-week experiment followed by semi-structured interviews was conducted in which 
      participants used ChatGPT for mental health support. Semi-structured interviews 
      were conducted with 24 individuals with mental health conditions. RESULTS: Eight 
      positive factors (psychoeducation, emotional support, goal setting and 
      motivation, referral and resource information, self-assessment and monitoring, 
      cognitive behavioral therapy, crisis interventions, and psychotherapeutic 
      exercises) and four negative factors (ethical and legal considerations, accuracy 
      and reliability, limited assessment capabilities, and cultural and linguistic 
      considerations) were associated with the use of ChatGPT for mental health 
      support. CONCLUSION: It is important to carefully consider the ethical, 
      reliability, accuracy, and legal challenges and develop appropriate strategies to 
      mitigate them in order to ensure safe and effective use of AI-based applications 
      like ChatGPT in mental health support.
CI  - © 2024 Alanezi.
FAU - Alanezi, Fahad
AU  - Alanezi F
AUID- ORCID: 0000-0002-5458-7818
AD  - College of Business Administration, Department Management Information Systems, 
      Imam Abdulrahman Bin Faisal University, Dammam, 31441, Saudi Arabia.
LA  - eng
PT  - Journal Article
DEP - 20240131
PL  - New Zealand
TA  - J Multidiscip Healthc
JT  - Journal of multidisciplinary healthcare
JID - 101512691
PMC - PMC10838501
OTO - NOTNLM
OT  - ChatGPT
OT  - anxiety
OT  - artificial intelligence
OT  - mentally-ill patients
OT  - motivation
OT  - support
COIS- The author reports no conflicts of interest in this work.
EDAT- 2024/02/05 06:42
MHDA- 2024/02/05 06:43
PMCR- 2024/01/31
CRDT- 2024/02/05 04:55
PHST- 2023/10/30 00:00 [received]
PHST- 2024/01/08 00:00 [accepted]
PHST- 2024/02/05 06:43 [medline]
PHST- 2024/02/05 06:42 [pubmed]
PHST- 2024/02/05 04:55 [entrez]
PHST- 2024/01/31 00:00 [pmc-release]
AID - 447368 [pii]
AID - 10.2147/JMDH.S447368 [doi]
PST - epublish
SO  - J Multidiscip Healthc. 2024 Jan 31;17:461-471. doi: 10.2147/JMDH.S447368. 
      eCollection 2024.

PMID- 38365990
OWN - NLM
STAT- Publisher
LR  - 20240216
IS  - 1434-4726 (Electronic)
IS  - 0937-4477 (Linking)
DP  - 2024 Feb 16
TI  - A cross-sectional comparative study: ChatGPT 3.5 versus diverse levels of medical 
      experts in the diagnosis of ENT diseases.
LID - 10.1007/s00405-024-08509-z [doi]
AB  - PURPOSE: With recent advances in artificial intelligence (AI), it has become 
      crucial to thoroughly evaluate its applicability in healthcare. This study aimed 
      to assess the accuracy of ChatGPT in diagnosing ear, nose, and throat (ENT) 
      pathology, and comparing its performance to that of medical experts. METHODS: We 
      conducted a cross-sectional comparative study where 32 ENT cases were presented 
      to ChatGPT 3.5, ENT physicians, ENT residents, family medicine (FM) specialists, 
      second-year medical students (Med2), and third-year medical students (Med3). Each 
      participant provided three differential diagnoses. The study analyzed diagnostic 
      accuracy rates and inter-rater agreement within and between participant groups 
      and ChatGPT. RESULTS: The accuracy rate of ChatGPT was 70.8%, being not 
      significantly different from ENT physicians or ENT residents. However, a 
      significant difference in correctness rate existed between ChatGPT and FM 
      specialists (49.8%, p &lt; 0.001), and between ChatGPT and medical students (Med2 
      47.5%, p &lt; 0.001; Med3 47%, p &lt; 0.001). Inter-rater agreement for the 
      differential diagnosis between ChatGPT and each participant group was either poor 
      or fair. In 68.75% of cases, ChatGPT failed to mention the most critical 
      diagnosis. CONCLUSIONS: ChatGPT demonstrated accuracy comparable to that of ENT 
      physicians and ENT residents in diagnosing ENT pathology, outperforming FM 
      specialists, Med2 and Med3. However, it showed limitations in identifying the 
      most critical diagnosis.
CI  - © 2024. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Makhoul, Mikhael
AU  - Makhoul M
AUID- ORCID: 0000-0001-5518-5549
AD  - Department of Otolaryngology-Head and Neck Surgery, Hotel Dieu de France 
      Hospital, Saint Joseph University, Alfred Naccache Boulevard, Ashrafieh, PO Box: 
      166830, Beirut, Lebanon. mikhaelmakhoul651@gmail.com.
FAU - Melkane, Antoine E
AU  - Melkane AE
AD  - Department of Otolaryngology-Head and Neck Surgery, Hotel Dieu de France 
      Hospital, Saint Joseph University, Alfred Naccache Boulevard, Ashrafieh, PO Box: 
      166830, Beirut, Lebanon.
FAU - Khoury, Patrick El
AU  - Khoury PE
AD  - Department of Otolaryngology-Head and Neck Surgery, Hotel Dieu de France 
      Hospital, Saint Joseph University, Alfred Naccache Boulevard, Ashrafieh, PO Box: 
      166830, Beirut, Lebanon.
FAU - Hadi, Christopher El
AU  - Hadi CE
AD  - Department of Otolaryngology-Head and Neck Surgery, Hotel Dieu de France 
      Hospital, Saint Joseph University, Alfred Naccache Boulevard, Ashrafieh, PO Box: 
      166830, Beirut, Lebanon.
FAU - Matar, Nayla
AU  - Matar N
AD  - Department of Otolaryngology-Head and Neck Surgery, Hotel Dieu de France 
      Hospital, Saint Joseph University, Alfred Naccache Boulevard, Ashrafieh, PO Box: 
      166830, Beirut, Lebanon.
LA  - eng
PT  - Journal Article
DEP - 20240216
PL  - Germany
TA  - Eur Arch Otorhinolaryngol
JT  - European archives of oto-rhino-laryngology : official journal of the European 
      Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the 
      German Society for Oto-Rhino-Laryngology - Head and Neck Surgery
JID - 9002937
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Case scenarios
OT  - ChatGPT
OT  - Diagnostic accuracy
OT  - ENT
OT  - Otolaryngology
EDAT- 2024/02/17 10:42
MHDA- 2024/02/17 10:42
CRDT- 2024/02/16 23:57
PHST- 2024/01/01 00:00 [received]
PHST- 2024/01/24 00:00 [accepted]
PHST- 2024/02/17 10:42 [medline]
PHST- 2024/02/17 10:42 [pubmed]
PHST- 2024/02/16 23:57 [entrez]
AID - 10.1007/s00405-024-08509-z [pii]
AID - 10.1007/s00405-024-08509-z [doi]
PST - aheadofprint
SO  - Eur Arch Otorhinolaryngol. 2024 Feb 16. doi: 10.1007/s00405-024-08509-z.

PMID- 37692649
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230913
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 8
DP  - 2023 Aug
TI  - The Use of AI in Diagnosing Diseases and Providing Management Plans: A 
      Consultation on Cardiovascular Disorders With ChatGPT.
PG  - e43106
LID - 10.7759/cureus.43106 [doi]
LID - e43106
AB  - BACKGROUND: Cardiovascular diseases (CVDs) have remained the leading causes of 
      death worldwide and substantially contribute to loss of health and excess health 
      system costs. According to WHO, cardiovascular diseases (CVDs) take an estimated 
      17.9 million lives each year. One of the reasons for an immensely high fatality 
      in CVDs is lack of efficient diagnosis and prompt treatment. Timely recognition 
      and management are crucial to minimize mortality. In the advancing world, AI 
      (artificial intelligence) and machine learning technologies continue to progress, 
      this advancement has opened new avenues for innovative approaches in the field of 
      medicine. Despite the rapid development in the field of AI, there is a limited 
      understanding of the potential benefits among clinicians and medical 
      practitioners. METHODS: In this study, we aimed to investigate the potential that 
      the AI language model holds to assist health practitioners in the diagnosis and 
      treatment of cardiovascular disorders. We asked Chat Generative Pre-trained 
      Transformer (ChatGPT) 10 hypothetical questions simulating clinical consultation. 
      The responses given by ChatGPT were accessed for its accuracy and accessibility 
      by a team of medical specialists and cardiologists with extensive experience in 
      managing cardiovascular disorders.&nbsp; Result:&nbsp;Out of the 10 clinical scenarios 
      inserted in ChatGPT, eight were perfectly diagnosed, however, the other two 
      answers given by ChatGPT were not entirely incorrect since those conditions were 
      associated with the actual diagnosis. Furthermore, the management plans and the 
      treatment protocols that were given by ChatGPT were in line with the literature 
      and current medical knowledge. The exact drug names and regimens were not 
      provided but a general guideline that was given by this AI tool is definitely 
      beneficial for junior doctors in getting an idea on how to proceed or refresh 
      their previous knowledge. CONCLUSION: ChatGPT is a valuable resource in the field 
      of medicine. Its comprehensive&nbsp;and properly organized response in an 
      understandable language has made it an effective and efficient tool to be used. 
      However, it is crucial to note that its limitations, such as the need for all 
      associated and typical signs, symptoms, and physical examination findings, and 
      its inability to personalize treatments need to be acknowledged.
CI  - Copyright © 2023, Rizwan et al.
FAU - Rizwan, Ayesha
AU  - Rizwan A
AD  - Medicine and Surgery, Islamic International Medical College, Islamabad, PAK.
FAU - Sadiq, Tahira
AU  - Sadiq T
AD  - Community Medicine, Islamic International Medical College, Islamabad, PAK.
LA  - eng
PT  - Journal Article
DEP - 20230807
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10483170
OTO - NOTNLM
OT  - ai and robotics in healthcare
OT  - ai consultation
OT  - artificial intelligence chatgpt-4
OT  - cardiovascular disorders
OT  - diagnosis and management
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/09/11 06:43
MHDA- 2023/09/11 06:44
PMCR- 2023/08/07
CRDT- 2023/09/11 04:41
PHST- 2023/08/07 00:00 [accepted]
PHST- 2023/09/11 06:44 [medline]
PHST- 2023/09/11 06:43 [pubmed]
PHST- 2023/09/11 04:41 [entrez]
PHST- 2023/08/07 00:00 [pmc-release]
AID - 10.7759/cureus.43106 [doi]
PST - epublish
SO  - Cureus. 2023 Aug 7;15(8):e43106. doi: 10.7759/cureus.43106. eCollection 2023 Aug.

PMID- 38180108
OWN - NLM
STAT- Publisher
LR  - 20240105
IS  - 1365-2230 (Electronic)
IS  - 0307-6938 (Linking)
DP  - 2024 Jan 5
TI  - Comparing the Quality of ChatGPT- and Physician-Generated Responses to Patients' 
      Dermatologic Questions in the Electronic Medical Record.
LID - llad456 [pii]
LID - 10.1093/ced/llad456 [doi]
AB  - BACKGROUND: Chat Generated Pre-trained Transformer ('ChatGPT,' Open AI, San 
      Francisco, USA) is a free artificial intelligence (AI)-based natural language 
      processing tool that generates complex responses to inputs from users. OBJECTIVE: 
      To determine whether ChatGPT is able to generate high-quality responses to 
      patient-submitted questions in the patient portal. METHODS: Patient-submitted 
      questions and their corresponding responses from their dermatology physician were 
      extracted from the electronic medical record for analysis. The questions were 
      input into ChatGPT (version 3.5), and the outputs were extracted for analysis, 
      with manual removal of verbiage pertaining to ChatGPT's inability to provide 
      medical advice. Ten blinded reviewers (n=7 physicians, n=3 non-physicians) rated 
      and selected their preference in terms of 'overall quality,' 'readability,' 
      'accuracy,' 'thoroughness,' and 'level of empathy,' of the physician- and 
      ChatGPT-generated responses. RESULTS: Thirty-one messages and responses were 
      analyzed. The physician-generated response was vastly preferred over the ChatGPT 
      response by both physician and non-physician reviewers and received significantly 
      higher ratings for 'readability' and 'level of empathy.' CONCLUSIONS: The results 
      of this study suggest that physician-generated responses to patients' portal 
      messages are still preferred over ChatGPT, but generative AI tools may still be 
      helpful in generating first drafts of responses and education resources for 
      patients.
CI  - © The Author(s) 2024. Published by Oxford University Press on behalf of British 
      Association of Dermatologists. All rights reserved. For permissions, please 
      e-mail: journals.permissions@oup.com.
FAU - Reynolds, Kelly
AU  - Reynolds K
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Nadelman, Daniel
AU  - Nadelman D
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Durgin, Joseph
AU  - Durgin J
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Ansah-Addo, Stephen
AU  - Ansah-Addo S
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Cole, Daniel
AU  - Cole D
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Fayne, Rachel
AU  - Fayne R
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Harrell, Jane
AU  - Harrell J
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Ratycz, Madison
AU  - Ratycz M
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Runge, Mason
AU  - Runge M
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Shepard-Hayes, Amanda
AU  - Shepard-Hayes A
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Wenzel, Daniel
AU  - Wenzel D
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
FAU - Tejasvi, Trilokraj
AU  - Tejasvi T
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, USA.
LA  - eng
PT  - Journal Article
DEP - 20240105
PL  - England
TA  - Clin Exp Dermatol
JT  - Clinical and experimental dermatology
JID - 7606847
SB  - IM
EDAT- 2024/01/05 12:43
MHDA- 2024/01/05 12:43
CRDT- 2024/01/05 09:48
PHST- 2023/10/01 00:00 [received]
PHST- 2023/11/28 00:00 [revised]
PHST- 2024/01/03 00:00 [accepted]
PHST- 2024/01/05 12:43 [medline]
PHST- 2024/01/05 12:43 [pubmed]
PHST- 2024/01/05 09:48 [entrez]
AID - 7511330 [pii]
AID - 10.1093/ced/llad456 [doi]
PST - aheadofprint
SO  - Clin Exp Dermatol. 2024 Jan 5:llad456. doi: 10.1093/ced/llad456.

PMID- 37676187
OWN - NLM
STAT- MEDLINE
DCOM- 20231030
LR  - 20231102
IS  - 1943-4723 (Electronic)
IS  - 0002-8177 (Linking)
VI  - 154
IP  - 11
DP  - 2023 Nov
TI  - The performance of artificial intelligence language models in board-style dental 
      knowledge assessment: A preliminary study on ChatGPT.
PG  - 970-974
LID - S0002-8177(23)00458-0 [pii]
LID - 10.1016/j.adaj.2023.07.016 [doi]
AB  - BACKGROUND: Although Chat Generative Pre-trained Transformer (ChatGPT) (OpenAI) 
      may be an appealing educational resource for students, the chatbot responses can 
      be subject to misinformation. This study was designed to evaluate the performance 
      of ChatGPT on a board-style multiple-choice dental knowledge assessment to gauge 
      its capacity to output accurate dental content and in turn the risk of 
      misinformation associated with use of the chatbot as an educational resource by 
      dental students. METHODS: ChatGPT3.5 and ChatGPT4 were asked questions obtained 
      from 3 different sources: INBDE Bootcamp, ITDOnline, and a list of board-style 
      questions provided by the Joint Commission on National Dental Examinations. 
      Image-based questions were excluded, as ChatGPT only takes text-based inputs. The 
      mean performance across 3 trials was reported for each model. RESULTS: ChatGPT3.5 
      and ChatGPT4 answered 61.3% and 76.9% of the questions correctly on average, 
      respectively. A 2-tailed t test was used to compare 2 independent sample means, 
      and a 2-tailed χ(2) test was used to compare 2 sample proportions. A P value less 
      than .05 was considered to be statistically significant. CONCLUSION: ChatGPT3.5 
      did not perform sufficiently well on the board-style knowledge assessment. 
      ChatGPT4, however, displayed a competent ability to output accurate dental 
      content. Future research should evaluate the proficiency of emerging models of 
      ChatGPT in dentistry to assess its evolving role in dental education. PRACTICAL 
      IMPLICATIONS: Although ChatGPT showed an impressive ability to output accurate 
      dental content, our findings should encourage dental students to incorporate 
      ChatGPT to supplement their existing learning program instead of using it as 
      their primary learning resource.
CI  - Copyright © 2023 American Dental Association. Published by Elsevier Inc. All 
      rights reserved.
FAU - Danesh, Arman
AU  - Danesh A
FAU - Pazouki, Hirad
AU  - Pazouki H
FAU - Danesh, Kasra
AU  - Danesh K
FAU - Danesh, Farzad
AU  - Danesh F
FAU - Danesh, Arsalan
AU  - Danesh A
LA  - eng
PT  - Journal Article
DEP - 20230905
PL  - England
TA  - J Am Dent Assoc
JT  - Journal of the American Dental Association (1939)
JID - 7503060
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Language
MH  - Educational Status
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Integrated National Board Dental Examination
OT  - dental board examination
OT  - dental education
OT  - dentistry
EDAT- 2023/09/07 12:42
MHDA- 2023/10/30 06:47
CRDT- 2023/09/07 10:33
PHST- 2023/06/30 00:00 [received]
PHST- 2023/07/27 00:00 [revised]
PHST- 2023/07/29 00:00 [accepted]
PHST- 2023/10/30 06:47 [medline]
PHST- 2023/09/07 12:42 [pubmed]
PHST- 2023/09/07 10:33 [entrez]
AID - S0002-8177(23)00458-0 [pii]
AID - 10.1016/j.adaj.2023.07.016 [doi]
PST - ppublish
SO  - J Am Dent Assoc. 2023 Nov;154(11):970-974. doi: 10.1016/j.adaj.2023.07.016. Epub 
      2023 Sep 5.

PMID- 38545309
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240329
IS  - 2277-9531 (Print)
IS  - 2319-6440 (Electronic)
IS  - 2277-9531 (Linking)
VI  - 13
DP  - 2024
TI  - Efficacy of ChatGPT in solving attitude, ethics, and communication case scenario 
      used for competency-based medical education in India: A case study.
PG  - 22
LID - 10.4103/jehp.jehp_625_23 [doi]
LID - 22
AB  - BACKGROUND: Competency-based medical education (CBME) is a method of medical 
      training that focuses on developing learners' competencies rather than simply 
      assessing their knowledge and skills. Attitude, ethics, and communication 
      (AETCOM) are important components of CBME, and the use of artificial intelligence 
      (AI) tools such as ChatGPT for CBME has not been studied. Hence, we aimed to 
      assess the capability of ChatGPT in solving AETCOM case scenarios used for CBME 
      in India. MATERIALS AND METHODS: A total of 11 case scenarios were developed 
      based on the AETCOM competencies. The scenarios were presented to ChatGPT, and 
      the responses generated by ChatGPT were evaluated by three independent experts by 
      awarding score ranging from 0 to 5. The scores were compared with a predefined 
      score of 2.5 (50% accuracy) and 4 (80% accuracy) of a one-sample median test. 
      Scores among the three raters were compared by the Kruskal-Wallis H test. The 
      inter-rater reliability of the evaluations was assessed using the intraclass 
      correlation coefficient (ICC). RESULTS: The mean score of solution provided by 
      ChatGPT was 3.88 ± 0.47 (out of 5), indicating an accuracy of approximately 78%. 
      The responses evaluated by three raters were similar (Kruskal-Wallis H P value 
      0.51), and the ICC value was 0.796, which indicates a relatively high level of 
      agreement among the raters. CONCLUSION: ChatGPT shows moderate capability in 
      solving AETCOM case scenarios used for CBME in India. The inter-rater reliability 
      of the evaluations suggests that ChatGPT's responses were consistent and 
      reliable. Further studies are needed to explore the potential of ChatGPT and 
      other AI tools in CBME and to determine the optimal use of these tools in medical 
      education.
CI  - Copyright: © 2024 Journal of Education and Health Promotion.
FAU - Roy, Asitava Deb
AU  - Roy AD
AD  - Department of Pathology, Mata Gujri Memorial Medical College and Lions Seva 
      Kendra Hospital, Kishanganj, Bihar, India.
FAU - Das, Dipmala
AU  - Das D
AD  - Department Microbiology, Mata Gujri Memorial Medical College and Lions Seva 
      Kendra Hospital, Kishanganj, Bihar, India.
FAU - Mondal, Himel
AU  - Mondal H
AD  - Department of Physiology, All India Institute of Medical Sciences, Deoghar, 
      Jharkhand, India.
LA  - eng
PT  - Journal Article
DEP - 20240207
PL  - India
TA  - J Educ Health Promot
JT  - Journal of education and health promotion
JID - 101593794
PMC - PMC10967926
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - communication
OT  - competency-based education
OT  - medical education
COIS- There are no conflicts of interest.
EDAT- 2024/03/28 06:46
MHDA- 2024/03/28 06:47
PMCR- 2024/02/07
CRDT- 2024/03/28 04:08
PHST- 2023/05/07 00:00 [received]
PHST- 2023/07/05 00:00 [accepted]
PHST- 2024/03/28 06:47 [medline]
PHST- 2024/03/28 06:46 [pubmed]
PHST- 2024/03/28 04:08 [entrez]
PHST- 2024/02/07 00:00 [pmc-release]
AID - JEHP-13-22 [pii]
AID - 10.4103/jehp.jehp_625_23 [doi]
PST - epublish
SO  - J Educ Health Promot. 2024 Feb 7;13:22. doi: 10.4103/jehp.jehp_625_23. 
      eCollection 2024.

PMID- 38201398
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240121
IS  - 2075-4418 (Print)
IS  - 2075-4418 (Electronic)
IS  - 2075-4418 (Linking)
VI  - 14
IP  - 1
DP  - 2023 Dec 30
TI  - Validation of a Deep Learning Chest X-ray Interpretation Model: Integrating 
      Large-Scale AI and Large Language Models for Comparative Analysis with ChatGPT.
LID - 10.3390/diagnostics14010090 [doi]
LID - 90
AB  - This study evaluates the diagnostic accuracy and clinical utility of two 
      artificial intelligence (AI) techniques: Kakao Brain Artificial Neural Network 
      for Chest X-ray Reading (KARA-CXR), an assistive technology developed using 
      large-scale AI and large language models (LLMs), and ChatGPT, a well-known LLM. 
      The study was conducted to validate the performance of the two technologies in 
      chest X-ray reading and explore their potential applications in the medical 
      imaging diagnosis domain. The study methodology consisted of randomly selecting 
      2000 chest X-ray images from a single institution's patient database, and two 
      radiologists evaluated the readings provided by KARA-CXR and ChatGPT. The study 
      used five qualitative factors to evaluate the readings generated by each model: 
      accuracy, false findings, location inaccuracies, count inaccuracies, and 
      hallucinations. Statistical analysis showed that KARA-CXR achieved significantly 
      higher diagnostic accuracy compared to ChatGPT. In the 'Acceptable' accuracy 
      category, KARA-CXR was rated at 70.50% and 68.00% by two observers, while ChatGPT 
      achieved 40.50% and 47.00%. Interobserver agreement was moderate for both 
      systems, with KARA at 0.74 and GPT4 at 0.73. For 'False Findings', KARA-CXR 
      scored 68.00% and 68.50%, while ChatGPT scored 37.00% for both observers, with 
      high interobserver agreements of 0.96 for KARA and 0.97 for GPT4. In 'Location 
      Inaccuracy' and 'Hallucinations', KARA-CXR outperformed ChatGPT with significant 
      margins. KARA-CXR demonstrated a non-hallucination rate of 75%, which is 
      significantly higher than ChatGPT's 38%. The interobserver agreement was high for 
      KARA (0.91) and moderate to high for GPT4 (0.85) in the hallucination category. 
      In conclusion, this study demonstrates the potential of AI and large-scale 
      language models in medical imaging and diagnostics. It also shows that in the 
      chest X-ray domain, KARA-CXR has relatively higher accuracy than ChatGPT.
FAU - Lee, Kyu Hong
AU  - Lee KH
AUID- ORCID: 0000-0003-1069-0810
AD  - Department of Radiology, College of Medicine, Inha University, Incheon 22212, 
      Republic of Korea.
FAU - Lee, Ro Woon
AU  - Lee RW
AUID- ORCID: 0000-0002-8678-8059
AD  - Department of Radiology, College of Medicine, Inha University, Incheon 22212, 
      Republic of Korea.
FAU - Kwon, Ye Eun
AU  - Kwon YE
AD  - Department of Radiology, College of Medicine, Inha University, Incheon 22212, 
      Republic of Korea.
LA  - eng
GR  - 2023-22-001/Kakaobrain/
PT  - Journal Article
DEP - 20231230
PL  - Switzerland
TA  - Diagnostics (Basel)
JT  - Diagnostics (Basel, Switzerland)
JID - 101658402
PMC - PMC10795741
OTO - NOTNLM
OT  - ChatGPT
OT  - KARA-CXR
OT  - LLM
OT  - chest X-ray
COIS- Ro Woon Lee is a co-researcher in developing Kakaobrain KARA-CXR and has received 
      research funding from Kakaobrain. The other authors have no potential conflict of 
      interest to disclose.
EDAT- 2024/01/11 07:41
MHDA- 2024/01/11 07:42
PMCR- 2023/12/30
CRDT- 2024/01/11 01:05
PHST- 2023/12/05 00:00 [received]
PHST- 2023/12/28 00:00 [revised]
PHST- 2023/12/29 00:00 [accepted]
PHST- 2024/01/11 07:42 [medline]
PHST- 2024/01/11 07:41 [pubmed]
PHST- 2024/01/11 01:05 [entrez]
PHST- 2023/12/30 00:00 [pmc-release]
AID - diagnostics14010090 [pii]
AID - diagnostics-14-00090 [pii]
AID - 10.3390/diagnostics14010090 [doi]
PST - epublish
SO  - Diagnostics (Basel). 2023 Dec 30;14(1):90. doi: 10.3390/diagnostics14010090.

PMID- 38007922
OWN - NLM
STAT- MEDLINE
DCOM- 20240123
LR  - 20240131
IS  - 1532-2688 (Electronic)
IS  - 1059-1311 (Linking)
VI  - 114
DP  - 2024 Jan
TI  - Assessing the performance of ChatGPT's responses to questions related to 
      epilepsy: A cross-sectional study on natural language processing and medical 
      information retrieval.
PG  - 1-8
LID - S1059-1311(23)00300-X [pii]
LID - 10.1016/j.seizure.2023.11.013 [doi]
AB  - BACKGROUND: Epilepsy is a neurological condition marked by frequent seizures and 
      various cognitive and psychological effects. Reliable information is essential 
      for effective treatment. Natural language processing models like ChatGPT are 
      increasingly used in healthcare for information access and data analysis, making 
      it crucial to assess their accuracy. OBJECTIVE: This study aimed to investigate 
      the accuracy of ChatGPT in providing educational information related to epilepsy. 
      METHODS: We compared the answers from ChatGPT-4 and ChatGPT-3.5 to 57 common 
      epilepsy questions based on the Korean Epilepsy Society's "Epilepsy Patient and 
      Caregiver Guide." Two epileptologists reviewed the responses, with a third 
      serving as an arbiter in cases of disagreement. RESULTS: Out of 57 questions, 40 
      responses from ChatGPT-4 had "sufficient educational value," 16 were "correct but 
      inadequate," and one was "mixed with correct and incorrect" information. No 
      answers were entirely incorrect. GPT-4 generally outperformed GPT-3.5 and was 
      often on par with or better than the official guide. CONCLUSIONS: ChatGPT-4 shows 
      promise as a tool for delivering reliable epilepsy-related information and could 
      help alleviate the educational burden on healthcare professionals. Further 
      research is needed to explore the benefits and limitations of using such models 
      in medical contexts.
CI  - Copyright © 2023. Published by Elsevier Ltd.
FAU - Kim, Hyun-Woo
AU  - Kim HW
AD  - Department of Neurology, Pusan National University Yangsan Hospital, 50612 
      Geumoro 20, Yangsan, South Korea.
FAU - Shin, Dong-Hyeon
AU  - Shin DH
AD  - Department of Neurology, Pusan National University Yangsan Hospital, 50612 
      Geumoro 20, Yangsan, South Korea.
FAU - Kim, Jiyoung
AU  - Kim J
AD  - Department of Neurology, Pusan National University Hospital, Busan, South Korea; 
      Pusan National University School of Medicine, Research Institute for Convergence 
      of Biomedical Science and Technology, Yangsan, South Korea.
FAU - Lee, Gha-Hyun
AU  - Lee GH
AD  - Department of Neurology, Pusan National University Hospital, Busan, South Korea; 
      Pusan National University School of Medicine, Research Institute for Convergence 
      of Biomedical Science and Technology, Yangsan, South Korea.
FAU - Cho, Jae Wook
AU  - Cho JW
AD  - Department of Neurology, Pusan National University Yangsan Hospital, 50612 
      Geumoro 20, Yangsan, South Korea; Pusan National University School of Medicine, 
      Research Institute for Convergence of Biomedical Science and Technology, Yangsan, 
      South Korea. Electronic address: sleepcho@pusan.ac.kr.
LA  - eng
PT  - Journal Article
DEP - 20231123
PL  - England
TA  - Seizure
JT  - Seizure
JID - 9306979
SB  - IM
MH  - Humans
MH  - Cross-Sectional Studies
MH  - *Natural Language Processing
MH  - *Epilepsy
MH  - Information Storage and Retrieval
MH  - Educational Status
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Epilepsy
OT  - Natural language processing
COIS- Declaration of Competing Interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2023/11/27 00:43
MHDA- 2024/01/23 06:43
CRDT- 2023/11/26 18:06
PHST- 2023/09/28 00:00 [received]
PHST- 2023/11/18 00:00 [revised]
PHST- 2023/11/22 00:00 [accepted]
PHST- 2024/01/23 06:43 [medline]
PHST- 2023/11/27 00:43 [pubmed]
PHST- 2023/11/26 18:06 [entrez]
AID - S1059-1311(23)00300-X [pii]
AID - 10.1016/j.seizure.2023.11.013 [doi]
PST - ppublish
SO  - Seizure. 2024 Jan;114:1-8. doi: 10.1016/j.seizure.2023.11.013. Epub 2023 Nov 23.

PMID- 38180782
OWN - NLM
STAT- MEDLINE
DCOM- 20240108
LR  - 20240122
IS  - 2369-3762 (Electronic)
IS  - 2369-3762 (Linking)
VI  - 10
DP  - 2024 Jan 5
TI  - Pure Wisdom or Potemkin Villages? A Comparison of ChatGPT 3.5 and ChatGPT 4 on 
      USMLE Step 3 Style Questions: Quantitative Analysis.
PG  - e51148
LID - 10.2196/51148 [doi]
LID - e51148
AB  - BACKGROUND: The United States Medical Licensing Examination (USMLE) has been 
      critical in medical education since 1992, testing various aspects of a medical 
      student's knowledge and skills through different steps, based on their training 
      level. Artificial intelligence (AI) tools, including chatbots like ChatGPT, are 
      emerging technologies with potential applications in medicine. However, 
      comprehensive studies analyzing ChatGPT's performance on USMLE Step 3 in 
      large-scale scenarios and comparing different versions of ChatGPT are limited. 
      OBJECTIVE: This paper aimed to analyze ChatGPT's performance on USMLE Step 3 
      practice test questions to better elucidate the strengths and weaknesses of AI 
      use in medical education and deduce evidence-based strategies to counteract AI 
      cheating. METHODS: A total of 2069 USMLE Step 3 practice questions were extracted 
      from the AMBOSS study platform. After including 229 image-based questions, a 
      total of 1840 text-based questions were further categorized and entered into 
      ChatGPT 3.5, while a subset of 229 questions were entered into ChatGPT 4. 
      Responses were recorded, and the accuracy of ChatGPT answers as well as its 
      performance in different test question categories and for different difficulty 
      levels were compared between both versions. RESULTS: Overall, ChatGPT 4 
      demonstrated a statistically significant superior performance compared to ChatGPT 
      3.5, achieving an accuracy of 84.7% (194/229) and 56.9% (1047/1840), 
      respectively. A noteworthy correlation was observed between the length of test 
      questions and the performance of ChatGPT 3.5 (ρ=-0.069; P=.003), which was absent 
      in ChatGPT 4 (P=.87). Additionally, the difficulty of test questions, as 
      categorized by AMBOSS hammer ratings, showed a statistically significant 
      correlation with performance for both ChatGPT versions, with ρ=-0.289 for ChatGPT 
      3.5 and ρ=-0.344 for ChatGPT 4. ChatGPT 4 surpassed ChatGPT 3.5 in all levels of 
      test question difficulty, except for the 2 highest difficulty tiers (4 and 5 
      hammers), where statistical significance was not reached. CONCLUSIONS: In this 
      study, ChatGPT 4 demonstrated remarkable proficiency in taking the USMLE Step 3, 
      with an accuracy rate of 84.7% (194/229), outshining ChatGPT 3.5 with an accuracy 
      rate of 56.9% (1047/1840). Although ChatGPT 4 performed exceptionally, it 
      encountered difficulties in questions requiring the application of theoretical 
      concepts, particularly in cardiology and neurology. These insights are pivotal 
      for the development of examination strategies that are resilient to AI and 
      underline the promising role of AI in the realm of medical education and 
      diagnostics.
CI  - ©Leonard Knoedler, Michael Alfertshofer, Samuel Knoedler, Cosima C Hoch, Paul F 
      Funk, Sebastian Cotofana, Bhagvat Maheta, Konstantin Frank, Vanessa Brébant, 
      Lukas Prantl, Philipp Lamby. Originally published in JMIR Medical Education 
      (https://mededu.jmir.org), 05.01.2024.
FAU - Knoedler, Leonard
AU  - Knoedler L
AUID- ORCID: 0000-0002-8949-3168
AD  - Department of Plastic, Hand and Reconstructive Surgery, University Hospital 
      Regensburg, Regensburg, Germany.
FAU - Alfertshofer, Michael
AU  - Alfertshofer M
AUID- ORCID: 0000-0002-4892-2376
AD  - Division of Hand, Plastic and Aesthetic Surgery, Ludwig-Maximilians University 
      Munich, Munich, Germany.
FAU - Knoedler, Samuel
AU  - Knoedler S
AUID- ORCID: 0000-0001-5798-8003
AD  - Department of Plastic, Hand and Reconstructive Surgery, University Hospital 
      Regensburg, Regensburg, Germany.
AD  - Division of Plastic Surgery, Brigham and Women's Hospital, Harvard Medical 
      School, Boston, MA, United States.
FAU - Hoch, Cosima C
AU  - Hoch CC
AUID- ORCID: 0000-0002-3875-7389
AD  - Department of Otolaryngology, Head and Neck Surgery, School of Medicine, 
      Technical University of Munich, Munich, Germany.
FAU - Funk, Paul F
AU  - Funk PF
AUID- ORCID: 0009-0000-4316-4249
AD  - Department of Otolaryngology, Head and Neck Surgery, University Hospital Jena, 
      Friedrich Schiller University Jena, Jena, Germany.
FAU - Cotofana, Sebastian
AU  - Cotofana S
AUID- ORCID: 0000-0001-7210-6566
AD  - Department of Dermatology, Erasmus Hospital, Rotterdam, Netherlands.
AD  - Centre for Cutaneous Research, Blizard Institute, Queen Mary University of 
      London, London, United Kingdom.
FAU - Maheta, Bhagvat
AU  - Maheta B
AUID- ORCID: 0000-0002-5318-3088
AD  - College of Medicine, California Northstate University, Elk Grove, CA, United 
      States.
FAU - Frank, Konstantin
AU  - Frank K
AUID- ORCID: 0000-0001-6994-8877
AD  - Ocean Clinic, Marbella, Spain.
FAU - Brébant, Vanessa
AU  - Brébant V
AUID- ORCID: 0000-0003-3144-4459
AD  - Department of Plastic, Hand and Reconstructive Surgery, University Hospital 
      Regensburg, Regensburg, Germany.
FAU - Prantl, Lukas
AU  - Prantl L
AUID- ORCID: 0000-0003-2454-2499
AD  - Department of Plastic, Hand and Reconstructive Surgery, University Hospital 
      Regensburg, Regensburg, Germany.
FAU - Lamby, Philipp
AU  - Lamby P
AUID- ORCID: 0000-0003-0815-5712
AD  - Department of Plastic, Hand and Reconstructive Surgery, University Hospital 
      Regensburg, Regensburg, Germany.
LA  - eng
PT  - Journal Article
DEP - 20240105
PL  - Canada
TA  - JMIR Med Educ
JT  - JMIR medical education
JID - 101684518
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Medicine
MH  - *Cardiology
MH  - *Education, Medical
MH  - Educational Status
PMC - PMC10799278
OTO - NOTNLM
OT  - ChatGPT
OT  - OpenAI
OT  - USMLE
OT  - USMLE Step 1
OT  - United States Medical Licensing Examination
OT  - artificial intelligence
OT  - clinical decision-making
OT  - medical education
COIS- Conflicts of Interest: None declared.
EDAT- 2024/01/05 12:42
MHDA- 2024/01/08 06:43
PMCR- 2024/01/05
CRDT- 2024/01/05 11:52
PHST- 2023/07/22 00:00 [received]
PHST- 2023/10/20 00:00 [accepted]
PHST- 2023/09/30 00:00 [revised]
PHST- 2024/01/08 06:43 [medline]
PHST- 2024/01/05 12:42 [pubmed]
PHST- 2024/01/05 11:52 [entrez]
PHST- 2024/01/05 00:00 [pmc-release]
AID - v10i1e51148 [pii]
AID - 10.2196/51148 [doi]
PST - epublish
SO  - JMIR Med Educ. 2024 Jan 5;10:e51148. doi: 10.2196/51148.

PMID- 37903939
OWN - NLM
STAT- MEDLINE
DCOM- 20240321
LR  - 20240321
IS  - 1432-5241 (Electronic)
IS  - 0364-216X (Linking)
VI  - 48
IP  - 4
DP  - 2024 Feb
TI  - Consulting the Digital Doctor: Google Versus ChatGPT as Sources of Information on 
      Breast Implant-Associated Anaplastic Large Cell Lymphoma and Breast Implant 
      Illness.
PG  - 590-607
LID - 10.1007/s00266-023-03713-4 [doi]
AB  - BACKGROUND: Breast implant-associated anaplastic large cell lymphoma (BIA-ALCL) 
      is a rare complication associated with the use of breast implants. Breast implant 
      illness (BII) is another potentially concerning issue related to breast implants. 
      This study aims to assess the quality of ChatGPT as a potential source of patient 
      education by comparing the answers to frequently asked questions on BIA-ALCL and 
      BII provided by ChatGPT and Google. METHODS: The Google and ChatGPT answers to 
      the 10 most frequently asked questions on the search terms "breast implant 
      associated anaplastic large cell lymphoma" and "breast implant illness" were 
      recorded. Five blinded breast plastic surgeons were then asked to grade the 
      quality of the answers according to the Global Quality Score (GQS). A Wilcoxon 
      paired t-test was performed to evaluate the difference in GQS ratings for Google 
      and ChatGPT answers. The sources provided by Google and ChatGPT were also 
      categorized and assessed. RESULTS: In a comparison of answers provided by Google 
      and ChatGPT on BIA-ALCL and BII, ChatGPT significantly outperformed Google. For 
      BIA-ALCL, Google's average score was 2.72 ± 1.44, whereas ChatGPT scored an 
      average of 4.18 ± 1.04 (p &lt; 0.01). For BII, Google's average score was 
      2.66 ± 1.24, while ChatGPT scored an average of 4.28 ± 0.97 (p &lt; 0.01). The 
      superiority of ChatGPT's responses was attributed to their comprehensive nature 
      and recognition of existing knowledge gaps. However, some of ChatGPT's answers 
      had inaccessible sources. CONCLUSION: ChatGPT outperforms Google in providing 
      high-quality answers to commonly asked questions on BIA-ALCL and BII, 
      highlighting the potential of AI technologies in patient education. LEVEL OF 
      EVIDENCE: Level III, comparative study LEVEL OF EVIDENCE III: This journal 
      requires that authors assign a level of evidence to each article. For a full 
      description of these Evidence-Based Medicine ratings, please refer to the Table 
      of Contents or the online Instructions to Authors www.springer.com/00266 .
CI  - © 2023. Springer Science+Business Media, LLC, part of Springer Nature and 
      International Society of Aesthetic Plastic Surgery.
FAU - Liu, Hilary Y
AU  - Liu HY
AD  - Department of Plastic Surgery, University of Pittsburgh Medical Center, 1350 
      Locust Street, G103, Pittsburgh, PA, 15219, USA.
FAU - Alessandri Bonetti, Mario
AU  - Alessandri Bonetti M
AD  - Department of Plastic Surgery, University of Pittsburgh Medical Center, 1350 
      Locust Street, G103, Pittsburgh, PA, 15219, USA.
FAU - De Lorenzi, Francesca
AU  - De Lorenzi F
AD  - Department of Plastic Surgery, IRCCS European Institute of Oncology, Via Giuseppe 
      Ripamonti 345, 20122, Milan, Italy.
FAU - Gimbel, Michael L
AU  - Gimbel ML
AD  - Department of Plastic Surgery, University of Pittsburgh Medical Center, 1350 
      Locust Street, G103, Pittsburgh, PA, 15219, USA.
FAU - Nguyen, Vu T
AU  - Nguyen VT
AD  - Department of Plastic Surgery, University of Pittsburgh Medical Center, 1350 
      Locust Street, G103, Pittsburgh, PA, 15219, USA.
FAU - Egro, Francesco M
AU  - Egro FM
AUID- ORCID: 0000-0003-1536-7713
AD  - Department of Plastic Surgery, University of Pittsburgh Medical Center, 1350 
      Locust Street, G103, Pittsburgh, PA, 15219, USA. francescoegro@gmail.com.
LA  - eng
PT  - Journal Article
DEP - 20231030
PL  - United States
TA  - Aesthetic Plast Surg
JT  - Aesthetic plastic surgery
JID - 7701756
SB  - IM
MH  - Humans
MH  - Female
MH  - *Breast Implants/adverse effects
MH  - *Lymphoma, Large-Cell, Anaplastic/epidemiology/etiology/pathology
MH  - Search Engine
MH  - *Breast Implantation/adverse effects
MH  - *Surgeons
MH  - Information Sources
MH  - *Breast Neoplasms/epidemiology/etiology/surgery
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Breast
OT  - Breast implant associated anaplastic large cell lymphoma
OT  - ChatGPT
OT  - Google
OT  - Implant
OT  - Implant illness
OT  - Patient education
EDAT- 2023/10/31 06:42
MHDA- 2024/03/21 12:48
CRDT- 2023/10/31 00:34
PHST- 2023/08/19 00:00 [received]
PHST- 2023/10/10 00:00 [accepted]
PHST- 2024/03/21 12:48 [medline]
PHST- 2023/10/31 06:42 [pubmed]
PHST- 2023/10/31 00:34 [entrez]
AID - 10.1007/s00266-023-03713-4 [pii]
AID - 10.1007/s00266-023-03713-4 [doi]
PST - ppublish
SO  - Aesthetic Plast Surg. 2024 Feb;48(4):590-607. doi: 10.1007/s00266-023-03713-4. 
      Epub 2023 Oct 30.

PMID- 37626010
OWN - NLM
STAT- MEDLINE
DCOM- 20231229
LR  - 20240305
IS  - 1365-2125 (Electronic)
IS  - 0306-5251 (Linking)
VI  - 90
IP  - 1
DP  - 2024 Jan
TI  - Evaluating the performance of ChatGPT in clinical pharmacy: A comparative study 
      of ChatGPT and clinical pharmacists.
PG  - 232-238
LID - 10.1111/bcp.15896 [doi]
AB  - AIMS: To evaluate the performance of chat generative pretrained transformer 
      (ChatGPT) in key domains of clinical pharmacy practice, including prescription 
      review, patient medication education, adverse drug reaction (ADR) recognition, 
      ADR causality assessment and drug counselling. METHODS: Questions and clinical 
      pharmacist's answers were collected from real clinical cases and clinical 
      pharmacist competency assessment. ChatGPT's responses were generated by inputting 
      the same question into the 'New Chat' box of ChatGPT Mar 23 Version. Five 
      licensed clinical pharmacists independently rated these answers on a scale of 0 
      (Completely incorrect) to 10 (Completely correct). The mean scores of ChatGPT and 
      clinical pharmacists were compared using a paired 2-tailed Student's t-test. The 
      text content of the answers was also descriptively summarized together. RESULTS: 
      The quantitative results indicated that ChatGPT was excellent in drug counselling 
      (ChatGPT: 8.77 vs. clinical pharmacist: 9.50, P = .0791) and weak in prescription 
      review (5.23 vs. 9.90, P = .0089), patient medication education (6.20 vs. 9.07, 
      P = .0032), ADR recognition (5.07 vs. 9.70, P = .0483) and ADR causality 
      assessment (4.03 vs. 9.73, P = .023). The capabilities and limitations of ChatGPT 
      in clinical pharmacy practice were summarized based on the completeness and 
      accuracy of the answers. ChatGPT revealed robust retrieval, information 
      integration and dialogue capabilities. It lacked medicine-specific datasets as 
      well as the ability for handling advanced reasoning and complex instructions. 
      CONCLUSIONS: While ChatGPT holds promise in clinical pharmacy practice as a 
      supplementary tool, the ability of ChatGPT to handle complex problems needs 
      further improvement and refinement.
CI  - © 2023 British Pharmacological Society.
FAU - Huang, Xiaoru
AU  - Huang X
AD  - Department of Pharmacy, Peking University Third Hospital, Beijing, China.
AD  - Department of Pharmaceutical Management and Clinical Pharmacy, College of 
      Pharmacy, Peking University, Beijing, China.
FAU - Estau, Dannya
AU  - Estau D
AD  - Department of Pharmacy, Peking University Third Hospital, Beijing, China.
AD  - Department of Pharmaceutical Management and Clinical Pharmacy, College of 
      Pharmacy, Peking University, Beijing, China.
FAU - Liu, Xuening
AU  - Liu X
AD  - Department of Pharmacy, Peking University Third Hospital, Beijing, China.
AD  - Department of Pharmaceutical Management and Clinical Pharmacy, College of 
      Pharmacy, Peking University, Beijing, China.
FAU - Yu, Yang
AU  - Yu Y
AD  - Department of Pharmacy, Peking University Third Hospital, Beijing, China.
AD  - Department of Pharmaceutical Management and Clinical Pharmacy, College of 
      Pharmacy, Peking University, Beijing, China.
FAU - Qin, Jiguang
AU  - Qin J
AD  - Department of Pharmacy, Peking University Third Hospital, Beijing, China.
AD  - Department of Pharmaceutical Management and Clinical Pharmacy, College of 
      Pharmacy, Peking University, Beijing, China.
FAU - Li, Zijian
AU  - Li Z
AUID- ORCID: 0000-0002-8057-2257
AD  - Department of Pharmacy, Peking University Third Hospital, Beijing, China.
AD  - Department of Pharmaceutical Management and Clinical Pharmacy, College of 
      Pharmacy, Peking University, Beijing, China.
AD  - Department of Cardiology and Institute of Vascular Medicine, Peking University 
      Third Hospital, Beijing Key Laboratory of Cardiovascular Receptors Research, Key 
      Laboratory of Cardiovascular Molecular Biology and Regulatory Peptides, Ministry 
      of Health, State Key Laboratory of Vascular Homeostasis and Remodeling, Peking 
      University, Beijing, China.
LA  - eng
GR  - 7222218/Beijing Municipal Natural Science Foundation/
GR  - 2021RU003/Medical Research Management/Basic and Clinical Research Unit of 
      Metabolic Cardiovascular Diseases, Chinese Academy of Medical Sciences/
GR  - 2022YFC3602400/National Key Research and Development Program of China/
GR  - 81820108031/National Natural Science Foundation of China/
GR  - 91939301/National Natural Science Foundation of China/
GR  - U21A20336/National Natural Science Foundation of China/
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20230913
PL  - England
TA  - Br J Clin Pharmacol
JT  - British journal of clinical pharmacology
JID - 7503323
SB  - IM
MH  - Humans
MH  - Pharmacists
MH  - Clinical Competence
MH  - *Drug-Related Side Effects and Adverse Reactions/prevention &amp; control
MH  - *Pharmacy Service, Hospital
MH  - *Pharmacy
OTO - NOTNLM
OT  - ChatGPT
OT  - clinical pharmacy
EDAT- 2023/08/26 05:41
MHDA- 2023/12/29 06:43
CRDT- 2023/08/25 22:02
PHST- 2023/08/01 00:00 [revised]
PHST- 2023/04/26 00:00 [received]
PHST- 2023/08/14 00:00 [accepted]
PHST- 2023/12/29 06:43 [medline]
PHST- 2023/08/26 05:41 [pubmed]
PHST- 2023/08/25 22:02 [entrez]
AID - 10.1111/bcp.15896 [doi]
PST - ppublish
SO  - Br J Clin Pharmacol. 2024 Jan;90(1):232-238. doi: 10.1111/bcp.15896. Epub 2023 
      Sep 13.

PMID- 38420484
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240301
IS  - 2405-8440 (Print)
IS  - 2405-8440 (Electronic)
IS  - 2405-8440 (Linking)
VI  - 10
IP  - 4
DP  - 2024 Feb 29
TI  - Understanding learners' perceptions of ChatGPT: A thematic analysis of peer 
      interviews among undergraduates and postgraduates in China.
PG  - e26239
LID - 10.1016/j.heliyon.2024.e26239 [doi]
LID - e26239
AB  - ChatGPT, an artificial intelligence (AI)-driven language model engineered by 
      OpenAI, has experienced a substantial upsurge in adoption within higher education 
      due to its versatile applications and sophisticated capabilities. Although 
      prevailing research on ChatGPT has predominantly concentrated on its 
      technological aspects and pedagogical ramifications, a comprehensive 
      understanding of students' perceptions and experiences regarding ChatGPT remains 
      elusive. To address this gap, this study employed a peer interview methodology, 
      conducting a thematic analysis of 106 first-year undergraduates and 81 first-year 
      postgraduate students' perceptions from diverse disciplines at a comprehensive 
      university in East China. The data analysis revealed that among the four factors 
      examined-grade, age, gender, and major-grade emerged as the most influential 
      determinant, followed by age and major. Postgraduate students demonstrated 
      heightened awareness of the potential limitations of ChatGPT in addressing 
      academic challenges and exhibited greater concern for security issues associated 
      with its application. This research offers essential insights into students' 
      perceptions and experiences with ChatGPT, emphasizing the importance of 
      recognizing potential limitations and ethical concerns associated with ChatGPT 
      usage. Additionally, the findings highlight ethical concerns, as students noted 
      the importance of responsible data handling and academic integrity in ChatGPT 
      usage, underscoring the need for ethical guidance in AI utilization. Moreover, 
      further research is essential to optimize AI use in education, aiming to improve 
      learning outcomes effectively.
CI  - © 2024 The Authors.
FAU - Xu, Xiaoshu
AU  - Xu X
AD  - School of Foreign Studies, Wenzhou University, Wenzhou City, Zhejiang Province, 
      China.
AD  - Stamford International University Thailand.
AD  - Macao Polytechnique University.
FAU - Su, Yujie
AU  - Su Y
AD  - School of Foreign Studies, Wenzhou University, Wenzhou City, Zhejiang Province, 
      China.
FAU - Zhang, Yunfeng
AU  - Zhang Y
AD  - The Faculty of Languages and Translation, R. de Luís Gonzaga Gomes, Macao 
      Polytechnic University.
FAU - Wu, Yunyang
AU  - Wu Y
AD  - School of Foreign Studies, Wenzhou University, Wenzhou City, Zhejiang Province, 
      China.
FAU - Xu, Xinyu
AU  - Xu X
AD  - School of Foreign Studies, Wenzhou University, Wenzhou City, Zhejiang Province, 
      China.
LA  - eng
PT  - Journal Article
DEP - 20240215
PL  - England
TA  - Heliyon
JT  - Heliyon
JID - 101672560
PMC - PMC10900412
OTO - NOTNLM
OT  - ChatGPT
OT  - Chatbot
OT  - Higher education
OT  - Learners' perceptions
OT  - Peer interview
OT  - thematic analysis
COIS- The authors declare that they have no known competing financial interests or 
      personal relationships that could have appeared to influence the work reported in 
      this paper.
EDAT- 2024/02/29 06:42
MHDA- 2024/02/29 06:43
PMCR- 2024/02/15
CRDT- 2024/02/29 04:13
PHST- 2023/07/28 00:00 [received]
PHST- 2024/02/02 00:00 [revised]
PHST- 2024/02/08 00:00 [accepted]
PHST- 2024/02/29 06:43 [medline]
PHST- 2024/02/29 06:42 [pubmed]
PHST- 2024/02/29 04:13 [entrez]
PHST- 2024/02/15 00:00 [pmc-release]
AID - S2405-8440(24)02270-9 [pii]
AID - e26239 [pii]
AID - 10.1016/j.heliyon.2024.e26239 [doi]
PST - epublish
SO  - Heliyon. 2024 Feb 15;10(4):e26239. doi: 10.1016/j.heliyon.2024.e26239. 
      eCollection 2024 Feb 29.

PMID- 38440047
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240306
IS  - 2376-5992 (Electronic)
IS  - 2376-5992 (Linking)
VI  - 10
DP  - 2024
TI  - An integrative decision-making framework to guide policies on regulating ChatGPT 
      usage.
PG  - e1845
LID - 10.7717/peerj-cs.1845 [doi]
LID - e1845
AB  - Generative artificial intelligence has created a moment in history where human 
      beings have begin to closely interact with artificial intelligence (AI) tools, 
      putting policymakers in a position to restrict or legislate such tools. One 
      particular example of such a tool is ChatGPT which is the first and world's most 
      popular multipurpose generative AI tool. This study aims to put forward a 
      policy-making framework of generative artificial intelligence based on the risk, 
      reward, and resilience framework. A systematic search was conducted, by using 
      carefully chosen keywords, excluding non-English content, conference articles, 
      book chapters, and editorials. Published research were filtered based on their 
      relevance to ChatGPT ethics, yielding a total of 41 articles. Key elements 
      surrounding ChatGPT concerns and motivations were systematically deduced and 
      classified under the risk, reward, and resilience categories to serve as 
      ingredients for the proposed decision-making framework. The decision-making 
      process and rules were developed as a primer to help policymakers navigate 
      decision-making conundrums. Then, the framework was practically tailored towards 
      some of the concerns surrounding ChatGPT in the context of higher education. In 
      the case of the interconnection between risk and reward, the findings show that 
      providing students with access to ChatGPT presents an opportunity for increased 
      efficiency in tasks such as text summarization and workload reduction. However, 
      this exposes them to risks such as plagiarism and cheating. Similarly, pursuing 
      certain opportunities such as accessing vast amounts of information, can lead to 
      rewards, but it also introduces risks like misinformation and copyright issues. 
      Likewise, focusing on specific capabilities of ChatGPT, such as developing tools 
      to detect plagiarism and misinformation, may enhance resilience in some areas 
      (e.g., academic integrity). However, it may also create vulnerabilities in other 
      domains, such as the digital divide, educational equity, and job losses. 
      Furthermore, the finding indicates second-order effects of legislation regarding 
      ChatGPT which have implications both positively and negatively. One potential 
      effect is a decrease in rewards due to the limitations imposed by the 
      legislation, which may hinder individuals from fully capitalizing on the 
      opportunities provided by ChatGPT. Hence, the risk, reward, and resilience 
      framework provides a comprehensive and flexible decision-making model that allows 
      policymakers and in this use case, higher education institutions to navigate the 
      complexities and trade-offs associated with ChatGPT, which have theoretical and 
      practical implications for the future.
CI  - © 2024 Bukar et al.
FAU - Bukar, Umar Ali
AU  - Bukar UA
AD  - Centre for Intelligent Cloud Computing (CICC), Faculty of Information Science &amp; 
      Technology, Multimedia University, Melaka, Malaysia.
FAU - Sayeed, Md Shohel
AU  - Sayeed MS
AUID- ORCID: 0000-0002-0052-4870
AD  - Centre for Intelligent Cloud Computing (CICC), Faculty of Information Science &amp; 
      Technology, Multimedia University, Melaka, Malaysia.
FAU - Razak, Siti Fatimah Abdul
AU  - Razak SFA
AUID- ORCID: 0000-0002-6108-3183
AD  - Centre for Intelligent Cloud Computing (CICC), Faculty of Information Science &amp; 
      Technology, Multimedia University, Melaka, Malaysia.
FAU - Yogarayan, Sumendra
AU  - Yogarayan S
AD  - Centre for Intelligent Cloud Computing (CICC), Faculty of Information Science &amp; 
      Technology, Multimedia University, Melaka, Malaysia.
FAU - Amodu, Oluwatosin Ahmed
AU  - Amodu OA
AD  - Information and Communication Engineering Department, Elizade University, 
      Ilara-Mokin, Ondo State, Nigeria.
LA  - eng
PT  - Journal Article
DEP - 20240229
PL  - United States
TA  - PeerJ Comput Sci
JT  - PeerJ. Computer science
JID - 101660598
PMC - PMC10911759
OTO - NOTNLM
OT  - ChatGPT
OT  - Decision making
OT  - Ethics
OT  - Generative AI
OT  - Higher education
OT  - Policy making
OT  - Resilience
OT  - Reward
OT  - Risk
OT  - Systematic review
COIS- The authors declare that they have no competing interests.
EDAT- 2024/03/05 06:46
MHDA- 2024/03/05 06:47
PMCR- 2024/02/29
CRDT- 2024/03/05 03:48
PHST- 2023/08/09 00:00 [received]
PHST- 2024/01/09 00:00 [accepted]
PHST- 2024/03/05 06:47 [medline]
PHST- 2024/03/05 06:46 [pubmed]
PHST- 2024/03/05 03:48 [entrez]
PHST- 2024/02/29 00:00 [pmc-release]
AID - cs-1845 [pii]
AID - 10.7717/peerj-cs.1845 [doi]
PST - epublish
SO  - PeerJ Comput Sci. 2024 Feb 29;10:e1845. doi: 10.7717/peerj-cs.1845. eCollection 
      2024.

PMID- 38163001
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240103
IS  - 2772-5294 (Electronic)
IS  - 2772-5294 (Linking)
VI  - 4
DP  - 2024
TI  - Probing artificial intelligence in neurosurgical training: ChatGPT takes a 
      neurosurgical residents written exam.
PG  - 102715
LID - 10.1016/j.bas.2023.102715 [doi]
LID - 102715
AB  - INTRODUCTION: Artificial Intelligence tools are being introduced in almost every 
      field of human life, including medical sciences and medical education, among 
      scepticism and enthusiasm. RESEARCH QUESTION: to assess how a generative language 
      tool (Generative Pretrained Transformer 3.5, ChatGPT) performs at both generating 
      questions and answering a neurosurgical residents' written exam. Namely, to 
      assess how ChatGPT generates questions, how it answers human-generated questions, 
      how residents answer AI-generated questions and how AI answers its self-generated 
      question. MATERIALS AND METHODS: 50 questions were included in the written exam, 
      46 questions were generated by humans (senior staff members) and 4 were generated 
      by ChatGPT. 11 participants took the exam (ChatGPT and 10 residents). Questions 
      were both open-ended and multiple-choice.8 questions were not submitted to 
      ChatGPT since they contained images or schematic drawings to interpret. RESULTS: 
      formulating requests to ChatGPT required an iterative process to precise both 
      questions and answers. Chat GPT scored among the lowest ranks (9/11) among all 
      the participants). There was no difference in response rate for residents' 
      between human-generated vs AI-generated questions that could have been attributed 
      to less clarity of the question. ChatGPT answered correctly to all its 
      self-generated questions. DISCUSSION AND CONCLUSIONS: AI is a promising and 
      powerful tool for medical education and for specific medical purposes, which need 
      to be further determined. To request AI to generate logical and sound questions, 
      that request must be formulated as precise as possible, framing the content, the 
      type of question and its correct answers.
CI  - © 2023 The Authors.
FAU - Bartoli, A
AU  - Bartoli A
AD  - Department of Clinical Neurosciences, Division of Neurosurgery, Geneva University 
      Medical Center, Geneva, Switzerland.
AD  - Switzerland &amp; Faculty of Medicine, University of Geneva, Switzerland.
FAU - May, A T
AU  - May AT
AD  - Department of Clinical Neurosciences, Division of Neurosurgery, Geneva University 
      Medical Center, Geneva, Switzerland.
AD  - Switzerland &amp; Faculty of Medicine, University of Geneva, Switzerland.
FAU - Al-Awadhi, A
AU  - Al-Awadhi A
AD  - Department of Clinical Neurosciences, Division of Neurosurgery, Geneva University 
      Medical Center, Geneva, Switzerland.
AD  - Switzerland &amp; Faculty of Medicine, University of Geneva, Switzerland.
FAU - Schaller, K
AU  - Schaller K
AD  - Department of Clinical Neurosciences, Division of Neurosurgery, Geneva University 
      Medical Center, Geneva, Switzerland.
AD  - Switzerland &amp; Faculty of Medicine, University of Geneva, Switzerland.
LA  - eng
PT  - Journal Article
DEP - 20231129
PL  - Netherlands
TA  - Brain Spine
JT  - Brain &amp; spine
JID - 9918470888906676
PMC - PMC10753430
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Neurosurgical education
OT  - Residents
OT  - Written exam
COIS- None.
EDAT- 2024/01/02 11:42
MHDA- 2024/01/02 11:43
PMCR- 2023/11/29
CRDT- 2024/01/01 04:33
PHST- 2023/08/02 00:00 [received]
PHST- 2023/11/12 00:00 [revised]
PHST- 2023/11/17 00:00 [accepted]
PHST- 2024/01/02 11:43 [medline]
PHST- 2024/01/02 11:42 [pubmed]
PHST- 2024/01/01 04:33 [entrez]
PHST- 2023/11/29 00:00 [pmc-release]
AID - S2772-5294(23)01003-2 [pii]
AID - 102715 [pii]
AID - 10.1016/j.bas.2023.102715 [doi]
PST - epublish
SO  - Brain Spine. 2023 Nov 29;4:102715. doi: 10.1016/j.bas.2023.102715. eCollection 
      2024.

PMID- 38263214
OWN - NLM
STAT- MEDLINE
DCOM- 20240125
LR  - 20240201
IS  - 2045-2322 (Electronic)
IS  - 2045-2322 (Linking)
VI  - 14
IP  - 1
DP  - 2024 Jan 23
TI  - A multinational study on the factors influencing university students' attitudes 
      and usage of ChatGPT.
PG  - 1983
LID - 10.1038/s41598-024-52549-8 [doi]
LID - 1983
AB  - Artificial intelligence models, like ChatGPT, have the potential to revolutionize 
      higher education when implemented properly. This study aimed to investigate the 
      factors influencing university students' attitudes and usage of ChatGPT in Arab 
      countries. The survey instrument "TAME-ChatGPT" was administered to 2240 
      participants from Iraq, Kuwait, Egypt, Lebanon, and Jordan. Of those, 46.8% heard 
      of ChatGPT, and 52.6% used it before the study. The results indicated that a 
      positive attitude and usage of ChatGPT were determined by factors like ease of 
      use, positive attitude towards technology, social influence, perceived 
      usefulness, behavioral/cognitive influences, low perceived risks, and low 
      anxiety. Confirmatory factor analysis indicated the adequacy of the 
      "TAME-ChatGPT" constructs. Multivariate analysis demonstrated that the attitude 
      towards ChatGPT usage was significantly influenced by country of residence, age, 
      university type, and recent academic performance. This study validated 
      "TAME-ChatGPT" as a useful tool for assessing ChatGPT adoption among university 
      students. The successful integration of ChatGPT in higher education relies on the 
      perceived ease of use, perceived usefulness, positive attitude towards 
      technology, social influence, behavioral/cognitive elements, low anxiety, and 
      minimal perceived risks. Policies for ChatGPT adoption in higher education should 
      be tailored to individual contexts, considering the variations in student 
      attitudes observed in this study.
CI  - © 2024. The Author(s).
FAU - Abdaljaleel, Maram
AU  - Abdaljaleel M
AD  - Department of Pathology, Microbiology and Forensic Medicine, School of Medicine, 
      The University of Jordan, Amman, 11942, Jordan.
AD  - Department of Clinical Laboratories and Forensic Medicine, Jordan University 
      Hospital, Amman, 11942, Jordan.
FAU - Barakat, Muna
AU  - Barakat M
AD  - Department of Clinical Pharmacy and Therapeutics, Faculty of Pharmacy, Applied 
      Science Private University, Amman, 11931, Jordan.
FAU - Alsanafi, Mariam
AU  - Alsanafi M
AD  - Department of Pharmacy Practice, Faculty of Pharmacy, Kuwait University, Kuwait 
      City, Kuwait.
AD  - Department of Pharmaceutical Sciences, Public Authority for Applied Education and 
      Training, College of Health Sciences, Safat, Kuwait.
FAU - Salim, Nesreen A
AU  - Salim NA
AD  - Prosthodontic Department, School of Dentistry, The University of Jordan, Amman, 
      11942, Jordan.
AD  - Prosthodontic Department, Jordan University Hospital, Amman, 11942, Jordan.
FAU - Abazid, Husam
AU  - Abazid H
AD  - Department of Clinical Pharmacy and Therapeutics, Faculty of Pharmacy, Applied 
      Science Private University, Amman, 11931, Jordan.
FAU - Malaeb, Diana
AU  - Malaeb D
AD  - College of Pharmacy, Gulf Medical University, P.O. Box 4184, Ajman, United Arab 
      Emirates.
FAU - Mohammed, Ali Haider
AU  - Mohammed AH
AD  - School of Pharmacy, Monash University Malaysia, Jalan Lagoon Selatan, 47500, 
      Bandar Sunway, Selangor Darul Ehsan, Malaysia.
FAU - Hassan, Bassam Abdul Rasool
AU  - Hassan BAR
AD  - Department of Pharmacy, Al Rafidain University College, Baghdad, 10001, Iraq.
FAU - Wayyes, Abdulrasool M
AU  - Wayyes AM
AD  - Department of Pharmacy, Al Rafidain University College, Baghdad, 10001, Iraq.
FAU - Farhan, Sinan Subhi
AU  - Farhan SS
AD  - Department of Anesthesia, Al Rafidain University College, Baghdad, 10001, Iraq.
FAU - Khatib, Sami El
AU  - Khatib SE
AD  - Department of Biomedical Sciences, School of Arts and Sciences, Lebanese 
      International University, Bekaa, Lebanon.
AD  - Center for Applied Mathematics and Bioinformatics (CAMB), Gulf University for 
      Science and Technology (GUST), 32093, Hawally, Kuwait.
FAU - Rahal, Mohamad
AU  - Rahal M
AD  - School of Pharmacy, Lebanese International University, Beirut, 961, Lebanon.
FAU - Sahban, Ali
AU  - Sahban A
AD  - School of Dentistry, The University of Jordan, Amman, 11942, Jordan.
FAU - Abdelaziz, Doaa H
AU  - Abdelaziz DH
AD  - Pharmacy Practice and Clinical Pharmacy Department, Faculty of Pharmacy, Future 
      University in Egypt, Cairo, 11835, Egypt.
AD  - Department of Clinical Pharmacy, The National Hepatology and Tropical Medicine 
      Research Institute, Cairo, 11835, Egypt.
FAU - Mansour, Noha O
AU  - Mansour NO
AD  - Clinical Pharmacy and Pharmacy Practice Department, Faculty of Pharmacy, Mansoura 
      University, Mansoura, 35516, Egypt.
AD  - Clinical Pharmacy and Pharmacy Practice Department, Faculty of Pharmacy, Mansoura 
      National University, Dakahlia Governorate, 7723730, Egypt.
FAU - AlZayer, Reem
AU  - AlZayer R
AD  - Clinical Pharmacy Practice, Department of Pharmacy, Mohammed Al-Mana College for 
      Medical Sciences, 34222, Dammam, Saudi Arabia.
FAU - Khalil, Roaa
AU  - Khalil R
AD  - Department of Pathology, Microbiology and Forensic Medicine, School of Medicine, 
      The University of Jordan, Amman, 11942, Jordan.
FAU - Fekih-Romdhane, Feten
AU  - Fekih-Romdhane F
AD  - The Tunisian Center of Early Intervention in Psychosis, Department of Psychiatry 
      "Ibn Omrane", Razi Hospital, 2010, Manouba, Tunisia.
AD  - Faculty of Medicine of Tunis, Tunis El Manar University, Tunis, Tunisia.
FAU - Hallit, Rabih
AU  - Hallit R
AD  - School of Medicine and Medical Sciences, Holy Spirit University of Kaslik, 
      Jounieh, Lebanon.
AD  - Department of Infectious Disease, Bellevue Medical Center, Mansourieh, Lebanon.
AD  - Department of Infectious Disease, Notre Dame des Secours, University Hospital 
      Center, Byblos, Lebanon.
FAU - Hallit, Souheil
AU  - Hallit S
AD  - School of Medicine and Medical Sciences, Holy Spirit University of Kaslik, 
      Jounieh, Lebanon.
AD  - Research Department, Psychiatric Hospital of the Cross, Jal Eddib, Lebanon.
FAU - Sallam, Malik
AU  - Sallam M
AD  - Department of Pathology, Microbiology and Forensic Medicine, School of Medicine, 
      The University of Jordan, Amman, 11942, Jordan. malik.sallam@ju.edu.jo.
AD  - Department of Clinical Laboratories and Forensic Medicine, Jordan University 
      Hospital, Amman, 11942, Jordan. malik.sallam@ju.edu.jo.
LA  - eng
PT  - Journal Article
DEP - 20240123
PL  - England
TA  - Sci Rep
JT  - Scientific reports
JID - 101563288
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Universities
MH  - *Academic Performance
MH  - Anxiety
MH  - Students
PMC - PMC10806219
COIS- The authors declare no competing interests.
EDAT- 2024/01/24 00:42
MHDA- 2024/01/25 06:43
PMCR- 2024/01/23
CRDT- 2024/01/23 23:47
PHST- 2023/09/30 00:00 [received]
PHST- 2024/01/19 00:00 [accepted]
PHST- 2024/01/25 06:43 [medline]
PHST- 2024/01/24 00:42 [pubmed]
PHST- 2024/01/23 23:47 [entrez]
PHST- 2024/01/23 00:00 [pmc-release]
AID - 10.1038/s41598-024-52549-8 [pii]
AID - 52549 [pii]
AID - 10.1038/s41598-024-52549-8 [doi]
PST - epublish
SO  - Sci Rep. 2024 Jan 23;14(1):1983. doi: 10.1038/s41598-024-52549-8.

PMID- 37844413
OWN - NLM
STAT- MEDLINE
DCOM- 20231216
LR  - 20231216
IS  - 1532-818X (Electronic)
IS  - 0196-0709 (Linking)
VI  - 45
IP  - 1
DP  - 2024 Jan-Feb
TI  - Answering head and neck cancer questions: An assessment of ChatGPT responses.
PG  - 104085
LID - S0196-0709(23)00299-5 [pii]
LID - 10.1016/j.amjoto.2023.104085 [doi]
AB  - PURPOSE: To examine and compare ChatGPT versus Google websites in answering 
      common head and neck cancer questions. MATERIALS AND METHODS: Commonly asked 
      questions about head and neck cancer were obtained and inputted into both 
      ChatGPT-4 and Google search engine. For each question, the ChatGPT response and 
      first website search result were compiled and examined. Content quality was 
      assessed by independent reviewers using standardized grading criteria and the 
      modified Ensuring Quality Information for Patients (EQIP) tool. Readability was 
      determined using the Flesch reading ease scale. RESULTS: In total, 49 questions 
      related to head and neck cancer were included. Google sources were on average 
      significantly higher quality than ChatGPT responses (4.2 vs 3.6, p&nbsp;=&nbsp;0.005). 
      According to the EQIP tool, Google and ChatGPT had on average similar response 
      rates per criterion (24.4 vs 20.5, p&nbsp;=&nbsp;0.09) while Google had a significantly 
      higher average score per question than ChatGPT (13.8 vs 11.7, p&nbsp;&lt;&nbsp;0.001) 
      According to the Flesch reading ease scale, ChatGPT and Google sources were both 
      considered similarly difficult to read (33.1 vs 37.0, p&nbsp;=&nbsp;0.180) and at a college 
      level (14.3 vs 14.2, p&nbsp;=&nbsp;0.820.) CONCLUSION: ChatGPT responses were as 
      challenging to read as Google sources, but poorer quality due to decreased 
      reliability and accuracy in answering questions. Though promising, ChatGPT in its 
      current form should not be considered dependable. Google sources are a preferred 
      resource for patient educational materials.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Wei, Kimberly
AU  - Wei K
AD  - Department of Otorhinolaryngology - Head and Neck Surgery, University of 
      Pennsylvania, Philadelphia, PA, USA.
FAU - Fritz, Christian
AU  - Fritz C
AD  - Department of Otorhinolaryngology - Head and Neck Surgery, University of 
      Pennsylvania, Philadelphia, PA, USA.
FAU - Rajasekaran, Karthik
AU  - Rajasekaran K
AD  - Department of Otorhinolaryngology - Head and Neck Surgery, University of 
      Pennsylvania, Philadelphia, PA, USA; Leonard Davis Institute of Health Economics, 
      University of Pennsylvania, Philadelphia, PA, USA. Electronic address: 
      Karthik.rajasekaran@pennmedicine.upenn.edu.
LA  - eng
PT  - Journal Article
DEP - 20231005
PL  - United States
TA  - Am J Otolaryngol
JT  - American journal of otolaryngology
JID - 8000029
SB  - IM
MH  - Humans
MH  - Reproducibility of Results
MH  - *Head and Neck Neoplasms/therapy
MH  - Search Engine
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Chatgpt
OT  - Common questions
OT  - Head and neck cancer
OT  - Patient education
COIS- Declaration of competing interest None.
EDAT- 2023/10/17 00:42
MHDA- 2023/12/17 09:43
CRDT- 2023/10/16 18:04
PHST- 2023/08/23 00:00 [received]
PHST- 2023/10/01 00:00 [accepted]
PHST- 2023/12/17 09:43 [medline]
PHST- 2023/10/17 00:42 [pubmed]
PHST- 2023/10/16 18:04 [entrez]
AID - S0196-0709(23)00299-5 [pii]
AID - 10.1016/j.amjoto.2023.104085 [doi]
PST - ppublish
SO  - Am J Otolaryngol. 2024 Jan-Feb;45(1):104085. doi: 10.1016/j.amjoto.2023.104085. 
      Epub 2023 Oct 5.

PMID- 37212584
OWN - NLM
STAT- MEDLINE
DCOM- 20231204
LR  - 20231217
IS  - 1572-0241 (Electronic)
IS  - 0002-9270 (Linking)
VI  - 118
IP  - 12
DP  - 2023 Dec 1
TI  - Chat Generative Pretrained Transformer Fails the Multiple-Choice American College 
      of Gastroenterology Self-Assessment Test.
PG  - 2280-2282
LID - 10.14309/ajg.0000000000002320 [doi]
AB  - INTRODUCTION: Chat Generative Pretrained Transformer (ChatGPT) is a natural 
      language processing model that generates human-like text. METHODS: ChatGPT-3 and 
      ChatGPT-4 were used to answer the 2022 and 2021 American College of 
      Gastroenterology self-assessment tests. The exact questions were inputted in both 
      versions of ChatGPT. A score of 70% or higher was required to pass the 
      assessment. RESULTS: Overall, ChatGPT-3 scored 65.1% on 455 included questions 
      and GPT-4 scored 62.4%. DISCUSSION: ChatGPT did not pass the American College of 
      Gastroenterology self-assessment test. We do not recommend its use for medical 
      education in gastroenterology in its current form.
CI  - Copyright © 2023 by The American College of Gastroenterology.
FAU - Suchman, Kelly
AU  - Suchman K
AD  - Zucker School of Medicine at Hofstra/Northwell, Long Island Jewish Medical 
      Center, New Hyde Park, New York, USA.
FAU - Garg, Shashank
AU  - Garg S
AD  - Arkansas Gastroenterology, North Little Rock, Arkansas, USA.
FAU - Trindade, Arvind J
AU  - Trindade AJ
AD  - Zucker School of Medicine at Hofstra/Northwell, Long Island Jewish Medical 
      Center, New Hyde Park, New York, USA.
AD  - Institute of Health Innovations and Outcomes Research, Feinstein Institutes for 
      Medical Research, Northwell Health, Manhasset, New York, USA.
LA  - eng
PT  - Journal Article
DEP - 20230522
PL  - United States
TA  - Am J Gastroenterol
JT  - The American journal of gastroenterology
JID - 0421030
SB  - IM
CIN - Am J Gastroenterol. 2023 Dec 1;118(12):2305. PMID: 38033226
CIN - Am J Gastroenterol. 2023 Dec 1;118(12):2305-2306. PMID: 38033227
MH  - Humans
MH  - *Gastroenterology
MH  - Self-Assessment
MH  - Universities
EDAT- 2023/05/22 13:04
MHDA- 2023/12/04 12:43
CRDT- 2023/05/22 09:03
PHST- 2023/03/15 00:00 [received]
PHST- 2023/05/10 00:00 [accepted]
PHST- 2023/12/04 12:43 [medline]
PHST- 2023/05/22 13:04 [pubmed]
PHST- 2023/05/22 09:03 [entrez]
AID - 00000434-202312000-00032 [pii]
AID - 10.14309/ajg.0000000000002320 [doi]
PST - ppublish
SO  - Am J Gastroenterol. 2023 Dec 1;118(12):2280-2282. doi: 
      10.14309/ajg.0000000000002320. Epub 2023 May 22.

PMID- 38457373
OWN - NLM
STAT- MEDLINE
DCOM- 20240311
LR  - 20240311
IS  - 1932-6203 (Electronic)
IS  - 1932-6203 (Linking)
VI  - 19
IP  - 3
DP  - 2024
TI  - Exploring factors influencing user perspective of ChatGPT as a technology that 
      assists in healthcare decision making: A cross sectional survey study.
PG  - e0296151
LID - 10.1371/journal.pone.0296151 [doi]
LID - e0296151
AB  - As ChatGPT emerges as a potential ally in healthcare decision-making, it is 
      imperative to investigate how users leverage and perceive it. The repurposing of 
      technology is innovative but brings risks, especially since AI's effectiveness 
      depends on the data it's fed. In healthcare, ChatGPT might provide sound advice 
      based on current medical knowledge, which could turn into misinformation if its 
      data sources later include erroneous information. Our study assesses user 
      perceptions of ChatGPT, particularly of those who used ChatGPT for 
      healthcare-related queries. By examining factors such as competence, reliability, 
      transparency, trustworthiness, security, and persuasiveness of ChatGPT, the 
      research aimed to understand how users rely on ChatGPT for health-related 
      decision-making. A web-based survey was distributed to U.S. adults using ChatGPT 
      at least once a month. Bayesian Linear Regression was used to understand how much 
      ChatGPT aids in informed decision-making. This analysis was conducted on subsets 
      of respondents, both those who used ChatGPT for healthcare decisions and those 
      who did not. Qualitative data from open-ended questions were analyzed using 
      content analysis, with thematic coding to extract public opinions on urban 
      environmental policies. Six hundred and seven individuals responded to the 
      survey. Respondents were distributed across 306 US cities of which 20 
      participants were from rural cities. Of all the respondents, 44 used ChatGPT for 
      health-related queries and decision-making. In the healthcare context, the most 
      effective model highlights 'Competent + Trustworthy + ChatGPT for healthcare 
      queries', underscoring the critical importance of perceived competence and 
      trustworthiness specifically in the realm of healthcare applications of ChatGPT. 
      On the other hand, the non-healthcare context reveals a broader spectrum of 
      influential factors in its best model, which includes 'Trustworthy + Secure + 
      Benefits outweigh risks + Satisfaction + Willing to take decisions + Intent to 
      use + Persuasive'. In conclusion our study findings suggest a clear demarcation 
      in user expectations and requirements from AI systems based on the context of 
      their use. We advocate for a balanced approach where technological advancement 
      and user readiness are harmonized.
CI  - Copyright: © 2024 Choudhury et al. This is an open access article distributed 
      under the terms of the Creative Commons Attribution License, which permits 
      unrestricted use, distribution, and reproduction in any medium, provided the 
      original author and source are credited.
FAU - Choudhury, Avishek
AU  - Choudhury A
AUID- ORCID: 0000-0002-5342-0709
AD  - Industrial and Management Systems Engineering, Benjamin M. Statler College of 
      Engineering and Mineral Resources, West Virginia University, Morgantown, West 
      Virginia, United States of America.
FAU - Elkefi, Safa
AU  - Elkefi S
AD  - Columbia University School of Nursing, Columbia University Irving Medical Center, 
      New York, New York, United States of America.
FAU - Tounsi, Achraf
AU  - Tounsi A
AD  - ADP, New Jersey, United States of America.
LA  - eng
PT  - Journal Article
DEP - 20240308
PL  - United States
TA  - PLoS One
JT  - PloS one
JID - 101285081
SB  - IM
MH  - Adult
MH  - Humans
MH  - Bayes Theorem
MH  - Cross-Sectional Studies
MH  - Reproducibility of Results
MH  - *Technology
MH  - *Decision Making
PMC - PMC10923482
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/03/08 18:42
MHDA- 2024/03/11 06:42
PMCR- 2024/03/08
CRDT- 2024/03/08 13:23
PHST- 2023/12/05 00:00 [received]
PHST- 2024/01/25 00:00 [accepted]
PHST- 2024/03/11 06:42 [medline]
PHST- 2024/03/08 18:42 [pubmed]
PHST- 2024/03/08 13:23 [entrez]
PHST- 2024/03/08 00:00 [pmc-release]
AID - PONE-D-23-40635 [pii]
AID - 10.1371/journal.pone.0296151 [doi]
PST - epublish
SO  - PLoS One. 2024 Mar 8;19(3):e0296151. doi: 10.1371/journal.pone.0296151. 
      eCollection 2024.

PMID- 38237878
OWN - NLM
STAT- Publisher
LR  - 20240220
IS  - 1532-8406 (Electronic)
IS  - 0883-5403 (Linking)
DP  - 2024 Jan 17
TI  - Chat Generative Pretrained Transformer (ChatGPT) and Bard: Artificial 
      Intelligence Does not yet Provide Clinically Supported Answers for Hip and Knee 
      Osteoarthritis.
LID - S0883-5403(24)00027-5 [pii]
LID - 10.1016/j.arth.2024.01.029 [doi]
AB  - BACKGROUND: Advancements in artificial intelligence (AI) have led to the creation 
      of large language models (LLMs), such as Chat Generative Pretrained Transformer 
      (ChatGPT) and Bard, that analyze online resources to synthesize responses to user 
      queries. Despite their popularity, the accuracy of LLM responses to medical 
      questions remains unknown. This study aimed to compare the responses of ChatGPT 
      and Bard regarding treatments for hip and knee osteoarthritis with the American 
      Academy of Orthopaedic Surgeons (AAOS) Evidence-Based Clinical Practice 
      Guidelines (CPGs) recommendations. METHODS: Both ChatGPT (Open AI) and Bard 
      (Google) were queried regarding 20 treatments (10 for hip and 10 for knee 
      osteoarthritis) from the AAOS CPGs. Responses were classified by 2 reviewers as 
      being in "Concordance," "Discordance," or "No Concordance" with AAOS CPGs. A 
      Cohen's Kappa coefficient was used to assess inter-rater reliability, and 
      Chi-squared analyses were used to compare responses between LLMs. RESULTS: 
      Overall, ChatGPT and Bard provided responses that were concordant with the AAOS 
      CPGs for 16 (80%) and 12 (60%) treatments, respectively. Notably, ChatGPT and 
      Bard encouraged the use of non-recommended treatments in 30% and 60% of queries, 
      respectively. There were no differences in performance when evaluating by joint 
      or by recommended versus non-recommended treatments. Studies were referenced in 6 
      (30%) of the Bard responses and none (0%) of the ChatGPT responses. Of the 6 Bard 
      responses, studies could only be identified for 1 (16.7%). Of the remaining, 2 
      (33.3%) responses cited studies in journals that did not exist, 2 (33.3%) cited 
      studies that could not be found with the information given, and 1 (16.7%) 
      provided links to unrelated studies. CONCLUSIONS: Both ChatGPT and Bard do not 
      consistently provide responses that align with the AAOS CPGs. Consequently, 
      physicians and patients should temper expectations on the guidance AI platforms 
      can currently provide.
CI  - Copyright © 2024 Elsevier Inc. All rights reserved.
FAU - Yang, JaeWon
AU  - Yang J
AD  - Department of Orthopaedic Surgery, University of Washington, Seattle, Washington.
FAU - Ardavanis, Kyle S
AU  - Ardavanis KS
AD  - Department of Orthopaedic Surgery, Madigan Medical Center, Tacoma, Washington.
FAU - Slack, Katherine E
AU  - Slack KE
AD  - Elson S. Floyd College of Medicine, Washington State University, Spokane, 
      Washington.
FAU - Fernando, Navin D
AU  - Fernando ND
AD  - Department of Orthopaedic Surgery, University of Washington, Seattle, Washington.
FAU - Della Valle, Craig J
AU  - Della Valle CJ
AD  - Department of Orthopaedic Surgery, Rush University Medical Center, Chicago, 
      Illinois.
FAU - Hernandez, Nicholas M
AU  - Hernandez NM
AD  - Department of Orthopaedic Surgery, University of Washington, Seattle, Washington.
LA  - eng
PT  - Journal Article
DEP - 20240117
PL  - United States
TA  - J Arthroplasty
JT  - The Journal of arthroplasty
JID - 8703515
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - bard
OT  - large language models
OT  - machine learning
EDAT- 2024/01/19 00:42
MHDA- 2024/01/19 00:42
CRDT- 2024/01/18 19:17
PHST- 2023/08/20 00:00 [received]
PHST- 2024/01/08 00:00 [revised]
PHST- 2024/01/11 00:00 [accepted]
PHST- 2024/01/19 00:42 [pubmed]
PHST- 2024/01/19 00:42 [medline]
PHST- 2024/01/18 19:17 [entrez]
AID - S0883-5403(24)00027-5 [pii]
AID - 10.1016/j.arth.2024.01.029 [doi]
PST - aheadofprint
SO  - J Arthroplasty. 2024 Jan 17:S0883-5403(24)00027-5. doi: 
      10.1016/j.arth.2024.01.029.

PMID- 38465158
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240312
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 16
IP  - 2
DP  - 2024 Feb
TI  - Enhancing Postoperative Cochlear Implant Care With ChatGPT-4: A Study on 
      Artificial Intelligence (AI)-Assisted Patient Education and Support.
PG  - e53897
LID - 10.7759/cureus.53897 [doi]
LID - e53897
AB  - BACKGROUND: Cochlear implantation is a critical surgical intervention for 
      patients with severe hearing loss. Postoperative care is essential for successful 
      rehabilitation, yet access to timely medical advice can be challenging, 
      especially in remote or resource-limited settings. Integrating advanced 
      artificial intelligence (AI) tools like Chat Generative Pre-trained Transformer 
      (ChatGPT)-4&nbsp;in post-surgical care could bridge the patient education and support 
      gap. AIM: This study aimed to assess the effectiveness of ChatGPT-4 as a 
      supplementary information resource for postoperative cochlear implant patients. 
      The focus was on evaluating the AI chatbot's ability to provide accurate, clear, 
      and relevant information, particularly in scenarios where access to healthcare 
      professionals is limited. MATERIALS AND METHODS: Five common postoperative 
      questions related to cochlear implant care were posed to ChatGPT-4. The AI 
      chatbot's responses were analyzed for accuracy, response time, clarity, and 
      relevance. The aim was to determine whether ChatGPT-4 could serve as a reliable 
      source of information for patients in need, especially if the patients could not 
      reach out to the hospital or the specialists at that moment. RESULTS: ChatGPT-4 
      provided responses aligned with current medical guidelines, demonstrating 
      accuracy and relevance. The AI chatbot responded to each query within seconds, 
      indicating its potential as a timely resource. Additionally, the responses were 
      clear and understandable, making complex medical information accessible to 
      non-medical audiences. These findings suggest that ChatGPT-4 could effectively 
      supplement traditional patient education, providing valuable support in 
      postoperative care. CONCLUSION: The study concluded that ChatGPT-4 has 
      significant potential as a supportive tool for cochlear implant patients post 
      surgery. While it cannot replace professional medical advice, ChatGPT-4 can 
      provide immediate, accessible, and understandable information, which is 
      particularly beneficial in special moments. This underscores the utility of AI in 
      enhancing patient care and supporting cochlear implantation.
CI  - Copyright © 2024, Aliyeva et al.
FAU - Aliyeva, Aynur
AU  - Aliyeva A
AD  - Otorhinolaryngology-Head and Neck Surgery, Cincinnati Children's Hospital, 
      Cincinnati, USA.
FAU - Sari, Elif
AU  - Sari E
AD  - Otorhinolaryngology-Head and Neck Surgery, Istanbul Aydın University, VM Medikal 
      Park Florya Hospital, Istanbul, TUR.
FAU - Alaskarov, Elvin
AU  - Alaskarov E
AD  - Otorhinolaryngology-Head and Neck Surgery, Istanbul Medipol University Health 
      Care Practice and Research Center, Esenler Hospital, Istanbul, TUR.
FAU - Nasirov, Rauf
AU  - Nasirov R
AD  - Neurosurgery, University of Cincinnati College of Medicine, Cincinnati, USA.
LA  - eng
PT  - Journal Article
DEP - 20240209
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10924891
OTO - NOTNLM
OT  - artificial intelligence in medicine
OT  - chatgpt-4
OT  - cochlear implant
OT  - patient education
OT  - postoperative care
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/03/11 06:42
MHDA- 2024/03/11 06:43
PMCR- 2024/02/09
CRDT- 2024/03/11 04:46
PHST- 2024/02/09 00:00 [accepted]
PHST- 2024/03/11 06:43 [medline]
PHST- 2024/03/11 06:42 [pubmed]
PHST- 2024/03/11 04:46 [entrez]
PHST- 2024/02/09 00:00 [pmc-release]
AID - 10.7759/cureus.53897 [doi]
PST - epublish
SO  - Cureus. 2024 Feb 9;16(2):e53897. doi: 10.7759/cureus.53897. eCollection 2024 Feb.

PMID- 37123797
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230502
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - A Case Report on Ground-Level Alternobaric Vertigo Due to Eustachian Tube 
      Dysfunction With the Assistance of Conversational Generative Pre-trained 
      Transformer (ChatGPT).
PG  - e36830
LID - 10.7759/cureus.36830 [doi]
LID - e36830
AB  - Alternatenobaric vertigo (ABV) develops when the middle ear pressure (MEP) is not 
      equal at the same height in the sea or the air. This is possible when the 
      altitude changes. Eustachian tube dysfunction (ETD) is a common cause of ABV. In 
      this case report, we discuss a patient who experienced repeated bouts of 
      ground-level alternobaric vertigo (GLABV) due to ETD. We also discuss how 
      Conversational Generative Pre-trained Transformer (ChatGPT) might be used in the 
      creation of this case report. A 41-year-old male patient complained of vertigo at 
      ground level on several occasions. His medical history included chronic 
      sinusitis, nasal congestion, and laryngopharyngeal reflux (LPR). During the 
      physical exam, his tympanic membranes were dull and moved less. Tympanometry 
      showed that he had an asymmetric type A and that both of his middle ears had 
      negative pressure. The results of the audiometry test were normal, and the 
      laryngoscopy revealed LPR. The patient was found to have GLABV because of ETD, 
      and different treatment options, such as Eustachian tube catheterization (ETC), 
      were thought about. This case study demonstrates how ChatGPT can be used to 
      assist with medical documentation and the treatment of GLABV caused by ETD. Even 
      though ChatGPT did not provide specific diagnostic or treatment recommendations 
      for the patient's condition, it did assist the doctor in determining what was 
      wrong and how to treat it while writing the case report. It also aided the doctor 
      in writing the case report by allowing them to discuss it. The use of artificial 
      intelligence (AI) tools such as ChatGPT has the potential to improve the accuracy 
      and speed of medical documentation, thereby streamlining clinical workflows and 
      improving patient care. Nonetheless, it is critical to consider the ethical 
      implications of using AI in clinical practice This case study emphasizes the 
      importance of understanding that ETD is a common cause of GLABV and how ChatGPT 
      can aid in the diagnosis and treatment of this condition.&nbsp;More research is needed 
      to fully understand how long-term AI interventions in medicine work and how 
      reliable they are.
CI  - Copyright © 2023, Kim et al.
FAU - Kim, Hee-Young
AU  - Kim HY
AD  - Center for Executive and Continuing Professional Education, Harvard T.H. Chan 
      School of Public Health, Boston, USA.
AD  - Department of Otolaryngology - Head and Neck Surgery, Samsung Seoul Hospital, 
      Seoul, KOR.
AD  - Otolaryngology, Kim ENT Clinic, Seoul, KOR.
LA  - eng
PT  - Case Reports
DEP - 20230328
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10140002
OTO - NOTNLM
OT  - alternobaric vertigo
OT  - artificial intelligence
OT  - chatgpt
OT  - eustachian tube catheterization
OT  - eustachian tube dysfunction
OT  - ground-level alternobaric vertigo
OT  - laryngopharyngeal reflux
OT  - middle ear pressure
OT  - tympanometry
OT  - &nbsp;laryngoscopy
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/05/01 06:41
MHDA- 2023/05/01 06:42
PMCR- 2023/03/28
CRDT- 2023/05/01 03:34
PHST- 2023/03/28 00:00 [accepted]
PHST- 2023/05/01 06:42 [medline]
PHST- 2023/05/01 06:41 [pubmed]
PHST- 2023/05/01 03:34 [entrez]
PHST- 2023/03/28 00:00 [pmc-release]
AID - 10.7759/cureus.36830 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 28;15(3):e36830. doi: 10.7759/cureus.36830. eCollection 2023 
      Mar.

PMID- 38215455
OWN - NLM
STAT- Publisher
LR  - 20240112
IS  - 1539-2864 (Electronic)
IS  - 0275-004X (Linking)
DP  - 2024 Jan 9
TI  - The ability of artificial intelligence chatbots ChatGPT and Google Bard to 
      accurately convey pre-operative information for patients undergoing 
      ophthalmological surgeries.
LID - 10.1097/IAE.0000000000004044 [doi]
AB  - INTRODUCTION: To determine whether the two popular artificial intelligence (AI) 
      chatbots, ChatGPT and Bard, provide high-quality information concerning procedure 
      description, risks, benefits, and alternatives of various ophthalmological 
      surgeries. METHODS: ChatGPT and Bard were prompted with questions pertaining to 
      the description, potential risks, benefits, alternatives, and implications of not 
      proceeding with various surgeries in different subspecialties of ophthalmology. 
      Six common ophthalmic procedures were included in our analysis. Two comprehensive 
      ophthalmologists and one sub-specialist graded each response independently using 
      a five-point Likert scale. RESULTS: Likert grading for accuracy was significantly 
      higher for ChatGPT in comparison to Bard (4.5±0.6 vs 3.8±0.8, p&lt;0.0001). 
      Generally, ChatGPT performed better than Bard even when questions were stratified 
      by type of ophthalmological surgery. There was no significant difference between 
      ChatGPT and Bard for response length (2104.7±271.4 characters vs 2441.0±633.9 
      characters, p=0.12). ChatGPT responded significantly slower than Bard (46.0±3.0 
      seconds vs 6.6±1.2 seconds, p&lt;0.0001). CONCLUSIONS: Both ChatGPT and Bard may 
      offer accessible and high-quality information relevant to the informed consent 
      process for various ophthalmic procedures. Nonetheless, both AI chatbots 
      overlooked probability of adverse events, hence limiting their potential and 
      introducing patients to information that may be difficult to interpret.
FAU - Patil, Nikhil S
AU  - Patil NS
AUID- ORCID: 0000-0003-3929-0482
AD  - Michael G. DeGroote School of Medicine, McMaster University, Hamilton, Ontario, 
      Canada.
FAU - Huang, Ryan
AU  - Huang R
AD  - Temerty Faculty of Medicine, University of Toronto, Toronto, Ontario, Canada.
FAU - Mihalache, Andrew
AU  - Mihalache A
AD  - Temerty Faculty of Medicine, University of Toronto, Toronto, Ontario, Canada.
FAU - Kisilevsky, Eli
AU  - Kisilevsky E
AD  - Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, 
      Ontario, Canada.
AD  - Unity Health, St. Joseph's Health Centre, University of Toronto, Toronto, 
      Ontario, Canada.
FAU - Kwok, Jason
AU  - Kwok J
AD  - Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, 
      Ontario, Canada.
FAU - Popovic, Marko M
AU  - Popovic MM
AD  - Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, 
      Ontario, Canada.
FAU - Nassrallah, Georges
AU  - Nassrallah G
AD  - Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, 
      Ontario, Canada.
AD  - Department of Ophthalmology, Hospital for Sick Children, University of Toronto, 
      Ontario, Canada.
FAU - Chan, Clara
AU  - Chan C
AD  - Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, 
      Ontario, Canada.
FAU - Mallipatna, Ashwin
AU  - Mallipatna A
AD  - Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, 
      Ontario, Canada.
AD  - Department of Ophthalmology, Hospital for Sick Children, University of Toronto, 
      Ontario, Canada.
FAU - Kertes, Peter J
AU  - Kertes PJ
AD  - Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, 
      Ontario, Canada.
AD  - John and Liz Tory Eye Centre, Sunnybrook Health Sciences Centre, Toronto, 
      Ontario, Canada.
FAU - Muni, Rajeev H
AU  - Muni RH
AD  - Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, 
      Ontario, Canada.
AD  - Department of Ophthalmology, St. Michael's Hospital/Unity Health Toronto, 
      Toronto, Ontario, Canada.
LA  - eng
PT  - Journal Article
DEP - 20240109
PL  - United States
TA  - Retina
JT  - Retina (Philadelphia, Pa.)
JID - 8309919
SB  - IM
EDAT- 2024/01/12 18:42
MHDA- 2024/01/12 18:42
CRDT- 2024/01/12 17:12
PHST- 2024/01/12 18:42 [medline]
PHST- 2024/01/12 18:42 [pubmed]
PHST- 2024/01/12 17:12 [entrez]
AID - 00006982-990000000-00574 [pii]
AID - 10.1097/IAE.0000000000004044 [doi]
PST - aheadofprint
SO  - Retina. 2024 Jan 9. doi: 10.1097/IAE.0000000000004044.

PMID- 38126878
OWN - NLM
STAT- MEDLINE
DCOM- 20240219
LR  - 20240219
IS  - 1557-9034 (Electronic)
IS  - 1092-6429 (Linking)
VI  - 34
IP  - 2
DP  - 2024 Feb
TI  - Appropriateness of Online Chat-Based Artificial Intelligence (ChatGPT) Answers to 
      Common Questions on Inguinal Hernia Repair.
PG  - 141-143
LID - 10.1089/lap.2023.0403 [doi]
AB  - ChatGPT is a conversational AI model developed by OpenAI designed to generate 
      human-like text based on the input it receives. ChatGPT has become increasingly 
      popular, and the general public may use this tool to ask questions about 
      different medical conditions. There is a lack of data showing if ChatGPT is able 
      to provide reliable information on medical conditions to the general public. The 
      aim of our study is to assess the accuracy and appropriateness of ChatGPT answers 
      to questions on inguinal hernia management.
FAU - Lima, Diego Laurentino
AU  - Lima DL
AUID- ORCID: 0000-0001-7383-1284
AD  - Department of Surgery, Montefiore Medical Center, Bronx, New York, USA.
FAU - Nogueira, Raquel
AU  - Nogueira R
AD  - Department of Surgery, Montefiore Medical Center, Bronx, New York, USA.
FAU - Chin, Ryan
AU  - Chin R
AD  - Department of Surgery, Montefiore Medical Center, Bronx, New York, USA.
FAU - Claus, Christiano
AU  - Claus C
AD  - Minimally Invasive Surgery Department, Nossa Senhora das Graças Hospital, 
      Curitiba, Brazil.
FAU - Malcher, Flavio
AU  - Malcher F
AD  - Division of General Surgery, NYU Langone, New York, New York, USA.
FAU - Sreeramoju, Prashanth
AU  - Sreeramoju P
AD  - Department of Surgery, Montefiore Medical Center, Bronx, New York, USA.
FAU - Cavazzola, Leandro Totti
AU  - Cavazzola LT
AD  - Department of Surgery, Federal University of Rio Grande do Sul, Porto Alegre, 
      Brazil.
LA  - eng
PT  - Journal Article
DEP - 20231221
PL  - United States
TA  - J Laparoendosc Adv Surg Tech A
JT  - Journal of laparoendoscopic &amp; advanced surgical techniques. Part A
JID - 9706293
SB  - IM
MH  - Humans
MH  - *Hernia, Inguinal/surgery
MH  - Artificial Intelligence
MH  - Communication
OTO - NOTNLM
OT  - artificial intelligence
OT  - chatGPT
OT  - inguinal hernia
EDAT- 2023/12/21 12:42
MHDA- 2024/02/19 06:44
CRDT- 2023/12/21 09:53
PHST- 2024/02/19 06:44 [medline]
PHST- 2023/12/21 12:42 [pubmed]
PHST- 2023/12/21 09:53 [entrez]
AID - 10.1089/lap.2023.0403 [doi]
PST - ppublish
SO  - J Laparoendosc Adv Surg Tech A. 2024 Feb;34(2):141-143. doi: 
      10.1089/lap.2023.0403. Epub 2023 Dec 21.

PMID- 38245622
OWN - NLM
STAT- Publisher
LR  - 20240120
IS  - 1476-5454 (Electronic)
IS  - 0950-222X (Linking)
DP  - 2024 Jan 20
TI  - Reliability and accuracy of artificial intelligence ChatGPT in providing 
      information on ophthalmic diseases and management to patients.
LID - 10.1038/s41433-023-02906-0 [doi]
AB  - PURPOSE: To assess the accuracy of ophthalmic information provided by an 
      artificial intelligence chatbot (ChatGPT). METHODS: Five diseases from 8 
      subspecialties of Ophthalmology were assessed by ChatGPT version 3.5. Three 
      questions were asked to ChatGPT for each disease: what is x?; how is x 
      diagnosed?; how is x treated? (x = name of the disease). Responses were graded by 
      comparing them to the American Academy of Ophthalmology (AAO) guidelines for 
      patients, with scores ranging from -3 (unvalidated and potentially harmful to a 
      patient's health or well-being if they pursue such a suggestion) to 2 (correct 
      and complete). MAIN OUTCOMES: Accuracy of responses from ChatGPT in response to 
      prompts related to ophthalmic health information in the form of scores on a scale 
      from -3 to 2. RESULTS: Of the 120 questions, 93 (77.5%) scored ≥ 1. 27. (22.5%) 
      scored ≤ -1; among these, 9 (7.5%) obtained a score of -3. The overall median 
      score amongst all subspecialties was 2 for the question "What is x", 1.5 for "How 
      is x diagnosed", and 1 for "How is x treated", though this did not achieve 
      significance by Kruskal-Wallis testing. CONCLUSIONS: Despite the positive scores, 
      ChatGPT on its own still provides incomplete, incorrect, and potentially harmful 
      information about common ophthalmic conditions, defined as the recommendation of 
      invasive procedures or other interventions with potential for adverse sequelae 
      which are not supported by the AAO for the disease in question. ChatGPT may be a 
      valuable adjunct to patient education, but currently, it is not sufficient 
      without concomitant human medical supervision.
CI  - © 2024. The Author(s).
FAU - Cappellani, Francesco
AU  - Cappellani F
AUID- ORCID: 0009-0007-6807-9455
AD  - Retina Service, Wills Eye Hospital, Thomas Jefferson University, Philadelphia, 
      PA, USA.
FAU - Card, Kevin R
AU  - Card KR
AD  - Ocular Oncology Service, Wills Eye Hospital, Thomas Jefferson University, 
      Philadelphia, PA, USA.
FAU - Shields, Carol L
AU  - Shields CL
AUID- ORCID: 0000-0002-3288-3632
AD  - Ocular Oncology Service, Wills Eye Hospital, Thomas Jefferson University, 
      Philadelphia, PA, USA.
FAU - Pulido, Jose S
AU  - Pulido JS
AD  - Retina Service, Wills Eye Hospital, Thomas Jefferson University, Philadelphia, 
      PA, USA.
FAU - Haller, Julia A
AU  - Haller JA
AD  - Retina Service, Wills Eye Hospital, Thomas Jefferson University, Philadelphia, 
      PA, USA. JHaller@willseye.org.
LA  - eng
PT  - Journal Article
DEP - 20240120
PL  - England
TA  - Eye (Lond)
JT  - Eye (London, England)
JID - 8703986
SB  - IM
EDAT- 2024/01/21 00:42
MHDA- 2024/01/21 00:42
CRDT- 2024/01/20 23:21
PHST- 2023/03/10 00:00 [received]
PHST- 2023/12/13 00:00 [accepted]
PHST- 2023/12/05 00:00 [revised]
PHST- 2024/01/21 00:42 [medline]
PHST- 2024/01/21 00:42 [pubmed]
PHST- 2024/01/20 23:21 [entrez]
AID - 10.1038/s41433-023-02906-0 [pii]
AID - 10.1038/s41433-023-02906-0 [doi]
PST - aheadofprint
SO  - Eye (Lond). 2024 Jan 20. doi: 10.1038/s41433-023-02906-0.

PMID- 38340312
OWN - NLM
STAT- Publisher
LR  - 20240210
IS  - 1466-187X (Electronic)
IS  - 0142-159X (Linking)
DP  - 2024 Feb 10
TI  - Case-based MCQ generator: A custom ChatGPT based on published prompts in the 
      literature for automatic item generation.
PG  - 1-3
LID - 10.1080/0142159X.2024.2314723 [doi]
AB  - WHAT IS THE EDUCATIONAL CHALLENGE? A fundamental challenge in medical education 
      is creating high-quality, clinically relevant multiple-choice questions (MCQs). 
      ChatGPT-based automatic item generation (AIG) methods need well-designed prompts. 
      However, the use of these prompts is hindered by the time-consuming process of 
      copying and pasting, a lack of know-how among medical teachers, and the 
      generalist nature of standard ChatGPT, which often lacks the medical context. 
      WHAT ARE THE PROPOSED SOLUTIONS? The Case-based MCQ Generator, a custom GPT, 
      addresses these challenges. It has been trained by using GPT Builder, which is a 
      platform designed by OpenAI for customizing ChatGPT to meet specific needs, in 
      order to allow users to generate case-based MCQs. By using this free tool for 
      those who have ChatGPT Plus subscription, health professions educators can easily 
      select a prompt, input a learning objective or item-specific test point, and 
      generate clinically relevant questions. WHAT ARE THE POTENTIAL BENEFITS TO A 
      WIDER GLOBAL AUDIENCE? It enhances the efficiency of MCQ generation and ensures 
      the generation of contextually relevant questions, surpassing the capabilities of 
      standard ChatGPT. It streamlines the MCQ creation process by integrating prompts 
      published in medical education literature, eliminating the need for manual prompt 
      input. WHAT ARE THE NEXT STEPS? Future development aims at sustainability and 
      addressing ethical and accessibility issues. It requires regular updates, 
      integration of new prompts from emerging health professions education literature, 
      and a supportive digital ecosystem around the tool. Accessibility, especially for 
      educators in low-resource countries, is vital, demanding alternative access 
      models to overcome financial barriers.
FAU - Kıyak, Yavuz Selim
AU  - Kıyak YS
AUID- ORCID: 0000-0002-5026-3234
AD  - Department of Medical Education and Informatics, Faculty of Medicine, Gazi 
      University, Ankara, Turkey.
AD  - Department of Bioinformatics and Telemedicine, Jagiellonian University Medical 
      College, Kraków, Poland.
FAU - Kononowicz, Andrzej A
AU  - Kononowicz AA
AUID- ORCID: 0000-0003-2956-2093
AD  - Department of Bioinformatics and Telemedicine, Jagiellonian University Medical 
      College, Kraków, Poland.
LA  - eng
PT  - Journal Article
DEP - 20240210
PL  - England
TA  - Med Teach
JT  - Medical teacher
JID - 7909593
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - automatic item generation
OT  - large language models
OT  - multiple-choice questions
EDAT- 2024/02/10 14:50
MHDA- 2024/02/10 14:50
CRDT- 2024/02/10 11:33
PHST- 2024/02/10 14:50 [medline]
PHST- 2024/02/10 14:50 [pubmed]
PHST- 2024/02/10 11:33 [entrez]
AID - 10.1080/0142159X.2024.2314723 [doi]
PST - aheadofprint
SO  - Med Teach. 2024 Feb 10:1-3. doi: 10.1080/0142159X.2024.2314723.

PMID- 37936506
OWN - NLM
STAT- MEDLINE
DCOM- 20240115
LR  - 20240115
IS  - 2157-6580 (Electronic)
IS  - 2157-6564 (Print)
IS  - 2157-6564 (Linking)
VI  - 13
IP  - 1
DP  - 2024 Jan 12
TI  - Using A Google Web Search Analysis to Assess the Utility of ChatGPT in Stem Cell 
      Therapy.
PG  - 60-68
LID - 10.1093/stcltm/szad074 [doi]
AB  - OBJECTIVE: Since its introduction, the use of ChatGPT has increased significantly 
      for medically related purposes. However, current research has not captured its 
      applications in providing information on stem cell therapy. To address this gap, 
      the present study compared the effectiveness of ChatGPT to Google in answering 
      medical questions related to stem cell therapy. METHODS: The search term "stem 
      cell therapy" was used to perform a Google web search, and the top 20 frequently 
      asked questions along with answers were recorded together with relevant website 
      sources. Of these questions, the top 10 questions were separately entered into 
      ChatGPT, and the answers and the sources were recorded. Then, the following 
      statement was entered into ChatGPT: "Do a Google search with the search term 
      'stem cell therapy' and record 20 common questions related to the search term." 
      After obtaining these questions, each question was separately entered into 
      ChatGPT for an answer and source. RESULTS: A majority of the top 20 questions 
      provided by Google were related to fact, whereas a majority of the questions 
      provided by ChatGPT were related to policy. The answer sources used by Google 
      were mostly drawn from medical practice, while those used by ChatGPT were mostly 
      drawn from academic information. CONCLUSION: Compared to Google, ChatGPT exhibits 
      stronger capabilities in promoting awareness of stem cell therapy. ChatGPT has 
      the ability to eliminate misleading information by providing accurate and 
      reliable answers. However, the responses provided by ChatGPT are still general in 
      nature and cannot substitute academic sources for providing specialized 
      knowledge.
CI  - © The Author(s) 2023. Published by Oxford University Press.
FAU - Chen, Long
AU  - Chen L
AD  - Arthritis Clinic and Research Center, Peking University People's Hospital, Peking 
      University, Beijing, People's Republic of China.
FAU - Li, Hui
AU  - Li H
AD  - Arthritis Clinic and Research Center, Peking University People's Hospital, Peking 
      University, Beijing, People's Republic of China.
FAU - Su, Yiqi
AU  - Su Y
AD  - Arthritis Clinic and Research Center, Peking University People's Hospital, Peking 
      University, Beijing, People's Republic of China.
FAU - Yang, Zhen
AU  - Yang Z
AD  - Arthritis Clinic and Research Center, Peking University People's Hospital, Peking 
      University, Beijing, People's Republic of China.
FAU - He, Zihao
AU  - He Z
AD  - Arthritis Clinic and Research Center, Peking University People's Hospital, Peking 
      University, Beijing, People's Republic of China.
FAU - Wang, Du
AU  - Wang D
AD  - Arthritis Clinic and Research Center, Peking University People's Hospital, Peking 
      University, Beijing, People's Republic of China.
FAU - Li, Jiao Jiao
AU  - Li JJ
AD  - School of Biomedical Engineering, Faculty of Engineering &amp; IT, University of 
      Technology Sydney, Sydney, NSW, Australia.
FAU - Xing, Dan
AU  - Xing D
AUID- ORCID: 0000-0001-6966-3134
AD  - Arthritis Clinic and Research Center, Peking University People's Hospital, Peking 
      University, Beijing, People's Republic of China.
LA  - eng
GR  - 81973606/National Natural Science Foundations of China/
GR  - 81973606/National Natural Science Foundation of China/
PT  - Journal Article
PL  - England
TA  - Stem Cells Transl Med
JT  - Stem cells translational medicine
JID - 101578022
SB  - IM
MH  - *Search Engine
MH  - *Stem Cell Transplantation
PMC - PMC10785216
OTO - NOTNLM
OT  - ChatGPT
OT  - Google
OT  - stem cell therapy
OT  - utility
OT  - web search
COIS- The authors declare no potential conflicts of interest.
EDAT- 2023/11/08 06:43
MHDA- 2024/01/15 12:43
PMCR- 2023/11/04
CRDT- 2023/11/08 03:44
PHST- 2023/06/05 00:00 [received]
PHST- 2023/09/29 00:00 [accepted]
PHST- 2024/01/15 12:43 [medline]
PHST- 2023/11/08 06:43 [pubmed]
PHST- 2023/11/08 03:44 [entrez]
PHST- 2023/11/04 00:00 [pmc-release]
AID - 7344723 [pii]
AID - szad074 [pii]
AID - 10.1093/stcltm/szad074 [doi]
PST - ppublish
SO  - Stem Cells Transl Med. 2024 Jan 12;13(1):60-68. doi: 10.1093/stcltm/szad074.

PMID- 38032062
OWN - NLM
STAT- MEDLINE
DCOM- 20240207
LR  - 20240207
IS  - 1365-2559 (Electronic)
IS  - 0309-0167 (Linking)
VI  - 84
IP  - 4
DP  - 2024 Mar
TI  - Bridging bytes and biopsies: A comparative analysis of ChatGPT and 
      histopathologists in pathology diagnosis and collaborative potential.
PG  - 601-613
LID - 10.1111/his.15100 [doi]
AB  - BACKGROUND AND AIMS: ChatGPT is a powerful artificial intelligence (AI) chatbot 
      developed by the OpenAI research laboratory which is capable of analysing human 
      input and generating human-like responses. Early research into the potential 
      application of ChatGPT in healthcare has focused mainly on clinical and 
      administrative functions. The diagnostic ability and utility of ChatGPT in 
      histopathology is not well defined. We benchmarked the performance of ChatGPT 
      against pathologists in diagnostic histopathology, and evaluated the 
      collaborative potential between pathologists and ChatGPT to deliver more accurate 
      diagnoses. METHODS AND RESULTS: In Part 1 of the study, pathologists and ChatGPT 
      were subjected to a series of questions encompassing common diagnostic conundrums 
      in histopathology. For Part 2, pathologists reviewed a series of challenging 
      virtual slides and provided their diagnoses before and after consultation with 
      ChatGPT. We found that ChatGPT performed worse than pathologists in reaching the 
      correct diagnosis. Consultation with ChatGPT provided limited help and 
      information generated from ChatGPT is dependent on the prompts provided by the 
      pathologists and is not always correct. Finally, we surveyed pathologists who 
      rated the diagnostic accuracy of ChatGPT poorly, but found it useful as an 
      advanced search engine. CONCLUSIONS: The use of ChatGPT4 as a diagnostic tool in 
      histopathology is limited by its inherent shortcomings. Judicious evaluation of 
      the information and histopathology diagnosis generated from ChatGPT4 is essential 
      and cannot replace the acuity and judgement of a pathologist. However, future 
      advances in generative AI may expand its role in the field of histopathology.
CI  - © 2023 The Authors. Histopathology published by John Wiley &amp; Sons Ltd.
FAU - Oon, Ming Liang
AU  - Oon ML
AUID- ORCID: 0000-0003-0493-3361
AD  - Department of Pathology, National University Hospital, Singapore, Singapore.
FAU - Syn, Nicholas L
AU  - Syn NL
AD  - Department of Pathology, National University Hospital, Singapore, Singapore.
FAU - Tan, Char Loo
AU  - Tan CL
AUID- ORCID: 0000-0002-1470-3175
AD  - Department of Pathology, National University Hospital, Singapore, Singapore.
FAU - Tan, Kong-Bing
AU  - Tan KB
AD  - Department of Pathology, National University Hospital, Singapore, Singapore.
AD  - Department of Pathology, Yong Loo Lin School of Medicine, National University of 
      Singapore, Singapore.
FAU - Ng, Siok-Bian
AU  - Ng SB
AD  - Department of Pathology, National University Hospital, Singapore, Singapore.
AD  - Department of Pathology, Yong Loo Lin School of Medicine, National University of 
      Singapore, Singapore.
AD  - Cancer Science Institute of Singapore, National University of Singapore, 
      Singapore, Singapore.
LA  - eng
GR  - National Medical Research Council/
PT  - Journal Article
DEP - 20231130
PL  - England
TA  - Histopathology
JT  - Histopathology
JID - 7704136
SB  - IM
MH  - Humans
MH  - *Pathologists
MH  - *Artificial Intelligence
MH  - Biopsy
MH  - Referral and Consultation
MH  - Software
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - chatbot
OT  - digital pathology
OT  - natural language processing
OT  - pathology
EDAT- 2023/11/30 12:42
MHDA- 2024/02/07 06:42
CRDT- 2023/11/30 07:53
PHST- 2023/10/03 00:00 [revised]
PHST- 2023/06/10 00:00 [received]
PHST- 2023/11/02 00:00 [accepted]
PHST- 2024/02/07 06:42 [medline]
PHST- 2023/11/30 12:42 [pubmed]
PHST- 2023/11/30 07:53 [entrez]
AID - 10.1111/his.15100 [doi]
PST - ppublish
SO  - Histopathology. 2024 Mar;84(4):601-613. doi: 10.1111/his.15100. Epub 2023 Nov 30.

PMID- 38270461
OWN - NLM
STAT- Publisher
LR  - 20240125
IS  - 1942-4426 (Electronic)
IS  - 0893-2174 (Linking)
VI  - 0
IP  - 0
DP  - 2024 Jan 24
TI  - Performance of an Artificial Intelligence-Based Chatbot (ChatGPT) Answering the 
      European Certification in Implant Dentistry Exam.
PG  - 1-5
LID - 10.11607/ijp.8852 [doi]
AB  - PURPOSE: To compare the performance of licensed dentists and two software 
      versions (3.5 legacy and 4.0) of an artificial intelligence (AI)-based chatbot 
      (ChatGPT) answering the exam for the 2022 Certification in Implant Dentistry of 
      the European Association for Osseointegration (EAO). MATERIALS AND METHODS: The 
      50 question, multiple-choice exam of the EAO for the 2022 Certification in 
      Implant Dentistry was obtained. Three groups were created based on the individual 
      or program answering the exam: licensed dentists (D group) and two software 
      versions of an artificial intelligence (AI)-based chatbot (ChatGPT)-3.5 legacy 
      (ChatGPT-3.5 group) and the 4.0 version (ChatGPT-4.0 group). The EAO provided the 
      results of the 2022 examinees (D group). For the ChatGPT groups, the 50 
      multiple-choice questions were introduced into both ChatGBT versions, and the 
      answers were recorded. Pearson correlation matrix was used to analyze the linear 
      relationship among the subgroups. The inter- and intraoperator reliability was 
      calculated using Cronbach's alpha coefficient. One-way ANOVA and Tukey post-hoc 
      tests were used to examine the data (α = .05). RESULTS: ChatGPT was able to pass 
      the exam for the 2022 Certification in Implant Dentistry of the EAO. 
      Additionally, the software version of ChatGPT impacted the score obtained. The 
      4.0 version was able to not only pass the exam but also obtained a significantly 
      higher score than the 3.5 version and licensed dentists completing the same exam. 
      CONCLUSIONS: The AI-based chatbot tested was able to not only pass the exam but 
      performed better than licensed dentists.
FAU - Revilla-León, Marta
AU  - Revilla-León M
FAU - Barmak, Basir A
AU  - Barmak BA
FAU - Sailer, Irena
AU  - Sailer I
FAU - Kois, John C
AU  - Kois JC
FAU - Att, Wael
AU  - Att W
LA  - eng
PT  - Journal Article
DEP - 20240124
PL  - United States
TA  - Int J Prosthodont
JT  - The International journal of prosthodontics
JID - 8900938
EDAT- 2024/01/25 12:42
MHDA- 2024/01/25 12:42
CRDT- 2024/01/25 09:13
PHST- 2024/01/25 12:42 [medline]
PHST- 2024/01/25 12:42 [pubmed]
PHST- 2024/01/25 09:13 [entrez]
AID - 4901389 [pii]
AID - 10.11607/ijp.8852 [doi]
PST - aheadofprint
SO  - Int J Prosthodont. 2024 Jan 24;0(0):1-5. doi: 10.11607/ijp.8852.

PMID- 37389659
OWN - NLM
STAT- MEDLINE
DCOM- 20240125
LR  - 20240206
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 52
IP  - 2
DP  - 2024 Feb
TI  - Enhancing Awareness and Self-diagnosis of Obstructive Sleep Apnea Using 
      AI-Powered Chatbots: The Role of ChatGPT in Revolutionizing Healthcare.
PG  - 136-138
LID - 10.1007/s10439-023-03298-8 [doi]
AB  - Since OpenAI (San Francisco, CA) released its generative AI chatbot, ChatGPT, we 
      are on the cusp of technological transformation. The tool is capable of 
      generating text according to the input that the user adds to it. Due to its 
      ability to imitate human speech tone while extracting encyclopedic knowledge, 
      ChatGPT can be a platform for personalized patient interaction. Thus, it has the 
      potential to revolutionize the healthcare system. Our study aims to evaluate how 
      ChatGPT can answer the queries of patients suffering from obstructive sleep apnea 
      and aid in self-diagnosis. By analyzing symptoms and guiding patients' behavior 
      toward prevention, ChatGPT can play a major role in avoiding serious health 
      repercussions that develop in the later course of obstructive sleep apnea.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Bilal, Maham
AU  - Bilal M
AUID- ORCID: 0009-0009-6094-2239
AD  - Dow Medical College, Dow University of Health Sciences, Karachi, Pakistan. 
      mahambilal33@gmail.com.
FAU - Jamil, Yumna
AU  - Jamil Y
AD  - Dow Medical College, Dow University of Health Sciences, Karachi, Pakistan.
FAU - Rana, Dua
AU  - Rana D
AD  - Dow Medical College, Dow University of Health Sciences, Karachi, Pakistan.
FAU - Shah, Hussain Haider
AU  - Shah HH
AD  - Dow Medical College, Dow University of Health Sciences, Karachi, Pakistan.
LA  - eng
PT  - Letter
DEP - 20230630
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - Humans
MH  - *Sleep Apnea, Obstructive
MH  - Software
MH  - Speech
MH  - Technology
MH  - Artificial Intelligence
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Healthcare
OT  - Obstructive sleep apnea
OT  - Patient interaction
OT  - Self-diagnosis
EDAT- 2023/06/30 13:11
MHDA- 2024/01/25 06:43
CRDT- 2023/06/30 11:06
PHST- 2023/06/16 00:00 [received]
PHST- 2023/06/22 00:00 [accepted]
PHST- 2024/01/25 06:43 [medline]
PHST- 2023/06/30 13:11 [pubmed]
PHST- 2023/06/30 11:06 [entrez]
AID - 10.1007/s10439-023-03298-8 [pii]
AID - 10.1007/s10439-023-03298-8 [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2024 Feb;52(2):136-138. doi: 10.1007/s10439-023-03298-8. Epub 
      2023 Jun 30.

PMID- 37855397
OWN - NLM
STAT- MEDLINE
DCOM- 20240320
LR  - 20240320
IS  - 1445-2197 (Electronic)
IS  - 1445-1433 (Linking)
VI  - 94
IP  - 3
DP  - 2024 Mar
TI  - Exploring the role of an artificial intelligence chatbot on appendicitis 
      management: an experimental study on ChatGPT.
PG  - 342-352
LID - 10.1111/ans.18736 [doi]
AB  - BACKGROUND: Appendicitis is a common surgical condition that requires urgent 
      medical attention. Recent advancements in artificial intelligence and large 
      language processing, such as ChatGPT, have demonstrated potential in supporting 
      healthcare management and scientific research. This study aims to evaluate the 
      accuracy and comprehensiveness of ChatGPT's knowledge on appendicitis management. 
      METHODS: Six questions related to appendicitis management were created by 
      experienced RACS qualified general surgeons to assess ChatGPT's ability to 
      provide accurate information. The criteria of ChatGPT answers' accuracy were 
      compared with current healthcare guidelines for appendicitis and subjective 
      evaluation by two RACS qualified General Surgeons. Additionally, ChatGPT was then 
      asked to provide five high level evidence references to support its responses. 
      RESULTS: ChatGPT provided clinically relevant information on appendicitis 
      management, however, was inconsistent in doing so and often provided superficial 
      information. Further to this, ChatGPT encountered difficulties in generating 
      relevant references, with some being either non-existent or incorrect. 
      CONCLUSION: ChatGPT has the potential to provide timely and comprehensible 
      medical information on appendicitis management to laypersons. However, its issue 
      of inaccuracy in information and production of non-existent or erroneous 
      references presents a challenge for researchers and clinicians who may 
      inadvertently employ such information in their research or healthcare. Therefore, 
      clinicians should exercise caution when using ChatGPT for these purposes.
CI  - © 2023 Royal Australasian College of Surgeons.
FAU - Gracias, Dylan
AU  - Gracias D
AUID- ORCID: 0000-0002-6873-0144
AD  - Department of Surgery, Townsville Hospital, Townsville, Queensland, Australia.
FAU - Siu, Adrian
AU  - Siu A
AUID- ORCID: 0000-0001-8151-9805
AD  - Faculty of Medicine and Health, Central Clinical School, The University of 
      Sydney, Sydney, New South Wales, Australia.
AD  - Surgical Outcomes Research Centre (SOuRCe), Royal Prince Alfred Hospital, 
      Camperdown, New South Wales, Australia.
AD  - Concord Institute of Academic Surgery, Concord Hospital, Concord, New South 
      Wales, Australia.
FAU - Seth, Ishith
AU  - Seth I
AUID- ORCID: 0000-0001-5444-8925
AD  - Department of Surgery, Peninsula Health, Melbourne, Victoria, Australia.
AD  - Department of Surgery, Bendigo Health, Bendigo, Victoria, Australia.
FAU - Dooreemeah, Dilshad
AU  - Dooreemeah D
AD  - Department of Surgery, Bendigo Health, Bendigo, Victoria, Australia.
FAU - Lee, Angus
AU  - Lee A
AUID- ORCID: 0000-0002-2276-9924
AD  - Department of Surgery, Bendigo Health, Bendigo, Victoria, Australia.
LA  - eng
PT  - Journal Article
DEP - 20231019
PL  - Australia
TA  - ANZ J Surg
JT  - ANZ journal of surgery
JID - 101086634
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Appendicitis/drug therapy/surgery
MH  - Exercise
MH  - Health Facilities
MH  - Knowledge
OTO - NOTNLM
OT  - ChatGPT
OT  - appendicitis
OT  - large language model
OT  - management
EDAT- 2023/10/19 12:42
MHDA- 2024/03/20 06:45
CRDT- 2023/10/19 08:03
PHST- 2023/09/27 00:00 [revised]
PHST- 2023/08/05 00:00 [received]
PHST- 2023/10/02 00:00 [accepted]
PHST- 2024/03/20 06:45 [medline]
PHST- 2023/10/19 12:42 [pubmed]
PHST- 2023/10/19 08:03 [entrez]
AID - 10.1111/ans.18736 [doi]
PST - ppublish
SO  - ANZ J Surg. 2024 Mar;94(3):342-352. doi: 10.1111/ans.18736. Epub 2023 Oct 19.

PMID- 38113774
OWN - NLM
STAT- MEDLINE
DCOM- 20240315
LR  - 20240315
IS  - 1532-818X (Electronic)
IS  - 0196-0709 (Linking)
VI  - 45
IP  - 2
DP  - 2024 Mar-Apr
TI  - Risk stratification of thyroid nodules: Assessing the suitability of ChatGPT for 
      text-based analysis.
PG  - 104144
LID - S0196-0709(23)00358-7 [pii]
LID - 10.1016/j.amjoto.2023.104144 [doi]
AB  - PURPOSE: Accurate risk stratification of thyroid nodules is essential for optimal 
      patient management. This study aimed to assess the suitability of ChatGPT for 
      risk stratification of thyroid nodules using a text-based evaluation. METHODS: A 
      dataset was compiled comprising 50 anonymized clinical reports and associated 
      risk assessments for thyroid nodules. The Chat Generative Pre-trained Transformer 
      (ChatGPT) was used to classify sonographic patterns in accordance with the 
      Thyroid Imaging Reporting and Data System (TI-RADS). The model's performance was 
      assessed using various criteria, including sensitivity, specificity, and 
      accuracy. A comparative analysis was conducted, evaluating the model against 
      investigator-based risk stratification as well as histology. RESULTS: With an 
      overall agreement rate of 42&nbsp;% in comparison with examiner-based evaluation 
      (TI-RADS 1-5), the results show that ChatGPT has moderate potential for 
      predicting the risk of malignancy in thyroid nodules using text-based reports. 
      The chatbot model achieved a sensitivity of 86.7&nbsp;%, a specificity of 10.7&nbsp;%, and 
      an overall accuracy of 68&nbsp;% when distinguishing between low-risk (TI-RADS 2 and 
      3) and high-risk (TI-RADS 4 and 5) categories. Interrater reliability was 
      calculated with a Cohen's kappa of 0.686. CONCLUSION: This study highlights the 
      potential of ChatGPT in assisting clinicians with risk stratification of thyroid 
      nodules. The results suggest that ChatGPT can facilitate personalized treatment 
      decisions, although the agreement rate is still low. Further research and 
      validation studies are necessary to establish the clinical applicability and 
      generalizability of ChatGPT in routine practice. The integration of ChatGPT into 
      clinical workflows has the potential to enhance thyroid nodule risk assessment 
      and improve patient care.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Sievert, Matti
AU  - Sievert M
AD  - Department of Otorhinolaryngology, Head and Neck Surgery, Friedrich Alexander 
      University of Erlangen-Nuremberg, Erlangen University Hospital, Germany.
FAU - Conrad, Olaf
AU  - Conrad O
AD  - Department of Otorhinolaryngology, Head and Neck Surgery, Friedrich Alexander 
      University of Erlangen-Nuremberg, Erlangen University Hospital, Germany. 
      Electronic address: olafmconrad@gmail.com.
FAU - Mueller, Sarina Katrin
AU  - Mueller SK
AD  - Department of Otorhinolaryngology, Head and Neck Surgery, Friedrich Alexander 
      University of Erlangen-Nuremberg, Erlangen University Hospital, Germany.
FAU - Rupp, Robin
AU  - Rupp R
AD  - Department of Otorhinolaryngology, Head and Neck Surgery, Friedrich Alexander 
      University of Erlangen-Nuremberg, Erlangen University Hospital, Germany.
FAU - Balk, Matthias
AU  - Balk M
AD  - Department of Otorhinolaryngology, Head and Neck Surgery, Friedrich Alexander 
      University of Erlangen-Nuremberg, Erlangen University Hospital, Germany.
FAU - Richter, Daniel
AU  - Richter D
AD  - Department of Otorhinolaryngology, Head and Neck Surgery, Friedrich Alexander 
      University of Erlangen-Nuremberg, Erlangen University Hospital, Germany.
FAU - Mantsopoulos, Konstantinos
AU  - Mantsopoulos K
AD  - Department of Otorhinolaryngology, Head and Neck Surgery, Friedrich Alexander 
      University of Erlangen-Nuremberg, Erlangen University Hospital, Germany.
FAU - Iro, Heinrich
AU  - Iro H
AD  - Department of Otorhinolaryngology, Head and Neck Surgery, Friedrich Alexander 
      University of Erlangen-Nuremberg, Erlangen University Hospital, Germany.
FAU - Koch, Michael
AU  - Koch M
AD  - Department of Otorhinolaryngology, Head and Neck Surgery, Friedrich Alexander 
      University of Erlangen-Nuremberg, Erlangen University Hospital, Germany.
LA  - eng
PT  - Journal Article
DEP - 20231207
PL  - United States
TA  - Am J Otolaryngol
JT  - American journal of otolaryngology
JID - 8000029
SB  - IM
MH  - Humans
MH  - *Thyroid Nodule/diagnostic imaging/pathology
MH  - Reproducibility of Results
MH  - Retrospective Studies
MH  - Ultrasonography/methods
MH  - Risk Assessment
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - Risk stratification
OT  - Thyroid nodules
OT  - Ultrasound
EDAT- 2023/12/19 19:54
MHDA- 2024/03/15 06:43
CRDT- 2023/12/19 18:12
PHST- 2023/11/22 00:00 [received]
PHST- 2023/12/03 00:00 [accepted]
PHST- 2024/03/15 06:43 [medline]
PHST- 2023/12/19 19:54 [pubmed]
PHST- 2023/12/19 18:12 [entrez]
AID - S0196-0709(23)00358-7 [pii]
AID - 10.1016/j.amjoto.2023.104144 [doi]
PST - ppublish
SO  - Am J Otolaryngol. 2024 Mar-Apr;45(2):104144. doi: 10.1016/j.amjoto.2023.104144. 
      Epub 2023 Dec 7.

PMID- 38428889
OWN - NLM
STAT- MEDLINE
DCOM- 20240304
LR  - 20240304
IS  - 1536-5964 (Electronic)
IS  - 0025-7974 (Print)
IS  - 0025-7974 (Linking)
VI  - 103
IP  - 9
DP  - 2024 Mar 1
TI  - Comparison of the problem-solving performance of ChatGPT-3.5, ChatGPT-4, Bing 
      Chat, and Bard for the Korean emergency medicine board examination question bank.
PG  - e37325
LID - 10.1097/MD.0000000000037325 [doi]
LID - e37325
AB  - Large language models (LLMs) have been deployed in diverse fields, and the 
      potential for their application in medicine has been explored through numerous 
      studies. This study aimed to evaluate and compare the performance of ChatGPT-3.5, 
      ChatGPT-4, Bing Chat, and Bard for the Emergency Medicine Board Examination 
      question bank in the Korean language. Of the 2353 questions in the question bank, 
      150 questions were randomly selected, and 27 containing figures were excluded. 
      Questions that required abilities such as analysis, creative thinking, 
      evaluation, and synthesis were classified as higher-order questions, and those 
      that required only recall, memory, and factual information in response were 
      classified as lower-order questions. The answers and explanations obtained by 
      inputting the 123 questions into the LLMs were analyzed and compared. ChatGPT-4 
      (75.6%) and Bing Chat (70.7%) showed higher correct response rates than 
      ChatGPT-3.5 (56.9%) and Bard (51.2%). ChatGPT-4 showed the highest correct 
      response rate for the higher-order questions at 76.5%, and Bard and Bing Chat 
      showed the highest rate for the lower-order questions at 71.4%. The 
      appropriateness of the explanation for the answer was significantly higher for 
      ChatGPT-4 and Bing Chat than for ChatGPT-3.5 and Bard (75.6%, 68.3%, 52.8%, and 
      50.4%, respectively). ChatGPT-4 and Bing Chat outperformed ChatGPT-3.5 and Bard 
      in answering a random selection of Emergency Medicine Board Examination questions 
      in the Korean language.
CI  - Copyright © 2024 the Author(s). Published by Wolters Kluwer Health, Inc.
FAU - Lee, Go Un
AU  - Lee GU
AD  - Department of Emergency Medicine, Konkuk University Medical Center, Seoul, 
      Republic of Korea.
FAU - Hong, Dae Young
AU  - Hong DY
AUID- ORCID: 0000-0003-2209-7213
AD  - Department of Emergency Medicine, Konkuk University School of Medicine, Seoul, 
      Republic of Korea.
FAU - Kim, Sin Young
AU  - Kim SY
AD  - Department of Emergency Medicine, Konkuk University Medical Center, Seoul, 
      Republic of Korea.
FAU - Kim, Jong Won
AU  - Kim JW
AD  - Department of Emergency Medicine, Konkuk University School of Medicine, Seoul, 
      Republic of Korea.
FAU - Lee, Young Hwan
AU  - Lee YH
AD  - Department of Emergency Medicine, Konkuk University School of Medicine, Seoul, 
      Republic of Korea.
FAU - Park, Sang O
AU  - Park SO
AD  - Department of Emergency Medicine, Konkuk University School of Medicine, Seoul, 
      Republic of Korea.
FAU - Lee, Kyeong Ryong
AU  - Lee KR
AD  - Department of Emergency Medicine, Konkuk University School of Medicine, Seoul, 
      Republic of Korea.
LA  - eng
PT  - Comparative Study
PT  - Journal Article
PL  - United States
TA  - Medicine (Baltimore)
JT  - Medicine
JID - 2985248R
SB  - IM
MH  - *Emergency Medicine/education
MH  - Language
MH  - Republic of Korea
MH  - *Artificial Intelligence
MH  - *Educational Measurement
PMC - PMC10906566
COIS- The authors have no funding and conflicts of interest to disclose.
EDAT- 2024/03/02 10:44
MHDA- 2024/03/04 06:49
PMCR- 2024/03/01
CRDT- 2024/03/01 19:23
PHST- 2024/03/04 06:49 [medline]
PHST- 2024/03/02 10:44 [pubmed]
PHST- 2024/03/01 19:23 [entrez]
PHST- 2024/03/01 00:00 [pmc-release]
AID - 00005792-202403010-00048 [pii]
AID - 10.1097/MD.0000000000037325 [doi]
PST - ppublish
SO  - Medicine (Baltimore). 2024 Mar 1;103(9):e37325. doi: 10.1097/MD.0000000000037325.

PMID- 38448201
OWN - NLM
STAT- Publisher
LR  - 20240306
IS  - 1468-2079 (Electronic)
IS  - 0007-1161 (Linking)
DP  - 2024 Mar 6
TI  - Exploring AI-chatbots' capability to suggest surgical planning in ophthalmology: 
      ChatGPT versus Google Gemini analysis of retinal detachment cases.
LID - bjo-2023-325143 [pii]
LID - 10.1136/bjo-2023-325143 [doi]
AB  - BACKGROUND: We aimed to define the capability of three different publicly 
      available large language models, Chat Generative Pretrained Transformer 
      (ChatGPT-3.5), ChatGPT-4 and Google Gemini in analysing retinal detachment cases 
      and suggesting the best possible surgical planning. METHODS: Analysis of 54 
      retinal detachments records entered into ChatGPT and Gemini's interfaces. After 
      asking 'Specify what kind of surgical planning you would suggest and the eventual 
      intraocular tamponade.' and collecting the given answers, we assessed the level 
      of agreement with the common opinion of three expert vitreoretinal surgeons. 
      Moreover, ChatGPT and Gemini answers were graded 1-5 (from poor to excellent 
      quality), according to the Global Quality Score (GQS). RESULTS: After excluding 4 
      controversial cases, 50 cases were included. Overall, ChatGPT-3.5, ChatGPT-4 and 
      Google Gemini surgical choices agreed with those of vitreoretinal surgeons in 
      40/50 (80%), 42/50 (84%) and 35/50 (70%) of cases. Google Gemini was not able to 
      respond in five cases. Contingency analysis showed significant differences 
      between ChatGPT-4 and Gemini (p=0.03). ChatGPT's GQS were 3.9±0.8 and 4.2±0.7 for 
      versions 3.5 and 4, while Gemini scored 3.5±1.1. There was no statistical 
      difference between the two ChatGPTs (p=0.22), while both outperformed Gemini 
      scores (p=0.03 and p=0.002, respectively). The main source of error was 
      endotamponade choice (14% for ChatGPT-3.5 and 4, and 12% for Google Gemini). Only 
      ChatGPT-4 was able to suggest a combined phacovitrectomy approach. CONCLUSION: In 
      conclusion, Google Gemini and ChatGPT evaluated vitreoretinal patients' records 
      in a coherent manner, showing a good level of agreement with expert surgeons. 
      According to the GQS, ChatGPT's recommendations were much more accurate and 
      precise.
CI  - © Author(s) (or their employer(s)) 2024. No commercial re-use. See rights and 
      permissions. Published by BMJ.
FAU - Carlà, Matteo Mario
AU  - Carlà MM
AUID- ORCID: 0000-0003-2979-1638
AD  - Ophthalmology Department, Catholic University "Sacro Cuore", Rome, Italy 
      mm.carla94@gmail.com.
AD  - Ophthalmology Department, Fondazione Policlinico Universitario "A. Gemelli", 
      IRCCS, Rome, Italy.
FAU - Gambini, Gloria
AU  - Gambini G
AUID- ORCID: 0000-0001-9500-049X
AD  - Ophthalmology Department, Catholic University "Sacro Cuore", Rome, Italy.
AD  - Ophthalmology Department, Fondazione Policlinico Universitario "A. Gemelli", 
      IRCCS, Rome, Italy.
FAU - Baldascino, Antonio
AU  - Baldascino A
AD  - Ophthalmology Department, Catholic University "Sacro Cuore", Rome, Italy.
AD  - Ophthalmology Department, Fondazione Policlinico Universitario "A. Gemelli", 
      IRCCS, Rome, Italy.
FAU - Giannuzzi, Federico
AU  - Giannuzzi F
AD  - Ophthalmology Department, Catholic University "Sacro Cuore", Rome, Italy.
AD  - Ophthalmology Department, Fondazione Policlinico Universitario "A. Gemelli", 
      IRCCS, Rome, Italy.
FAU - Boselli, Francesco
AU  - Boselli F
AD  - Ophthalmology Department, Catholic University "Sacro Cuore", Rome, Italy.
AD  - Ophthalmology Department, Fondazione Policlinico Universitario "A. Gemelli", 
      IRCCS, Rome, Italy.
FAU - Crincoli, Emanuele
AU  - Crincoli E
AUID- ORCID: 0000-0001-9996-9871
AD  - Ophthalmology Department, Catholic University "Sacro Cuore", Rome, Italy.
AD  - Ophthalmology Department, Fondazione Policlinico Universitario "A. Gemelli", 
      IRCCS, Rome, Italy.
FAU - D'Onofrio, Nicola Claudio
AU  - D'Onofrio NC
AD  - Ophthalmology Department, Catholic University "Sacro Cuore", Rome, Italy.
AD  - Ophthalmology Department, Fondazione Policlinico Universitario "A. Gemelli", 
      IRCCS, Rome, Italy.
FAU - Rizzo, Stanislao
AU  - Rizzo S
AUID- ORCID: 0000-0001-6302-063X
AD  - Ophthalmology Department, Catholic University "Sacro Cuore", Rome, Italy.
AD  - Ophthalmology Department, Fondazione Policlinico Universitario "A. Gemelli", 
      IRCCS, Rome, Italy.
LA  - eng
PT  - Journal Article
DEP - 20240306
PL  - England
TA  - Br J Ophthalmol
JT  - The British journal of ophthalmology
JID - 0421041
SB  - IM
OTO - NOTNLM
OT  - Medical Education
OT  - Ophthalmologic Surgical Procedures
OT  - Retina
OT  - Surveys and Questionnaires
OT  - Vitreous
COIS- Competing interests: None declared.
EDAT- 2024/03/07 00:42
MHDA- 2024/03/07 00:42
CRDT- 2024/03/06 21:13
PHST- 2023/12/31 00:00 [received]
PHST- 2024/02/16 00:00 [accepted]
PHST- 2024/03/07 00:42 [medline]
PHST- 2024/03/07 00:42 [pubmed]
PHST- 2024/03/06 21:13 [entrez]
AID - bjo-2023-325143 [pii]
AID - 10.1136/bjo-2023-325143 [doi]
PST - aheadofprint
SO  - Br J Ophthalmol. 2024 Mar 6:bjo-2023-325143. doi: 10.1136/bjo-2023-325143.

PMID- 38430368
OWN - NLM
STAT- Publisher
LR  - 20240302
IS  - 1654-7209 (Electronic)
IS  - 0044-7447 (Linking)
DP  - 2024 Mar 2
TI  - "In the end, the story of climate change was one of hope and redemption": 
      ChatGPT's narrative on global warming.
LID - 10.1007/s13280-024-01997-7 [doi]
AB  - AI chatbots such as ChatGPT help people produce texts. According to media 
      reporting, these texts are also used for educational purposes. Thus, AI 
      influences people's knowledge and perception of current issues. This paper 
      examines the narrative of ChatGPT's stories on climate change. Our explorative 
      analysis reveals that ChatGPT's stories on climate change show a relatively 
      uniform structure and similar content. Generally, the narrative is in line with 
      scientific knowledge on climate change; the stories convey no significant 
      misinformation. However, specific topics in current debates on global warming are 
      conspicuously missing. According to the ChatGPT narrative, humans as a species 
      are responsible for climate change and specific economic activities or actors 
      associated with carbon emissions play no role. Analogously, the social 
      structuration of vulnerability to climate impacts and issues of climate justice 
      are hardly addressed. ChatGPT's narrative consists of de-politicized stories that 
      are highly optimistic about technological progress.
CI  - © 2024. The Author(s).
FAU - Sommer, Bernd
AU  - Sommer B
AUID- ORCID: 0000-0001-6057-1567
AD  - Environmental Sociology and Transformation Research, TU Dortmund University, 
      Dortmund, Germany. bernd.sommer@tu-dortmund.de.
FAU - von Querfurth, Sarah
AU  - von Querfurth S
AUID- ORCID: 0000-0002-2338-5236
AD  - Department of Social Sciences, TU Dortmund University, Emil-Figge-Str. 50, 44221, 
      Dortmund, Germany.
LA  - eng
PT  - Journal Article
DEP - 20240302
PL  - Sweden
TA  - Ambio
JT  - Ambio
JID - 0364220
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence (AI)
OT  - ChatGPT
OT  - Climate change
OT  - Climate change narratives
OT  - Climate justice
OT  - De-politicization
EDAT- 2024/03/02 12:46
MHDA- 2024/03/02 12:46
CRDT- 2024/03/02 11:09
PHST- 2023/06/15 00:00 [received]
PHST- 2024/02/09 00:00 [accepted]
PHST- 2023/12/23 00:00 [revised]
PHST- 2024/03/02 12:46 [medline]
PHST- 2024/03/02 12:46 [pubmed]
PHST- 2024/03/02 11:09 [entrez]
AID - 10.1007/s13280-024-01997-7 [pii]
AID - 10.1007/s13280-024-01997-7 [doi]
PST - aheadofprint
SO  - Ambio. 2024 Mar 2. doi: 10.1007/s13280-024-01997-7.

PMID- 38248805
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240128
IS  - 2075-4426 (Print)
IS  - 2075-4426 (Electronic)
IS  - 2075-4426 (Linking)
VI  - 14
IP  - 1
DP  - 2024 Jan 18
TI  - Assessment of Quality and Readability of Information Provided by ChatGPT in 
      Relation to Anterior Cruciate Ligament Injury.
LID - 10.3390/jpm14010104 [doi]
LID - 104
AB  - The aim of our study was to evaluate the potential role of Artificial 
      Intelligence tools like ChatGPT in patient education. To do this, we assessed 
      both the quality and readability of information provided by ChatGPT 3.5 and 4 in 
      relation to Anterior Cruciate Ligament (ACL) injury and treatment. ChatGPT 3.5 
      and 4 were used to answer common patient queries relating to ACL injuries and 
      treatment. The quality of the information was assessed using the DISCERN 
      criteria. Readability was assessed with the use of seven readability formulae: 
      the Flesch-Kincaid Reading Grade Level, the Flesch Reading Ease Score, the Raygor 
      Estimate, the SMOG, the Fry, the FORCAST, and the Gunning Fog. The mean reading 
      grade level (RGL) was compared with the recommended 8th-grade reading level, the 
      mean RGL among adults in America. The perceived quality and mean RGL of answers 
      given by both ChatGPT 3.5 and 4 was also compared. Both ChatGPT 3.5 and 4 yielded 
      DISCERN scores suggesting "good" quality of information, with ChatGPT 4 slightly 
      outperforming 3.5. However, readability levels for both versions significantly 
      exceeded the average 8th-grade reading level for American patients. ChatGPT 3.5 
      had a mean RGL of 18.08, while the mean RGL of ChatGPT 4 was 17.9, exceeding the 
      average American reading grade level by 10.08 grade levels and 9.09 grade levels, 
      respectively. While ChatGPT can provide both reliable and good quality 
      information on ACL injuries and treatment options, the readability of the content 
      may limit its utility. Additionally, the consistent lack of source citation 
      represents a significant area of concern for patients and clinicians alike. If AI 
      is to play a role in patient education, it must reliably produce information 
      which is accurate, easily comprehensible, and clearly sourced.
FAU - Fahy, Stephen
AU  - Fahy S
AUID- ORCID: 0000-0002-4503-1842
AD  - Centrum für Muskuloskeletale Chirurgie, Charité Universitätsmedizin, 10117 
      Berlin, Germany.
FAU - Oehme, Stephan
AU  - Oehme S
AUID- ORCID: 0000-0001-5044-2657
AD  - Centrum für Muskuloskeletale Chirurgie, Charité Universitätsmedizin, 10117 
      Berlin, Germany.
FAU - Milinkovic, Danko
AU  - Milinkovic D
AD  - Centrum für Muskuloskeletale Chirurgie, Charité Universitätsmedizin, 10117 
      Berlin, Germany.
FAU - Jung, Tobias
AU  - Jung T
AD  - Centrum für Muskuloskeletale Chirurgie, Charité Universitätsmedizin, 10117 
      Berlin, Germany.
FAU - Bartek, Benjamin
AU  - Bartek B
AD  - Centrum für Muskuloskeletale Chirurgie, Charité Universitätsmedizin, 10117 
      Berlin, Germany.
LA  - eng
PT  - Journal Article
DEP - 20240118
PL  - Switzerland
TA  - J Pers Med
JT  - Journal of personalized medicine
JID - 101602269
PMC - PMC10817257
OTO - NOTNLM
OT  - ACL reconstruction surgery (ACL-R)
OT  - ChatGPT
OT  - DISCERN criteria
OT  - anterior cruciate ligament (ACL)
OT  - artificial intelligence (AI)
OT  - health literacy
OT  - natural language processing
OT  - orthopaedic injuries
OT  - patient education materials (PEMS)
OT  - readability
COIS- The authors declare no conflicts of interest.
EDAT- 2024/01/22 06:43
MHDA- 2024/01/22 06:44
PMCR- 2024/01/18
CRDT- 2024/01/22 04:44
PHST- 2023/11/03 00:00 [received]
PHST- 2023/12/17 00:00 [revised]
PHST- 2023/12/20 00:00 [accepted]
PHST- 2024/01/22 06:44 [medline]
PHST- 2024/01/22 06:43 [pubmed]
PHST- 2024/01/22 04:44 [entrez]
PHST- 2024/01/18 00:00 [pmc-release]
AID - jpm14010104 [pii]
AID - jpm-14-00104 [pii]
AID - 10.3390/jpm14010104 [doi]
PST - epublish
SO  - J Pers Med. 2024 Jan 18;14(1):104. doi: 10.3390/jpm14010104.

PMID- 38361368
OWN - NLM
STAT- Publisher
LR  - 20240216
IS  - 1541-3144 (Electronic)
IS  - 0194-2638 (Linking)
DP  - 2024 Feb 15
TI  - Evaluation of Informative Content on Cerebral Palsy in the Era of Artificial 
      Intelligence: The Value of ChatGPT.
PG  - 1-10
LID - 10.1080/01942638.2024.2316178 [doi]
AB  - AIMS: In addition to the popular search engines on the Internet, ChatGPT may 
      provide accurate and reliable health information. The aim of this study was to 
      examine whether ChatGPT's responses to frequently asked questions concerning 
      cerebral palsy (CP) by families were reliable and useful. METHODS: Google trends 
      were used to find the most frequently searched keywords for CP. Five independent 
      physiatrists assessed ChatGPT responses to 10 questions. Seven-point Likert-type 
      scales were used to rate information reliability and usefulness based on whether 
      the answer can be validated and is understandable. RESULTS: The median ratings 
      for reliability of information for each question varied from 2 (very unsafe) to 5 
      (relatively very reliable). The median rating was 4 (reliable) for four 
      questions. The median ratings for usefulness of information varied from 2 (very 
      little useful) to 5 (moderately useful). The median rating was 4 (partly useful) 
      for seven questions. CONCLUSION: Although ChatGPT appears promising as an 
      additional tool for informing family members of individuals with CP about medical 
      information, it should be emphasized that both consumers and health care 
      providers should be aware of the limitations of artificial intelligence-generated 
      information.
FAU - Ata, Ayşe Merve
AU  - Ata AM
AD  - Department of Physical Medicine and Rehabilitation, Ankara Bilkent City Hospital, 
      Physical Therapy and Rehabilitation Hospital, Ankara, Turkey.
FAU - Aras, Berke
AU  - Aras B
AD  - Department of Physical Medicine and Rehabilitation, Ankara Bilkent City Hospital, 
      Physical Therapy and Rehabilitation Hospital, Ankara, Turkey.
FAU - Yılmaz Taşdelen, Özlem
AU  - Yılmaz Taşdelen Ö
AD  - Department of Physical Medicine and Rehabilitation, Ankara Bilkent City Hospital, 
      Physical Therapy and Rehabilitation Hospital, Ankara, Turkey.
FAU - Çelik, Canan
AU  - Çelik C
AD  - Department of Physical Medicine and Rehabilitation, Ankara Bilkent City Hospital, 
      Physical Therapy and Rehabilitation Hospital, Ankara, Turkey.
FAU - Çulha, Canan
AU  - Çulha C
AD  - Department of Physical Medicine and Rehabilitation, Ankara Bilkent City Hospital, 
      Physical Therapy and Rehabilitation Hospital, Ankara, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20240215
PL  - England
TA  - Phys Occup Ther Pediatr
JT  - Physical &amp; occupational therapy in pediatrics
JID - 8109120
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - cerebral palsy
OT  - health information
OT  - large language model
EDAT- 2024/02/16 06:43
MHDA- 2024/02/16 06:43
CRDT- 2024/02/16 02:04
PHST- 2024/02/16 06:43 [medline]
PHST- 2024/02/16 06:43 [pubmed]
PHST- 2024/02/16 02:04 [entrez]
AID - 10.1080/01942638.2024.2316178 [doi]
PST - aheadofprint
SO  - Phys Occup Ther Pediatr. 2024 Feb 15:1-10. doi: 10.1080/01942638.2024.2316178.

PMID- 38149617
OWN - NLM
STAT- MEDLINE
DCOM- 20240226
LR  - 20240227
IS  - 1466-187X (Electronic)
IS  - 0142-159X (Linking)
VI  - 46
IP  - 3
DP  - 2024 Mar
TI  - The tools of the future are the challenges of today: The use of ChatGPT in 
      problem-based learning medical education.
PG  - 320-322
LID - 10.1080/0142159X.2023.2290997 [doi]
AB  - What is the educational challenge?Incorporation of large language model (LLM) or 
      generative artificial intelligence (AI) software poses a challenge to various 
      areas of medical education, including problem-based learning (PBL). LLMs, such as 
      ChatGPT, have incredible potential to transform educational systems and enhance 
      student learning outcomes when used responsibly.What are the proposed 
      solutions?ChatGPT can provide several ways to support students and assist 
      facilitators with course responsibilities. Here we address factors of 
      implementation and describe how ChatGPT can be responsibly utilized to support 
      key elements of PBL.How was the solution implemented?Providing reasonable access 
      is an essential element of novel software implementation. Additionally, training 
      for both faculty and staff is vital to foster responsible usage, provide 
      base-line proficiency, and guide users to critically evaluate the quality of 
      output.What lessons were learned that are relevant to a wider audience?The use of 
      LLMs or other generative AI is dramatically rising in the world. Appropriate and 
      conscientious incorporation of AI into educational programs can foster 
      responsible use and potentially enhance student learning.What are the next 
      steps?Assessment of learning outcomes, student self-efficacy, group dynamics, and 
      stakeholder feedback are required to measure the effects of ChatGPT in the PBL 
      curriculum. Additionally, software programs competitive with ChatGPT are 
      currently under development and will also need to be investigated for their 
      potential role in education.
FAU - Divito, Christopher B
AU  - Divito CB
AD  - College of Medicine, Lake Erie College of Osteopathic Medicine at Seton Hill, 
      Greensburg, PA, USA.
FAU - Katchikian, Bryan M
AU  - Katchikian BM
AD  - College of Medicine, Lake Erie College of Osteopathic Medicine at Seton Hill, 
      Greensburg, PA, USA.
FAU - Gruenwald, Jenna E
AU  - Gruenwald JE
AD  - College of Medicine, Lake Erie College of Osteopathic Medicine at Seton Hill, 
      Greensburg, PA, USA.
FAU - Burgoon, Jennifer M
AU  - Burgoon JM
AUID- ORCID: 0000-0001-9824-7965
AD  - College of Medicine, Lake Erie College of Osteopathic Medicine at Seton Hill, 
      Greensburg, PA, USA.
LA  - eng
PT  - Journal Article
DEP - 20231227
PL  - England
TA  - Med Teach
JT  - Medical teacher
JID - 7909593
SB  - IM
MH  - Humans
MH  - *Problem-Based Learning
MH  - Artificial Intelligence
MH  - Learning
MH  - Curriculum
MH  - *Education, Medical
OTO - NOTNLM
OT  - ChatGPT
OT  - generative artificial intelligence (AI)
OT  - large language models (LLM)
OT  - medical education
OT  - problem-based learning (PBL)
EDAT- 2023/12/27 12:41
MHDA- 2024/02/26 06:44
CRDT- 2023/12/27 07:49
PHST- 2024/02/26 06:44 [medline]
PHST- 2023/12/27 12:41 [pubmed]
PHST- 2023/12/27 07:49 [entrez]
AID - 10.1080/0142159X.2023.2290997 [doi]
PST - ppublish
SO  - Med Teach. 2024 Mar;46(3):320-322. doi: 10.1080/0142159X.2023.2290997. Epub 2023 
      Dec 27.

PMID- 37423349
OWN - NLM
STAT- MEDLINE
DCOM- 20231103
LR  - 20231113
IS  - 1558-349X (Electronic)
IS  - 1546-1440 (Linking)
VI  - 20
IP  - 10
DP  - 2023 Oct
TI  - Use of Large Language Models to Predict Neuroimaging.
PG  - 1004-1009
LID - S1546-1440(23)00483-0 [pii]
LID - 10.1016/j.jacr.2023.06.008 [doi]
AB  - PURPOSE: Large language models (LLMs) have demonstrated a level of competency 
      within the medical field. The aim of this study was to explore the ability of 
      LLMs to predict the best neuroradiologic imaging modality given specific clinical 
      presentations. In addition, the authors seek to determine if LLMs can outperform 
      an experienced neuroradiologist in this regard. METHODS: ChatGPT and Glass AI, a 
      health care-based LLM by Glass Health, were used. ChatGPT was prompted to rank 
      the three best neuroimaging modalities while taking the best responses from Glass 
      AI and the neuroradiologist. The responses were compared with the ACR 
      Appropriateness Criteria for 147 conditions. Clinical scenarios were passed into 
      each LLM twice to account for stochasticity. Each output was scored out of 3 on 
      the basis of the criteria. Partial scores were given for nonspecific answers. 
      RESULTS: ChatGPT and Glass AI scored 1.75 and 1.83, respectively, with no 
      statistically significant difference. The neuroradiologist scored 2.20, 
      significantly outperforming both LLMs. ChatGPT was also found to be the more 
      inconsistent of the two LLMs, with the score difference between both outputs 
      being statistically significant. Additionally, scores between different ranks 
      output by ChatGPT were statistically significant. CONCLUSIONS: LLMs perform well 
      in selecting appropriate neuroradiologic imaging procedures when prompted with 
      specific clinical scenarios. ChatGPT performed the same as Glass AI, suggesting 
      that with medical text training, ChatGPT could significantly improve its function 
      in this application. LLMs did not outperform an experienced neuroradiologist, 
      indicating the need for continued improvement in the medical context.
CI  - Copyright © 2023 American College of Radiology. Published by Elsevier Inc. All 
      rights reserved.
FAU - Nazario-Johnson, Lleayem
AU  - Nazario-Johnson L
AD  - Department of Diagnostic Imaging, The Warren Alpert Medical School of Brown 
      University/Rhode Island Hospital, Providence, Rhode Island.
FAU - Zaki, Hossam A
AU  - Zaki HA
AD  - Department of Diagnostic Imaging, The Warren Alpert Medical School of Brown 
      University/Rhode Island Hospital, Providence, Rhode Island. Electronic address: 
      hossam_zaki@brown.edu.
FAU - Tung, Glenn A
AU  - Tung GA
AD  - Associate Dean for Clinical Affairs, Department of Diagnostic Imaging, The Warren 
      Alpert Medical School of Brown University/Rhode Island Hospital, Providence, 
      Rhode Island.
LA  - eng
PT  - Journal Article
DEP - 20230708
PL  - United States
TA  - J Am Coll Radiol
JT  - Journal of the American College of Radiology : JACR
JID - 101190326
SB  - IM
MH  - Humans
MH  - *Language
MH  - *Neuroimaging
MH  - Radiologists
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - clinical decision making
EDAT- 2023/07/10 00:41
MHDA- 2023/11/03 06:44
CRDT- 2023/07/09 19:24
PHST- 2023/05/11 00:00 [received]
PHST- 2023/06/15 00:00 [revised]
PHST- 2023/06/16 00:00 [accepted]
PHST- 2023/11/03 06:44 [medline]
PHST- 2023/07/10 00:41 [pubmed]
PHST- 2023/07/09 19:24 [entrez]
AID - S1546-1440(23)00483-0 [pii]
AID - 10.1016/j.jacr.2023.06.008 [doi]
PST - ppublish
SO  - J Am Coll Radiol. 2023 Oct;20(10):1004-1009. doi: 10.1016/j.jacr.2023.06.008. 
      Epub 2023 Jul 8.

PMID- 38354285
OWN - NLM
STAT- Publisher
LR  - 20240214
IS  - 1557-9034 (Electronic)
IS  - 1092-6429 (Linking)
DP  - 2024 Feb 14
TI  - How Appropriate Are Recommendations of Online Chat-Based Artificial Intelligence 
      (ChatGPT) to Common Questions on Ventral Hernia Repair?
LID - 10.1089/lap.2023.0475 [doi]
AB  - ChatGPT is a conversational AI model developed by OpenAI to generate human-like 
      text based on the input it receives. ChatGPT has become increasingly popular, and 
      the general public may use this tool to ask questions about different medical 
      conditions. There is a lack of data to demonstrate ChatGPT is able to provide 
      reliable information on medical conditions. The aim of our study is to assess the 
      accuracy and appropriateness of ChatGPT answers to questions on ventral hernia 
      management.
FAU - Lima, Diego Laurentino
AU  - Lima DL
AUID- ORCID: 0000-0001-7383-1284
AD  - Department of Surgery, Montefiore Medical Center, Bronx, New York, USA.
FAU - Nogueira, Raquel
AU  - Nogueira R
AD  - Department of Surgery, Montefiore Medical Center, Bronx, New York, USA.
FAU - Liu, Jack
AU  - Liu J
AD  - Department of Surgery, Montefiore Medical Center, Bronx, New York, USA.
FAU - Claus, Christiano
AU  - Claus C
AD  - Minimally Invasive Surgery Department, Nossa Senhora das Graças Hospital, 
      Curitiba, Brazil.
FAU - Malcher, Flavio
AU  - Malcher F
AD  - Division of General Surgery, NYU Langone Health, New York, New York, USA.
FAU - Sreeramoju, Prashanth
AU  - Sreeramoju P
AD  - Department of Surgery, Montefiore Medical Center, Bronx, New York, USA.
FAU - Cavazzola, Leandro Totti
AU  - Cavazzola LT
AD  - Department of Surgery, Federal University of Rio Grande do Sul, Porto Alegre, 
      Brazil.
LA  - eng
PT  - Journal Article
DEP - 20240214
PL  - United States
TA  - J Laparoendosc Adv Surg Tech A
JT  - Journal of laparoendoscopic &amp; advanced surgical techniques. Part A
JID - 9706293
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - ventral hernia
EDAT- 2024/02/14 18:42
MHDA- 2024/02/14 18:42
CRDT- 2024/02/14 14:23
PHST- 2024/02/14 18:42 [medline]
PHST- 2024/02/14 18:42 [pubmed]
PHST- 2024/02/14 14:23 [entrez]
AID - 10.1089/lap.2023.0475 [doi]
PST - aheadofprint
SO  - J Laparoendosc Adv Surg Tech A. 2024 Feb 14. doi: 10.1089/lap.2023.0475.

PMID- 38486402
OWN - NLM
STAT- MEDLINE
DCOM- 20240318
LR  - 20240318
IS  - 1975-5937 (Electronic)
IS  - 1975-5937 (Linking)
VI  - 21
DP  - 2024
TI  - Opportunities, challenges, and future directions of large language models, 
      including ChatGPT in medical education: a systematic scoping review.
PG  - 6
LID - 10.3352/jeehp.2024.21.6 [doi]
AB  - BACKGROUND: ChatGPT is a large language model (LLM) based on artificial 
      intelligence (AI) capable of responding in multiple languages and generating 
      nuanced and highly complex responses. While ChatGPT holds promising applications 
      in medical education, its limitations and potential risks cannot be ignored. 
      METHODS: A scoping review was conducted for English articles discussing ChatGPT 
      in the context of medical education published after 2022. A literature search was 
      performed using PubMed/MEDLINE, Embase, and Web of Science databases, and 
      information was extracted from the relevant studies that were ultimately 
      included. RESULTS: ChatGPT exhibits various potential applications in medical 
      education, such as providing personalized learning plans and materials, creating 
      clinical practice simulation scenarios, and assisting in writing articles. 
      However, challenges associated with academic integrity, data accuracy, and 
      potential harm to learning were also highlighted in the literature. The paper 
      emphasizes certain recommendations for using ChatGPT, including the establishment 
      of guidelines. Based on the review, 3 key research areas were proposed: 
      cultivating the ability of medical students to use ChatGPT correctly, integrating 
      ChatGPT into teaching activities and processes, and proposing standards for the 
      use of AI by medical students. CONCLUSION: ChatGPT has the potential to transform 
      medical education, but careful consideration is required for its full 
      integration. To harness the full potential of ChatGPT in medical education, 
      attention should not only be given to the capabilities of AI but also to its 
      impact on students and teachers.
FAU - Xu, Xiaojun
AU  - Xu X
AD  - Division of Hematology/Oncology, Children's Hospital, Zhejiang University School 
      of Medicine, National Clinical Research Centre for Child Health, Zhejiang, China.
FAU - Chen, Yixiao
AU  - Chen Y
AD  - Division of Hematology/Oncology, Children's Hospital, Zhejiang University School 
      of Medicine, National Clinical Research Centre for Child Health, Zhejiang, China.
FAU - Miao, Jing
AU  - Miao J
AD  - Division of Hematology/Oncology, Children's Hospital, Zhejiang University School 
      of Medicine, National Clinical Research Centre for Child Health, Zhejiang, China.
LA  - eng
PT  - Journal Article
PT  - Systematic Review
DEP - 20240315
PL  - Korea (South)
TA  - J Educ Eval Health Prof
JT  - Journal of educational evaluation for health professions
JID - 101490061
SB  - IM
MH  - *Artificial Intelligence
MH  - Computer Simulation
MH  - *Education, Medical
MH  - Language
MH  - Learning
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Attention
OT  - Data accuracy
OT  - Medical education
OT  - Medical students
EDAT- 2024/03/15 06:44
MHDA- 2024/03/18 06:42
CRDT- 2024/03/15 01:52
PHST- 2024/01/12 00:00 [received]
PHST- 2024/03/05 00:00 [accepted]
PHST- 2024/03/18 06:42 [medline]
PHST- 2024/03/15 06:44 [pubmed]
PHST- 2024/03/15 01:52 [entrez]
AID - jeehp.2024.21.6 [pii]
AID - 10.3352/jeehp.2024.21.6 [doi]
PST - ppublish
SO  - J Educ Eval Health Prof. 2024;21:6. doi: 10.3352/jeehp.2024.21.6. Epub 2024 Mar 
      15.

PMID- 37962570
OWN - NLM
STAT- MEDLINE
DCOM- 20240119
LR  - 20240301
IS  - 1434-4726 (Electronic)
IS  - 0937-4477 (Linking)
VI  - 281
IP  - 2
DP  - 2024 Feb
TI  - Is artificial intelligence ready to replace specialist doctors entirely? ENT 
      specialists vs ChatGPT: 1-0, ball at the center.
PG  - 995-1023
LID - 10.1007/s00405-023-08321-1 [doi]
AB  - PURPOSE: The purpose of this study is to evaluate ChatGPT's responses to Ear, 
      Nose and Throat (ENT) clinical cases and compare them with the responses of ENT 
      specialists. METHODS: We have hypothesized 10 scenarios, based on ENT daily 
      experience, with the same primary symptom. We have constructed 20 clinical cases, 
      2 for each scenario. We described them to 3 ENT specialists and ChatGPT. The 
      difficulty of the clinical cases was assessed by the 5 ENT authors of this 
      article. The responses of ChatGPT were evaluated by the 5 ENT authors of this 
      article for correctness and consistency with the responses of the 3 ENT experts. 
      To verify the stability of ChatGPT's responses, we conducted the searches, always 
      from the same account, for 5 consecutive days. RESULTS: Among the 20 cases, 8 
      were rated as low complexity, 6 as moderate complexity and 6 as high complexity. 
      The overall mean correctness and consistency score of ChatGPT responses was 3.80 
      (SD 1.02) and 2.89 (SD 1.24), respectively. We did not find a statistically 
      significant difference in the average ChatGPT correctness and coherence score 
      according to case complexity. The total intraclass correlation coefficient (ICC) 
      for the stability of the correctness and consistency of ChatGPT was 0.763 (95% 
      confidence interval [CI] 0.553-0.895) and 0.837 (95% CI 0.689-0.927), 
      respectively. CONCLUSIONS: Our results revealed the potential usefulness of 
      ChatGPT in ENT diagnosis. The instability in responses and the inability to 
      recognise certain clinical elements are its main limitations.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Dallari, Virginia
AU  - Dallari V
AD  - Young Confederation of European ORL-HNS, Y-CEORL-HNS, Dublin, Ireland.
AD  - Unit of Otorhinolaryngology, Head &amp; Neck Department, University of Verona, 
      Piazzale L.A. Scuro 10, 37134, Verona, Italy.
FAU - Sacchetto, Andrea
AU  - Sacchetto A
AUID- ORCID: 0000-0002-0272-6652
AD  - Young Confederation of European ORL-HNS, Y-CEORL-HNS, Dublin, Ireland. 
      andrea.sacchetto89@gmail.com.
AD  - Department of Otolaryngology, Ospedale San Bortolo, AULSS 8 Berica, Vicenza, 
      Italy. andrea.sacchetto89@gmail.com.
FAU - Saetti, Roberto
AU  - Saetti R
AD  - Department of Otolaryngology, Ospedale San Bortolo, AULSS 8 Berica, Vicenza, 
      Italy.
FAU - Calabrese, Luca
AU  - Calabrese L
AD  - Department of Otorhinolaryngology-Head and Neck Surgery, Hospital of Bolzano 
      (SABES-ASDAA), Teaching Hospital of Paracelsus Medical University (PMU), 
      Bolzano-Bozen, Italy.
FAU - Vittadello, Fabio
AU  - Vittadello F
AD  - Explora-Research and Statistical Analysis, Padua, Italy.
FAU - Gazzini, Luca
AU  - Gazzini L
AD  - Young Confederation of European ORL-HNS, Y-CEORL-HNS, Dublin, Ireland.
AD  - Department of Otorhinolaryngology-Head and Neck Surgery, Hospital of Bolzano 
      (SABES-ASDAA), Teaching Hospital of Paracelsus Medical University (PMU), 
      Bolzano-Bozen, Italy.
LA  - eng
PT  - Journal Article
DEP - 20231114
PL  - Germany
TA  - Eur Arch Otorhinolaryngol
JT  - European archives of oto-rhino-laryngology : official journal of the European 
      Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the 
      German Society for Oto-Rhino-Laryngology - Head and Neck Surgery
JID - 9002937
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Pharynx
MH  - Neck
MH  - Nose
OTO - NOTNLM
OT  - ChatGPT
OT  - Machine learning
OT  - Natural language processing
OT  - Otolaryngology
OT  - Research
EDAT- 2023/11/14 12:42
MHDA- 2024/01/19 06:42
CRDT- 2023/11/14 11:03
PHST- 2023/08/17 00:00 [received]
PHST- 2023/10/27 00:00 [accepted]
PHST- 2024/01/19 06:42 [medline]
PHST- 2023/11/14 12:42 [pubmed]
PHST- 2023/11/14 11:03 [entrez]
AID - 10.1007/s00405-023-08321-1 [pii]
AID - 10.1007/s00405-023-08321-1 [doi]
PST - ppublish
SO  - Eur Arch Otorhinolaryngol. 2024 Feb;281(2):995-1023. doi: 
      10.1007/s00405-023-08321-1. Epub 2023 Nov 14.

PMID- 38249792
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240123
IS  - 2624-8212 (Electronic)
IS  - 2624-8212 (Linking)
VI  - 6
DP  - 2023
TI  - A longitudinal study on artificial intelligence adoption: understanding the 
      drivers of ChatGPT usage behavior change in higher education.
PG  - 1324398
LID - 10.3389/frai.2023.1324398 [doi]
LID - 1324398
AB  - As the field of artificial intelligence (AI) continues to progress, the use of 
      AI-powered chatbots, such as ChatGPT, in higher education settings has gained 
      significant attention. This paper addresses a well-defined problem pertaining to 
      the critical need for a comprehensive examination of students' ChatGPT adoption 
      in higher education. To examine such adoption, it is imperative to focus on 
      measuring actual user behavior. While measuring students' ChatGPT usage behavior 
      at a specific point in time can be valuable, a more holistic approach is 
      necessary to understand the temporal dynamics of AI adoption. To address this 
      need, a longitudinal survey was conducted, examining how students' ChatGPT usage 
      behavior changes over time among students, and unveiling the drivers of such 
      behavior change. The empirical examination of 222 Dutch higher education students 
      revealed a significant decline in students' ChatGPT usage behavior over an 8 
      month period. This period was defined by two distinct data collection phases: the 
      initial phase (T1) and a follow-up phase conducted 8 months later (T2). 
      Furthermore, the results demonstrate that changes in trust, emotional creepiness, 
      and Perceived Behavioral Control significantly predicted the observed change in 
      usage behavior. The findings of this research carry significant academic and 
      managerial implications, as they advance our comprehension of the temporal 
      aspects of AI adoption in higher education. The findings also provide actionable 
      guidance for AI developers and educational institutions seeking to optimize 
      student engagement with AI technologies.
CI  - Copyright © 2024 Polyportis.
FAU - Polyportis, Athanasios
AU  - Polyportis A
AD  - Department of Biotechnology, Faculty of Applied Sciences, Delft University of 
      Technology, Delft, Netherlands.
LA  - eng
PT  - Journal Article
DEP - 20240105
PL  - Switzerland
TA  - Front Artif Intell
JT  - Frontiers in artificial intelligence
JID - 101770551
PMC - PMC10797058
OTO - NOTNLM
OT  - ChatGPT
OT  - Perceived Behavioral Control
OT  - artificial intelligence adoption
OT  - chatbot in higher education
OT  - emotional creepiness
OT  - longitudinal survey
OT  - student behavior change
OT  - trust in artificial intelligence
COIS- The author declares that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2024/01/22 06:42
MHDA- 2024/01/22 06:43
PMCR- 2024/01/05
CRDT- 2024/01/22 05:05
PHST- 2023/10/19 00:00 [received]
PHST- 2023/12/13 00:00 [accepted]
PHST- 2024/01/22 06:43 [medline]
PHST- 2024/01/22 06:42 [pubmed]
PHST- 2024/01/22 05:05 [entrez]
PHST- 2024/01/05 00:00 [pmc-release]
AID - 10.3389/frai.2023.1324398 [doi]
PST - epublish
SO  - Front Artif Intell. 2024 Jan 5;6:1324398. doi: 10.3389/frai.2023.1324398. 
      eCollection 2023.

PMID- 38148925
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240125
IS  - 0972-978X (Print)
IS  - 0972-978X (Electronic)
IS  - 0972-978X (Linking)
VI  - 50
DP  - 2024 Apr
TI  - The ability of artificial intelligence tools to formulate orthopaedic clinical 
      decisions in comparison to human clinicians: An analysis of ChatGPT 3.5, ChatGPT 
      4, and Bard.
PG  - 1-7
LID - 10.1016/j.jor.2023.11.063 [doi]
AB  - BACKGROUND: Recent advancements in artificial intelligence (AI) have sparked 
      interest in its integration into clinical medicine and education. This study 
      evaluates the performance of three AI tools compared to human clinicians in 
      addressing complex orthopaedic decisions in real-world clinical cases. 
      QUESTIONS/PURPOSES: To evaluate the ability of commonly used AI tools to 
      formulate orthopaedic clinical decisions in comparison to human clinicians. 
      PATIENTS AND METHODS: The study used OrthoBullets Cases, a publicly available 
      clinical cases collaboration platform where surgeons from around the world choose 
      treatment options based on peer-reviewed standardised treatment polls. The 
      clinical cases cover various orthopaedic categories. Three AI tools, (ChatGPT 
      3.5, ChatGPT 4, and Bard), were evaluated. Uniform prompts were used to input 
      case information including questions relating to the case, and the AI tools' 
      responses were analysed for alignment with the most popular response, within 10%, 
      and within 20% of the most popular human responses. RESULTS: In total, 8 clinical 
      categories comprising of 97 questions were analysed. ChatGPT 4 demonstrated the 
      highest proportion of most popular responses (proportion of most popular 
      response: ChatGPT 4 68.0%, ChatGPT 3.5 40.2%, Bard 45.4%, P value&nbsp;&lt;&nbsp;0.001), 
      outperforming other AI tools. AI tools performed poorer in questions that were 
      considered controversial (where disagreement occurred in human responses). 
      Inter-tool agreement, as evaluated using Cohen's kappa coefficient, ranged from 
      0.201 (ChatGPT 4 vs. Bard) to 0.634 (ChatGPT 3.5 vs. Bard). However, AI tool 
      responses varied widely, reflecting a need for consistency in real-world clinical 
      applications. CONCLUSIONS: While AI tools demonstrated potential use in 
      educational contexts, their integration into clinical decision-making requires 
      caution due to inconsistent responses and deviations from peer consensus. Future 
      research should focus on specialised clinical AI tool development to maximise 
      utility in clinical decision-making. LEVEL OF EVIDENCE: IV.
CI  - © 2023 The Authors.
FAU - Agharia, Suzen
AU  - Agharia S
AD  - Department of Orthopaedic Surgery, St. Vincent's Hospital, Melbourne, Victoria, 
      Australia.
FAU - Szatkowski, Jan
AU  - Szatkowski J
AD  - Department of Orthopaedic Surgery, Indiana University Health Methodist Hospital, 
      Indianapolis, IN, USA.
FAU - Fraval, Andrew
AU  - Fraval A
AD  - Department of Orthopaedic Surgery, St. Vincent's Hospital, Melbourne, Victoria, 
      Australia.
FAU - Stevens, Jarrad
AU  - Stevens J
AD  - Department of Orthopaedic Surgery, St. Vincent's Hospital, Melbourne, Victoria, 
      Australia.
FAU - Zhou, Yushy
AU  - Zhou Y
AD  - Department of Orthopaedic Surgery, St. Vincent's Hospital, Melbourne, Victoria, 
      Australia.
LA  - eng
PT  - Journal Article
DEP - 20231201
PL  - India
TA  - J Orthop
JT  - Journal of orthopaedics
JID - 101233220
PMC - PMC10749221
COIS- All authors declare they have no conflicts of interest relating to this study.
EDAT- 2023/12/27 06:42
MHDA- 2023/12/27 06:43
PMCR- 2023/12/01
CRDT- 2023/12/27 03:40
PHST- 2023/11/18 00:00 [received]
PHST- 2023/11/22 00:00 [accepted]
PHST- 2023/12/27 06:43 [medline]
PHST- 2023/12/27 06:42 [pubmed]
PHST- 2023/12/27 03:40 [entrez]
PHST- 2023/12/01 00:00 [pmc-release]
AID - S0972-978X(23)00339-2 [pii]
AID - 10.1016/j.jor.2023.11.063 [doi]
PST - epublish
SO  - J Orthop. 2023 Dec 1;50:1-7. doi: 10.1016/j.jor.2023.11.063. eCollection 2024 
      Apr.

PMID- 38076813
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240212
DP  - 2023 Oct 31
TI  - Comparison of Large Language Models in Answering Immuno-Oncology Questions: A 
      Cross-Sectional Study.
LID - 2023.10.31.23297825 [pii]
LID - 10.1101/2023.10.31.23297825 [doi]
AB  - BACKGROUND: The capability of large language models (LLMs) to understand and 
      generate human-readable text has prompted the investigation of their potential as 
      educational and management tools for cancer patients and healthcare providers. 
      MATERIALS AND METHODS: We conducted a cross-sectional study aimed at evaluating 
      the ability of ChatGPT-4, ChatGPT-3.5, and Google Bard to answer questions 
      related to four domains of immuno-oncology (Mechanisms, Indications, Toxicities, 
      and Prognosis). We generated 60 open-ended questions (15 for each section). 
      Questions were manually submitted to LLMs, and responses were collected on June 
      30th, 2023. Two reviewers evaluated the answers independently. RESULTS: ChatGPT-4 
      and ChatGPT-3.5 answered all questions, whereas Google Bard answered only 53.3% 
      (p &lt;0.0001). The number of questions with reproducible answers was higher for 
      ChatGPT-4 (95%) and ChatGPT3.5 (88.3%) than for Google Bard (50%) (p &lt;0.0001). In 
      terms of accuracy, the number of answers deemed fully correct were 75.4%, 58.5%, 
      and 43.8% for ChatGPT-4, ChatGPT-3.5, and Google Bard, respectively (p = 0.03). 
      Furthermore, the number of responses deemed highly relevant was 71.9%, 77.4%, and 
      43.8% for ChatGPT-4, ChatGPT-3.5, and Google Bard, respectively (p = 0.04). 
      Regarding readability, the number of highly readable was higher for ChatGPT-4 and 
      ChatGPT-3.5 (98.1%) and (100%) compared to Google Bard (87.5%) (p = 0.02). 
      CONCLUSION: ChatGPT-4 and ChatGPT-3.5 are potentially powerful tools in 
      immuno-oncology, whereas Google Bard demonstrated relatively poorer performance. 
      However, the risk of inaccuracy or incompleteness in the responses was evident in 
      all three LLMs, highlighting the importance of expert-driven verification of the 
      outputs returned by these technologies.
FAU - Iannantuono, Giovanni Maria
AU  - Iannantuono GM
AD  - Genitourinary Malignancies Branch, Center for Cancer Research, National Cancer 
      Institute, National Institutes of Health, Bethesda, MD, United States.
FAU - Bracken-Clarke, Dara
AU  - Bracken-Clarke D
AD  - Center for Immuno-Oncology, Center for Cancer Research, National Cancer 
      Institute, National Institutes of Health, Bethesda, MD, United States.
FAU - Karzai, Fatima
AU  - Karzai F
AD  - Genitourinary Malignancies Branch, Center for Cancer Research, National Cancer 
      Institute, National Institutes of Health, Bethesda, MD, United States.
FAU - Choo-Wosoba, Hyoyoung
AU  - Choo-Wosoba H
AD  - Biostatistics and Data Management Section, Center for Cancer Research, National 
      Cancer Institute, National Institutes of Health, Bethesda, MD, United States.
FAU - Gulley, James L
AU  - Gulley JL
AD  - Center for Immuno-Oncology, Center for Cancer Research, National Cancer 
      Institute, National Institutes of Health, Bethesda, MD, United States.
FAU - Floudas, Charalampos S
AU  - Floudas CS
AD  - Center for Immuno-Oncology, Center for Cancer Research, National Cancer 
      Institute, National Institutes of Health, Bethesda, MD, United States.
LA  - eng
PT  - Preprint
DEP - 20231031
PL  - United States
TA  - medRxiv
JT  - medRxiv : the preprint server for health sciences
JID - 101767986
UIN - Oncologist. 2024 Feb 03;:. PMID: 38309720
PMC - PMC10705618
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Google Bard
OT  - Immuno-oncology
OT  - Large language models
COIS- Conflicts of Interest Authors declare no conflict of interest.
EDAT- 2023/12/11 12:43
MHDA- 2023/12/11 12:44
PMCR- 2023/12/08
CRDT- 2023/12/11 06:19
PHST- 2023/12/11 12:43 [pubmed]
PHST- 2023/12/11 12:44 [medline]
PHST- 2023/12/11 06:19 [entrez]
PHST- 2023/12/08 00:00 [pmc-release]
AID - 2023.10.31.23297825 [pii]
AID - 10.1101/2023.10.31.23297825 [doi]
PST - epublish
SO  - medRxiv [Preprint]. 2023 Oct 31:2023.10.31.23297825. doi: 
      10.1101/2023.10.31.23297825.

PMID- 37385548
OWN - NLM
STAT- MEDLINE
DCOM- 20230925
LR  - 20231027
IS  - 1097-6779 (Electronic)
IS  - 0016-5107 (Linking)
VI  - 98
IP  - 4
DP  - 2023 Oct
TI  - Harnessing language models for streamlined postcolonoscopy patient management: a 
      novel approach.
PG  - 639-641.e4
LID - S0016-5107(23)02675-5 [pii]
LID - 10.1016/j.gie.2023.06.025 [doi]
AB  - BACKGROUND AND AIMS: ChatGPT, an advanced language model, is increasingly used in 
      diverse fields, including medicine. This study explores using ChatGPT to optimize 
      postcolonoscopy management by providing guideline-based recommendations and 
      addressing low compliance rates and timing issues. METHODS: In this 
      proof-of-concept study, 20 clinical scenarios were prepared as structured reports 
      and free-text notes, and ChatGPT's responses were evaluated by 2 senior 
      gastroenterologists. Compliance with guidelines and accuracy were assessed, and 
      inter-rater agreement was calculated using Fleiss' kappa coefficient. RESULTS: 
      ChatGPT exhibited 90% compliance with guidelines and 85% accuracy, with a very 
      good inter-rater agreement (Fleiss' kappa coefficient of .84, P&nbsp;&lt; .01). ChatGPT 
      handled multiple variations and descriptions and crafted concise patient letters. 
      CONCLUSIONS: Results suggest that ChatGPT could aid healthcare providers in 
      making informed decisions and improve compliance with postcolonoscopy 
      surveillance guidelines. Future research should investigate integrating ChatGPT 
      into electronic health record systems and evaluating its effectiveness in 
      different healthcare settings and populations.
CI  - Copyright © 2023 American Society for Gastrointestinal Endoscopy. Published by 
      Elsevier Inc. All rights reserved.
FAU - Gorelik, Yuri
AU  - Gorelik Y
AD  - Gastroenterology Institute, Rambam Health Care Campus and Technion Institute of 
      Technology, Faculty of Medicine, Haifa, Israel.
FAU - Ghersin, Itai
AU  - Ghersin I
AD  - Gastroenterology Institute, Rambam Health Care Campus and Technion Institute of 
      Technology, Faculty of Medicine, Haifa, Israel.
FAU - Maza, Itay
AU  - Maza I
AD  - Gastroenterology Institute, Rambam Health Care Campus and Technion Institute of 
      Technology, Faculty of Medicine, Haifa, Israel.
FAU - Klein, Amir
AU  - Klein A
AD  - Gastroenterology Institute, Rambam Health Care Campus and Technion Institute of 
      Technology, Faculty of Medicine, Haifa, Israel.
LA  - eng
PT  - Journal Article
DEP - 20230627
PL  - United States
TA  - Gastrointest Endosc
JT  - Gastrointestinal endoscopy
JID - 0010505
SB  - IM
MH  - Humans
MH  - *Colonoscopy/methods
MH  - Guideline Adherence
MH  - *Gastroenterologists
COIS- Disclosure All authors disclosed no financial relationships.
EDAT- 2023/06/30 01:06
MHDA- 2023/09/25 06:42
CRDT- 2023/06/29 19:17
PHST- 2023/04/24 00:00 [received]
PHST- 2023/05/21 00:00 [revised]
PHST- 2023/06/08 00:00 [accepted]
PHST- 2023/09/25 06:42 [medline]
PHST- 2023/06/30 01:06 [pubmed]
PHST- 2023/06/29 19:17 [entrez]
AID - S0016-5107(23)02675-5 [pii]
AID - 10.1016/j.gie.2023.06.025 [doi]
PST - ppublish
SO  - Gastrointest Endosc. 2023 Oct;98(4):639-641.e4. doi: 10.1016/j.gie.2023.06.025. 
      Epub 2023 Jun 27.

PMID- 38507989
OWN - NLM
STAT- MEDLINE
DCOM- 20240402
LR  - 20240402
IS  - 1872-6968 (Electronic)
IS  - 0303-8467 (Linking)
VI  - 239
DP  - 2024 Apr
TI  - Chat-GPT on brain tumors: An examination of Artificial Intelligence/Machine 
      Learning's ability to provide diagnoses and treatment plans for example 
      neuro-oncology cases.
PG  - 108238
LID - S0303-8467(24)00125-2 [pii]
LID - 10.1016/j.clineuro.2024.108238 [doi]
AB  - OBJECTIVE: Assess the capabilities of ChatGPT-3.5 and 4 to provide accurate 
      diagnoses, treatment options, and treatment plans for brain tumors in example 
      neuro-oncology cases. METHODS: ChatGPT-3.5 and 4 were provided with twenty 
      example neuro-oncology cases of brain tumors, all selected from medical 
      textbooks. The artificial intelligence programs were asked to give a diagnosis, 
      treatment option, and treatment plan for each of these twenty example cases. Team 
      members first determined in which cases ChatGPT-3.5 and 4 provided the correct 
      diagnosis or treatment plan. Twenty neurosurgeons from the researchers' 
      institution then independently rated the diagnoses, treatment options, and 
      treatment plans provided by both artificial intelligence programs for each of the 
      twenty example cases, on a scale of one to ten, with ten being the highest score. 
      To determine whether the difference between the scores of ChatGPT-3.5 and 4 was 
      statistically significant, a paired t-test was conducted for the average scores 
      given to the programs for each example case. RESULTS: In the initial analysis of 
      correct responses, ChatGPT-4 had an accuracy of 85% for its diagnoses of example 
      brain tumors and an accuracy of 75% for its provided treatment plans, while 
      ChatGPT-3.5 only had an accuracy of 65% and 10%, respectively. The average scores 
      given by the twenty independent neurosurgeons to ChatGPT-4 for its accuracy of 
      diagnosis, provided treatment options, and provided treatment plan were 8.3, 8.4, 
      and 8.5 out of 10, respectively, while ChatGPT-3.5's average scores for these 
      categories of assessment were 5.9, 5.7, and 5.7. These differences in average 
      score are statistically significant on a paired t-test, with a p-value of less 
      than 0.001 for each difference. CONCLUSIONS: ChatGPT-4 demonstrates great promise 
      as a diagnostic tool for brain tumors in neuro-oncology, as attested to by the 
      program's performance in this study and its assessment by surveyed neurosurgeon 
      reviewers.
CI  - Copyright © 2024 Elsevier B.V. All rights reserved.
FAU - Kozel, Giovanni
AU  - Kozel G
AD  - Warren Alpert Medical School, Brown University,&nbsp;Providence,&nbsp;RI,&nbsp;USA.
FAU - Gurses, Muhammet Enes
AU  - Gurses ME
AD  - Department of Neurosurgery, Miller School of Medicine, University of Miami, 
      Miami, FL, USA. Electronic address: megursesmd@gmail.com.
FAU - Gecici, Neslihan Nisa
AU  - Gecici NN
AD  - Hacettepe University School of Medicine, Ankara, Turkey.
FAU - Gökalp, Elif
AU  - Gökalp E
AD  - Ankara University School of Medicine, Ankara, Turkey.
FAU - Bahadir, Siyar
AU  - Bahadir S
AD  - Feinstein Institute, New York, USA.
FAU - Merenzon, Martin A
AU  - Merenzon MA
AD  - Department of Neurosurgery, Yale University School of Medicine, New Haven, CT, 
      USA.
FAU - Shah, Ashish H
AU  - Shah AH
AD  - Department of Neurosurgery, Miller School of Medicine, University of Miami, 
      Miami, FL, USA.
FAU - Komotar, Ricardo J
AU  - Komotar RJ
AD  - Department of Neurosurgery, Miller School of Medicine, University of Miami, 
      Miami, FL, USA.
FAU - Ivan, Michael E
AU  - Ivan ME
AD  - Department of Neurosurgery, Miller School of Medicine, University of Miami, 
      Miami, FL, USA.
LA  - eng
PT  - Journal Article
DEP - 20240309
PL  - Netherlands
TA  - Clin Neurol Neurosurg
JT  - Clinical neurology and neurosurgery
JID - 7502039
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Brain Neoplasms/diagnosis/therapy
MH  - Neurosurgeons
MH  - Research Personnel
MH  - Machine Learning
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT 3.5
OT  - ChatGPT 4
OT  - Neurosurgery
OT  - Neurosurgical Treatment
COIS- Declaration of Competing Interest The authors report no personal, financial, or 
      institutional interest in the materials or devices described in this article.
EDAT- 2024/03/21 00:43
MHDA- 2024/04/02 06:45
CRDT- 2024/03/20 19:10
PHST- 2024/03/05 00:00 [received]
PHST- 2024/03/07 00:00 [accepted]
PHST- 2024/04/02 06:45 [medline]
PHST- 2024/03/21 00:43 [pubmed]
PHST- 2024/03/20 19:10 [entrez]
AID - S0303-8467(24)00125-2 [pii]
AID - 10.1016/j.clineuro.2024.108238 [doi]
PST - ppublish
SO  - Clin Neurol Neurosurg. 2024 Apr;239:108238. doi: 10.1016/j.clineuro.2024.108238. 
      Epub 2024 Mar 9.

PMID- 37701430
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230915
IS  - 2949-9127 (Electronic)
IS  - 2949-9127 (Linking)
VI  - 1
DP  - 2023
TI  - The potential of chatbots in chronic venous disease patient management.
LID - 100019 [pii]
LID - 10.1016/j.jvsvi.2023.100019 [doi]
AB  - OBJECTIVE: Health care providers and recipients have been using artificial 
      intelligence and its subfields, such as natural language processing and machine 
      learning technologies, in the form of search engines to obtain medical 
      information for some time now. Although a search engine returns a ranked list of 
      webpages in response to a query and allows the user to obtain information from 
      those links directly, ChatGPT has elevated the interface between humans with 
      artificial intelligence by attempting to provide relevant information in a 
      human-like textual conversation. This technology is being adopted rapidly and has 
      enormous potential to impact various aspects of health care, including patient 
      education, research, scientific writing, pre-visit/post-visit queries, 
      documentation assistance, and more. The objective of this study is to assess 
      whether chatbots could assist with answering patient questions and electronic 
      health record inbox management. METHODS: We devised two questionnaires: (1) 
      administrative and non-complex medical questions (based on actual inbox 
      questions); and (2) complex medical questions on the topic of chronic venous 
      disease. We graded the performance of publicly available chatbots regarding their 
      potential to assist with electronic health record inbox management. The study was 
      graded by an internist and a vascular medicine specialist independently. RESULTS: 
      On administrative and non-complex medical questions, ChatGPT 4.0 performed better 
      than ChatGPT 3.5. ChatGPT 4.0 received a grade of 1 on all the questions: 20 of 
      20 (100%). ChatGPT 3.5 received a grade of 1 on 14 of 20 questions (70%), grade 2 
      on 4 of 16 questions (20%), grade 3 on 0 questions (0%), and grade 4 on 2/20 
      questions (10%). On complex medical questions, ChatGPT 4.0 performed the best. 
      ChatGPT 4.0 received a grade of 1 on 15 of 20 questions (75%), grade 2 on 2 of 20 
      questions (10%), grade 3 on 2 of 20 questions (10%), and grade 4 on 1 of 20 
      questions (5%). ChatGPT 3.5 received a grade of 1 on 9 of 20 questions (45%), 
      grade 2 on 4 of 20 questions (20%), grade 3 on 4 of 20 questions (20%), and grade 
      4 on 3 of 20 questions (15%). Clinical Camel received a grade of 1 on 0 of 20 
      questions (0%), grade 2 on 5 of 20 questions (25%), grade 3 on 5 of 20 questions 
      (25%), and grade 4 on 10 of 20 questions (50%). CONCLUSIONS: Based on our 
      interactions with ChatGPT regarding the topic of chronic venous disease, it is 
      plausible that in the future, this technology may be used to assist with 
      electronic health record inbox management and offload medical staff. However, for 
      this technology to receive regulatory approval to be used for that purpose, it 
      will require extensive supervised training by subject experts, have guardrails to 
      prevent "hallucinations" and maintain confidentiality, and prove that it can 
      perform at a level comparable to (if not better than) humans. (JVS-Vascular 
      Insights 2023;1:100019.).
FAU - Athavale, Anand
AU  - Athavale A
AD  - Division of Vascular Surgery, Stanford University School of Medicine, Palo Alto.
FAU - Baier, Jonathan
AU  - Baier J
AD  - NextNext LLC, Lovettsville.
FAU - Ross, Elsie
AU  - Ross E
AD  - Division of Vascular Surgery, Stanford University School of Medicine, Palo Alto.
FAU - Fukaya, Eri
AU  - Fukaya E
AD  - Division of Vascular Surgery, Stanford University School of Medicine, Palo Alto.
LA  - eng
GR  - K01 HL148639/HL/NHLBI NIH HHS/United States
PT  - Journal Article
DEP - 20230619
PL  - United States
TA  - JVS Vasc Insights
JT  - JVS-vascular insights
JID - 9918663888006676
PMC - PMC10497234
MID - NIHMS1925830
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT 3.5
OT  - ChatGPT 4.0
OT  - Chronic venous disease
OT  - Clinical Camel
OT  - Electronic health record inbox management
OT  - Generative AI
COIS- Author conflict of interest: none.
EDAT- 2023/09/13 06:42
MHDA- 2023/09/13 06:43
PMCR- 2023/09/12
CRDT- 2023/09/13 04:05
PHST- 2023/09/13 06:43 [medline]
PHST- 2023/09/13 06:42 [pubmed]
PHST- 2023/09/13 04:05 [entrez]
PHST- 2023/09/12 00:00 [pmc-release]
AID - 100019 [pii]
AID - 10.1016/j.jvsvi.2023.100019 [doi]
PST - ppublish
SO  - JVS Vasc Insights. 2023;1:100019. doi: 10.1016/j.jvsvi.2023.100019. Epub 2023 Jun 
      19.

PMID- 37529688
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231106
IS  - 2234-943X (Print)
IS  - 2234-943X (Electronic)
IS  - 2234-943X (Linking)
VI  - 13
DP  - 2023
TI  - Evaluating large language models on a highly-specialized topic, radiation 
      oncology physics.
PG  - 1219326
LID - 10.3389/fonc.2023.1219326 [doi]
LID - 1219326
AB  - PURPOSE: We present the first study to investigate Large Language Models (LLMs) 
      in answering radiation oncology physics questions. Because popular exams like AP 
      Physics, LSAT, and GRE have large test-taker populations and ample test 
      preparation resources in circulation, they may not allow for accurately assessing 
      the true potential of LLMs. This paper proposes evaluating LLMs on a 
      highly-specialized topic, radiation oncology physics, which may be more pertinent 
      to scientific and medical communities in addition to being a valuable benchmark 
      of LLMs. METHODS: We developed an exam consisting of 100 radiation oncology 
      physics questions based on our expertise. Four LLMs, ChatGPT (GPT-3.5), ChatGPT 
      (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and 
      non-experts. The performance of ChatGPT (GPT-4) was further explored by being 
      asked to explain first, then answer. The deductive reasoning capability of 
      ChatGPT (GPT-4) was evaluated using a novel approach (substituting the correct 
      answer with "None of the above choices is the correct answer."). A majority vote 
      analysis was used to approximate how well each group could score when working 
      together. RESULTS: ChatGPT GPT-4 outperformed all other LLMs and medical 
      physicists, on average, with improved accuracy when prompted to explain before 
      answering. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its 
      answer choices across a number of trials, whether correct or incorrect, a 
      characteristic that was not observed in the human test groups or Bard (LaMDA). In 
      evaluating deductive reasoning ability, ChatGPT (GPT-4) demonstrated surprising 
      accuracy, suggesting the potential presence of an emergent ability. Finally, 
      although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not 
      allow for further improvement when scoring based on a majority vote across 
      trials. In contrast, a team of medical physicists were able to greatly outperform 
      ChatGPT (GPT-4) using a majority vote. CONCLUSION: This study suggests a great 
      potential for LLMs to work alongside radiation oncology experts as highly 
      knowledgeable assistants.
CI  - Copyright © 2023 Holmes, Liu, Zhang, Ding, Sio, McGee, Ashman, Li, Liu, Shen and 
      Liu.
FAU - Holmes, Jason
AU  - Holmes J
AD  - Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, United States.
FAU - Liu, Zhengliang
AU  - Liu Z
AD  - School of Computing, The University of Georgia, Athens, GA, United States.
FAU - Zhang, Lian
AU  - Zhang L
AD  - Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, United States.
FAU - Ding, Yuzhen
AU  - Ding Y
AD  - Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, United States.
FAU - Sio, Terence T
AU  - Sio TT
AD  - Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, United States.
FAU - McGee, Lisa A
AU  - McGee LA
AD  - Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, United States.
FAU - Ashman, Jonathan B
AU  - Ashman JB
AD  - Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, United States.
FAU - Li, Xiang
AU  - Li X
AD  - Department of Radiology, Massachusetts General Hospital, Boston, MA, United 
      States.
FAU - Liu, Tianming
AU  - Liu T
AD  - School of Computing, The University of Georgia, Athens, GA, United States.
FAU - Shen, Jiajian
AU  - Shen J
AD  - Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, United States.
FAU - Liu, Wei
AU  - Liu W
AD  - Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, United States.
LA  - eng
GR  - K25 CA168984/CA/NCI NIH HHS/United States
PT  - Journal Article
DEP - 20230717
PL  - Switzerland
TA  - Front Oncol
JT  - Frontiers in oncology
JID - 101568867
PMC - PMC10388568
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - large language model
OT  - medical physics
OT  - natural language processing
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2023/08/02 06:43
MHDA- 2023/08/02 06:44
PMCR- 2023/01/01
CRDT- 2023/08/02 04:00
PHST- 2023/05/08 00:00 [received]
PHST- 2023/06/12 00:00 [accepted]
PHST- 2023/08/02 06:44 [medline]
PHST- 2023/08/02 06:43 [pubmed]
PHST- 2023/08/02 04:00 [entrez]
PHST- 2023/01/01 00:00 [pmc-release]
AID - 10.3389/fonc.2023.1219326 [doi]
PST - epublish
SO  - Front Oncol. 2023 Jul 17;13:1219326. doi: 10.3389/fonc.2023.1219326. eCollection 
      2023.

PMID- 37502981
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240315
DP  - 2023 Jul 24
TI  - Evaluating Capabilities of Large Language Models: Performance of GPT4 on Surgical 
      Knowledge Assessments.
LID - 2023.07.16.23292743 [pii]
LID - 10.1101/2023.07.16.23292743 [doi]
AB  - BACKGROUND: Artificial intelligence (AI) has the potential to dramatically alter 
      healthcare by enhancing how we diagnosis and treat disease. One promising AI 
      model is ChatGPT, a large general-purpose language model trained by OpenAI. The 
      chat interface has shown robust, human-level performance on several professional 
      and academic benchmarks. We sought to probe its performance and stability over 
      time on surgical case questions. METHODS: We evaluated the performance of 
      ChatGPT-4 on two surgical knowledge assessments: the Surgical Council on Resident 
      Education (SCORE) and a second commonly used knowledge assessment, referred to as 
      Data-B. Questions were entered in two formats: open-ended and multiple choice. 
      ChatGPT output were assessed for accuracy and insights by surgeon evaluators. We 
      categorized reasons for model errors and the stability of performance on repeat 
      encounters. RESULTS: A total of 167 SCORE and 112 Data-B questions were presented 
      to the ChatGPT interface. ChatGPT correctly answered 71% and 68% of 
      multiple-choice SCORE and Data-B questions, respectively. For both open-ended and 
      multiple-choice questions, approximately two-thirds of ChatGPT responses 
      contained non-obvious insights. Common reasons for inaccurate responses included: 
      inaccurate information in a complex question (n=16, 36.4%); inaccurate 
      information in fact-based question (n=11, 25.0%); and accurate information with 
      circumstantial discrepancy (n=6, 13.6%). Upon repeat query, the answer selected 
      by ChatGPT varied for 36.4% of inaccurate questions; the response accuracy 
      changed for 6/16 questions. CONCLUSION: Consistent with prior findings, we 
      demonstrate robust near or above human-level performance of ChatGPT within the 
      surgical domain. Unique to this study, we demonstrate a substantial inconsistency 
      in ChatGPT responses with repeat query. This finding warrants future 
      consideration and presents an opportunity to further train these models to 
      provide safe and consistent responses. Without mental and/or conceptual models, 
      it is unclear whether language models such as ChatGPT would be able to safely 
      assist clinicians in providing care.
FAU - Beaulieu-Jones, Brendin R
AU  - Beaulieu-Jones BR
AD  - Department of Surgery, Beth Israel Deaconess Medical Center, Boston, MA.
AD  - Department of Biomedical Informatics, Harvard Medical School, Boston, MA.
FAU - Shah, Sahaj
AU  - Shah S
AD  - Geisinger Commonwealth School of Medicine, Scranton, PA.
FAU - Berrigan, Margaret T
AU  - Berrigan MT
AD  - Department of Surgery, Beth Israel Deaconess Medical Center, Boston, MA.
FAU - Marwaha, Jayson S
AU  - Marwaha JS
AD  - Division of Colorectal Surgery, National Taiwan University Hospital, Taipei, 
      Taiwan.
FAU - Lai, Shuo-Lun
AU  - Lai SL
AD  - Division of Colorectal Surgery, National Taiwan University Hospital, Taipei, 
      Taiwan.
FAU - Brat, Gabriel A
AU  - Brat GA
AD  - Department of Surgery, Beth Israel Deaconess Medical Center, Boston, MA.
AD  - Department of Biomedical Informatics, Harvard Medical School, Boston, MA.
LA  - eng
GR  - T15 LM007092/LM/NLM NIH HHS/United States
PT  - Preprint
DEP - 20230724
PL  - United States
TA  - medRxiv
JT  - medRxiv : the preprint server for health sciences
JID - 101767986
UIN - Surgery. 2024 Jan 20;:. PMID: 38246839
PMC - PMC10371188
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - language models
OT  - surgery
OT  - surgical education
EDAT- 2023/07/28 06:42
MHDA- 2023/07/28 06:43
PMCR- 2023/07/26
CRDT- 2023/07/28 04:25
PHST- 2023/07/28 06:42 [pubmed]
PHST- 2023/07/28 06:43 [medline]
PHST- 2023/07/28 04:25 [entrez]
PHST- 2023/07/26 00:00 [pmc-release]
AID - 2023.07.16.23292743 [pii]
AID - 10.1101/2023.07.16.23292743 [doi]
PST - epublish
SO  - medRxiv [Preprint]. 2023 Jul 24:2023.07.16.23292743. doi: 
      10.1101/2023.07.16.23292743.

PMID- 38485486
OWN - NLM
STAT- MEDLINE
DCOM- 20240318
LR  - 20240318
IS  - 2044-6055 (Electronic)
IS  - 2044-6055 (Linking)
VI  - 14
IP  - 3
DP  - 2024 Mar 14
TI  - Application of generative language models to orthopaedic practice.
PG  - e076484
LID - 10.1136/bmjopen-2023-076484 [doi]
LID - e076484
AB  - OBJECTIVE: To explore whether large language models (LLMs) Generated Pre-trained 
      Transformer (GPT)-3 and ChatGPT can write clinical letters and predict management 
      plans for common orthopaedic scenarios. DESIGN: Fifteen scenarios were generated 
      and ChatGPT and GPT-3 prompted to write clinical letters and separately generate 
      management plans for identical scenarios with plans removed. MAIN OUTCOME 
      MEASURES: Letters were assessed for readability using the Readable Tool. Accuracy 
      of letters and management plans were assessed by three independent orthopaedic 
      surgery clinicians. RESULTS: Both models generated complete letters for all 
      scenarios after single prompting. Readability was compared using Flesch-Kincade 
      Grade Level (ChatGPT: 8.77 (SD 0.918); GPT-3: 8.47 (SD 0.982)), Flesch 
      Readability Ease (ChatGPT: 58.2 (SD 4.00); GPT-3: 59.3 (SD 6.98)), Simple Measure 
      of Gobbledygook (SMOG) Index (ChatGPT: 11.6 (SD 0.755); GPT-3: 11.4 (SD 1.01)), 
      and reach (ChatGPT: 81.2%; GPT-3: 80.3%). ChatGPT produced more accurate letters 
      (8.7/10 (SD 0.60) vs 7.3/10 (SD 1.41), p=0.024) and management plans (7.9/10 (SD 
      0.63) vs 6.8/10 (SD 1.06), p&lt;0.001) than GPT-3. However, both LLMs sometimes 
      omitted key information or added additional guidance which was at worst 
      inaccurate. CONCLUSIONS: This study shows that LLMs are effective for generation 
      of clinical letters. With little prompting, they are readable and mostly 
      accurate. However, they are not consistent, and include inappropriate omissions 
      or insertions. Furthermore, management plans produced by LLMs are generic but 
      often accurate. In the future, a healthcare specific language model trained on 
      accurate and secure data could provide an excellent tool for increasing the 
      efficiency of clinicians through summarisation of large volumes of data into a 
      single clinical letter.
CI  - © Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No 
      commercial re-use. See rights and permissions. Published by BMJ.
FAU - Caterson, Jessica
AU  - Caterson J
AD  - London School of Hygiene &amp; Tropical Medicine, London, UK.
FAU - Ambler, Olivia
AU  - Ambler O
AD  - Plastic Surgery, Morriston Hospital, Swansea, Wales, UK.
FAU - Cereceda-Monteoliva, Nicholas
AU  - Cereceda-Monteoliva N
AD  - Guy's and St Thomas' Hospitals NHS Trust, London, London, UK.
FAU - Horner, Matthew
AU  - Horner M
AD  - Trauma Department, University Hospital of Wales, Cardiff, Cardiff, UK.
AD  - Trauma and Orthopaedic Surgery, University Hospital of Wales, Cardiff, Cardiff, 
      UK.
FAU - Jones, Andrew
AU  - Jones A
AD  - Orthopaedic Surgery, University Hospital of Wales, Cardiff, Cardiff, UK.
FAU - Poacher, Arwel Tomos
AU  - Poacher AT
AUID- ORCID: 0000-0002-4200-4929
AD  - Trauma Department, University Hospital of Wales, Cardiff, Cardiff, UK 
      drarwelpoacher@gmail.com.
AD  - School of Biosciences, Cardiff University, Cardiff, Cardiff, UK.
LA  - eng
PT  - Journal Article
DEP - 20240314
PL  - England
TA  - BMJ Open
JT  - BMJ open
JID - 101552874
RN  - 0 (Drugs, Generic)
SB  - IM
MH  - Humans
MH  - *Orthopedics
MH  - *Orthopedic Procedures
MH  - Drugs, Generic
MH  - Health Facilities
MH  - Language
PMC - PMC10941106
OTO - NOTNLM
OT  - HEALTH SERVICES ADMINISTRATION &amp; MANAGEMENT
OT  - Health informatics
OT  - ORTHOPAEDIC &amp; TRAUMA SURGERY
OT  - Organisational development
COIS- Competing interests: None declared.
EDAT- 2024/03/15 00:43
MHDA- 2024/03/18 06:44
PMCR- 2024/03/14
CRDT- 2024/03/14 22:33
PHST- 2024/03/18 06:44 [medline]
PHST- 2024/03/15 00:43 [pubmed]
PHST- 2024/03/14 22:33 [entrez]
PHST- 2024/03/14 00:00 [pmc-release]
AID - bmjopen-2023-076484 [pii]
AID - 10.1136/bmjopen-2023-076484 [doi]
PST - epublish
SO  - BMJ Open. 2024 Mar 14;14(3):e076484. doi: 10.1136/bmjopen-2023-076484.

PMID- 37822477
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231031
IS  - 0976-5662 (Print)
IS  - 2213-3445 (Electronic)
IS  - 0976-5662 (Linking)
VI  - 44
DP  - 2023 Sep
TI  - Perception of Chat Generative Pre-trained Transformer (Chat-GPT) AI tool amongst 
      MSK clinicians.
PG  - 102253
LID - 10.1016/j.jcot.2023.102253 [doi]
LID - 102253
AB  - BACKGROUND: Chat Generative Pre-trained Transformer (ChatGPT); an open access 
      artificial intelligence (AI) tool has been in the limelight with its ability to 
      respond to prompts, analyse data information using algorithms to augment 
      efficiency in day-to-day activities across a spectrum of human activities 
      including MSK/Orthopaedic science. PURPOSE OF THE STUDY: The purpose of this 
      cross-sectional survey has been to analyse the knowledge, understanding of the 
      role of Chat Generative Pre-trained Transformer (ChatGPT) and its implications in 
      clinical practice as well as research in medicine. MATERIAL &amp; METHODS: An online 
      cross-sectional survey of 10 questions (multiple choice and free text) was 
      circulated amongst orthopaedic surgeons, musculoskeletal radiologists and 
      Rheumatologists in India and UK, to evaluate perception of Chat Generative 
      Pre-trained Transformer (ChatGPT) AI Tool. RESULTS: We had 125 responses with 
      majority being aware of ChatGPT though a minority had used it. There was 
      consensus that its going have detrimental effect on workforce with majority of 
      the opinion that they would be used to create radiology reports. Mixed responses 
      were noted regarding the quality of research and role of ChatGPT being an 
      anonymous author. CONCLUSION: There is a considerable debate amongst clinicians 
      of orthopaedic, radiology and rheumatology -specialities. The attitudes are mixed 
      but mainly positive, although there are many concerns about the still-evolving 
      new technology. LEVEL OF STUDY: Diagnostic Study level 4.
CI  - © 2023 Delhi Orthopedic Association. All rights reserved.
FAU - Iyengar, Karthikeyan P
AU  - Iyengar KP
AD  - Department of Orthopaedics, Southport and Ormskirk Hospitals, Mersey and West 
      Lancashire Teaching NHS Trust, Southport, UK.
FAU - Yousef, Mina Malak Abed
AU  - Yousef MMA
AD  - Department of Orthopaedics, Southport and Ormskirk Hospitals, Mersey and West 
      Lancashire Teaching NHS Trust, Southport, UK.
FAU - Nune, Arvind
AU  - Nune A
AD  - Department of Rheumatology, Southport and Ormskirk Hospitals, Mersey and West 
      Lancahire Teaching NHS Trust, Southport, UK.
FAU - Sharma, Gaurav Kant
AU  - Sharma GK
AD  - Jaipur Institute of Pain and Sports Injuries (JIPSI), Jaipur, India.
FAU - Botchu, Rajesh
AU  - Botchu R
AD  - Department of Musculoskeletal Radiology, Royal Orthopedic Hospital, Birmingham, 
      UK.
LA  - eng
PT  - Journal Article
DEP - 20230923
PL  - India
TA  - J Clin Orthop Trauma
JT  - Journal of clinical orthopaedics and trauma
JID - 101559469
PMC - PMC10562840
OTO - NOTNLM
OT  - Algorithms
OT  - Artificial intelligence
OT  - Orthopaedics
OT  - Radiology
OT  - Surveys and questionnaires
COIS- The authors declare the following financial interests/personal relationships 
      which may be considered as potential competing interests:KP and RB are on 
      editorial board of JCOT
EDAT- 2023/10/12 06:43
MHDA- 2023/10/12 06:44
PMCR- 2024/09/23
CRDT- 2023/10/12 04:03
PHST- 2023/02/28 00:00 [received]
PHST- 2023/08/03 00:00 [revised]
PHST- 2023/09/21 00:00 [accepted]
PHST- 2024/09/23 00:00 [pmc-release]
PHST- 2023/10/12 06:44 [medline]
PHST- 2023/10/12 06:43 [pubmed]
PHST- 2023/10/12 04:03 [entrez]
AID - S0976-5662(23)00161-3 [pii]
AID - 102253 [pii]
AID - 10.1016/j.jcot.2023.102253 [doi]
PST - epublish
SO  - J Clin Orthop Trauma. 2023 Sep 23;44:102253. doi: 10.1016/j.jcot.2023.102253. 
      eCollection 2023 Sep.

PMID- 38037955
OWN - NLM
STAT- Publisher
LR  - 20231201
IS  - 1478-7083 (Electronic)
IS  - 0035-8843 (Linking)
DP  - 2023 Dec 1
TI  - Performance of large language models at the MRCS Part A: a tool for medical 
      education?
LID - 10.1308/rcsann.2023.0085 [doi]
AB  - INTRODUCTION: The Intercollegiate Membership of the Royal College of Surgeons 
      examination (MRCS) Part A assesses generic surgical sciences and applied 
      knowledge using 300 multiple-choice Single Best Answer items. Large Language 
      Models (LLMs) are trained on vast amounts of text to generate natural language 
      outputs, and applications in healthcare and medical education are rising. 
      METHODS: Two LLMs, ChatGPT (OpenAI) and Bard (Google AI), were tested using 300 
      questions from a popular MRCS Part A question bank without/with need for 
      justification (NJ/J). LLM outputs were scored according to accuracy, concordance 
      and insight. RESULTS: ChatGPT achieved 85.7%/84.3% accuracy for NJ/J encodings. 
      Bard achieved 64%/64.3% accuracy for NJ/J encodings. ChatGPT and Bard displayed 
      high levels of concordance for NJ (95.3%; 81.7%) and J (93.7%; 79.7%) encodings, 
      respectively. ChatGPT and Bard provided an insightful statement in &gt;98% and &gt;86% 
      outputs, respectively. DISCUSSION: This study demonstrates that ChatGPT achieves 
      passing-level accuracy at MRCS Part A, and both LLMs achieve high concordance and 
      provide insightful responses to test questions. Instances of clinically 
      inappropriate or inaccurate decision-making, incomplete appreciation of nuanced 
      clinical scenarios and utilisation of out-of-date guidance was, however, noted. 
      LLMs are accessible and time-efficient tools, access vast clinical knowledge, and 
      may reduce the emphasis on factual recall in medical education and assessment. 
      CONCLUSION: ChatGPT achieves passing-level accuracy for MRCS Part A with 
      concordant and insightful outputs. Future applications of LLMs in healthcare must 
      be cautious of hallucinations and incorrect reasoning but have the potential to 
      develop AI-supported clinicians.
FAU - Yiu, A
AU  - Yiu A
AD  - King's College Hospital NHS Foundation Trust, UK.
AD  - Imperial College London, UK.
FAU - Lam, K
AU  - Lam K
AD  - Imperial College London, UK.
LA  - eng
PT  - Journal Article
DEP - 20231201
PL  - England
TA  - Ann R Coll Surg Engl
JT  - Annals of the Royal College of Surgeons of England
JID - 7506860
SB  - IM
OTO - NOTNLM
OT  - Bard
OT  - ChatGPT
OT  - Education
OT  - Examination
OT  - Large language model
OT  - MRCS
EDAT- 2023/12/01 12:41
MHDA- 2023/12/01 12:41
CRDT- 2023/12/01 06:34
PHST- 2023/12/01 12:41 [medline]
PHST- 2023/12/01 12:41 [pubmed]
PHST- 2023/12/01 06:34 [entrez]
AID - 10.1308/rcsann.2023.0085 [doi]
PST - aheadofprint
SO  - Ann R Coll Surg Engl. 2023 Dec 1. doi: 10.1308/rcsann.2023.0085.

PMID- 37923639
OWN - NLM
STAT- MEDLINE
DCOM- 20231205
LR  - 20231205
IS  - 1877-1300 (Electronic)
IS  - 1877-1297 (Linking)
VI  - 15
IP  - 12
DP  - 2023 Dec
TI  - Exploratory study on student perception on the use of chat AI in process-driven 
      problem-based learning.
PG  - 1017-1025
LID - S1877-1297(23)00283-6 [pii]
LID - 10.1016/j.cptl.2023.10.001 [doi]
AB  - INTRODUCTION: With the increasing prevalence of artificial intelligence (AI) 
      technology, it is imperative to investigate its influence on education and the 
      resulting impact on student learning outcomes. This includes exploring the 
      potential application of AI in process-driven problem-based learning (PDPBL). 
      This study aimed to investigate the perceptions of students towards the use of 
      ChatGPT) build on GPT-3.5 in PDPBL in the Bachelor of Pharmacy program. METHODS: 
      Eighteen students with prior experience in traditional PDPBL processes 
      participated in the study, divided into three groups to perform PDPBL sessions 
      with various triggers from pharmaceutical chemistry, pharmaceutics, and clinical 
      pharmacy fields, while utilizing chat AI provided by ChatGPT to assist with data 
      searching and problem-solving. Questionnaires were used to collect data on the 
      impact of ChatGPT on students' satisfaction, engagement, participation, and 
      learning experience during the PBL sessions. RESULTS: The survey revealed that 
      ChatGPT improved group collaboration and engagement during PDPBL, while 
      increasing motivation and encouraging more questions. Nevertheless, some students 
      encountered difficulties understanding ChatGPT's information and questioned its 
      reliability and credibility. Despite these challenges, most students saw 
      ChatGPT's potential to eventually replace traditional information-seeking 
      methods. CONCLUSIONS: The study suggests that ChatGPT has the potential to 
      enhance PDPBL in pharmacy education. However, further research is needed to 
      examine the validity and reliability of the information provided by ChatGPT, and 
      its impact on a larger sample size.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Hamid, Hazrina
AU  - Hamid H
AD  - Faculty of Pharmacy, Lincoln University College, 12-18, Jalan SS 6/12, 47301 
      Petaling Jaya, Selangor Darul Ehsan, Malaysia.
FAU - Zulkifli, Khadjizah
AU  - Zulkifli K
AD  - Faculty of Pharmacy, Lincoln University College, 12-18, Jalan SS 6/12, 47301 
      Petaling Jaya, Selangor Darul Ehsan, Malaysia.
FAU - Naimat, Faiza
AU  - Naimat F
AD  - Department of Pharmacy, Malaysia National Heart Institute College, 145, Jalan Tun 
      Razak, 50400 Kuala Lumpur, Malaysia.
FAU - Che Yaacob, Nor Liana
AU  - Che Yaacob NL
AD  - Faculty of Pharmacy, University Sultan Zainal Abidin, 20400 Kuala Terengganu, 
      Terengganu Darul Iman, Malaysia.
FAU - Ng, Kwok Wen
AU  - Ng KW
AD  - Faculty of Pharmacy, Quest International University, 227, Jalan Raja Permaisuri 
      Bainun, 30250 Ipoh, Perak, Malaysia. Electronic address: kwokwen.ng@qiu.edu.my.
LA  - eng
PT  - Journal Article
DEP - 20231102
PL  - United States
TA  - Curr Pharm Teach Learn
JT  - Currents in pharmacy teaching &amp; learning
JID - 101560815
SB  - IM
MH  - Humans
MH  - *Problem-Based Learning
MH  - Artificial Intelligence
MH  - Reproducibility of Results
MH  - Students
MH  - *Pharmacy Service, Hospital
MH  - Perception
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Process-driven problem-based learning
OT  - Students' perception
COIS- Declaration of Competing Interests None.
EDAT- 2023/11/06 01:19
MHDA- 2023/12/05 12:42
CRDT- 2023/11/03 22:56
PHST- 2023/02/08 00:00 [received]
PHST- 2023/08/08 00:00 [revised]
PHST- 2023/10/17 00:00 [accepted]
PHST- 2023/12/05 12:42 [medline]
PHST- 2023/11/06 01:19 [pubmed]
PHST- 2023/11/03 22:56 [entrez]
AID - S1877-1297(23)00283-6 [pii]
AID - 10.1016/j.cptl.2023.10.001 [doi]
PST - ppublish
SO  - Curr Pharm Teach Learn. 2023 Dec;15(12):1017-1025. doi: 
      10.1016/j.cptl.2023.10.001. Epub 2023 Nov 2.

PMID- 38124357
OWN - NLM
STAT- Publisher
LR  - 20231221
IS  - 1537-1921 (Electronic)
IS  - 0898-4921 (Linking)
DP  - 2023 Dec 19
TI  - Utilizing Artificial Intelligence and Chat Generative Pretrained Transformer to 
      Answer Questions About Clinical Scenarios in Neuroanesthesiology.
LID - 10.1097/ANA.0000000000000949 [doi]
AB  - OBJECTIVE: We tested the ability of chat generative pretrained transformer 
      (ChatGPT), an artificial intelligence chatbot, to answer questions relevant to 
      scenarios covered in 3 clinical guidelines, published by the Society for 
      Neuroscience in Anesthesiology and Critical Care (SNACC), which has published 
      management guidelines: endovascular treatment of stroke, perioperative stroke 
      (Stroke), and care of patients undergoing complex spine surgery (Spine). METHODS: 
      Four neuroanesthesiologists independently assessed whether ChatGPT could apply 52 
      high-quality recommendations (HQRs) included in the 3 SNACC guidelines. HQRs were 
      deemed present in the ChatGPT responses if noted by at least 3 of the 4 
      reviewers. Reviewers also identified incorrect references, potentially harmful 
      recommendations, and whether ChatGPT cited the SNACC guidelines. RESULTS: The 
      overall reviewer agreement for the presence of HQRs in the ChatGPT answers ranged 
      from 0% to 100%. Only 4 of 52 (8%) HQRs were deemed present by at least 3 of the 
      4 reviewers after 5 generic questions, and 23 (44%) HQRs were deemed present 
      after at least 1 additional targeted question. Potentially harmful 
      recommendations were identified for each of the 3 clinical scenarios and ChatGPT 
      failed to cite the SNACC guidelines. CONCLUSIONS: The ChatGPT answers were open 
      to human interpretation regarding whether the responses included the HQRs. Though 
      targeted questions resulted in the inclusion of more HQRs than generic questions, 
      fewer than 50% of HQRs were noted even after targeted questions. This suggests 
      that ChatGPT should not currently be considered a reliable source of information 
      for clinical decision-making. Future iterations of ChatGPT may refine algorithms 
      to improve its reliability as a source of clinical information.
CI  - Copyright © 2023 Wolters Kluwer Health, Inc. All rights reserved.
FAU - Blacker, Samuel N
AU  - Blacker SN
AD  - Department of Anesthesiology, University of North Carolina at Chapel Hill.
FAU - Kang, Mia
AU  - Kang M
AD  - Department of Anesthesiology, University of North Carolina at Chapel Hill.
FAU - Chakraborty, Indranil
AU  - Chakraborty I
AD  - Department of Anesthesiology, University of Arkansas.
FAU - Chowdhury, Tumul
AU  - Chowdhury T
AD  - Department of Anesthesiology, University of Toronto.
FAU - Williams, James
AU  - Williams J
AD  - Department of Anesthesiology, University of North Carolina at Chapel Hill.
FAU - Lewis, Carol
AU  - Lewis C
AD  - Department of Anesthesiology, University of North Carolina at Chapel Hill.
FAU - Zimmer, Michael
AU  - Zimmer M
AD  - Department of Anesthesiology, University of North Carolina at Chapel Hill.
FAU - Wilson, Brad
AU  - Wilson B
AD  - Department of Anesthesiology, University of North Carolina at Chapel Hill.
FAU - Lele, Abhijit V
AU  - Lele AV
AD  - Department of Anesthesiology, University of Washington, Seattle, WA.
LA  - eng
PT  - Journal Article
DEP - 20231219
PL  - United States
TA  - J Neurosurg Anesthesiol
JT  - Journal of neurosurgical anesthesiology
JID - 8910749
SB  - IM
COIS- A.V.L. reports salary support from Life Center Northwest. The remaining authors 
      have no conflicts of interest to declare.
EDAT- 2023/12/21 06:43
MHDA- 2023/12/21 06:43
CRDT- 2023/12/21 01:23
PHST- 2023/08/14 00:00 [received]
PHST- 2023/11/20 00:00 [accepted]
PHST- 2023/12/21 06:43 [medline]
PHST- 2023/12/21 06:43 [pubmed]
PHST- 2023/12/21 01:23 [entrez]
AID - 00008506-990000000-00090 [pii]
AID - 10.1097/ANA.0000000000000949 [doi]
PST - aheadofprint
SO  - J Neurosurg Anesthesiol. 2023 Dec 19. doi: 10.1097/ANA.0000000000000949.

PMID- 38238871
OWN - NLM
STAT- Publisher
LR  - 20240118
IS  - 0972-9062 (Print)
IS  - 0972-9062 (Linking)
DP  - 2024 Jan 16
TI  - AI in the repurposing of potential herbs for filariasis therapy.
LID - 10.4103/0972-9062.393975 [doi]
AB  - BACKGROUND OBJECTIVES: The goal of this study was to see how well an AI language 
      model called Chat Generative Pre-trained Transformer (ChatGPT) assisted 
      healthcare personnel in selecting relevant medications for filariasis therapy. A 
      team of medical specialists and tropical medicine experts reviewed ChatGPT's 
      recommendations for ten hypothetical filariasis clinical situations. METHODS: The 
      purpose of this study was to look at the effectiveness of an AI language model 
      called Chat Generative Pre-trained Transformer (ChatGPT) in supporting healthcare 
      providers in picking appropriate drugs for filariasis treatment. Ten hypothetical 
      filariasis clinical cases were submitted to ChatGPT, and its recommendations were 
      evaluated by a panel of medical professionals and tropical medicine experts. 
      RESULTS: ChatGPT gave appropriate suggestions for potential medication 
      repurposing in filariasis treatment in all ten clinical scenarios. Its drug 
      recommendations were in line with current medical research and literature. 
      Despite the lack of particular treatment regimens, ChatGPT's general ideas proved 
      useful for healthcare practitioners, providing insights and updates on 
      prospective drug repurposing tactics. INTERPRETATION CONCLUSION: ChatGPT shows 
      promise as a useful method for repurposing drugs in the treatment of filariasis. 
      Its thorough and brief responses make it useful for finding possible 
      pharmacological candidates. However, it is critical to recognize ChatGPT's 
      limitations, such as the requirement for additional clinical information and the 
      inability to change therapy. Further research and development are required to 
      optimize its use in filariasis therapy settings.
CI  - Copyright © 2024 Copyright: © 2024 Journal of Vector Borne Diseases.
FAU - Wiwanitmkit, Somsri
AU  - Wiwanitmkit S
AD  - Private Academic and Editorial Consultant, Bangkok Thailand.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AD  - Center for global health research Saveetha medical college Saveetha Institute of 
      medical and technical sciences, India.
LA  - eng
PT  - Journal Article
DEP - 20240116
PL  - India
TA  - J Vector Borne Dis
JT  - Journal of vector borne diseases
JID - 101212761
SB  - IM
EDAT- 2024/01/19 00:42
MHDA- 2024/01/19 00:42
CRDT- 2024/01/18 23:45
PHST- 2023/09/20 00:00 [received]
PHST- 2023/11/20 00:00 [accepted]
PHST- 2024/01/19 00:42 [medline]
PHST- 2024/01/19 00:42 [pubmed]
PHST- 2024/01/18 23:45 [entrez]
AID - 01196045-990000000-00024 [pii]
AID - 10.4103/0972-9062.393975 [doi]
PST - aheadofprint
SO  - J Vector Borne Dis. 2024 Jan 16. doi: 10.4103/0972-9062.393975.

PMID- 38435597
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240305
IS  - 2376-5992 (Electronic)
IS  - 2376-5992 (Linking)
VI  - 10
DP  - 2024
TI  - A bilingual benchmark for evaluating large language models.
PG  - e1893
LID - 10.7717/peerj-cs.1893 [doi]
LID - e1893
AB  - This work introduces a new benchmark for the bilingual evaluation of large 
      language models (LLMs) in English and Arabic. While LLMs have transformed various 
      fields, their evaluation in Arabic remains limited. This work addresses this gap 
      by proposing a novel evaluation method for LLMs in both Arabic and English, 
      allowing for a direct comparison between the performance of the two languages. We 
      build a new evaluation dataset based on the General Aptitude Test (GAT), a 
      standardized test widely used for university admissions in the Arab world, that 
      we utilize to measure the linguistic capabilities of LLMs. We conduct several 
      experiments to examine the linguistic capabilities of ChatGPT and quantify how 
      much better it is at English than Arabic. We also examine the effect of changing 
      task descriptions from Arabic to English and vice-versa. In addition to that, we 
      find that fastText can surpass ChatGPT in finding Arabic word analogies. We 
      conclude by showing that GPT-4 Arabic linguistic capabilities are much better 
      than ChatGPT's Arabic capabilities and are close to ChatGPT's English 
      capabilities.
CI  - ©2024 Alkaoud.
FAU - Alkaoud, Mohamed
AU  - Alkaoud M
AD  - Department of Computer Science, College of Computer and Information Sciences, 
      King Saud University, Riyadh, Saudi Arabia.
LA  - eng
PT  - Journal Article
DEP - 20240229
PL  - United States
TA  - PeerJ Comput Sci
JT  - PeerJ. Computer science
JID - 101660598
PMC - PMC10909174
OTO - NOTNLM
OT  - Arabic NLP
OT  - ChatGPT
OT  - LLM evaluation
OT  - Large language models
OT  - Multilingual NLP
OT  - Natural language processing
COIS- The author declares there are no competing interests.
EDAT- 2024/03/04 06:43
MHDA- 2024/03/04 06:44
PMCR- 2024/02/29
CRDT- 2024/03/04 05:03
PHST- 2023/07/28 00:00 [received]
PHST- 2024/01/30 00:00 [accepted]
PHST- 2024/03/04 06:44 [medline]
PHST- 2024/03/04 06:43 [pubmed]
PHST- 2024/03/04 05:03 [entrez]
PHST- 2024/02/29 00:00 [pmc-release]
AID - cs-1893 [pii]
AID - 10.7717/peerj-cs.1893 [doi]
PST - epublish
SO  - PeerJ Comput Sci. 2024 Feb 29;10:e1893. doi: 10.7717/peerj-cs.1893. eCollection 
      2024.

PMID- 37713147
OWN - NLM
STAT- MEDLINE
DCOM- 20230918
LR  - 20230918
IS  - 1536-3708 (Electronic)
IS  - 0148-7043 (Linking)
VI  - 91
IP  - 4
DP  - 2023 Oct 1
TI  - Comparison of Patient Education Materials Generated by Chat Generative 
      Pre-Trained Transformer Versus Experts: An Innovative Way to Increase Readability 
      of Patient Education Materials.
PG  - 409-412
LID - 10.1097/SAP.0000000000003634 [doi]
AB  - INTRODUCTION: Improving patient education materials may improve patient outcomes. 
      This study aims to explore the possibility of generating patient education 
      materials with the assistance of a large language model, Chat Generative 
      Pre-Trained Transformer (ChatGPT). In addition, we compare the accuracy and 
      readability of ChatGPT-generated materials versus expert-generated materials. 
      METHODS: Patient education materials in implant-based breast reconstruction were 
      generated by experts and ChatGPT independently. Readability and accuracy of the 
      materials are the main outcomes. Readability of the materials was compared using 
      Flesch-Kincaid score. Accuracy of the materials generated by ChatGPT was 
      evaluated by 2 independent reviewers. Content errors are categorized into 
      information errors, statistical errors, and multiple errors (errors more than 2 
      types). RESULTS: The content generated by experts had higher readability. The 
      Flesch-Kincaid score is at the 7.5 grade for expert-generated materials, whereas 
      the content generated by ChatGPT is at the 10.5 grade (despite ChatGPT being 
      asked to generate content at the seventh grade level). The accuracy of 
      ChatGPT-generated content is 50%, with most errors being information errors. 
      ChatGPT often provides information about breast reduction or breast augmentation, 
      despite being asked specifically about breast reconstruction. Despite its 
      limitation, ChatGPT significantly reduced the time required to generate patient 
      education materials. Although it takes experts 1 month to generate patient 
      education materials, ChatGPT generates materials within 30 minutes. CONCLUSIONS: 
      ChatGPT can be a powerful starting tool to generate patient education materials. 
      However, its readability and accuracy still require improvements.
CI  - Copyright © 2023 Wolters Kluwer Health, Inc. All rights reserved.
FAU - Hung, Ya-Ching
AU  - Hung YC
FAU - Chaker, Sara C
AU  - Chaker SC
AD  - From the Department of Plastic Surgery, Vanderbilt University Medical Center, 
      Nashville, TN.
FAU - Sigel, Matthew
AU  - Sigel M
AD  - Vanderbilt University, Nashville, TN.
FAU - Saad, Mariam
AU  - Saad M
AD  - From the Department of Plastic Surgery, Vanderbilt University Medical Center, 
      Nashville, TN.
FAU - Slater, Elizabeth D
AU  - Slater ED
AD  - From the Department of Plastic Surgery, Vanderbilt University Medical Center, 
      Nashville, TN.
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Ann Plast Surg
JT  - Annals of plastic surgery
JID - 7805336
SB  - IM
MH  - Humans
MH  - *Comprehension
MH  - Patient Education as Topic
MH  - Language
MH  - *Mammaplasty
COIS- Conflicts of interest and sources of funding: none declared.
EDAT- 2023/09/15 12:44
MHDA- 2023/09/18 12:43
CRDT- 2023/09/15 11:17
PHST- 2023/09/18 12:43 [medline]
PHST- 2023/09/15 12:44 [pubmed]
PHST- 2023/09/15 11:17 [entrez]
AID - 00000637-202310000-00002 [pii]
AID - 10.1097/SAP.0000000000003634 [doi]
PST - ppublish
SO  - Ann Plast Surg. 2023 Oct 1;91(4):409-412. doi: 10.1097/SAP.0000000000003634.

PMID- 38309720
OWN - NLM
STAT- Publisher
LR  - 20240212
IS  - 1549-490X (Electronic)
IS  - 1083-7159 (Linking)
DP  - 2024 Feb 3
TI  - Comparison of Large Language Models in Answering Immuno-Oncology Questions: A 
      Cross-Sectional Study.
LID - oyae009 [pii]
LID - 10.1093/oncolo/oyae009 [doi]
AB  - BACKGROUND: The capability of large language models (LLMs) to understand and 
      generate human-readable text has prompted the investigation of their potential as 
      educational and management tools for patients with cancer and healthcare 
      providers. MATERIALS AND METHODS: We conducted a cross-sectional study aimed at 
      evaluating the ability of ChatGPT-4, ChatGPT-3.5, and Google Bard to answer 
      questions related to 4 domains of immuno-oncology (Mechanisms, Indications, 
      Toxicities, and Prognosis). We generated 60 open-ended questions (15 for each 
      section). Questions were manually submitted to LLMs, and responses were collected 
      on June 30, 2023. Two reviewers evaluated the answers independently. RESULTS: 
      ChatGPT-4 and ChatGPT-3.5 answered all questions, whereas Google Bard answered 
      only 53.3% (P &lt; .0001). The number of questions with reproducible answers was 
      higher for ChatGPT-4 (95%) and ChatGPT3.5 (88.3%) than for Google Bard (50%) 
      (P &lt; .0001). In terms of accuracy, the number of answers deemed fully correct 
      were 75.4%, 58.5%, and 43.8% for ChatGPT-4, ChatGPT-3.5, and Google Bard, 
      respectively (P = .03). Furthermore, the number of responses deemed highly 
      relevant was 71.9%, 77.4%, and 43.8% for ChatGPT-4, ChatGPT-3.5, and Google Bard, 
      respectively (P = .04). Regarding readability, the number of highly readable was 
      higher for ChatGPT-4 and ChatGPT-3.5 (98.1%) and (100%) compared to Google Bard 
      (87.5%) (P = .02). CONCLUSION: ChatGPT-4 and ChatGPT-3.5 are potentially powerful 
      tools in immuno-oncology, whereas Google Bard demonstrated relatively poorer 
      performance. However, the risk of inaccuracy or incompleteness in the responses 
      was evident in all 3 LLMs, highlighting the importance of expert-driven 
      verification of the outputs returned by these technologies.
CI  - Published by Oxford University Press 2024. This work is written by (a) US 
      Government employee(s) and is in the public domain in the US.
FAU - Iannantuono, Giovanni Maria
AU  - Iannantuono GM
AD  - Genitourinary Malignancies Branch, Center for Cancer Research, National Cancer 
      Institute, National Institutes of Health, Bethesda, MD, USA.
FAU - Bracken-Clarke, Dara
AU  - Bracken-Clarke D
AD  - Center for Immuno-Oncology, Center for Cancer Research, National Cancer 
      Institute, National Institutes of Health, Bethesda, MD, USA.
FAU - Karzai, Fatima
AU  - Karzai F
AD  - Genitourinary Malignancies Branch, Center for Cancer Research, National Cancer 
      Institute, National Institutes of Health, Bethesda, MD, USA.
FAU - Choo-Wosoba, Hyoyoung
AU  - Choo-Wosoba H
AD  - Biostatistics and Data Management Section, Center for Cancer Research, National 
      Cancer Institute, National Institutes of Health, Bethesda, MD, USA.
FAU - Gulley, James L
AU  - Gulley JL
AD  - Center for Immuno-Oncology, Center for Cancer Research, National Cancer 
      Institute, National Institutes of Health, Bethesda, MD, USA.
FAU - Floudas, Charalampos S
AU  - Floudas CS
AUID- ORCID: 0000-0002-0020-237X
AD  - Center for Immuno-Oncology, Center for Cancer Research, National Cancer 
      Institute, National Institutes of Health, Bethesda, MD, USA.
LA  - eng
GR  - NH/NIH HHS/United States
GR  - CA/NCI NIH HHS/United States
PT  - Journal Article
DEP - 20240203
PL  - England
TA  - Oncologist
JT  - The oncologist
JID - 9607837
SB  - IM
UOF - medRxiv. 2023 Oct 31;:. PMID: 38076813
OTO - NOTNLM
OT  - ChatGPT
OT  - Google Bard
OT  - artificial intelligence
OT  - immuno-oncology
OT  - large language models
EDAT- 2024/02/04 00:42
MHDA- 2024/02/04 00:42
CRDT- 2024/02/03 20:32
PHST- 2023/11/10 00:00 [received]
PHST- 2024/01/11 00:00 [accepted]
PHST- 2024/02/04 00:42 [medline]
PHST- 2024/02/04 00:42 [pubmed]
PHST- 2024/02/03 20:32 [entrez]
AID - 7600405 [pii]
AID - 10.1093/oncolo/oyae009 [doi]
PST - aheadofprint
SO  - Oncologist. 2024 Feb 3:oyae009. doi: 10.1093/oncolo/oyae009.

PMID- 37296802
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230612
IS  - 2075-4418 (Print)
IS  - 2075-4418 (Electronic)
IS  - 2075-4418 (Linking)
VI  - 13
IP  - 11
DP  - 2023 Jun 2
TI  - Evaluating the Utility of a Large Language Model in Answering Common Patients' 
      Gastrointestinal Health-Related Questions: Are We There Yet?
LID - 10.3390/diagnostics13111950 [doi]
LID - 1950
AB  - BACKGROUND AND AIMS: Patients frequently have concerns about their disease and 
      find it challenging to obtain accurate Information. OpenAI's ChatGPT chatbot 
      (ChatGPT) is a new large language model developed to provide answers to a wide 
      range of questions in various fields. Our aim is to evaluate the performance of 
      ChatGPT in answering patients' questions regarding gastrointestinal health. 
      METHODS: To evaluate the performance of ChatGPT in answering patients' questions, 
      we used a representative sample of 110 real-life questions. The answers provided 
      by ChatGPT were rated in consensus by three experienced gastroenterologists. The 
      accuracy, clarity, and efficacy of the answers provided by ChatGPT were assessed. 
      RESULTS: ChatGPT was able to provide accurate and clear answers to patients' 
      questions in some cases, but not in others. For questions about treatments, the 
      average accuracy, clarity, and efficacy scores (1 to 5) were 3.9 ± 0.8, 3.9 ± 
      0.9, and 3.3 ± 0.9, respectively. For symptoms questions, the average accuracy, 
      clarity, and efficacy scores were 3.4 ± 0.8, 3.7 ± 0.7, and 3.2 ± 0.7, 
      respectively. For diagnostic test questions, the average accuracy, clarity, and 
      efficacy scores were 3.7 ± 1.7, 3.7 ± 1.8, and 3.5 ± 1.7, respectively. 
      CONCLUSIONS: While ChatGPT has potential as a source of information, further 
      development is needed. The quality of information is contingent upon the quality 
      of the online information provided. These findings may be useful for healthcare 
      providers and patients alike in understanding the capabilities and limitations of 
      ChatGPT.
FAU - Lahat, Adi
AU  - Lahat A
AUID- ORCID: 0000-0003-1513-7280
AD  - Chaim Sheba Medical Center, Department of Gastroenterology, Affiliated to Tel 
      Aviv University, Tel Aviv 69978, Israel.
FAU - Shachar, Eyal
AU  - Shachar E
AD  - Chaim Sheba Medical Center, Department of Gastroenterology, Affiliated to Tel 
      Aviv University, Tel Aviv 69978, Israel.
FAU - Avidan, Benjamin
AU  - Avidan B
AD  - Chaim Sheba Medical Center, Department of Gastroenterology, Affiliated to Tel 
      Aviv University, Tel Aviv 69978, Israel.
FAU - Glicksberg, Benjamin
AU  - Glicksberg B
AUID- ORCID: 0000-0003-4515-8090
AD  - Mount Sinai Clinical Intelligence Center, Icahn School of Medicine at Mount 
      Sinai, New York, NY 10029, USA.
FAU - Klang, Eyal
AU  - Klang E
AD  - The Sami Sagol AI Hub, ARC Innovation Center, Chaim Sheba Medical Center, 
      Affiliated to Tel-Aviv University, Tel Aviv 69978, Israel.
LA  - eng
PT  - Journal Article
DEP - 20230602
PL  - Switzerland
TA  - Diagnostics (Basel)
JT  - Diagnostics (Basel, Switzerland)
JID - 101658402
PMC - PMC10252924
OTO - NOTNLM
OT  - OpenAI’s ChatGPT
OT  - chatbot
OT  - gastroenterology
OT  - medical information
OT  - natural language processing (NLP)
OT  - patients’ questions
COIS- The authors declare no conflict of interest.
EDAT- 2023/06/10 15:13
MHDA- 2023/06/10 15:14
PMCR- 2023/06/02
CRDT- 2023/06/10 01:02
PHST- 2023/03/27 00:00 [received]
PHST- 2023/05/28 00:00 [revised]
PHST- 2023/06/01 00:00 [accepted]
PHST- 2023/06/10 15:14 [medline]
PHST- 2023/06/10 15:13 [pubmed]
PHST- 2023/06/10 01:02 [entrez]
PHST- 2023/06/02 00:00 [pmc-release]
AID - diagnostics13111950 [pii]
AID - diagnostics-13-01950 [pii]
AID - 10.3390/diagnostics13111950 [doi]
PST - epublish
SO  - Diagnostics (Basel). 2023 Jun 2;13(11):1950. doi: 10.3390/diagnostics13111950.

PMID- 37330210
OWN - NLM
STAT- MEDLINE
DCOM- 20230929
LR  - 20230929
IS  - 1535-7732 (Electronic)
IS  - 1051-0443 (Linking)
VI  - 34
IP  - 10
DP  - 2023 Oct
TI  - Evaluation of an Artificial Intelligence Chatbot for Delivery of IR Patient 
      Education Material: A Comparison with Societal Website Content.
PG  - 1760-1768.e32
LID - S1051-0443(23)00423-2 [pii]
LID - 10.1016/j.jvir.2023.05.037 [doi]
AB  - PURPOSE: To assess the accuracy, completeness, and readability of patient 
      educational material produced by a machine learning model and compare the output 
      to that provided by a societal website. MATERIALS AND METHODS: Content from the 
      Society of Interventional Radiology Patient Center website was retrieved, 
      categorized, and organized into discrete questions. These questions were entered 
      into the ChatGPT platform, and the output was analyzed for word and sentence 
      counts, readability using multiple validated scales, factual correctness, and 
      suitability for patient education using the Patient Education Materials 
      Assessment Tool for Printable Materials (PEMAT-P) instrument. RESULTS: A total of 
      21,154 words were analyzed, including 7,917 words from the website and 13,377 
      words representing the total output of the ChatGPT platform across 22 text 
      passages. Compared to the societal website, output from the ChatGPT platform was 
      longer and more difficult to read on 4 of 5 readability scales. The ChatGPT 
      output was incorrect for 12 (11.5%) of 104 questions. When reviewed using the 
      PEMAT-P tool, the ChatGPT content scored lower than the website material. Content 
      from both the website and ChatGPT were significantly above the recommended fifth 
      or sixth grade level for patient education, with a mean Flesch-Kincaid grade 
      level of 11.1 (±1.3) for the website and 11.9 (±1.6) for the ChatGPT content. 
      CONCLUSIONS: The ChatGPT platform may produce incomplete or inaccurate patient 
      educational content, and providers should be familiar with the limitations of the 
      system in its current form. Opportunities may exist to fine-tune existing large 
      language models, which could be optimized for the delivery of patient educational 
      content.
CI  - Copyright © 2023 SIR. Published by Elsevier Inc. All rights reserved.
FAU - McCarthy, Colin J
AU  - McCarthy CJ
AD  - Division of Vascular and Interventional Radiology, Beth Israel Deaconess Medical 
      Center, Harvard Medical School, Boston, Massachusetts. Electronic address: 
      cmccar11@bidmc.harvard.edu.
FAU - Berkowitz, Seth
AU  - Berkowitz S
AD  - Division of Vascular and Interventional Radiology, Beth Israel Deaconess Medical 
      Center, Harvard Medical School, Boston, Massachusetts.
FAU - Ramalingam, Vijay
AU  - Ramalingam V
AD  - Division of Vascular and Interventional Radiology, Beth Israel Deaconess Medical 
      Center, Harvard Medical School, Boston, Massachusetts.
FAU - Ahmed, Muneeb
AU  - Ahmed M
AD  - Division of Vascular and Interventional Radiology, Beth Israel Deaconess Medical 
      Center, Harvard Medical School, Boston, Massachusetts.
LA  - eng
PT  - Journal Article
DEP - 20230616
PL  - United States
TA  - J Vasc Interv Radiol
JT  - Journal of vascular and interventional radiology : JVIR
JID - 9203369
SB  - IM
MH  - Humans
MH  - *Health Literacy
MH  - Artificial Intelligence
MH  - Patient Education as Topic
MH  - Software
MH  - Societies, Medical
EDAT- 2023/06/18 01:07
MHDA- 2023/09/29 06:44
CRDT- 2023/06/17 19:18
PHST- 2023/02/01 00:00 [received]
PHST- 2023/04/05 00:00 [revised]
PHST- 2023/05/08 00:00 [accepted]
PHST- 2023/09/29 06:44 [medline]
PHST- 2023/06/18 01:07 [pubmed]
PHST- 2023/06/17 19:18 [entrez]
AID - S1051-0443(23)00423-2 [pii]
AID - 10.1016/j.jvir.2023.05.037 [doi]
PST - ppublish
SO  - J Vasc Interv Radiol. 2023 Oct;34(10):1760-1768.e32. doi: 
      10.1016/j.jvir.2023.05.037. Epub 2023 Jun 16.

PMID- 37930057
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 1097-0096 (Electronic)
IS  - 0091-2751 (Linking)
VI  - 52
IP  - 2
DP  - 2024 Feb
TI  - An investigation study on the interpretation of ultrasonic medical reports using 
      OpenAI's GPT-3.5-turbo model.
PG  - 105-111
LID - 10.1002/jcu.23590 [doi]
AB  - OBJECTIVES: Ultrasound medical reports are an important means of diagnosing 
      diseases and assessing treatment effectiveness. However, their professional terms 
      and complex sentences often make it difficult for ordinary people to understand. 
      Therefore, this study explores the clinical value of using artificial 
      intelligence systems based on ChatGPT to interpret ultrasound medical reports. 
      METHODS: In this study, a combination of online and offline questionnaires were 
      used to survey both physicians and non-medical individuals. The questionnaires 
      evaluated ChatGPT's interpretation of ultrasound reports from both professional 
      and comprehensibility perspectives, and the results were analyzed using Excel 
      spreadsheets. Additionally, a portion of the research content was evaluated using 
      the Likert Scale 5-point method in the questionnaire. RESULTS: According to 
      survey results, 67.4% of surveyed doctors believe that using ChatGPT for 
      interpreting ultrasound medical reports can help improve work efficiency. At the 
      same time, 69.72% of non-medical professionals believe it is necessary to enhance 
      their understanding of medical ultrasound reports through ChatGPT interpretation, 
      and 62.58% support the application of ChatGPT to ultrasound medical reports. The 
      non-medical group's understanding of ultrasound medical reports significantly 
      improved (p &lt; 0.01) after implementing ChatGPT, However, 67.49% of the general 
      public are concerned about ChatGPT's imperfect functionality, which may cause 
      misleading information. This reflects that the public's trust in new technology 
      is not high enough, and they are also worried about possible privacy leaks and 
      security issues with ChatGPT technology. CONCLUSIONS: The higher acceptance and 
      support of non-medical individuals for the interpretation of medical reports by 
      ChatGPT might be due to the system's natural language processing abilities that 
      allow them to better understand and evaluate report contents. However, the 
      expertise and experience of physicians are still irreplaceable. This suggests 
      that the ChatGPT-based ultrasound medical report interpretation system has 
      certain clinical value and application prospects, but further optimization is 
      necessary to address its shortcomings in data quality and professionalism. This 
      study provides a reference and inspiration for promoting the application and 
      development of ultrasound technology and artificial intelligence systems in the 
      medical field.
CI  - © 2023 Wiley Periodicals LLC.
FAU - Wang, Wen Hui
AU  - Wang WH
AUID- ORCID: 0000-0001-7495-100X
AD  - Department of Ultrasound, West China Hospital of Sichuan University, Chengdu, 
      China.
FAU - Wang, Shi Yu
AU  - Wang SY
AD  - School of Life Science and Technology, University of Electronic Science and 
      Technology of China, Chengdu, China.
FAU - Huang, Jia Yan
AU  - Huang JY
AD  - Department of Ultrasound, West China Hospital of Sichuan University, Chengdu, 
      China.
FAU - Liu, Xiao di
AU  - Liu XD
AD  - Department of Ultrasound, West China Hospital of Sichuan University, Chengdu, 
      China.
FAU - Yang, Jie
AU  - Yang J
AD  - Department of Ultrasound, West China Hospital of Sichuan University, Chengdu, 
      China.
FAU - Liao, Min
AU  - Liao M
AD  - Department of Ultrasound, West China Hospital of Sichuan University, Chengdu, 
      China.
FAU - Lu, Qiang
AU  - Lu Q
AD  - Department of Ultrasound, West China Hospital of Sichuan University, Chengdu, 
      China.
FAU - Wu, Zhe
AU  - Wu Z
AD  - School of Life Science and Technology, University of Electronic Science and 
      Technology of China, Chengdu, China.
LA  - eng
GR  - 82027808/The National Key Scientific Instrument Development Program/
GR  - 2019YFE0196700/The National Science and Technology Ministry of China/
PT  - Journal Article
DEP - 20231106
PL  - United States
TA  - J Clin Ultrasound
JT  - Journal of clinical ultrasound : JCU
JID - 0401663
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Ultrasonics
MH  - Data Accuracy
MH  - *Physicians
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - GPT-3.5
OT  - Ultrasound
EDAT- 2023/11/06 12:43
MHDA- 2024/02/13 12:45
CRDT- 2023/11/06 08:14
PHST- 2023/09/23 00:00 [revised]
PHST- 2023/07/31 00:00 [received]
PHST- 2023/10/04 00:00 [accepted]
PHST- 2024/02/13 12:45 [medline]
PHST- 2023/11/06 12:43 [pubmed]
PHST- 2023/11/06 08:14 [entrez]
AID - 10.1002/jcu.23590 [doi]
PST - ppublish
SO  - J Clin Ultrasound. 2024 Feb;52(2):105-111. doi: 10.1002/jcu.23590. Epub 2023 Nov 
      6.

PMID- 37779351
OWN - NLM
STAT- MEDLINE
DCOM- 20240220
LR  - 20240223
IS  - 2194-802X (Electronic)
IS  - 2194-802X (Linking)
VI  - 11
IP  - 1
DP  - 2024 Feb 1
TI  - Enhancing clinical reasoning with Chat Generative Pre-trained Transformer: a 
      practical guide.
PG  - 102-105
LID - 10.1515/dx-2023-0116 [doi]
AB  - OBJECTIVES: This study aimed to elucidate effective methodologies for utilizing 
      the generative artificial intelligence (AI) system, namely the Chat Generative 
      Pre-trained Transformer (ChatGPT), in improving clinical reasoning abilities 
      among clinicians. METHODS: We conducted a comprehensive exploration of the 
      capabilities of ChatGPT, emphasizing two main areas: (1)&nbsp;efficient utilization of 
      ChatGPT, with a focus on application and language selection, input methodology, 
      and output verification; and (2) specific strategies to bolster clinical 
      reasoning using ChatGPT, including self-learning via simulated clinical case 
      creation and engagement with published case reports. RESULTS: Effective AI-based 
      clinical reasoning development requires a clear delineation of both system roles 
      and user needs. All outputs from the system necessitate rigorous verification 
      against credible medical resources. When used in self-learning scenarios, 
      capabilities of ChatGPT in clinical case creation notably enhanced disease 
      comprehension. CONCLUSIONS: The efficient use of generative AIs, as exemplified 
      by ChatGPT, can impressively enhance clinical reasoning among medical 
      professionals. Adopting these cutting-edge tools promises a bright future for 
      continuous advancements in clinicians' diagnostic skills, heralding a 
      transformative era in digital healthcare.
CI  - © 2023 Walter de Gruyter GmbH, Berlin/Boston.
FAU - Hirosawa, Takanobu
AU  - Hirosawa T
AUID- ORCID: 0000-0002-3573-8203
AD  - Department of Diagnostic and Generalist Medicine, Dokkyo Medical University, 
      Tochigi, Japan.
FAU - Shimizu, Taro
AU  - Shimizu T
AD  - Department of Diagnostic and Generalist Medicine, Dokkyo Medical University, 
      Tochigi, Japan.
LA  - eng
PT  - Journal Article
DEP - 20231003
PL  - Germany
TA  - Diagnosis (Berl)
JT  - Diagnosis (Berlin, Germany)
JID - 101654734
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Clinical Reasoning
MH  - Language
MH  - Learning
OTO - NOTNLM
OT  - diagnosis
OT  - diagnostic excellence
OT  - large language model
OT  - natural language processing
OT  - self-learning
EDAT- 2023/10/02 06:42
MHDA- 2024/02/20 11:50
CRDT- 2023/10/02 02:23
PHST- 2023/08/30 00:00 [received]
PHST- 2023/09/04 00:00 [accepted]
PHST- 2024/02/20 11:50 [medline]
PHST- 2023/10/02 06:42 [pubmed]
PHST- 2023/10/02 02:23 [entrez]
AID - dx-2023-0116 [pii]
AID - 10.1515/dx-2023-0116 [doi]
PST - epublish
SO  - Diagnosis (Berl). 2023 Oct 3;11(1):102-105. doi: 10.1515/dx-2023-0116. 
      eCollection 2024 Feb 1.

PMID- 38246839
OWN - NLM
STAT- MEDLINE
DCOM- 20240318
LR  - 20240320
IS  - 1532-7361 (Electronic)
IS  - 0039-6060 (Print)
IS  - 0039-6060 (Linking)
VI  - 175
IP  - 4
DP  - 2024 Apr
TI  - Evaluating capabilities of large language models: Performance of GPT-4 on 
      surgical knowledge assessments.
PG  - 936-942
LID - S0039-6060(23)00954-6 [pii]
LID - 10.1016/j.surg.2023.12.014 [doi]
AB  - BACKGROUND: Artificial intelligence has the potential to dramatically alter 
      health care by enhancing how we diagnose and treat disease. One promising 
      artificial intelligence model is ChatGPT, a general-purpose large language model 
      trained by OpenAI. ChatGPT has shown human-level performance on several 
      professional and academic benchmarks. We sought to evaluate its performance on 
      surgical knowledge questions and assess the stability of this performance on 
      repeat queries. METHODS: We evaluated the performance of ChatGPT-4 on questions 
      from the Surgical Council on Resident Education question bank and a second 
      commonly used surgical knowledge assessment, referred to as Data-B. Questions 
      were entered in 2 formats: open-ended and multiple-choice. ChatGPT outputs were 
      assessed for accuracy and insights by surgeon evaluators. We categorized reasons 
      for model errors and the stability of performance on repeat queries. RESULTS: A 
      total of 167 Surgical Council on Resident Education and 112 Data-B questions were 
      presented to the ChatGPT interface. ChatGPT correctly answered 71.3% and 67.9% of 
      multiple choice and 47.9% and 66.1% of open-ended questions for Surgical Council 
      on Resident Education and Data-B, respectively. For both open-ended and 
      multiple-choice questions, approximately two-thirds of ChatGPT responses 
      contained nonobvious insights. Common reasons for incorrect responses included 
      inaccurate information in a complex question (n&nbsp;= 16, 36.4%), inaccurate 
      information in a fact-based question (n&nbsp;= 11, 25.0%), and accurate information 
      with circumstantial discrepancy (n&nbsp;= 6, 13.6%). Upon repeat query, the answer 
      selected by ChatGPT varied for 36.4% of questions answered incorrectly on the 
      first query; the response accuracy changed for 6/16 (37.5%) questions. 
      CONCLUSION: Consistent with findings in other academic and professional domains, 
      we demonstrate near or above human-level performance of ChatGPT on surgical 
      knowledge questions from 2 widely used question banks. ChatGPT performed better 
      on multiple-choice than open-ended questions, prompting questions regarding its 
      potential for clinical application. Unique to this study, we demonstrate 
      inconsistency in ChatGPT responses on repeat queries. This finding warrants 
      future consideration including efforts at training large language models to 
      provide the safe and consistent responses required for clinical application. 
      Despite near or above human-level performance on question banks and given these 
      observations, it is unclear whether large language models such as ChatGPT are 
      able to safely assist clinicians in providing care.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Beaulieu-Jones, Brendin R
AU  - Beaulieu-Jones BR
AD  - Department of Surgery, Beth Israel Deaconess Medical Center, Boston, MA; 
      Department of Biomedical Informatics, Harvard Medical School, Boston, MA. 
      Electronic address: https://twitter.com/bratogram.
FAU - Berrigan, Margaret T
AU  - Berrigan MT
AD  - Department of Surgery, Beth Israel Deaconess Medical Center, Boston, MA.
FAU - Shah, Sahaj
AU  - Shah S
AD  - Geisinger Commonwealth School of Medicine, Scranton, PA.
FAU - Marwaha, Jayson S
AU  - Marwaha JS
AD  - Division of Colorectal Surgery, National Taiwan University Hospital, Taipei, 
      Taiwan.
FAU - Lai, Shuo-Lun
AU  - Lai SL
AD  - Division of Colorectal Surgery, National Taiwan University Hospital, Taipei, 
      Taiwan.
FAU - Brat, Gabriel A
AU  - Brat GA
AD  - Department of Surgery, Beth Israel Deaconess Medical Center, Boston, MA; 
      Department of Biomedical Informatics, Harvard Medical School, Boston, MA. 
      Electronic address: bbeaulieujones@gmail.com.
LA  - eng
GR  - T15 LM007092/LM/NLM NIH HHS/United States
PT  - Journal Article
DEP - 20240120
PL  - United States
TA  - Surgery
JT  - Surgery
JID - 0417347
SB  - IM
UOF - medRxiv. 2023 Jul 24;:. PMID: 37502981
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Surgeons
MH  - Educational Status
MH  - Benchmarking
MH  - Language
PMC - PMC10947829
MID - NIHMS1961919
COIS- Conflict of Interest/Disclosure: The authors have no relevant financial 
      disclosures.
EDAT- 2024/01/22 00:42
MHDA- 2024/03/18 06:42
PMCR- 2025/04/01
CRDT- 2024/01/21 21:54
PHST- 2023/07/17 00:00 [received]
PHST- 2023/12/09 00:00 [revised]
PHST- 2023/12/15 00:00 [accepted]
PHST- 2025/04/01 00:00 [pmc-release]
PHST- 2024/03/18 06:42 [medline]
PHST- 2024/01/22 00:42 [pubmed]
PHST- 2024/01/21 21:54 [entrez]
AID - S0039-6060(23)00954-6 [pii]
AID - 10.1016/j.surg.2023.12.014 [doi]
PST - ppublish
SO  - Surgery. 2024 Apr;175(4):936-942. doi: 10.1016/j.surg.2023.12.014. Epub 2024 Jan 
      20.

PMID- 38135548
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 1878-7452 (Electronic)
IS  - 1878-7452 (Linking)
VI  - 81
IP  - 3
DP  - 2024 Mar
TI  - Development and Evaluation of Aeyeconsult: A Novel Ophthalmology Chatbot 
      Leveraging Verified Textbook Knowledge and GPT-4.
PG  - 438-443
LID - S1931-7204(23)00432-4 [pii]
LID - 10.1016/j.jsurg.2023.11.019 [doi]
AB  - OBJECTIVE: There has been much excitement on the use of large language models 
      (LLMs) such as ChatGPT in ophthalmology. However, LLMs are limited in that they 
      are trained on unverified information and do not cite their sources. This paper 
      highlights a new methodology to create a generative AI chatbot to answer eye care 
      related questions which uses only verified ophthalmology textbooks as data and 
      cites its sources. SETTING: Yale School of Medicine Department of Ophthalmology 
      and Visual Science. DESIGN/METHODS: Aeyeconsult, an ophthalmology chatbot, was 
      developed using GPT-4 (the LLM used to power the publicly available chatbot 
      ChatGPT-4), LangChain, and Pinecone. Ophthalmology textbooks were processed into 
      embeddings and stored in Pinecone. User queries were similarly converted, 
      compared to stored embeddings, and GPT-4 generated responses. The interface was 
      adapted from public code. Both Aeyeconsult and ChatGPT-4 were tested on the same 
      260 questions from OphthoQuestions.com, with the first response from Aeyeconsult 
      and ChatGPT-4 recorded as the answer. RESULTS: Aeyeconsult outperformed ChatGPT-4 
      on the OKAP dataset, with 83.4% correct answers compared to 69.2% (p = 0.0118). 
      Aeyeconsult also had fewer instances of no answer and multiple answers. Both 
      systems performed best in General Medicine, with Aeyeconsult achieving 96.2% 
      accuracy. Aeyeconsult's weakest performance was in Clinical Optics at 68.1%, but 
      it still outperformed ChatGPT-4 in this category (45.5%). CONCLUSION: LLMs may be 
      useful in answering ophthalmology questions but their trustworthiness and 
      accuracy is limited due to training on unverified internet data and lack of 
      source citation. We used a new methodology, using verified ophthalmology 
      textbooks as source material and providing citations, to mitigate these issues, 
      resulting in a chatbot more accurate than ChatGPT-4 in answering OKAPs style 
      questions.
CI  - Copyright © 2023 Association of Program Directors in Surgery. Published by 
      Elsevier Inc. All rights reserved.
FAU - Singer, Maxwell B
AU  - Singer MB
AD  - Department of Ophthalmology and Visual Science, Yale School of Medicine, New 
      Haven, Connecticut. Electronic address: maxwell.singer@yale.edu.
FAU - Fu, Julia J
AU  - Fu JJ
AD  - Yale School of Medicine, New Haven, Connecticut.
FAU - Chow, Jessica
AU  - Chow J
AD  - Department of Ophthalmology and Visual Science, Yale School of Medicine, New 
      Haven, Connecticut.
FAU - Teng, Christopher C
AU  - Teng CC
AD  - Department of Ophthalmology and Visual Science, Yale School of Medicine, New 
      Haven, Connecticut.
LA  - eng
PT  - Journal Article
DEP - 20231221
PL  - United States
TA  - J Surg Educ
JT  - Journal of surgical education
JID - 101303204
SB  - IM
MH  - *Internet
MH  - *Ophthalmology
MH  - Schools
MH  - Software
OTO - NOTNLM
OT  - ChatGPT
OT  - OKAPs
OT  - artificial intelligence
OT  - chatbot
OT  - large language models
EDAT- 2023/12/23 12:42
MHDA- 2024/02/14 12:45
CRDT- 2023/12/22 21:57
PHST- 2023/08/14 00:00 [received]
PHST- 2023/11/08 00:00 [revised]
PHST- 2023/11/25 00:00 [accepted]
PHST- 2024/02/14 12:45 [medline]
PHST- 2023/12/23 12:42 [pubmed]
PHST- 2023/12/22 21:57 [entrez]
AID - S1931-7204(23)00432-4 [pii]
AID - 10.1016/j.jsurg.2023.11.019 [doi]
PST - ppublish
SO  - J Surg Educ. 2024 Mar;81(3):438-443. doi: 10.1016/j.jsurg.2023.11.019. Epub 2023 
      Dec 21.

PMID- 38069953
OWN - NLM
STAT- MEDLINE
DCOM- 20240205
LR  - 20240206
IS  - 1531-6564 (Electronic)
IS  - 0363-5023 (Linking)
VI  - 49
IP  - 2
DP  - 2024 Feb
TI  - Appropriateness and Reliability of an Online Artificial Intelligence Platform's 
      Responses to Common Questions Regarding Distal Radius Fractures.
PG  - 91-98
LID - S0363-5023(23)00589-0 [pii]
LID - 10.1016/j.jhsa.2023.10.019 [doi]
AB  - PURPOSE: Chat Generative Pre-Trained Transformer (ChatGPT) is a novel artificial 
      intelligence chatbot that is changing the way humans gather information online. 
      The purpose of this study was to investigate ChatGPT's ability to appropriately 
      and reliably answer common questions regarding distal radius fractures. METHODS: 
      Thirty common questions regarding distal radius fractures were presented in an 
      identical manner to the online ChatGPT-3.5 interface three separate times, 
      yielding 90 unique responses because ChatGPT produces an original answer with 
      each query. All responses were graded as "appropriate," "appropriate but 
      incomplete," or "inappropriate" by a consensus discussion among three hand 
      surgeon reviewers. The questions were additionally subcategorized into one of 
      four domains based on Bloom's cognitive learning taxonomy, and descriptive 
      statistics were reported. RESULTS: Seventy of the 90 total responses (78%) 
      produced by ChatGPT were "appropriate," and 29 of the 30 questions (97%) had at 
      least one response considered appropriate (of the three possible). However, only 
      17 of the 30 questions (57%) were answered appropriately on all three iterations. 
      The test-retest reliability of ChatGPT was poor with an intraclass correlation 
      coefficient of 0.12. Finally, ChatGPT performed best answering questions 
      requiring lower-order thinking skills (Bloom's levels 1-3) and less well on level 
      4 questions. CONCLUSIONS: This study found that although ChatGPT has the 
      capability to answer common questions regarding distal radius fractures, caution 
      should be taken before implementing its use, given ChatGPT's inconsistency in 
      providing a complete and accurate response to the same question every time. 
      CLINICAL RELEVANCE: As the popularity and technology of ChatGPT continue to grow, 
      it is important to understand the potential and limitations of this platform to 
      determine how it may&nbsp;be best implemented to improve patient care.
CI  - Copyright © 2024 American Society for Surgery of the Hand. Published by Elsevier 
      Inc. All rights reserved.
FAU - Christy, Michele
AU  - Christy M
AD  - Department of Orthopaedic Surgery, Washington University in St. Louis, St. Louis, 
      MO.
FAU - Morris, Marie T
AU  - Morris MT
AD  - Department of Orthopaedic Surgery, Washington University in St. Louis, St. Louis, 
      MO.
FAU - Goldfarb, Charles A
AU  - Goldfarb CA
AD  - Department of Orthopaedic Surgery, Washington University in St. Louis, St. Louis, 
      MO.
FAU - Dy, Christopher J
AU  - Dy CJ
AD  - Department of Orthopaedic Surgery, Washington University in St. Louis, St. Louis, 
      MO. Electronic address: Dyc@wustl.edu.
LA  - eng
PT  - Journal Article
DEP - 20231208
PL  - United States
TA  - J Hand Surg Am
JT  - The Journal of hand surgery
JID - 7609631
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Wrist Fractures
MH  - Reproducibility of Results
MH  - Software
MH  - *Surgeons
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - distal radius fractures
OT  - patient education
EDAT- 2023/12/10 06:41
MHDA- 2024/02/05 06:42
CRDT- 2023/12/09 10:34
PHST- 2023/06/19 00:00 [received]
PHST- 2023/10/25 00:00 [revised]
PHST- 2023/10/26 00:00 [accepted]
PHST- 2024/02/05 06:42 [medline]
PHST- 2023/12/10 06:41 [pubmed]
PHST- 2023/12/09 10:34 [entrez]
AID - S0363-5023(23)00589-0 [pii]
AID - 10.1016/j.jhsa.2023.10.019 [doi]
PST - ppublish
SO  - J Hand Surg Am. 2024 Feb;49(2):91-98. doi: 10.1016/j.jhsa.2023.10.019. Epub 2023 
      Dec 8.

PMID- 38243411
OWN - NLM
STAT- Publisher
LR  - 20240120
IS  - 1538-6724 (Electronic)
IS  - 0031-9023 (Linking)
DP  - 2024 Jan 18
TI  - Detecting Artificial Intelligence-Generated Personal Statements in Professional 
      Physical Therapist Education Program Applications: A Lexical Analysis.
LID - pzae006 [pii]
LID - 10.1093/ptj/pzae006 [doi]
AB  - OBJECTIVE: The objective of this study was to compare the lexical sophistication 
      of personal statements submitted by professional physical therapist education 
      program applicants with those generated by OpenAI's Chat Generative Pre-trained 
      Transformer (ChatGPT). METHODS: Personal statements from 152 applicants and 20 
      generated by ChatGPT were collected, all in response to a standardized prompt. 
      These statements were coded numerically, then analyzed with recurrence 
      quantification analyses (RQAs). RQA indices including recurrence, determinism, 
      max line, mean line, and entropy were compared with t tests. A receiver operating 
      characteristic (ROC) curve analysis was used to examine discriminative validity 
      of RQA indices to distinguish between ChatGPT and human-generated personal 
      statements. RESULTS: ChatGPT-generated personal statements exhibited higher 
      recurrence, determinism, mean line, and entropy values than did human-generated 
      personal statements. The strongest discriminator was a 13.04% determinism rate, 
      which differentiated ChatGPT from human-generated writing samples with 70% 
      sensitivity and 91.4% specificity (positive likelihood ratio = 8.14). Personal 
      statements with determinism rates exceeding 13% were 8 times more likely to have 
      been ChatGPT than human generated. CONCLUSIONS: While RQA can distinguish 
      artificial intelligence-generated text from human-generated text, it is not 
      absolute. Thus, artificial intelligence introduces additional challenges to the 
      authenticity and utility of personal statements. Admissions committees along with 
      organizations providing guidelines in professional physical therapist education 
      program admissions should re-evaluate the role of personal statements in 
      applications. IMPACT: As artificial intelligence-driven chatbots like ChatGPT 
      complicate the evaluation of personal statements, RQA emerges as a potential tool 
      for admissions committees to detect artificial intelligence-generated statements.
CI  - © The Author(s) 2024. Published by Oxford University Press on behalf of the 
      American Physical Therapy Association. All rights reserved. For permissions, 
      please e-mail: journals.permissions@oup.com.
FAU - Hollman, John H
AU  - Hollman JH
AD  - Mayo Clinic School of Health Sciences, Mayo Clinic College of Medicine and 
      Science, and Department of Physical Medicine and Rehabilitation, Mayo Clinic, 
      Rochester, Minnesota.
FAU - Cloud-Biebl, Beth A
AU  - Cloud-Biebl BA
AD  - Mayo Clinic School of Health Sciences, Mayo Clinic College of Medicine and 
      Science, and Department of Physical Medicine and Rehabilitation, Mayo Clinic, 
      Rochester, Minnesota.
FAU - Krause, David A
AU  - Krause DA
AD  - Mayo Clinic School of Health Sciences, Mayo Clinic College of Medicine and 
      Science, and Department of Physical Medicine and Rehabilitation, Mayo Clinic, 
      Rochester, Minnesota.
FAU - Calley, Darren Q
AU  - Calley DQ
AD  - Mayo Clinic School of Health Sciences, Mayo Clinic College of Medicine and 
      Science, and Department of Physical Medicine and Rehabilitation, Mayo Clinic, 
      Rochester, Minnesota.
LA  - eng
PT  - Journal Article
DEP - 20240118
PL  - United States
TA  - Phys Ther
JT  - Physical therapy
JID - 0022623
SB  - IM
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - Entropy
OT  - Linguistics
OT  - ROC Curve
OT  - Schools
EDAT- 2024/01/20 10:42
MHDA- 2024/01/20 10:42
CRDT- 2024/01/20 00:52
PHST- 2023/03/17 00:00 [received]
PHST- 2023/09/21 00:00 [revised]
PHST- 2024/01/15 00:00 [accepted]
PHST- 2024/01/20 10:42 [medline]
PHST- 2024/01/20 10:42 [pubmed]
PHST- 2024/01/20 00:52 [entrez]
AID - 7577670 [pii]
AID - 10.1093/ptj/pzae006 [doi]
PST - aheadofprint
SO  - Phys Ther. 2024 Jan 18:pzae006. doi: 10.1093/ptj/pzae006.

PMID- 38366690
OWN - NLM
STAT- Publisher
LR  - 20240307
IS  - 1365-2648 (Electronic)
IS  - 0309-2402 (Linking)
DP  - 2024 Feb 17
TI  - A comparative vignette study: Evaluating the potential role of a generative AI 
      model in enhancing clinical decision-making in nursing.
LID - 10.1111/jan.16101 [doi]
AB  - AIM: This study explores the potential of a generative artificial intelligence 
      tool (ChatGPT) as clinical support for nurses. Specifically, we aim to assess 
      whether ChatGPT can demonstrate clinical decision-making equivalent to that of 
      expert nurses and novice nursing students. This will be evaluated by comparing 
      ChatGPT responses to clinical scenarios to those of nurses on different levels of 
      experience. DESIGN: This is a cross-sectional study. METHODS: Emergency room 
      registered nurses (i.e. experts; n = 30) and nursing students (i.e. novices; 
      n = 38) were recruited during March-April 2023. Clinical decision-making was 
      measured using three validated clinical scenarios involving an initial assessment 
      and reevaluation. Clinical decision-making aspects assessed were the accuracy of 
      initial assessments, the appropriateness of recommended tests and resource use 
      and the capacity to reevaluate decisions. Performance was also compared by timing 
      response generations and word counts. Expert nurses and novice students completed 
      online questionnaires (via Qualtrics), while ChatGPT responses were obtained from 
      OpenAI. RESULTS: Concerning aspects of clinical decision-making and compared to 
      novices and experts: (1) ChatGPT exhibited indecisiveness in initial assessments; 
      (2) ChatGPT tended to suggest unnecessary diagnostic tests; (3) When new 
      information required re-evaluation, ChatGPT responses demonstrated inaccurate 
      understanding and inappropriate modifications. In terms of performance, the mean 
      number of words utilized in ChatGPT answers was 27-41 times greater than that 
      utilized by both experts and novices; and responses were provided approximately 4 
      times faster than those of novices and twice faster than expert nurses. ChatGPT 
      responses maintained logical structure and clarity. CONCLUSIONS: A generative AI 
      tool demonstrated indecisiveness and a tendency towards over-triage compared to 
      human clinicians. IMPACT: The study shows that it is important to approach the 
      implementation of ChatGPT as a nurse's digital assistant with caution. More study 
      is needed to optimize the model's training and algorithms to provide accurate 
      healthcare support that aids clinical decision-making. REPORTING METHOD: This 
      study adhered to relevant EQUATOR guidelines for reporting observational studies. 
      PATIENT OR PUBLIC CONTRIBUTION: Patients were not directly involved in the 
      conduct of this study.
CI  - © 2024 The Authors. Journal of Advanced Nursing published by John Wiley &amp; Sons 
      Ltd.
FAU - Saban, Mor
AU  - Saban M
AUID- ORCID: 0000-0001-6869-0907
AD  - Nursing Department, Steyer School of Health Professions, Faculty of Medical and 
      Health Sciences, Tel Aviv University, Tel Aviv, Israel.
FAU - Dubovi, Ilana
AU  - Dubovi I
AUID- ORCID: 0000-0002-7087-856X
AD  - Nursing Department, Steyer School of Health Professions, Faculty of Medical and 
      Health Sciences, Tel Aviv University, Tel Aviv, Israel.
LA  - eng
PT  - Journal Article
DEP - 20240217
PL  - England
TA  - J Adv Nurs
JT  - Journal of advanced nursing
JID - 7609811
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - Clinical decision-making
OT  - Generative AI
OT  - Novice
OT  - Nurse education
OT  - Nurses
EDAT- 2024/02/17 12:44
MHDA- 2024/02/17 12:44
CRDT- 2024/02/17 03:22
PHST- 2024/01/23 00:00 [revised]
PHST- 2023/11/28 00:00 [received]
PHST- 2024/02/06 00:00 [accepted]
PHST- 2024/02/17 12:44 [pubmed]
PHST- 2024/02/17 12:44 [medline]
PHST- 2024/02/17 03:22 [entrez]
AID - 10.1111/jan.16101 [doi]
PST - aheadofprint
SO  - J Adv Nurs. 2024 Feb 17. doi: 10.1111/jan.16101.

PMID- 38350517
OWN - NLM
STAT- Publisher
LR  - 20240317
IS  - 1532-8406 (Electronic)
IS  - 0883-5403 (Linking)
DP  - 2024 Feb 11
TI  - An Artificial Intelligence Chatbot is an Accurate and Useful Online Patient 
      Resource Prior to Total Knee Arthroplasty.
LID - S0883-5403(24)00104-9 [pii]
LID - 10.1016/j.arth.2024.02.005 [doi]
AB  - BACKGROUND: Online information is a useful resource for patients seeking advice 
      on their orthopaedic care. While traditional websites provide responses to 
      specific frequently asked questions (FAQs), sophisticated artificial intelligence 
      tools may be able to provide the same information to patients in a more 
      accessible manner. Chat Generative Pretrained Transformer (ChatGPT) is a powerful 
      artificial intelligence chatbot that has been shown to effectively draw on its 
      large reserves of information in a conversational context with a user. The 
      purpose of this study was to assess the accuracy and reliability of 
      ChatGPT-generated responses to FAQs regarding total knee arthroplasty. METHODS: 
      We distributed a survey that challenged arthroplasty surgeons to identify which 
      of the 2 responses to FAQs on our institution's website was human-written and 
      which was generated by ChatGPT. All questions were total knee 
      arthroplasty-related. The second portion of the survey investigated the potential 
      to further leverage ChatGPT to assist with translation and accessibility as a 
      means to better meet the needs of our diverse patient population. RESULTS: 
      Surgeons correctly identified the ChatGPT-generated responses 4 out of 10 times 
      on average (range: 0 to 7). No consensus was reached on any of the responses to 
      the FAQs. Additionally, over 90% of our surgeons strongly encouraged the use of 
      ChatGPT to more effectively accommodate the diverse patient populations that seek 
      information from our hospital's online resources. CONCLUSIONS: ChatGPT provided 
      accurate, reliable answers to our website's FAQs. Surgeons also agreed that 
      ChatGPT's ability to provide targeted, language-specific responses to FAQs would 
      be of benefit to our diverse patient population.
CI  - Copyright © 2024 Elsevier Inc. All rights reserved.
FAU - Taylor, Walter L 4th
AU  - Taylor WL 4th
AD  - Department of Adult Reconstruction and Joint Replacement, Hospital for Special 
      Surgery, New York, New York.
FAU - Cheng, Ryan
AU  - Cheng R
AD  - Department of Adult Reconstruction and Joint Replacement, Hospital for Special 
      Surgery, New York, New York.
FAU - Weinblatt, Aaron I
AU  - Weinblatt AI
AD  - Department of Adult Reconstruction and Joint Replacement, Hospital for Special 
      Surgery, New York, New York.
FAU - Bergstein, Victoria
AU  - Bergstein V
AD  - Department of Adult Reconstruction and Joint Replacement, Hospital for Special 
      Surgery, New York, New York.
FAU - Long, William J
AU  - Long WJ
AD  - Department of Adult Reconstruction and Joint Replacement, Hospital for Special 
      Surgery, New York, New York.
LA  - eng
PT  - Journal Article
DEP - 20240211
PL  - United States
TA  - J Arthroplasty
JT  - The Journal of arthroplasty
JID - 8703515
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - eHealth literacy
OT  - health equity
OT  - patient education
OT  - total knee arthroplasty
EDAT- 2024/02/14 00:44
MHDA- 2024/02/14 00:44
CRDT- 2024/02/13 19:11
PHST- 2023/11/08 00:00 [received]
PHST- 2024/02/05 00:00 [accepted]
PHST- 2024/02/14 00:44 [pubmed]
PHST- 2024/02/14 00:44 [medline]
PHST- 2024/02/13 19:11 [entrez]
AID - S0883-5403(24)00104-9 [pii]
AID - 10.1016/j.arth.2024.02.005 [doi]
PST - aheadofprint
SO  - J Arthroplasty. 2024 Feb 11:S0883-5403(24)00104-9. doi: 
      10.1016/j.arth.2024.02.005.

PMID- 38331591
OWN - NLM
STAT- Publisher
LR  - 20240208
IS  - 1768-3122 (Electronic)
IS  - 0248-8663 (Linking)
DP  - 2024 Feb 7
TI  - [The spring of artificial intelligence: AI vs. expert for internal medicine 
      cases].
LID - S0248-8663(24)00032-8 [pii]
LID - 10.1016/j.revmed.2024.01.012 [doi]
AB  - INTRODUCTION: The "Printemps de la Médecine Interne" are training days for 
      Francophone internists. The clinical cases presented during these days are 
      complex. This study aims to evaluate the diagnostic capabilities of 
      non-specialized artificial intelligence (language models) ChatGPT-4 and Bard by 
      confronting them with the puzzles of the "Printemps de la Médecine Interne". 
      METHOD: Clinical cases from the "Printemps de la Médecine Interne" 2021 and 2022 
      were submitted to two language models: ChatGPT-4 and Bard. In case of a wrong 
      answer, a second attempt was offered. We then compared the responses of human 
      internist experts to those of artificial intelligence. RESULTS: Of the 12 
      clinical cases submitted, human internist experts diagnosed nine, ChatGPT-4 
      diagnosed three, and Bard diagnosed one. One of the cases solved by ChatGPT-4 was 
      not solved by the internist expert. The artificial intelligence had a response 
      time of a few seconds. CONCLUSIONS: Currently, the diagnostic skills of ChatGPT-4 
      and Bard are inferior to those of human experts in solving complex clinical cases 
      but are very promising. Recently made available to the general public, they 
      already have impressive capabilities, questioning the role of the diagnostic 
      physician. It would be advisable to adapt the rules or subjects of future 
      "Printemps de la Médecine Interne" so that they are not solved by a public 
      language model.
CI  - Copyright © 2024 The Author(s). Published by Elsevier Masson SAS.. All rights 
      reserved.
FAU - Albaladejo, A
AU  - Albaladejo A
AD  - Médecine interne et immunologie clinique, CHU de Rennes, 2, rue 
      Henri-le-Guilloux, 35000 Rennes, France. Electronic address: 
      adrien.albaladejo@chu-rennes.fr.
FAU - Lorleac'h, A
AU  - Lorleac'h A
AD  - Groupement hospitalier Bretagne Sud, 5, avenue Choiseul, 56100 Lorient, France. 
      Electronic address: a.lorleach@ghbs.bzh.
FAU - Allain, J-S
AU  - Allain JS
AD  - Groupement hospitalier Bretagne Sud, 5, avenue Choiseul, 56100 Lorient, France. 
      Electronic address: js.allain@ghbs.bzh.
LA  - fre
PT  - English Abstract
PT  - Journal Article
TT  - Les Printemps de la Médecine Interne&nbsp;: l’intelligence artificielle face aux 
      experts internistes.
DEP - 20240207
PL  - France
TA  - Rev Med Interne
JT  - La Revue de medecine interne
JID - 8101383
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Bard
OT  - Case report
OT  - ChatGPT
OT  - Diagnostic
OT  - Intelligence artificielle
EDAT- 2024/02/09 00:42
MHDA- 2024/02/09 00:42
CRDT- 2024/02/08 21:53
PHST- 2023/10/11 00:00 [received]
PHST- 2024/01/09 00:00 [revised]
PHST- 2024/01/17 00:00 [accepted]
PHST- 2024/02/09 00:42 [medline]
PHST- 2024/02/09 00:42 [pubmed]
PHST- 2024/02/08 21:53 [entrez]
AID - S0248-8663(24)00032-8 [pii]
AID - 10.1016/j.revmed.2024.01.012 [doi]
PST - aheadofprint
SO  - Rev Med Interne. 2024 Feb 7:S0248-8663(24)00032-8. doi: 
      10.1016/j.revmed.2024.01.012.

PMID- 38533615
OWN - NLM
STAT- MEDLINE
DCOM- 20240328
LR  - 20240328
IS  - 2368-7959 (Electronic)
IS  - 2368-7959 (Linking)
VI  - 11
DP  - 2024 Mar 18
TI  - Comparing the Perspectives of Generative AI, Mental Health Experts, and the 
      General Public on Schizophrenia Recovery: Case Vignette Study.
PG  - e53043
LID - 10.2196/53043 [doi]
AB  - BACKGROUND: The current paradigm in mental health care focuses on clinical 
      recovery and symptom remission. This model's efficacy is influenced by therapist 
      trust in patient recovery potential and the depth of the therapeutic 
      relationship. Schizophrenia is a chronic illness with severe symptoms where the 
      possibility of recovery is a matter of debate. As artificial intelligence (AI) 
      becomes integrated into the health care field, it is important to examine its 
      ability to assess recovery potential in major psychiatric disorders such as 
      schizophrenia. OBJECTIVE: This study aimed to evaluate the ability of large 
      language models (LLMs) in comparison to mental health professionals to assess the 
      prognosis of schizophrenia with and without professional treatment and the 
      long-term positive and negative outcomes. METHODS: Vignettes were inputted into 
      LLMs interfaces and assessed 10 times by 4 AI platforms: ChatGPT-3.5, ChatGPT-4, 
      Google Bard, and Claude. A total of 80 evaluations were collected and benchmarked 
      against existing norms to analyze what mental health professionals (general 
      practitioners, psychiatrists, clinical psychologists, and mental health nurses) 
      and the general public think about schizophrenia prognosis with and without 
      professional treatment and the positive and negative long-term outcomes of 
      schizophrenia interventions. RESULTS: For the prognosis of schizophrenia with 
      professional treatment, ChatGPT-3.5 was notably pessimistic, whereas ChatGPT-4, 
      Claude, and Bard aligned with professional views but differed from the general 
      public. All LLMs believed untreated schizophrenia would remain static or worsen 
      without professional treatment. For long-term outcomes, ChatGPT-4 and Claude 
      predicted more negative outcomes than Bard and ChatGPT-3.5. For positive 
      outcomes, ChatGPT-3.5 and Claude were more pessimistic than Bard and ChatGPT-4. 
      CONCLUSIONS: The finding that 3 out of the 4 LLMs aligned closely with the 
      predictions of mental health professionals when considering the "with treatment" 
      condition is a demonstration of the potential of this technology in providing 
      professional clinical prognosis. The pessimistic assessment of ChatGPT-3.5 is a 
      disturbing finding since it may reduce the motivation of patients to start or 
      persist with treatment for schizophrenia. Overall, although LLMs hold promise in 
      augmenting health care, their application necessitates rigorous validation and a 
      harmonious blend with human expertise.
CI  - © Zohar Elyoseph, Inbar Levkovich. Originally published in JMIR Mental Health 
      (https://mental.jmir.org).
FAU - Elyoseph, Zohar
AU  - Elyoseph Z
AUID- ORCID: 0000-0002-5717-4074
AD  - Department of Brain Sciences, Faculty of Medicine, Imperial College London, 
      London, United Kingdom.
AD  - The Center for Psychobiological Research, Department of Psychology and 
      Educational Counseling, Max Stern Yezreel Valley College, Emek Yezreel, Israel.
FAU - Levkovich, Inbar
AU  - Levkovich I
AUID- ORCID: 0000-0003-1582-3889
AD  - Faculty of Graduate Studies, Oranim Academic College, Kiryat Tiv'on, Israel.
LA  - eng
PT  - Journal Article
DEP - 20240318
PL  - Canada
TA  - JMIR Ment Health
JT  - JMIR mental health
JID - 101658926
SB  - IM
MH  - Humans
MH  - Mental Health
MH  - *Schizophrenia
MH  - Artificial Intelligence
MH  - Health Occupations
MH  - *General Practitioners
OTO - NOTNLM
OT  - ChatGPT
OT  - GPT
OT  - Generative Pre-trained Transformers
OT  - LLM
OT  - LLMs
OT  - NLP
OT  - artificial intelligence
OT  - language model
OT  - language models
OT  - large language models
OT  - mental
OT  - natural language processing
OT  - outcome
OT  - outcomes
OT  - prognosis
OT  - prognostic
OT  - prognostics
OT  - recovery
OT  - schizophrenia
OT  - vignette
OT  - vignettes
EDAT- 2024/03/27 06:43
MHDA- 2024/03/28 06:44
CRDT- 2024/03/27 04:33
PHST- 2023/09/24 00:00 [received]
PHST- 2024/01/24 00:00 [revised]
PHST- 2024/02/11 00:00 [accepted]
PHST- 2024/03/28 06:44 [medline]
PHST- 2024/03/27 06:43 [pubmed]
PHST- 2024/03/27 04:33 [entrez]
AID - v11i1e53043 [pii]
AID - 10.2196/53043 [doi]
PST - epublish
SO  - JMIR Ment Health. 2024 Mar 18;11:e53043. doi: 10.2196/53043.

PMID- 38197996
OWN - NLM
STAT- MEDLINE
DCOM- 20240111
LR  - 20240226
IS  - 1433-8726 (Electronic)
IS  - 0724-4983 (Linking)
VI  - 42
IP  - 1
DP  - 2024 Jan 10
TI  - How does artificial intelligence master urological board examinations? A 
      comparative analysis of different Large Language Models' accuracy and reliability 
      in the 2022 In-Service Assessment of the European Board of Urology.
PG  - 20
LID - 10.1007/s00345-023-04749-6 [doi]
AB  - PURPOSE: This study is a comparative analysis of three Large Language Models 
      (LLMs) evaluating their rate of correct answers (RoCA) and the reliability of 
      generated answers on a set of urological knowledge-based questions spanning 
      different levels of complexity. METHODS: ChatGPT-3.5, ChatGPT-4, and Bing AI 
      underwent two testing rounds, with a 48-h gap in between, using the 100 
      multiple-choice questions from the 2022 European Board of Urology (EBU) 
      In-Service Assessment (ISA). For conflicting responses, an additional consensus 
      round was conducted to establish conclusive answers. RoCA&nbsp;was compared across 
      various question complexities. Ten weeks after the consensus round, a subsequent 
      testing round was conducted to assess potential knowledge gain and improvement in 
      RoCA, respectively. RESULTS: Over three testing rounds, ChatGPT-3.5 achieved RoCa 
      scores of 58%, 62%, and 59%. In contrast, ChatGPT-4 achieved RoCA scores of 63%, 
      77%, and 77%, while Bing AI yielded scores of 81%, 73%, and 77%, respectively. 
      Agreement rates between rounds 1 and 2 were 84% (κ = 0.67, p &lt; 0.001) for 
      ChatGPT-3.5, 74% (κ = 0.40, p &lt; 0.001) for ChatGPT-4, and 76% (κ = 0.33, 
      p &lt; 0.001) for BING AI. In the consensus round, ChatGPT-4 and Bing AI 
      significantly outperformed ChatGPT-3.5 (77% and 77% vs. 59%, both&nbsp;p = 0.010). All 
      LLMs demonstrated decreasing RoCA scores with increasing question complexity 
      (p &lt; 0.001).&nbsp;In the fourth round, no significant improvement in RoCA was observed 
      across all three LLMs. CONCLUSIONS: The performance of the tested LLMs in 
      addressing urological specialist inquiries warrants further refinement. Moreover, 
      the deficiency in response reliability contributes to existing challenges related 
      to their current utility for educational purposes.
CI  - © 2024. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Kollitsch, Lisa
AU  - Kollitsch L
AD  - Department of Urology and Andrology, Klinik Donaustadt, Vienna, Austria.
FAU - Eredics, Klaus
AU  - Eredics K
AD  - Department of Urology and Andrology, Klinik Donaustadt, Vienna, Austria.
AD  - Department of Urology, Paracelsus Medical University, Salzburg, Austria.
FAU - Marszalek, Martin
AU  - Marszalek M
AD  - Department of Urology and Andrology, Klinik Donaustadt, Vienna, Austria.
FAU - Rauchenwald, Michael
AU  - Rauchenwald M
AD  - Department of Urology and Andrology, Klinik Donaustadt, Vienna, Austria.
AD  - European Board of Urology, Arnhem, The Netherlands.
FAU - Brookman-May, Sabine D
AU  - Brookman-May SD
AD  - Department of Urology, University of Munich, LMU, Munich, Germany.
AD  - Johnson and Johnson Innovative Medicine, Research and Development, Spring House, 
      PA, USA.
FAU - Burger, Maximilian
AU  - Burger M
AD  - Department of Urology, Caritas St. Josef Medical Centre, University of 
      Regensburg, Regensburg, Germany.
FAU - Körner-Riffard, Katharina
AU  - Körner-Riffard K
AD  - Department of Urology, Caritas St. Josef Medical Centre, University of 
      Regensburg, Regensburg, Germany.
FAU - May, Matthias
AU  - May M
AUID- ORCID: 0000-0003-3896-0405
AD  - Department of Urology, St. Elisabeth Hospital Straubing, Brothers of Mercy 
      Hospital, Straubing, Germany. matthias.may@klinikum-straubing.de.
LA  - eng
PT  - Journal Article
DEP - 20240110
PL  - Germany
TA  - World J Urol
JT  - World journal of urology
JID - 8307716
SB  - IM
CIN - World J Urol. 2024 Feb 26;42(1):104. PMID: 38407653
MH  - Humans
MH  - *Artificial Intelligence
MH  - Reproducibility of Results
MH  - *Urology
MH  - Physical Examination
MH  - Language
OTO - NOTNLM
OT  - AI
OT  - BING AI
OT  - ChatGPT-3.5
OT  - ChatGPT-4
OT  - EBU
OT  - ISA
OT  - LLM
OT  - Medical exam
OT  - Pass mark
OT  - Urology exam
EDAT- 2024/01/10 12:42
MHDA- 2024/01/11 07:42
CRDT- 2024/01/10 11:20
PHST- 2023/09/24 00:00 [received]
PHST- 2023/11/02 00:00 [accepted]
PHST- 2024/01/11 07:42 [medline]
PHST- 2024/01/10 12:42 [pubmed]
PHST- 2024/01/10 11:20 [entrez]
AID - 10.1007/s00345-023-04749-6 [pii]
AID - 10.1007/s00345-023-04749-6 [doi]
PST - epublish
SO  - World J Urol. 2024 Jan 10;42(1):20. doi: 10.1007/s00345-023-04749-6.

PMID- 36869927
OWN - NLM
STAT- MEDLINE
DCOM- 20230307
LR  - 20231231
IS  - 1573-689X (Electronic)
IS  - 0148-5598 (Print)
IS  - 0148-5598 (Linking)
VI  - 47
IP  - 1
DP  - 2023 Mar 4
TI  - Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple 
      Clinical and Research Scenarios.
PG  - 33
LID - 10.1007/s10916-023-01925-4 [doi]
LID - 33
AB  - This paper aims to highlight the potential applications and limits of a large 
      language model (LLM) in healthcare. ChatGPT is a recently developed LLM that was 
      trained on a massive dataset of text for dialogue with users. Although AI-based 
      language models like ChatGPT have demonstrated impressive capabilities, it is 
      uncertain how well they will perform in real-world scenarios, particularly in 
      fields such as medicine where high-level and complex thinking is necessary. 
      Furthermore, while the use of ChatGPT in writing scientific articles and other 
      scientific outputs may have potential benefits, important ethical concerns must 
      also be addressed. Consequently, we investigated the feasibility of ChatGPT in 
      clinical and research scenarios: (1) support of the clinical practice, (2) 
      scientific production, (3) misuse in medicine and research, and (4) reasoning 
      about public health topics. Results indicated that it is important to recognize 
      and promote education on the appropriate use and potential pitfalls of AI-based 
      LLMs in medicine.
CI  - © 2023. The Author(s).
FAU - Cascella, Marco
AU  - Cascella M
AD  - Department of Anesthesia and Critical Care, Istituto Nazionale Tumori - IRCCS, 
      Fondazione Pascale, Via Mariano Semmola, 53, 80131, Naples, Italy.
FAU - Montomoli, Jonathan
AU  - Montomoli J
AD  - Department of Anesthesia and Intensive Care, Infermi Hospital, AUSL Romagna, 
      Viale Settembrini 2, 47923, Rimini, Italy.
FAU - Bellini, Valentina
AU  - Bellini V
AD  - Anesthesiology, Critical Care and Pain Medicine Division, Department of Medicine 
      and Surgery, University of Parma, Viale Gramsci 14, 43126, Parma, Italy.
FAU - Bignami, Elena
AU  - Bignami E
AD  - Anesthesiology, Critical Care and Pain Medicine Division, Department of Medicine 
      and Surgery, University of Parma, Viale Gramsci 14, 43126, Parma, Italy. 
      elenagiovanna.bignami@unipr.it.
LA  - eng
PT  - Journal Article
DEP - 20230304
PL  - United States
TA  - J Med Syst
JT  - Journal of medical systems
JID - 7806056
SB  - IM
MH  - Humans
MH  - Feasibility Studies
MH  - Educational Status
MH  - *Language
MH  - Delivery of Health Care
PMC - PMC9985086
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Clinical resaerch
OT  - Medicine
COIS- The authors have no competing interests to declare that are relevant to the 
      content of this article.
EDAT- 2023/03/05 06:00
MHDA- 2023/03/08 06:00
PMCR- 2023/03/04
CRDT- 2023/03/04 11:15
PHST- 2023/01/21 00:00 [received]
PHST- 2023/02/20 00:00 [accepted]
PHST- 2023/03/04 11:15 [entrez]
PHST- 2023/03/05 06:00 [pubmed]
PHST- 2023/03/08 06:00 [medline]
PHST- 2023/03/04 00:00 [pmc-release]
AID - 10.1007/s10916-023-01925-4 [pii]
AID - 1925 [pii]
AID - 10.1007/s10916-023-01925-4 [doi]
PST - epublish
SO  - J Med Syst. 2023 Mar 4;47(1):33. doi: 10.1007/s10916-023-01925-4.

PMID- 38010917
OWN - NLM
STAT- MEDLINE
DCOM- 20240318
LR  - 20240318
IS  - 1557-9077 (Electronic)
IS  - 1050-7256 (Linking)
VI  - 34
IP  - 3
DP  - 2024 Mar
TI  - Evaluating ChatGPT Responses on Thyroid Nodules for Patient Education.
PG  - 371-377
LID - 10.1089/thy.2023.0491 [doi]
AB  - Background: ChatGPT, an artificial intelligence (AI) chatbot, is the fastest 
      growing consumer application in history. Given recent trends identifying 
      increasing patient use of Internet sources for self-education, we seek to 
      evaluate the quality of ChatGPT-generated responses for patient education on 
      thyroid nodules. Methods: ChatGPT was queried 4 times with 30 identical 
      questions. Queries differed by initial chatbot prompting: no prompting, 
      patient-friendly prompting, 8th-grade level prompting, and prompting for 
      references. Answers were scored on a hierarchical score: incorrect, partially 
      correct, correct, or correct with references. Proportions of responses at 
      incremental score thresholds were compared by prompt type using chi-squared 
      analysis. Flesch-Kincaid grade level was calculated for each answer. The 
      relationship between prompt type and grade level was assessed using analysis of 
      variance. References provided within ChatGPT answers were totaled and analyzed 
      for veracity. Results: Across all prompts (n = 120 questions), 83 answers (69.2%) 
      were at least correct. Proportions of responses that were at least partially 
      correct (p = 0.795) and correct (p = 0.402) did not differ by prompt; responses 
      that were correct with references did (p &lt; 0.0001). Responses from 8th-grade 
      level prompting were the lowest mean grade level (13.43 ± 2.86) and were 
      significantly lower than no prompting (14.97 ± 2.01, p = 0.01) and prompting for 
      references (16.43 ± 2.05, p &lt; 0.0001). Prompting for references generated 80/80 
      (100%) of referenced medical publications within answers. Seventy references 
      (87.5%) were legitimate citations, and 58/80 (72.5%) provided accurately reported 
      information from the referenced publication. Conclusion: ChatGPT overall provides 
      appropriate answers to most questions on thyroid nodules regardless of prompting. 
      Despite targeted prompting strategies, ChatGPT reliably generates responses 
      corresponding to grade levels well-above accepted recommendations for presenting 
      medical information to patients. Significant rates of AI hallucination may 
      preclude clinicians from recommending the current version of ChatGPT as an 
      educational tool for patients at this time.
FAU - Campbell, Daniel J
AU  - Campbell DJ
AUID- ORCID: 0000-0001-9001-5705
AD  - Department of Otolaryngology-Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania, USA.
FAU - Estephan, Leonard E
AU  - Estephan LE
AD  - Department of Otolaryngology-Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania, USA.
FAU - Sina, Elliott M
AU  - Sina EM
AD  - Department of Otolaryngology-Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania, USA.
FAU - Mastrolonardo, Eric V
AU  - Mastrolonardo EV
AD  - Department of Otolaryngology-Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania, USA.
FAU - Alapati, Rahul
AU  - Alapati R
AD  - Department of Otolaryngology-Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania, USA.
FAU - Amin, Dev R
AU  - Amin DR
AD  - Department of Otolaryngology-Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania, USA.
FAU - Cottrill, Elizabeth E
AU  - Cottrill EE
AD  - Department of Otolaryngology-Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania, USA.
LA  - eng
PT  - Journal Article
DEP - 20231226
PL  - United States
TA  - Thyroid
JT  - Thyroid : official journal of the American Thyroid Association
JID - 9104317
SB  - IM
MH  - Humans
MH  - *Thyroid Nodule/diagnosis
MH  - Artificial Intelligence
MH  - Patient Education as Topic
MH  - Educational Status
MH  - Internet
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - patient education
OT  - thyroid nodule
EDAT- 2023/11/27 18:44
MHDA- 2024/03/18 06:43
CRDT- 2023/11/27 12:33
PHST- 2024/03/18 06:43 [medline]
PHST- 2023/11/27 18:44 [pubmed]
PHST- 2023/11/27 12:33 [entrez]
AID - 10.1089/thy.2023.0491 [doi]
PST - ppublish
SO  - Thyroid. 2024 Mar;34(3):371-377. doi: 10.1089/thy.2023.0491. Epub 2023 Dec 26.

PMID- 37908959
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231102
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 9
DP  - 2023 Sep
TI  - Evaluating ChatGPT-3.5 and Claude-2 in Answering and Explaining Conceptual 
      Medical Physiology Multiple-Choice Questions.
PG  - e46222
LID - 10.7759/cureus.46222 [doi]
LID - e46222
AB  - Background Generative artificial intelligence (AI) systems such as ChatGPT-3.5 
      and Claude-2 may assist in explaining complex medical science topics. A few 
      studies have shown that AI can solve complicated physiology problems that require 
      critical thinking and analysis. However, further studies are required to validate 
      the effectiveness of AI in answering conceptual multiple-choice questions (MCQs) 
      in human physiology. Objective This study aimed to evaluate and compare the 
      proficiency of ChatGPT-3.5 and Claude-2 in answering and explaining a curated set 
      of MCQs in medical physiology. Methods In this cross-sectional study, a set of 55 
      MCQs from 10 competencies of medical physiology was purposefully constructed that 
      required comprehension, problem-solving, and analytical skills to solve them. The 
      MCQs and a structured prompt for response generation were presented to 
      ChatGPT-3.5 and Claude-2. The explanations provided by both AI systems were 
      documented in an Excel spreadsheet. All three authors subjected these 
      explanations to a rating process using a scale of 0 to 3. A rating of 0 was 
      assigned to an incorrect, 1 to a partially correct, 2 to a correct explanation 
      with some aspects missing, and 3 to a perfectly correct explanation. Both AI 
      models were evaluated for their ability to choose the correct answer (option) and 
      provide clear and comprehensive explanations of the MCQs. The Mann-Whitney U test 
      was used to compare AI responses. The Fleiss multi-rater kappa (κ) was used to 
      determine the score agreement among the three raters. The statistical 
      significance level was decided at P ≤ 0.05. Results Claude-2 answered 40 MCQs 
      correctly, which was significantly higher than the 26 correct responses from 
      ChatGPT-3.5. The rating distribution for the explanations generated by Claude-2 
      was significantly higher than that of ChatGPT-3.5. The κ values were 0.804 and 
      0.818 for Claude-2 and ChatGPT-3.5, respectively. Conclusion In terms of 
      answering and elucidating conceptual MCQs in medical physiology, Claude-2 
      surpassed ChatGPT-3.5. However, accessing Claude-2 from India requires the use of 
      a virtual private network, which may raise security concerns.
CI  - Copyright © 2023, Agarwal et al.
FAU - Agarwal, Mayank
AU  - Agarwal M
AD  - Physiology, All India Institute of Medical Sciences, Raebareli, IND.
FAU - Goswami, Ayan
AU  - Goswami A
AD  - Physiology, Santiniketan Medical College, Bolpur, IND.
FAU - Sharma, Priyanka
AU  - Sharma P
AD  - Physiology, School of Medical Sciences &amp; Research, Sharda University, Greater 
      Noida, IND.
LA  - eng
PT  - Journal Article
DEP - 20230929
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10613833
OTO - NOTNLM
OT  - artificial intelligence
OT  - chatgpt
OT  - claude
OT  - large language models
OT  - medical education
OT  - multiple choice questions
OT  - physiology
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/11/01 06:43
MHDA- 2023/11/01 06:44
PMCR- 2023/09/29
CRDT- 2023/11/01 04:04
PHST- 2023/09/29 00:00 [accepted]
PHST- 2023/11/01 06:44 [medline]
PHST- 2023/11/01 06:43 [pubmed]
PHST- 2023/11/01 04:04 [entrez]
PHST- 2023/09/29 00:00 [pmc-release]
AID - 10.7759/cureus.46222 [doi]
PST - epublish
SO  - Cureus. 2023 Sep 29;15(9):e46222. doi: 10.7759/cureus.46222. eCollection 2023 
      Sep.

PMID- 37553556
OWN - NLM
STAT- MEDLINE
DCOM- 20240315
LR  - 20240315
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 52
IP  - 4
DP  - 2024 Apr
TI  - Enhancing Diabetes Self-management and Education: A Critical Analysis of 
      ChatGPT's Role.
PG  - 741-744
LID - 10.1007/s10439-023-03317-8 [doi]
AB  - ChatGPT, an advanced natural language processing model, holds significant promise 
      in diabetes self-management and education. ChatGPT excels in providing 
      personalized educational experiences by tailoring information to meet individual 
      patient needs and preferences. It aids patients in developing self-management 
      skills and strategies, fostering proactive disease management. Additionally, 
      ChatGPT addresses healthcare access disparities by enabling patients to access 
      educational resources irrespective of their geographic location or physical 
      limitations. However, it is important to acknowledge and address the deficiencies 
      of ChatGPT, such as its limited medical expertise, contextual understanding, and 
      emotional support capabilities. Strategies for optimizing ChatGPT include regular 
      training and updating, integration of healthcare professionals' expertise, 
      improvement in contextual comprehension, and enhancing emotional support. By 
      addressing these limitations and striking a balance between the benefits and 
      limitations, ChatGPT can play a significant role in empowering patients to better 
      understand and manage diabetes. Further research and development are needed to 
      refine ChatGPT's capabilities and address ethical considerations, but its 
      integration in patient education holds the potential to transform healthcare 
      delivery and create a more informed and engaged patient population.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Zheng, Yue
AU  - Zheng Y
AUID- ORCID: 0000-0003-3865-7051
AD  - Cancer Center, West China Hospital, Sichuan University, Chengdu, 610041, Sichuan, 
      China.
FAU - Wu, Yijun
AU  - Wu Y
AD  - Cancer Center, West China Hospital, Sichuan University, Chengdu, 610041, Sichuan, 
      China.
FAU - Feng, Baijie
AU  - Feng B
AD  - West China School of Medicine, Sichuan University, Chengdu, 610041, China.
FAU - Wang, Laduona
AU  - Wang L
AD  - Cancer Center, West China Hospital, Sichuan University, Chengdu, 610041, Sichuan, 
      China.
FAU - Kang, Kai
AU  - Kang K
AD  - Cancer Center, West China Hospital, Sichuan University, Chengdu, 610041, Sichuan, 
      China. kaikang@wchscu.cn.
FAU - Zhao, Ailin
AU  - Zhao A
AD  - Department of Hematology, West China Hospital, Sichuan University, Chengdu, 
      610041, Sichuan, China. irenez20@outlook.com.
LA  - eng
GR  - 2023NSFSC1885/Natural Science Foundation of Sichuan Province/
GR  - 2022SCUH0025/"from zero to one" Innovation Research Project of Sichuan 
      University/
GR  - 2022-YF05-01443-SN/Chengdu Science and Technology Program/
GR  - 23ZDYF2836/Key Research and Development Program of Sichuan Province/
GR  - 2021M692310/China Postdoctoral Science Foundation/
GR  - 82204490/National Natural Science Foundation of China/
PT  - Letter
DEP - 20230808
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - Humans
MH  - *Self-Management
MH  - Disease Management
MH  - Health Personnel
MH  - Healthcare Disparities
MH  - *Diabetes Mellitus/therapy
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Diabetes self-management and education
EDAT- 2023/08/09 01:05
MHDA- 2024/03/15 06:44
CRDT- 2023/08/08 23:30
PHST- 2023/07/06 00:00 [received]
PHST- 2023/07/10 00:00 [accepted]
PHST- 2024/03/15 06:44 [medline]
PHST- 2023/08/09 01:05 [pubmed]
PHST- 2023/08/08 23:30 [entrez]
AID - 10.1007/s10439-023-03317-8 [pii]
AID - 10.1007/s10439-023-03317-8 [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2024 Apr;52(4):741-744. doi: 10.1007/s10439-023-03317-8. Epub 
      2023 Aug 8.

PMID- 38540647
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240330
IS  - 2227-9032 (Print)
IS  - 2227-9032 (Electronic)
IS  - 2227-9032 (Linking)
VI  - 12
IP  - 6
DP  - 2024 Mar 19
TI  - Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement.
LID - 10.3390/healthcare12060683 [doi]
LID - 683
AB  - This study explores the integration of large language models (LLMs), like 
      ChatGPT, to improve attention deficit hyperactivity disorder (ADHD) treatments. 
      Utilizing the Delphi method for its systematic forecasting capabilities, we 
      gathered a panel of child ADHD therapy experts. These experts interacted with our 
      custom ChatGPT through a specialized interface, thus engaging in simulated 
      therapy scenarios with behavioral prompts and commands. Using empirical tests and 
      expert feedback, we aimed to rigorously evaluate ChatGPT's effectiveness in 
      therapy settings to integrate AI into healthcare responsibly. We sought to ensure 
      that AI contributes positively and ethically to therapy and patient care, thus 
      filling a gap in ADHD treatment methods. Findings show ChatGPT's empathy, 
      adaptability, and communication strengths, thereby highlighting its potential to 
      significantly improve ADHD care. The study points to ChatGPT's capacity to 
      transform therapy practices through personalized and responsive patient care. 
      However, it also notes the need for enhancements in privacy, cultural 
      sensitivity, and interpreting nonverbal cues for ChatGPT's effective healthcare 
      integration. Our research advocates for merging technological innovation with a 
      comprehensive understanding of patient needs and ethical considerations, thereby 
      aiming to pioneer a new era of AI-assisted therapy. We emphasize the ongoing 
      refinement of AI tools like ChatGPT to meet ADHD therapy and patient care 
      requirements more effectively.
FAU - Berrezueta-Guzman, Santiago
AU  - Berrezueta-Guzman S
AUID- ORCID: 0000-0001-5559-2056
AD  - Applied Software Engineering Research Group, School of Computation, Information, 
      and Technology, Technical University of Munich, 80333 Munich, Germany.
FAU - Kandil, Mohanad
AU  - Kandil M
AUID- ORCID: 0009-0009-9713-098X
AD  - Applied Software Engineering Research Group, School of Computation, Information, 
      and Technology, Technical University of Munich, 80333 Munich, Germany.
FAU - Martín-Ruiz, María-Luisa
AU  - Martín-Ruiz ML
AUID- ORCID: 0000-0002-4355-3620
AD  - Grupo de Investigación Innovación Tecnológica para las Personas (InnoTep), 
      Departamento de Ingeniería Telemática y Electrónica, ETSIS de Telecomunicación, 
      Campus Sur, Universidad Politécnica de Madrid, 28031 Madrid, Spain.
FAU - Pau de la Cruz, Iván
AU  - Pau de la Cruz I
AUID- ORCID: 0000-0002-1183-4401
AD  - Grupo de Investigación Innovación Tecnológica para las Personas (InnoTep), 
      Departamento de Ingeniería Telemática y Electrónica, ETSIS de Telecomunicación, 
      Campus Sur, Universidad Politécnica de Madrid, 28031 Madrid, Spain.
FAU - Krusche, Stephan
AU  - Krusche S
AUID- ORCID: 0000-0002-4552-644X
AD  - Applied Software Engineering Research Group, School of Computation, Information, 
      and Technology, Technical University of Munich, 80333 Munich, Germany.
LA  - eng
PT  - Journal Article
DEP - 20240319
PL  - Switzerland
TA  - Healthcare (Basel)
JT  - Healthcare (Basel, Switzerland)
JID - 101666525
PMC - PMC10970191
OTO - NOTNLM
OT  - ADHD
OT  - AI in mental health
OT  - AI-driven decision making
OT  - ChatGPT
OT  - LLMs
OT  - artificial intelligence
OT  - cognitive therapy
OT  - computational cognitive tools
OT  - customizable AI bots
OT  - occupationaltherapy innovation
OT  - personalized therapy sessions
OT  - robotic systems in therapy
OT  - sensory data integration
COIS- The authors declare no conflict of interest.
EDAT- 2024/03/28 06:45
MHDA- 2024/03/28 06:46
PMCR- 2024/03/19
CRDT- 2024/03/28 01:08
PHST- 2024/01/22 00:00 [received]
PHST- 2024/02/22 00:00 [revised]
PHST- 2024/03/14 00:00 [accepted]
PHST- 2024/03/28 06:46 [medline]
PHST- 2024/03/28 06:45 [pubmed]
PHST- 2024/03/28 01:08 [entrez]
PHST- 2024/03/19 00:00 [pmc-release]
AID - healthcare12060683 [pii]
AID - healthcare-12-00683 [pii]
AID - 10.3390/healthcare12060683 [doi]
PST - epublish
SO  - Healthcare (Basel). 2024 Mar 19;12(6):683. doi: 10.3390/healthcare12060683.

PMID- 38327910
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240210
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 16
IP  - 1
DP  - 2024 Jan
TI  - Understanding the Landscape: The Emergence of Artificial Intelligence (AI), 
      ChatGPT, and Google Bard in Gastroenterology.
PG  - e51848
LID - 10.7759/cureus.51848 [doi]
LID - e51848
AB  - Introduction Artificial intelligence (AI) integration in healthcare, specifically 
      in gastroenterology, has opened new avenues for enhanced patient care and medical 
      decision-making. This study aims to assess the reliability and accuracy of two 
      prominent AI tools, ChatGPT 4.0 and Google Bard, in answering 
      gastroenterology-related queries, thereby evaluating their potential utility in 
      medical settings. Methods The study employed a structured approach where typical 
      gastroenterology questions were input into ChatGPT 4.0 and Google Bard. 
      Independent reviewers evaluated responses using a Likert scale and 
      cross-referenced them with guidelines from authoritative gastroenterology bodies. 
      Statistical analysis, including the Mann-Whitney U test, was conducted to assess 
      the significance of differences in ratings. Results ChatGPT 4.0 demonstrated 
      higher reliability and accuracy in its responses than Google Bard, as indicated 
      by higher mean ratings and statistically significant p-values in hypothesis 
      testing. However, limitations in the data structure, such as the inability to 
      conduct detailed correlation analysis, were noted. Conclusion The study concludes 
      that ChatGPT 4.0 outperforms Google Bard in providing reliable and accurate 
      responses to gastroenterology-related queries. This finding underscores the 
      potential of AI tools like ChatGPT in enhancing healthcare delivery. However, the 
      study also highlights the need for a broader and more diverse assessment of AI 
      capabilities in healthcare to leverage their potential in clinical practice 
      fully.
CI  - Copyright © 2024, Rammohan et al.
FAU - Rammohan, Rajmohan
AU  - Rammohan R
AD  - Gastroenterology, Nassau University Medical Center, East Meadow, USA.
FAU - Joy, Melvin V
AU  - Joy MV
AD  - Internal Medicine, Nassau University Medical Center, East Meadow, USA.
FAU - Magam, Sai Greeshma
AU  - Magam SG
AD  - Internal Medicine, Nassau University Medical Center, East Meadow, USA.
FAU - Natt, Dilman
AU  - Natt D
AD  - Internal Medicine, Nassau University Medical Center, East Meadow, USA.
FAU - Magam, Sai Reshma
AU  - Magam SR
AD  - Internal Medicine, Nassau University Medical Center, East Meadow, USA.
FAU - Pannikodu, Leeza
AU  - Pannikodu L
AD  - Internal Medicine, Nassau University Medical Center, East Meadow, USA.
FAU - Desai, Jiten
AU  - Desai J
AD  - Internal Medicine, Nassau University Medical Center, East Meadow, USA.
FAU - Akande, Olawale
AU  - Akande O
AD  - Internal Medicine, Nassau University Medical Center, East Meadow, USA.
FAU - Bunting, Susan
AU  - Bunting S
AD  - Internal Medicine, Nassau University Medical Center, East Meadow, USA.
FAU - Yost, Robert M
AU  - Yost RM
AD  - Internal Medicine, Nassau University Medical Center, East Meadow, USA.
FAU - Mustacchia, Paul
AU  - Mustacchia P
AD  - Gastroenterology and Hepatology, Nassau University Medical Center, East Meadow, 
      USA.
LA  - eng
PT  - Journal Article
DEP - 20240108
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10847895
OTO - NOTNLM
OT  - acute pancreatitis
OT  - chatgpt
OT  - gastroenterology
OT  - google bard
OT  - liver cirrhosis
OT  - openai
OT  - pancreatitis
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/02/08 06:42
MHDA- 2024/02/08 06:43
PMCR- 2024/01/08
CRDT- 2024/02/08 04:12
PHST- 2024/01/07 00:00 [accepted]
PHST- 2024/02/08 06:43 [medline]
PHST- 2024/02/08 06:42 [pubmed]
PHST- 2024/02/08 04:12 [entrez]
PHST- 2024/01/08 00:00 [pmc-release]
AID - 10.7759/cureus.51848 [doi]
PST - epublish
SO  - Cureus. 2024 Jan 8;16(1):e51848. doi: 10.7759/cureus.51848. eCollection 2024 Jan.

PMID- 37183438
OWN - NLM
STAT- MEDLINE
DCOM- 20230706
LR  - 20230718
IS  - 1097-0142 (Electronic)
IS  - 0008-543X (Linking)
VI  - 129
IP  - 15
DP  - 2023 Aug 1
TI  - ChatGPT-A promising generative AI tool and its implications for cancer care.
PG  - 2284-2289
LID - 10.1002/cncr.34827 [doi]
AB  - Since its launch, ChatGPT has taken the internet by storm and has the potential 
      to be used broadly in the health care system, particularly in a setting such as 
      medical oncology. ChatGPT is well suited to review and extract key content from 
      records of patients with cancer, interpret next-generation sequencing reports, 
      and offer a list of potential clinical trial options.
CI  - © 2023 American Cancer Society.
FAU - Uprety, Dipesh
AU  - Uprety D
AUID- ORCID: 0000-0002-4278-9703
AD  - Department of Medical Oncology, Barbara Ann Karmanos Cancer Institute, Detroit, 
      Michigan, USA.
FAU - Zhu, Dongxiao
AU  - Zhu D
AD  - Department of Computer Science, Wayne State University, Detroit, Michigan, USA.
FAU - West, Howard Jack
AU  - West HJ
AD  - Department of Medical Oncology, City of Hope Comprehensive Cancer Center, Duarte, 
      California, USA.
LA  - eng
PT  - Journal Article
DEP - 20230514
PL  - United States
TA  - Cancer
JT  - Cancer
JID - 0374236
SB  - IM
MH  - Humans
MH  - *High-Throughput Nucleotide Sequencing
MH  - Internet
MH  - Medical Oncology
MH  - *Neoplasms/therapy
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence (AI)
OT  - cancer care
OT  - oncology
EDAT- 2023/05/15 06:42
MHDA- 2023/07/06 06:42
CRDT- 2023/05/15 03:41
PHST- 2023/07/06 06:42 [medline]
PHST- 2023/05/15 06:42 [pubmed]
PHST- 2023/05/15 03:41 [entrez]
AID - 10.1002/cncr.34827 [doi]
PST - ppublish
SO  - Cancer. 2023 Aug 1;129(15):2284-2289. doi: 10.1002/cncr.34827. Epub 2023 May 14.

PMID- 37040823
OWN - NLM
STAT- MEDLINE
DCOM- 20230612
LR  - 20240213
IS  - 1532-8406 (Electronic)
IS  - 0883-5403 (Linking)
VI  - 38
IP  - 7
DP  - 2023 Jul
TI  - Using a Google Web Search Analysis to Assess the Utility of ChatGPT in Total 
      Joint Arthroplasty.
PG  - 1195-1202
LID - S0883-5403(23)00352-2 [pii]
LID - 10.1016/j.arth.2023.04.007 [doi]
AB  - BACKGROUND: Rapid technological advancements have laid the foundations for the 
      use of artificial intelligence in medicine. The promise of machine learning (ML) 
      lies in its potential ability to improve treatment decision making, predict 
      adverse outcomes, and streamline the management of perioperative healthcare. In 
      an increasing consumer-focused health care model, unprecedented access to 
      information may extend to patients using ChatGPT to gain insight into medical 
      questions. The main objective of our study was to replicate a patient's internet 
      search in order to assess the appropriateness of ChatGPT, a novel machine 
      learning tool released in 2022 that provides dialogue responses to queries, in 
      comparison to Google Web Search, the most widely used search engine in the United 
      States today, as a resource for patients for online health information. For the 2 
      different search engines, we compared i) the most frequently asked questions 
      (FAQs) associated with total knee arthroplasty (TKA) and total hip arthroplasty 
      (THA) by question type and topic; ii) the answers to the most frequently asked 
      questions; as well as iii) the FAQs yielding a numerical response. METHODS: A 
      Google web search was performed with the following search terms: "total knee 
      replacement" and "total hip replacement." These terms were individually entered 
      and the first 10 FAQs were extracted along with the source of the associated 
      website for each question. The following statements were inputted into ChatGPT: 
      1) "Perform a google search with the search term 'total knee replacement' and 
      record the 10 most FAQs related to the search term" as well as 2) "Perform a 
      google search with the search term 'total hip replacement' and record the 10 most 
      FAQs related to the search term." A Google web search was repeated with the same 
      search terms to identify the first 10 FAQs that included a numerical response for 
      both "total knee replacement" and "total hip replacement." These questions were 
      then inputted into ChatGPT and the questions and answers were recorded. RESULTS: 
      There were 5 of 20 (25%) questions that were similar when performing a Google web 
      search and a search of ChatGPT for all search terms. Of the 20 questions asked 
      for the Google Web Search, 13 of 20 were provided by commercial websites. For 
      ChatGPT, 15 of 20 (75%) questions were answered by government websites, with the 
      most frequent one being PubMed. In terms of numerical questions, 11 of 20 (55%) 
      of the most FAQs provided different responses between a Google web search and 
      ChatGPT. CONCLUSION: A comparison of the FAQs by a Google web search with 
      attempted replication by ChatGPT revealed heterogenous questions and responses 
      for open and discrete questions. ChatGPT should remain a trending use as a 
      potential resource to patients that needs further corroboration until its ability 
      to provide credible information is verified and concordant with the goals of the 
      physician and the patient alike.
CI  - Copyright © 2023. Published by Elsevier Inc.
FAU - Dubin, Jeremy A
AU  - Dubin JA
AD  - LifeBridge Health, Sinai Hospital of Baltimore, Rubin Institute for Advanced 
      Orthopedics, Baltimore, Maryland.
FAU - Bains, Sandeep S
AU  - Bains SS
AD  - LifeBridge Health, Sinai Hospital of Baltimore, Rubin Institute for Advanced 
      Orthopedics, Baltimore, Maryland.
FAU - Chen, Zhongming
AU  - Chen Z
AD  - LifeBridge Health, Sinai Hospital of Baltimore, Rubin Institute for Advanced 
      Orthopedics, Baltimore, Maryland.
FAU - Hameed, Daniel
AU  - Hameed D
AD  - LifeBridge Health, Sinai Hospital of Baltimore, Rubin Institute for Advanced 
      Orthopedics, Baltimore, Maryland.
FAU - Nace, James
AU  - Nace J
AD  - LifeBridge Health, Sinai Hospital of Baltimore, Rubin Institute for Advanced 
      Orthopedics, Baltimore, Maryland.
FAU - Mont, Michael A
AU  - Mont MA
AD  - LifeBridge Health, Sinai Hospital of Baltimore, Rubin Institute for Advanced 
      Orthopedics, Baltimore, Maryland.
FAU - Delanois, Ronald E
AU  - Delanois RE
AD  - LifeBridge Health, Sinai Hospital of Baltimore, Rubin Institute for Advanced 
      Orthopedics, Baltimore, Maryland.
LA  - eng
PT  - Journal Article
DEP - 20230410
PL  - United States
TA  - J Arthroplasty
JT  - The Journal of arthroplasty
JID - 8703515
SB  - IM
CIN - J Arthroplasty. 2023 Sep;38(9):e17. PMID: 37573081
CIN - J Arthroplasty. 2023 Sep;38(9):e18. PMID: 37573082
CIN - J Arthroplasty. 2023 Sep;38(9):e19-e20. PMID: 37573083
CIN - J Arthroplasty. 2023 Sep;38(9):e21. PMID: 37573084
MH  - Humans
MH  - Search Engine
MH  - Artificial Intelligence
MH  - *Arthroplasty, Replacement, Knee
MH  - *Arthroplasty, Replacement, Hip
OTO - NOTNLM
OT  - ChatGPT
OT  - google
OT  - total joint arthroplasty
OT  - utility
OT  - web search
EDAT- 2023/04/12 06:00
MHDA- 2023/06/12 06:42
CRDT- 2023/04/11 19:14
PHST- 2023/02/07 00:00 [received]
PHST- 2023/03/22 00:00 [revised]
PHST- 2023/04/03 00:00 [accepted]
PHST- 2023/06/12 06:42 [medline]
PHST- 2023/04/12 06:00 [pubmed]
PHST- 2023/04/11 19:14 [entrez]
AID - S0883-5403(23)00352-2 [pii]
AID - 10.1016/j.arth.2023.04.007 [doi]
PST - ppublish
SO  - J Arthroplasty. 2023 Jul;38(7):1195-1202. doi: 10.1016/j.arth.2023.04.007. Epub 
      2023 Apr 10.

PMID- 37693092
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230913
IS  - 2472-7245 (Electronic)
IS  - 2472-7245 (Linking)
VI  - 8
IP  - 3
DP  - 2023 Jul-Sep
TI  - Evaluating ChatGPT Performance on the Orthopaedic In-Training Examination.
LID - 10.2106/JBJS.OA.23.00056 [doi]
LID - e23.00056
AB  - BACKGROUND: Artificial intelligence (AI) holds potential in improving medical 
      education and healthcare delivery. ChatGPT is a state-of-the-art natural language 
      processing AI model which has shown impressive capabilities, scoring in the top 
      percentiles on numerous standardized examinations, including the Uniform Bar Exam 
      and Scholastic Aptitude Test. The goal of this study was to evaluate ChatGPT 
      performance on the Orthopaedic In-Training Examination (OITE), an assessment of 
      medical knowledge for orthopedic residents. METHODS: OITE 2020, 2021, and 2022 
      questions without images were inputted into ChatGPT version 3.5 and version 4 
      (GPT-4) with zero prompting. The performance of ChatGPT was evaluated as a 
      percentage of correct responses and compared with the national average of 
      orthopedic surgery residents at each postgraduate year (PGY) level. ChatGPT was 
      asked to provide a source for its answer, which was categorized as being a 
      journal article, book, or website, and if the source could be verified. Impact 
      factor for the journal cited was also recorded. RESULTS: ChatGPT answered 196 of 
      360 answers correctly (54.3%), corresponding to a PGY-1 level. ChatGPT cited a 
      verifiable source in 47.2% of questions, with an average median journal impact 
      factor of 5.4. GPT-4 answered 265 of 360 questions correctly (73.6%), 
      corresponding to the average performance of a PGY-5 and exceeding the 
      corresponding passing score for the American Board of Orthopaedic Surgery Part I 
      Examination of 67%. GPT-4 cited a verifiable source in 87.9% of questions, with 
      an average median journal impact factor of 5.2. CONCLUSIONS: ChatGPT performed 
      above the average PGY-1 level and GPT-4 performed better than the average PGY-5 
      level, showing major improvement. Further investigation is needed to determine 
      how successive versions of ChatGPT would perform and how to optimize this 
      technology to improve medical education. CLINICAL RELEVANCE: AI has the potential 
      to aid in medical education and healthcare delivery.
CI  - Copyright © 2023 The Authors. Published by The Journal of Bone and Joint Surgery, 
      Incorporated. All rights reserved.
FAU - Kung, Justin E
AU  - Kung JE
AUID- ORCID: 0000-0003-2536-5076
AD  - Department of Orthopedic Surgery, Prisma Health-Midlands University of South 
      Carolina, Columbia, South Carolina.
FAU - Marshall, Christopher
AU  - Marshall C
AUID- ORCID: 0009-0003-3212-0656
AD  - University of South Carolina School of Medicine, Columbia, South Carolina.
FAU - Gauthier, Chase
AU  - Gauthier C
AUID- ORCID: 0009-0004-4738-8468
AD  - Department of Orthopedic Surgery, Prisma Health-Midlands University of South 
      Carolina, Columbia, South Carolina.
FAU - Gonzalez, Tyler A
AU  - Gonzalez TA
AUID- ORCID: 0000-0002-3210-8097
AD  - Department of Orthopedic Surgery, Prisma Health-Midlands University of South 
      Carolina, Columbia, South Carolina.
FAU - Jackson, J Benjamin 3rd
AU  - Jackson JB 3rd
AD  - Department of Orthopedic Surgery, Prisma Health-Midlands University of South 
      Carolina, Columbia, South Carolina.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20230908
PL  - United States
TA  - JB JS Open Access
JT  - JB &amp; JS open access
JID - 101726219
PMC - PMC10484364
EDAT- 2023/09/11 06:43
MHDA- 2023/09/11 06:44
PMCR- 2023/09/08
CRDT- 2023/09/11 04:47
PHST- 2023/09/11 06:44 [medline]
PHST- 2023/09/11 06:43 [pubmed]
PHST- 2023/09/11 04:47 [entrez]
PHST- 2023/09/08 00:00 [pmc-release]
AID - JBJSOA-D-23-00056 [pii]
AID - 10.2106/JBJS.OA.23.00056 [doi]
PST - epublish
SO  - JB JS Open Access. 2023 Sep 8;8(3):e23.00056. doi: 10.2106/JBJS.OA.23.00056. 
      eCollection 2023 Jul-Sep.

PMID- 38144348
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231225
IS  - 2405-8440 (Print)
IS  - 2405-8440 (Electronic)
IS  - 2405-8440 (Linking)
VI  - 9
IP  - 12
DP  - 2023 Dec
TI  - A systematic review and meta-analysis on ChatGPT and its utilization in medical 
      and dental research.
PG  - e23050
LID - 10.1016/j.heliyon.2023.e23050 [doi]
LID - e23050
AB  - Since its release, ChatGPT has taken the world by storm with its utilization in 
      various fields of life. This review's main goal was to offer a thorough and 
      fact-based evaluation of ChatGPT's potential as a tool for medical and dental 
      research, which could direct subsequent research and influence clinical 
      practices. METHODS: Different online databases were scoured for relevant articles 
      that were in accordance with the study objectives. A team of reviewers was 
      assembled to devise a proper methodological framework for inclusion of articles 
      and meta-analysis. RESULTS: 11 descriptive studies were considered for this 
      review that evaluated the accuracy of ChatGPT in answering medical queries 
      related to different domains such as systematic reviews, cancer, liver diseases, 
      diagnostic imaging, education, and COVID-19 vaccination. The studies reported 
      different accuracy ranges, from 18.3&nbsp;% to 100&nbsp;%, across various datasets and 
      specialties. The meta-analysis showed an odds ratio (OR) of 2.25 and a relative 
      risk (RR) of 1.47 with a 95&nbsp;% confidence interval (CI), indicating that the 
      accuracy of ChatGPT in providing correct responses was significantly higher 
      compared to the total responses for queries. However, significant heterogeneity 
      was present among the studies, suggesting considerable variability in the effect 
      sizes across the included studies. CONCLUSION: The observations indicate that 
      ChatGPT has the ability to provide appropriate solutions to questions in the 
      medical and dentistry areas, but researchers and doctors should cautiously assess 
      its responses because they might not always be dependable. Overall, the 
      importance of this study rests in shedding light on ChatGPT's accuracy in the 
      medical and dentistry fields and emphasizing the need for additional 
      investigation to enhance its performance. © 2017 Elsevier Inc. All rights 
      reserved.
CI  - © 2023 The Authors.
FAU - Bagde, Hiroj
AU  - Bagde H
AD  - Department of Periodontology, Chhattisgarh Dental College and Research Institute, 
      Rajnandgaon, Chhattisgarh, India.
FAU - Dhopte, Ashwini
AU  - Dhopte A
AD  - Department of Oral Medicine and Radiology, Chhattisgarh Dental College and 
      Research Institute, Rajnandgaon, Chhattisgarh, India.
FAU - Alam, Mohammad Khursheed
AU  - Alam MK
AD  - Preventive Dentistry Department, College of Dentistry, Jouf University, Sakaka, 
      72345, Saudi Arabia.
AD  - Department of Dental Research Cell, Saveetha Dental College and Hospitals, 
      Saveetha Institute of Medical and Technical Sciences, Chennai, India.
AD  - Department of Public Health, Faculty of Allied Health Sciences, Daffodil 
      International University, Dhaka, Bangladesh.
FAU - Basri, Rehana
AU  - Basri R
AD  - Department of Internal Medicine, College of Medicine, Jouf University, Sakaka, 
      72345, Saudi Arabia.
LA  - eng
PT  - Journal Article
DEP - 20231129
PL  - England
TA  - Heliyon
JT  - Heliyon
JID - 101672560
PMC - PMC10746423
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Dentistry
OT  - Machine learning
OT  - Medicine
COIS- The authors declare that they have no known competing financial interests or 
      personal relationships that could have appeared to influence the work reported in 
      this paper.
EDAT- 2023/12/25 06:43
MHDA- 2023/12/25 06:44
PMCR- 2023/11/29
CRDT- 2023/12/25 04:30
PHST- 2023/05/07 00:00 [received]
PHST- 2023/10/24 00:00 [revised]
PHST- 2023/11/24 00:00 [accepted]
PHST- 2023/12/25 06:44 [medline]
PHST- 2023/12/25 06:43 [pubmed]
PHST- 2023/12/25 04:30 [entrez]
PHST- 2023/11/29 00:00 [pmc-release]
AID - S2405-8440(23)10258-1 [pii]
AID - e23050 [pii]
AID - 10.1016/j.heliyon.2023.e23050 [doi]
PST - epublish
SO  - Heliyon. 2023 Nov 29;9(12):e23050. doi: 10.1016/j.heliyon.2023.e23050. 
      eCollection 2023 Dec.

PMID- 37821602
OWN - NLM
STAT- Publisher
LR  - 20231011
IS  - 1432-0932 (Electronic)
IS  - 0940-6719 (Linking)
DP  - 2023 Oct 11
TI  - Large language models: Are artificial intelligence-based chatbots a reliable 
      source of patient information for spinal surgery?
LID - 10.1007/s00586-023-07975-z [doi]
AB  - PURPOSE: Large language models (LLM) have recently attracted attention because of 
      their enormous performance. Based on artificial intelligence, LLM enable dialogic 
      communication using quasi-natural language that approximates the quality of human 
      communication. Thus, LLM could play an important role for patients to become 
      informed. To evaluate the validity of an LLM in providing medical information, we 
      used one of the first high-performance LLM (ChatGPT) on the clinical example of 
      acute lumbar disc herniation (LDH). METHODS: Twenty-four spinal surgeons 
      experienced in LDH surgery directed questions to ChatGPT about the clinical 
      picture of LDH from a patient's perspective. They evaluated the quality of 
      ChatGPT responses and its potential use in medical communication. The responses 
      were compared with the information content of a standard informed consent form. 
      RESULTS: ChatGPT provided good results in terms of comprehensibility, 
      specificity, and satisfaction of responses and in terms of medical accuracy and 
      completeness. ChatGPT was not able to provide all the information that was 
      provided in the informed consent form, but did communicate information that was 
      not listed there. In some cases, albeit minor, ChatGPT made medically inaccurate 
      claims, such as listing kyphoplasty and vertebroplasty as surgical options for 
      LDH. CONCLUSION: With the incipient use of artificial intelligence in 
      communication, LLM will certainly become increasingly important to patients. Even 
      if LLM are unlikely to play a role in clinical communication between physicians 
      and patients at the moment, the opportunities-but also the risks-of this novel 
      technology should be alertly monitored.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Stroop, Anna
AU  - Stroop A
AD  - Faculty of Health, Department of Medicine, Witten-Herdecke University, 
      Alfred-Herrhausen-Straße 45, 58455, Witten, Germany.
FAU - Stroop, Tabea
AU  - Stroop T
AD  - Faculty of Health, Department of Medicine, Witten-Herdecke University, 
      Alfred-Herrhausen-Straße 45, 58455, Witten, Germany.
FAU - Zawy Alsofy, Samer
AU  - Zawy Alsofy S
AD  - Faculty of Health, Department of Medicine, Witten-Herdecke University, 
      Alfred-Herrhausen-Straße 45, 58455, Witten, Germany.
AD  - Department of Neurosurgery, St. Barbara-Hospital, Academic Hospital of 
      Westfälische Wilhelms-University Münster, Hamm, Germany.
FAU - Nakamura, Makoto
AU  - Nakamura M
AD  - Department of Neurosurgery, Academic Hospital Köln-Merheim, Witten-Herdecke 
      University, Cologne, Germany.
FAU - Möllmann, Frank
AU  - Möllmann F
AD  - Department for Neuro- and Spine Surgery, Niels Stensen Neuro Center, Osnabrück, 
      Germany.
FAU - Greiner, Christoph
AU  - Greiner C
AD  - Department for Neuro- and Spine Surgery, Niels Stensen Neuro Center, Osnabrück, 
      Germany.
FAU - Stroop, Ralf
AU  - Stroop R
AUID- ORCID: 0000-0001-8795-6790
AD  - Faculty of Health, Department of Medicine, Witten-Herdecke University, 
      Alfred-Herrhausen-Straße 45, 58455, Witten, Germany. ralf.stroop@uni-wh.de.
AD  - Medical School Hamburg, Hamburg, Germany. ralf.stroop@uni-wh.de.
LA  - eng
PT  - Journal Article
DEP - 20231011
PL  - Germany
TA  - Eur Spine J
JT  - European spine journal : official publication of the European Spine Society, the 
      European Spinal Deformity Society, and the European Section of the Cervical Spine 
      Research Society
JID - 9301980
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - Large language model
OT  - Patient information
OT  - Spinal surgery
EDAT- 2023/10/12 00:43
MHDA- 2023/10/12 00:43
CRDT- 2023/10/11 23:30
PHST- 2023/03/12 00:00 [received]
PHST- 2023/09/25 00:00 [accepted]
PHST- 2023/08/31 00:00 [revised]
PHST- 2023/10/12 00:43 [medline]
PHST- 2023/10/12 00:43 [pubmed]
PHST- 2023/10/11 23:30 [entrez]
AID - 10.1007/s00586-023-07975-z [pii]
AID - 10.1007/s00586-023-07975-z [doi]
PST - aheadofprint
SO  - Eur Spine J. 2023 Oct 11. doi: 10.1007/s00586-023-07975-z.

PMID- 37246194
OWN - NLM
STAT- Publisher
LR  - 20230816
IS  - 1476-5438 (Electronic)
IS  - 1018-4813 (Linking)
DP  - 2023 May 29
TI  - Analysis of large-language model versus human performance for genetics questions.
LID - 10.1038/s41431-023-01396-8 [doi]
AB  - Large-language models like ChatGPT have recently received a great deal of 
      attention. One area of interest pertains to how these models could be used in 
      biomedical contexts, including related to human genetics. To assess one facet of 
      this, we compared the performance of ChatGPT versus human respondents (13,642 
      human responses) in answering 85 multiple-choice questions about aspects of human 
      genetics. Overall, ChatGPT did not perform significantly differently (p = 0.8327) 
      than human respondents; ChatGPT was 68.2% accurate, compared to 66.6% accuracy 
      for human respondents. Both ChatGPT and humans performed better on 
      memorization-type questions versus critical thinking questions (p &lt; 0.0001). When 
      asked the same question multiple times, ChatGPT frequently provided different 
      answers (16% of initial responses), including for both initially correct and 
      incorrect answers, and gave plausible explanations for both correct and incorrect 
      answers. ChatGPT's performance was impressive, but currently demonstrates 
      significant shortcomings for clinical or other high-stakes use. Addressing these 
      limitations will be important to guide adoption in real-life situations.
CI  - © 2023. This is a U.S. Government work and not under copyright protection in the 
      US; foreign copyright protection may apply.
FAU - Duong, Dat
AU  - Duong D
AD  - Medical Genomics Unit, Medical Genetics Branch, National Human Genome Research 
      Institute, Bethesda, MD, USA.
FAU - Solomon, Benjamin D
AU  - Solomon BD
AUID- ORCID: 0000-0002-8826-1023
AD  - Medical Genomics Unit, Medical Genetics Branch, National Human Genome Research 
      Institute, Bethesda, MD, USA. solomonb@mail.nih.gov.
LA  - eng
GR  - N/A/U.S. Department of Health &amp; Human Services | NIH | National Human Genome 
      Research Institute (NHGRI)/
PT  - Journal Article
DEP - 20230529
PL  - England
TA  - Eur J Hum Genet
JT  - European journal of human genetics : EJHG
JID - 9302235
SB  - IM
UOF - medRxiv. 2023 Jan 28;:. PMID: 36789422
CIN - Eur J Hum Genet. 2023 Jul 5;:. PMID: 37407734
EDAT- 2023/05/29 00:42
MHDA- 2023/05/29 00:42
CRDT- 2023/05/28 23:05
PHST- 2023/02/08 00:00 [received]
PHST- 2023/05/16 00:00 [accepted]
PHST- 2023/05/09 00:00 [revised]
PHST- 2023/05/29 00:42 [pubmed]
PHST- 2023/05/29 00:42 [medline]
PHST- 2023/05/28 23:05 [entrez]
AID - 10.1038/s41431-023-01396-8 [pii]
AID - 10.1038/s41431-023-01396-8 [doi]
PST - aheadofprint
SO  - Eur J Hum Genet. 2023 May 29. doi: 10.1038/s41431-023-01396-8.

PMID- 37368124
OWN - NLM
STAT- MEDLINE
DCOM- 20240125
LR  - 20240204
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 52
IP  - 2
DP  - 2024 Feb
TI  - Need an AI-Enabled, Next-Generation, Advanced ChatGPT or Large Language Models 
      (LLMs) for Error-Free and Accurate Medical Information.
PG  - 134-135
LID - 10.1007/s10439-023-03297-9 [doi]
AB  - Recently, the interest in AI-guided ChatGPT has increased day-to-day, and 
      different applications have been explored, including the medical field. The 
      publication number is also increasing. At the same time, people are trying to get 
      medical information from this Chartbot. However, researchers found that ChatGPT 
      also provides partly correct or false information. Therefore, in this article, we 
      urge the researchers to develop an AI-enabled, next-generation, advanced ChatGPT 
      or large language models (LLMs) so that people can get accurate and error-free 
      medical information.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Chakraborty, Chiranjib
AU  - Chakraborty C
AUID- ORCID: 0000-0002-3958-239X
AD  - Department of Biotechnology, School of Life Science and Biotechnology, Adamas 
      University, Kolkata, West Bengal, 700126, India. drchiranjib@yahoo.com.
FAU - Bhattacharya, Manojit
AU  - Bhattacharya M
AUID- ORCID: 0000-0001-9669-1835
AD  - Department of Zoology, Fakir Mohan University, Vyasa Vihar, Balasore, Odisha, 
      756020, India.
FAU - Lee, Sang-Soo
AU  - Lee SS
AUID- ORCID: 0000-0001-5074-7581
AD  - Institute for Skeletal Aging &amp; Orthopedic Surgery, Hallym University-Chuncheon 
      Sacred Heart Hospital, Chuncheon-si, Gangwon-do, 24252, Republic of Korea.
LA  - eng
PT  - Letter
DEP - 20230627
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - Humans
MH  - *Language
MH  - *Artificial Intelligence
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - Large language models
OT  - Medical information
EDAT- 2023/06/27 13:11
MHDA- 2024/01/25 06:44
CRDT- 2023/06/27 11:10
PHST- 2023/06/14 00:00 [received]
PHST- 2023/06/20 00:00 [accepted]
PHST- 2024/01/25 06:44 [medline]
PHST- 2023/06/27 13:11 [pubmed]
PHST- 2023/06/27 11:10 [entrez]
AID - 10.1007/s10439-023-03297-9 [pii]
AID - 10.1007/s10439-023-03297-9 [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2024 Feb;52(2):134-135. doi: 10.1007/s10439-023-03297-9. Epub 
      2023 Jun 27.

PMID- 38478661
OWN - NLM
STAT- MEDLINE
DCOM- 20240315
LR  - 20240315
IS  - 1087-2108 (Electronic)
IS  - 1087-2108 (Linking)
VI  - 29
IP  - 6
DP  - 2023 Dec 15
TI  - ChatGPT offers an editorial on the opportunities for chatbots in dermatologic 
      research and patient care.
LID - 10.5070/D329662990 [doi]
AB  - ChatGPT is a chatbot developed by OpenAI, an artificial intelligence research 
      laboratory, that is trained on massive-scale internet text data to understand a 
      broad range of language styles and topics. As a mature, conversational chatbot, 
      ChatGPT can respond to follow-up questions and produce coherent primary texts 
      based on the user's request. We explore the opportunities and risks of 
      integrating chatbots into dermatologic patient care and research while presenting 
      ChatGPT's response to the same question.
FAU - Kim, Yong-Hun
AU  - Kim YH
FAU - Zhang, Michael Z
AU  - Zhang MZ
FAU - Vidal, Nahid Y
AU  - Vidal NY
AD  - Department of Dermatology, Mayo Clinic, Rochester, Minnesota, USA Division of 
      Dermatologic Surgery, Mayo Clinic, Rochester, Minnesota, USA. 
      Vidal.Nahid@mayo.edu.
LA  - eng
PT  - Journal Article
DEP - 20231215
PL  - United States
TA  - Dermatol Online J
JT  - Dermatology online journal
JID - 9610776
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Communication
MH  - Internet
MH  - Language
MH  - Patient Care
EDAT- 2024/03/13 18:46
MHDA- 2024/03/15 06:43
CRDT- 2024/03/13 14:13
PHST- 2024/01/18 00:00 [received]
PHST- 2024/01/18 00:00 [accepted]
PHST- 2024/03/15 06:43 [medline]
PHST- 2024/03/13 18:46 [pubmed]
PHST- 2024/03/13 14:13 [entrez]
AID - 10.5070/D329662990 [doi]
PST - epublish
SO  - Dermatol Online J. 2023 Dec 15;29(6). doi: 10.5070/D329662990.

PMID- 37058235
OWN - NLM
STAT- MEDLINE
DCOM- 20231204
LR  - 20231204
IS  - 1863-4362 (Electronic)
IS  - 0021-1265 (Linking)
VI  - 192
IP  - 6
DP  - 2023 Dec
TI  - Academic journals cannot simply require authors to declare that they used 
      ChatGPT.
PG  - 3195-3196
LID - 10.1007/s11845-023-03374-x [doi]
AB  - This letter to the editor points out weaknesses in the editorial policies of some 
      academic journals regarding the use of ChatGPT-generated content. Editorial 
      policies should provide more specific details on which parts of an academic paper 
      are allowed to use ChatGPT-generated content. If authors use ChatGPT-generated 
      content in the conclusion or results section, it may harm the academic paper's 
      originality and, therefore, should not be accepted.
CI  - © 2023. The Author(s), under exclusive licence to Royal Academy of Medicine in 
      Ireland.
FAU - Tang, Gengyan
AU  - Tang G
AUID- ORCID: 0000-0003-3221-3134
AD  - Sichuan Academy of Social Sciences, Chengdu, China. tanggengyan@outlook.com.
LA  - eng
PT  - Letter
DEP - 20230414
PL  - Ireland
TA  - Ir J Med Sci
JT  - Irish journal of medical science
JID - 7806864
SB  - IM
MH  - Humans
MH  - *Publishing
MH  - *Periodicals as Topic
MH  - Editorial Policies
OTO - NOTNLM
OT  - Academic journals
OT  - Research ethics
OT  - Research integrity
OT  - Research originality
OT  - Scholarly publishing
EDAT- 2023/04/15 06:00
MHDA- 2023/12/04 12:41
CRDT- 2023/04/14 11:13
PHST- 2023/04/06 00:00 [received]
PHST- 2023/04/11 00:00 [accepted]
PHST- 2023/12/04 12:41 [medline]
PHST- 2023/04/15 06:00 [pubmed]
PHST- 2023/04/14 11:13 [entrez]
AID - 10.1007/s11845-023-03374-x [pii]
AID - 10.1007/s11845-023-03374-x [doi]
PST - ppublish
SO  - Ir J Med Sci. 2023 Dec;192(6):3195-3196. doi: 10.1007/s11845-023-03374-x. Epub 
      2023 Apr 14.

PMID- 38490655
OWN - NLM
STAT- MEDLINE
DCOM- 20240318
LR  - 20240320
IS  - 2044-6055 (Electronic)
IS  - 2044-6055 (Linking)
VI  - 14
IP  - 3
DP  - 2024 Mar 15
TI  - Can ChatGPT pass the MRCP (UK) written examinations? Analysis of performance and 
      errors using a clinical decision-reasoning framework.
PG  - e080558
LID - 10.1136/bmjopen-2023-080558 [doi]
LID - e080558
AB  - OBJECTIVE: Large language models (LLMs) such as ChatGPT are being developed for 
      use in research, medical education and clinical decision systems. However, as 
      their usage increases, LLMs face ongoing regulatory concerns. This study aims to 
      analyse ChatGPT's performance on a postgraduate examination to identify areas of 
      strength and weakness, which may provide further insight into their role in 
      healthcare. DESIGN: We evaluated the performance of ChatGPT 4 (24 May 2023 
      version) on official MRCP (Membership of the Royal College of Physicians) parts 1 
      and 2 written examination practice questions. Statistical analysis was performed 
      using Python. Spearman rank correlation assessed the relationship between the 
      probability of correctly answering a question and two variables: question 
      difficulty and question length. Incorrectly answered questions were analysed 
      further using a clinical reasoning framework to assess the errors made. SETTING: 
      Online using ChatGPT web interface. PRIMARY AND SECONDARY OUTCOME MEASURES: 
      Primary outcome was the score (percentage questions correct) in the MRCP 
      postgraduate written examinations. Secondary outcomes were qualitative 
      categorisation of errors using a clinical decision-making framework. RESULTS: 
      ChatGPT achieved accuracy rates of 86.3% (part 1) and 70.3% (part 2). Weak but 
      significant correlations were found between ChatGPT's accuracy and both 
      just-passing rates in part 2 (r=0.34, p=0.0001) and question length in part 1 
      (r=-0.19, p=0.008). Eight types of error were identified, with the most frequent 
      being factual errors, context errors and omission errors. CONCLUSION: ChatGPT 
      performance greatly exceeded the passing mark for both exams. Multiple choice 
      examinations provide a benchmark for LLM performance which is comparable to human 
      demonstrations of knowledge, while also highlighting the errors LLMs make. 
      Understanding the reasons behind ChatGPT's errors allows us to develop strategies 
      to prevent them in medical devices that incorporate LLM technology.
CI  - © Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No 
      commercial re-use. See rights and permissions. Published by BMJ.
FAU - Maitland, Amy
AU  - Maitland A
AUID- ORCID: 0000-0002-9089-9034
AD  - Health Education England North East, Newcastle upon Tyne, UK.
FAU - Fowkes, Ross
AU  - Fowkes R
AD  - Health Education England North East, Newcastle upon Tyne, UK.
FAU - Maitland, Stuart
AU  - Maitland S
AD  - The Newcastle Upon Tyne NHS Hospitals Foundation Trust, Newcastle upon Tyne, UK 
      stu.maitland@newcastle.ac.uk.
AD  - Newcastle University Translational and Clinical Research Institute, Newcastle 
      upon Tyne, UK.
LA  - eng
PT  - Journal Article
DEP - 20240315
PL  - England
TA  - BMJ Open
JT  - BMJ open
JID - 101552874
SB  - IM
MH  - Humans
MH  - *Cholangiopancreatography, Magnetic Resonance
MH  - *Clinical Reasoning
MH  - Clinical Decision-Making
MH  - Benchmarking
MH  - United Kingdom
PMC - PMC10946340
OTO - NOTNLM
OT  - Clinical Decision-Making
OT  - Health informatics
OT  - MEDICAL EDUCATION &amp; TRAINING
OT  - QUALITATIVE RESEARCH
COIS- Competing interests: None declared.
EDAT- 2024/03/16 05:42
MHDA- 2024/03/18 06:43
PMCR- 2024/03/15
CRDT- 2024/03/15 20:34
PHST- 2024/03/18 06:43 [medline]
PHST- 2024/03/16 05:42 [pubmed]
PHST- 2024/03/15 20:34 [entrez]
PHST- 2024/03/15 00:00 [pmc-release]
AID - bmjopen-2023-080558 [pii]
AID - 10.1136/bmjopen-2023-080558 [doi]
PST - epublish
SO  - BMJ Open. 2024 Mar 15;14(3):e080558. doi: 10.1136/bmjopen-2023-080558.

PMID- 37989755
OWN - NLM
STAT- MEDLINE
DCOM- 20231123
LR  - 20231219
IS  - 2045-2322 (Electronic)
IS  - 2045-2322 (Linking)
VI  - 13
IP  - 1
DP  - 2023 Nov 21
TI  - ChatGPT's performance before and after teaching in mass casualty incident triage.
PG  - 20350
LID - 10.1038/s41598-023-46986-0 [doi]
LID - 20350
AB  - Since its initial launching, ChatGPT has gained significant attention from the 
      media, with many claiming that ChatGPT's arrival is a transformative milestone in 
      the advancement of the AI revolution. Our aim was to assess the performance of 
      ChatGPT before and after teaching&nbsp;the triage of mass casualty incidents by 
      utilizing a validated questionnaire specifically designed for such scenarios. In 
      addition, we compared the triage performance between&nbsp;ChatGPT and&nbsp;medical 
      students. Our cross-sectional study employed a mixed-methods analysis to assess 
      the performance of ChatGPT in mass casualty incident triage, pre- and 
      post-teaching of Simple Triage And Rapid Treatment (START) triage. After teaching 
      the START triage algorithm, ChatGPT scored an overall triage accuracy of 80%, 
      with only 20% of cases being over-triaged. The mean accuracy of medical students 
      on the same questionnaire yielded 64.3%. Qualitative analysis on pre-determined 
      themes on 'walking-wounded', 'respiration', 'perfusion', and 'mental status' on 
      ChatGPT showed similar performance in&nbsp;pre- and post-teaching of START triage. 
      Additional themes on 'disclaimer', 'prediction', 'management plan', and 
      'assumption' were identified during the thematic analysis. ChatGPT exhibited 
      promising results in effectively responding to mass casualty incident 
      questionnaires. Nevertheless, additional research is necessary to ensure its 
      safety and efficacy before clinical implementation.
CI  - © 2023. The Author(s).
FAU - Gan, Rick Kye
AU  - Gan RK
AUID- ORCID: 0000-0002-4211-7819
AD  - Unit for Research in Emergency and Disaster, Faculty of Medicine and Health 
      Sciences, University of Oviedo, 33006, Oviedo, Spain.
FAU - Uddin, Helal
AU  - Uddin H
AUID- ORCID: 0000-0002-0767-3174
AD  - Unit for Research in Emergency and Disaster, Faculty of Medicine and Health 
      Sciences, University of Oviedo, 33006, Oviedo, Spain. md.helal.uddin@stud.ki.se.
AD  - Department of Global Public Health, Karolinska Institute, 17177, Solna, Sweden. 
      md.helal.uddin@stud.ki.se.
AD  - Department of Sociology, East West University, Dhaka, 1212, Bangladesh. 
      md.helal.uddin@stud.ki.se.
FAU - Gan, Ann Zee
AU  - Gan AZ
AUID- ORCID: 0009-0001-4650-5528
AD  - Tenghilan Health Clinic, 89208, Tuaran, Sabah, Malaysia.
FAU - Yew, Ying Ying
AU  - Yew YY
AUID- ORCID: 0000-0003-2747-6730
AD  - Unit for Research in Emergency and Disaster, Faculty of Medicine and Health 
      Sciences, University of Oviedo, 33006, Oviedo, Spain.
FAU - González, Pedro Arcos
AU  - González PA
AUID- ORCID: 0000-0003-4882-5442
AD  - Unit for Research in Emergency and Disaster, Faculty of Medicine and Health 
      Sciences, University of Oviedo, 33006, Oviedo, Spain.
LA  - eng
PT  - Journal Article
DEP - 20231121
PL  - England
TA  - Sci Rep
JT  - Scientific reports
JID - 101563288
SB  - IM
MH  - Humans
MH  - *Triage/methods
MH  - *Mass Casualty Incidents
MH  - Cross-Sectional Studies
MH  - Computer Simulation
MH  - Algorithms
PMC - PMC10663620
COIS- The authors declare no competing interests.
EDAT- 2023/11/22 00:42
MHDA- 2023/11/23 06:42
PMCR- 2023/11/21
CRDT- 2023/11/21 23:37
PHST- 2023/05/27 00:00 [received]
PHST- 2023/11/07 00:00 [accepted]
PHST- 2023/11/23 06:42 [medline]
PHST- 2023/11/22 00:42 [pubmed]
PHST- 2023/11/21 23:37 [entrez]
PHST- 2023/11/21 00:00 [pmc-release]
AID - 10.1038/s41598-023-46986-0 [pii]
AID - 46986 [pii]
AID - 10.1038/s41598-023-46986-0 [doi]
PST - epublish
SO  - Sci Rep. 2023 Nov 21;13(1):20350. doi: 10.1038/s41598-023-46986-0.

PMID- 36981544
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231106
IS  - 2227-9032 (Print)
IS  - 2227-9032 (Electronic)
IS  - 2227-9032 (Linking)
VI  - 11
IP  - 6
DP  - 2023 Mar 19
TI  - ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic 
      Review on the Promising Perspectives and Valid Concerns.
LID - 10.3390/healthcare11060887 [doi]
LID - 887
AB  - ChatGPT is an artificial intelligence (AI)-based conversational large language 
      model (LLM). The potential applications of LLMs in health care education, 
      research, and practice could be promising if the associated valid concerns are 
      proactively examined and addressed. The current systematic review aimed to 
      investigate the utility of ChatGPT in health care education, research, and 
      practice and to highlight its potential limitations. Using the PRIMSA guidelines, 
      a systematic search was conducted to retrieve English records in PubMed/MEDLINE 
      and Google Scholar (published research or preprints) that examined ChatGPT in the 
      context of health care education, research, or practice. A total of 60 records 
      were eligible for inclusion. Benefits of ChatGPT were cited in 51/60 (85.0%) 
      records and included: (1) improved scientific writing and enhancing research 
      equity and versatility; (2) utility in health care research (efficient analysis 
      of datasets, code generation, literature reviews, saving time to focus on 
      experimental design, and drug discovery and development); (3) benefits in health 
      care practice (streamlining the workflow, cost saving, documentation, 
      personalized medicine, and improved health literacy); and (4) benefits in health 
      care education including improved personalized learning and the focus on critical 
      thinking and problem-based learning. Concerns regarding ChatGPT use were stated 
      in 58/60 (96.7%) records including ethical, copyright, transparency, and legal 
      issues, the risk of bias, plagiarism, lack of originality, inaccurate content 
      with risk of hallucination, limited knowledge, incorrect citations, cybersecurity 
      issues, and risk of infodemics. The promising applications of ChatGPT can induce 
      paradigm shifts in health care education, research, and practice. However, the 
      embrace of this AI chatbot should be conducted with extreme caution considering 
      its potential limitations. As it currently stands, ChatGPT does not qualify to be 
      listed as an author in scientific articles unless the ICMJE/COPE guidelines are 
      revised or amended. An initiative involving all stakeholders in health care 
      education, research, and practice is urgently needed. This will help to set a 
      code of ethics to guide the responsible use of ChatGPT among other LLMs in health 
      care and academia.
FAU - Sallam, Malik
AU  - Sallam M
AUID- ORCID: 0000-0002-0165-9670
AD  - Department of Pathology, Microbiology and Forensic Medicine, School of Medicine, 
      The University of Jordan, Amman 11942, Jordan.
AD  - Department of Clinical Laboratories and Forensic Medicine, Jordan University 
      Hospital, Amman 11942, Jordan.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20230319
PL  - Switzerland
TA  - Healthcare (Basel)
JT  - Healthcare (Basel, Switzerland)
JID - 101666525
PMC - PMC10048148
OTO - NOTNLM
OT  - artificial intelligence
OT  - digital health
OT  - ethics
OT  - healthcare
OT  - machine learning
COIS- The author declares no conflict of interest.
EDAT- 2023/03/30 06:00
MHDA- 2023/03/30 06:01
PMCR- 2023/03/19
CRDT- 2023/03/29 01:31
PHST- 2023/02/20 00:00 [received]
PHST- 2023/03/17 00:00 [revised]
PHST- 2023/03/17 00:00 [accepted]
PHST- 2023/03/30 06:01 [medline]
PHST- 2023/03/29 01:31 [entrez]
PHST- 2023/03/30 06:00 [pubmed]
PHST- 2023/03/19 00:00 [pmc-release]
AID - healthcare11060887 [pii]
AID - healthcare-11-00887 [pii]
AID - 10.3390/healthcare11060887 [doi]
PST - epublish
SO  - Healthcare (Basel). 2023 Mar 19;11(6):887. doi: 10.3390/healthcare11060887.

PMID- 36946005
OWN - NLM
STAT- MEDLINE
DCOM- 20230725
LR  - 20240109
IS  - 2287-285X (Electronic)
IS  - 2287-2728 (Print)
IS  - 2287-2728 (Linking)
VI  - 29
IP  - 3
DP  - 2023 Jul
TI  - Assessing the performance of ChatGPT in answering questions regarding cirrhosis 
      and hepatocellular carcinoma.
PG  - 721-732
LID - 10.3350/cmh.2023.0089 [doi]
AB  - BACKGROUND/AIMS: Patients with cirrhosis and hepatocellular carcinoma (HCC) 
      require extensive and personalized care to improve outcomes. ChatGPT (Generative 
      Pre-trained Transformer), a large language model, holds the potential to provide 
      professional yet patient-friendly support. We aimed to examine the accuracy and 
      reproducibility of ChatGPT in answering questions regarding knowledge, 
      management, and emotional support for cirrhosis and HCC. METHODS: ChatGPT's 
      responses to 164 questions were independently graded by two transplant 
      hepatologists and resolved by a third reviewer. The performance of ChatGPT was 
      also assessed using two published questionnaires and 26 questions formulated from 
      the quality measures of cirrhosis management. Finally, its emotional support 
      capacity was tested. RESULTS: We showed that ChatGPT regurgitated extensive 
      knowledge of cirrhosis (79.1% correct) and HCC (74.0% correct), but only small 
      proportions (47.3% in cirrhosis, 41.1% in HCC) were labeled as comprehensive. The 
      performance was better in basic knowledge, lifestyle, and treatment than in the 
      domains of diagnosis and preventive medicine. For the quality measures, the model 
      answered 76.9% of questions correctly but failed to specify decision-making 
      cut-offs and treatment durations. ChatGPT lacked knowledge of regional guidelines 
      variations, such as HCC screening criteria. However, it provided practical and 
      multifaceted advice to patients and caregivers regarding the next steps and 
      adjusting to a new diagnosis. CONCLUSION: We analyzed the areas of robustness and 
      limitations of ChatGPT's responses on the management of cirrhosis and HCC and 
      relevant emotional support. ChatGPT may have a role as an adjunct informational 
      tool for patients and physicians to improve outcomes.
FAU - Yeo, Yee Hui
AU  - Yeo YH
AD  - Karsh Division of Gastroenterology and Hepatology, Department of Medicine, 
      Cedars-Sinai Medical Center, Los Angeles, CA, USA.
FAU - Samaan, Jamil S
AU  - Samaan JS
AD  - Karsh Division of Gastroenterology and Hepatology, Department of Medicine, 
      Cedars-Sinai Medical Center, Los Angeles, CA, USA.
FAU - Ng, Wee Han
AU  - Ng WH
AD  - Bristol Medical School, University of Bristol, Bristol, UK.
FAU - Ting, Peng-Sheng
AU  - Ting PS
AD  - School of Medicine, Tulane University, New Orleans, LA, USA.
FAU - Trivedi, Hirsh
AU  - Trivedi H
AD  - Karsh Division of Gastroenterology and Hepatology, Department of Medicine, 
      Cedars-Sinai Medical Center, Los Angeles, CA, USA.
AD  - Comprehensive Transplant Center, Cedars-Sinai Medical Center, Los Angeles, CA, 
      USA.
FAU - Vipani, Aarshi
AU  - Vipani A
AD  - Karsh Division of Gastroenterology and Hepatology, Department of Medicine, 
      Cedars-Sinai Medical Center, Los Angeles, CA, USA.
FAU - Ayoub, Walid
AU  - Ayoub W
AD  - Karsh Division of Gastroenterology and Hepatology, Department of Medicine, 
      Cedars-Sinai Medical Center, Los Angeles, CA, USA.
AD  - Comprehensive Transplant Center, Cedars-Sinai Medical Center, Los Angeles, CA, 
      USA.
FAU - Yang, Ju Dong
AU  - Yang JD
AD  - Karsh Division of Gastroenterology and Hepatology, Department of Medicine, 
      Cedars-Sinai Medical Center, Los Angeles, CA, USA.
AD  - Comprehensive Transplant Center, Cedars-Sinai Medical Center, Los Angeles, CA, 
      USA.
AD  - Samuel Oschin Comprehensive Cancer Institute, Cedars- Sinai Medical Center, Los 
      Angeles, CA, USA.
FAU - Liran, Omer
AU  - Liran O
AD  - Department of Psychiatry and Behavioral Sciences, Cedars-Sinai, Los Angeles, CA, 
      USA.
AD  - Division of Health Services Research, Department of Medicine, Cedars-Sinai, Los 
      Angeles, CA, USA.
FAU - Spiegel, Brennan
AU  - Spiegel B
AD  - Karsh Division of Gastroenterology and Hepatology, Department of Medicine, 
      Cedars-Sinai Medical Center, Los Angeles, CA, USA.
AD  - Division of Health Services Research, Department of Medicine, Cedars-Sinai, Los 
      Angeles, CA, USA.
FAU - Kuo, Alexander
AU  - Kuo A
AD  - Karsh Division of Gastroenterology and Hepatology, Department of Medicine, 
      Cedars-Sinai Medical Center, Los Angeles, CA, USA.
AD  - Comprehensive Transplant Center, Cedars-Sinai Medical Center, Los Angeles, CA, 
      USA.
LA  - eng
PT  - Journal Article
DEP - 20230322
PL  - Korea (South)
TA  - Clin Mol Hepatol
JT  - Clinical and molecular hepatology
JID - 101586730
SB  - IM
CIN - Clin Mol Hepatol. 2023 Jul;29(3):813-814. PMID: 37211355
CIN - Clin Mol Hepatol. 2023 Jul;29(3):815-816. PMID: 37221834
CIN - Clin Mol Hepatol. 2024 Jan;30(1):111-112. PMID: 37828840
CIN - Clin Mol Hepatol. 2024 Jan;30(1):113-117. PMID: 37946373
MH  - Humans
MH  - *Carcinoma, Hepatocellular/diagnosis
MH  - Reproducibility of Results
MH  - *Liver Neoplasms/diagnosis
MH  - Liver Cirrhosis/complications/diagnosis
PMC - PMC10366809
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Chronic disease management
OT  - Health communication
OT  - Patient education as topic
OT  - Telemedicine
COIS- Conflicts of Interest The authors have no conflictsto disclose.
EDAT- 2023/03/23 06:00
MHDA- 2023/07/25 06:43
PMCR- 2023/07/01
CRDT- 2023/03/22 03:48
PHST- 2023/03/03 00:00 [received]
PHST- 2023/03/21 00:00 [accepted]
PHST- 2023/07/25 06:43 [medline]
PHST- 2023/03/23 06:00 [pubmed]
PHST- 2023/03/22 03:48 [entrez]
PHST- 2023/07/01 00:00 [pmc-release]
AID - cmh.2023.0089 [pii]
AID - cmh-2023-0089 [pii]
AID - 10.3350/cmh.2023.0089 [doi]
PST - ppublish
SO  - Clin Mol Hepatol. 2023 Jul;29(3):721-732. doi: 10.3350/cmh.2023.0089. Epub 2023 
      Mar 22.

PMID- 38234125
OWN - NLM
STAT- Publisher
LR  - 20240118
IS  - 1879-3479 (Electronic)
IS  - 0020-7292 (Linking)
DP  - 2024 Jan 17
TI  - It takes one to know one-Machine learning for identifying OBGYN abstracts written 
      by ChatGPT.
LID - 10.1002/ijgo.15365 [doi]
AB  - OBJECTIVES: To use machine learning to optimize the detection of obstetrics and 
      gynecology (OBGYN) Chat Generative Pre-trained Transformer (ChatGPT) -written 
      abstracts of all OBGYN journals. METHODS: We used Web of Science to identify all 
      original articles published in all OBGYN journals in 2022. Seventy-five original 
      articles were randomly selected. For each, we prompted ChatGPT to write an 
      abstract based on the title and results of the original abstracts. Each abstract 
      was tested by Grammarly software and reports were inserted into a database. 
      Machine-learning modes were trained and examined on the database created. 
      RESULTS: Overall, 75 abstracts from 12 different OBGYN journals were randomly 
      selected. There were seven (58%) Q1 journals, one (8%) Q2 journal, two (17%) Q3 
      journals, and two (17%) Q4 journals. Use of mixed dialects of English, absence of 
      comma-misuse, absence of incorrect verb forms, and improper formatting were 
      important prediction variables of ChatGPT-written abstracts. The deep-learning 
      model had the highest predictive performance of all examined models. This model 
      achieved the following performance: accuracy 0.90, precision 0.92, recall 0.85, 
      area under the curve 0.95. CONCLUSIONS: Machine-learning-based tools reach high 
      accuracy in identifying ChatGPT-written OBGYN abstracts.
CI  - © 2024 The Authors. International Journal of Gynecology &amp; Obstetrics published by 
      John Wiley &amp; Sons Ltd on behalf of International Federation of Gynecology and 
      Obstetrics.
FAU - Levin, Gabriel
AU  - Levin G
AUID- ORCID: 0000-0003-1282-5379
AD  - The Department of Obstetrics and Gynecology, Hadassah-Hebrew University Medical 
      Center, Jerusalem, Israel.
AD  - Lady Davis Institute for Cancer Research, Jewish General Hospital, McGill 
      University, Montreal, Quebec, Canada.
FAU - Meyer, Raanan
AU  - Meyer R
AD  - Division of Minimally Invasive Gynecologic Surgery, Department of Obstetrics and 
      Gynecology, Cedars Sinai Medical Center, Los Angeles, California, USA.
AD  - The Dr. Pinchas Bornstein Talpiot Medical Leadership Program, Sheba Medical 
      Center, Ramat-Gan, Israel.
FAU - Guigue, Paul-Adrien
AU  - Guigue PA
AD  - Lady Davis Institute for Cancer Research, Jewish General Hospital, McGill 
      University, Montreal, Quebec, Canada.
FAU - Brezinov, Yoav
AU  - Brezinov Y
AD  - Department of Experimental Surgery, McGill University, Montreal, Quebec, Canada.
LA  - eng
PT  - Journal Article
DEP - 20240117
PL  - United States
TA  - Int J Gynaecol Obstet
JT  - International journal of gynaecology and obstetrics: the official organ of the 
      International Federation of Gynaecology and Obstetrics
JID - 0210174
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Machine learning
OT  - Obstetrics and gynecology
OT  - performance
EDAT- 2024/01/18 06:43
MHDA- 2024/01/18 06:43
CRDT- 2024/01/18 02:03
PHST- 2023/12/08 00:00 [revised]
PHST- 2023/09/29 00:00 [received]
PHST- 2023/12/26 00:00 [accepted]
PHST- 2024/01/18 06:43 [medline]
PHST- 2024/01/18 06:43 [pubmed]
PHST- 2024/01/18 02:03 [entrez]
AID - 10.1002/ijgo.15365 [doi]
PST - aheadofprint
SO  - Int J Gynaecol Obstet. 2024 Jan 17. doi: 10.1002/ijgo.15365.

PMID- 37939643
OWN - NLM
STAT- MEDLINE
DCOM- 20231205
LR  - 20240125
IS  - 1878-0539 (Electronic)
IS  - 1748-6815 (Linking)
VI  - 87
DP  - 2023 Dec
TI  - Online patient education in body contouring: A comparison between Google and 
      ChatGPT.
PG  - 390-402
LID - S1748-6815(23)00642-3 [pii]
LID - 10.1016/j.bjps.2023.10.091 [doi]
AB  - Appropriate patient education and preparation prior to surgery represent a 
      fundamental step in managing expectations, avoiding unnecessary encounters and 
      eventually achieving optimal outcomes. Thus, the objective of this study is to 
      evaluate ChatGPT's potential as a viable source for patient education by 
      comparing its responses and provided references to frequently asked questions on 
      body contouring, with Google's. A Google search was conducted on July 15th, 2023, 
      using the search term "body contouring surgery". The first 15 questions under the 
      "People also ask" section and answers provided by Google were recorded. The 15 
      questions were then asked to ChatGPT-3.5. Four plastic surgeons evaluated the 
      answers from 1 to 5 according to the Global Quality Scale. The mean score for 
      responses given by Google was 2.55&nbsp;±&nbsp;1.29, indicating poor quality but some 
      information present, of very limited use to patients. The mean score for 
      responses produced by ChatGPT was 4.38&nbsp;±&nbsp;0.67, suggesting that the content was of 
      good quality, useful to patients, and encompassed the most important topics. The 
      difference was statistically significant (p&nbsp;=&nbsp;0.001). Deficiencies in providing 
      references represent one of the most evident weaknesses of ChatGPT. However, 
      ChatGPT did not appear to spread misinformation, and the content of the generated 
      responses was deemed of good quality and useful to patients. The integration of 
      AI technology as a source for patient education has the potential to optimize 
      patient queries on body contouring questions.
CI  - Copyright © 2023 British Association of Plastic, Reconstructive and Aesthetic 
      Surgeons. Published by Elsevier Ltd. All rights reserved.
FAU - Alessandri-Bonetti, Mario
AU  - Alessandri-Bonetti M
AD  - Department of Plastic Surgery, University of Pittsburgh Medical Center, 1350 
      Locust Street, G103, Pittsburgh, PA 15219, United States.
FAU - Liu, Hilary Y
AU  - Liu HY
AD  - Department of Plastic Surgery, University of Pittsburgh Medical Center, 1350 
      Locust Street, G103, Pittsburgh, PA 15219, United States.
FAU - Palmesano, Marco
AU  - Palmesano M
AD  - Department of Plastic Surgery, University of Rome "Tor Vergata", Rome, Italy.
FAU - Nguyen, Vu T
AU  - Nguyen VT
AD  - Department of Plastic Surgery, University of Pittsburgh Medical Center, 1350 
      Locust Street, G103, Pittsburgh, PA 15219, United States.
FAU - Egro, Francesco M
AU  - Egro FM
AD  - Department of Plastic Surgery, University of Pittsburgh Medical Center, 1350 
      Locust Street, G103, Pittsburgh, PA 15219, United States. Electronic address: 
      francescoegro@gmail.com.
LA  - eng
PT  - Journal Article
DEP - 20231020
PL  - Netherlands
TA  - J Plast Reconstr Aesthet Surg
JT  - Journal of plastic, reconstructive &amp; aesthetic surgery : JPRAS
JID - 101264239
SB  - IM
CIN - J Plast Reconstr Aesthet Surg. 2023 Dec;87:440-441. PMID: 37944454
MH  - Humans
MH  - *Body Contouring
MH  - Search Engine
MH  - Patient Education as Topic
MH  - *Education, Distance
MH  - Patients
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Body contouring
OT  - ChatGPT
OT  - Large language model
OT  - Patient education
COIS- Declaration of Competing Interest The authors declare that they have no conflicts 
      of interest to disclose.
EDAT- 2023/11/09 00:42
MHDA- 2023/12/05 12:42
CRDT- 2023/11/08 18:11
PHST- 2023/09/18 00:00 [received]
PHST- 2023/10/08 00:00 [accepted]
PHST- 2023/12/05 12:42 [medline]
PHST- 2023/11/09 00:42 [pubmed]
PHST- 2023/11/08 18:11 [entrez]
AID - S1748-6815(23)00642-3 [pii]
AID - 10.1016/j.bjps.2023.10.091 [doi]
PST - ppublish
SO  - J Plast Reconstr Aesthet Surg. 2023 Dec;87:390-402. doi: 
      10.1016/j.bjps.2023.10.091. Epub 2023 Oct 20.

PMID- 37855948
OWN - NLM
STAT- Publisher
LR  - 20231019
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
DP  - 2023 Oct 19
TI  - Examining the Potential of ChatGPT on Biomedical Information Retrieval: 
      Fact-Checking Drug-Disease Associations.
LID - 10.1007/s10439-023-03385-w [doi]
AB  - Large language models (LLMs) such as ChatGPT have recently attracted significant 
      attention due to their impressive performance on many real-world tasks. These 
      models have also demonstrated the potential in facilitating various biomedical 
      tasks. However, little is known of their potential in biomedical information 
      retrieval, especially identifying drug-disease associations. This study aims to 
      explore the potential of ChatGPT, a popular LLM, in discerning drug-disease 
      associations. We collected 2694 true drug-disease associations and 5662 false 
      drug-disease pairs. Our approach involved creating various prompts to instruct 
      ChatGPT in identifying these associations. Under varying prompt designs, 
      ChatGPT's capability to identify drug-disease associations with an accuracy of 
      74.6-83.5% and 96.2-97.6% for the true and false pairs, respectively. This study 
      shows that ChatGPT has the potential in identifying drug-disease associations and 
      may serve as a helpful tool in searching pharmacy-related information. However, 
      the accuracy of its insights warrants comprehensive examination before its 
      implementation in medical practice.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Gao, Zhenxiang
AU  - Gao Z
AUID- ORCID: 0000-0002-2223-5092
AD  - Center for Artificial Intelligence in Drug Discovery, School of Medicine, Case 
      Western Reserve University, Cleveland, OH, USA.
FAU - Li, Lingyao
AU  - Li L
AD  - School of Information, University of Michigan, Ann Arbor, MI, USA.
FAU - Ma, Siyuan
AU  - Ma S
AD  - Vanderbilt University Medical Center, Nashville, TN, USA.
FAU - Wang, Qinyong
AU  - Wang Q
AD  - Center for Artificial Intelligence in Drug Discovery, School of Medicine, Case 
      Western Reserve University, Cleveland, OH, USA.
FAU - Hemphill, Libby
AU  - Hemphill L
AD  - School of Information, University of Michigan, Ann Arbor, MI, USA.
FAU - Xu, Rong
AU  - Xu R
AD  - Center for Artificial Intelligence in Drug Discovery, School of Medicine, Case 
      Western Reserve University, Cleveland, OH, USA. rxx@case.edu.
LA  - eng
GR  - AA029831/AA/NIAAA NIH HHS/United States
GR  - AG057557/AG/NIA NIH HHS/United States
GR  - AG061388/AG/NIA NIH HHS/United States
GR  - AG062272/AG/NIA NIH HHS/United States
GR  - AG07664/AG/NIA NIH HHS/United States
PT  - Letter
DEP - 20231019
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
OTO - NOTNLM
OT  - Biomedical information retrieval
OT  - ChatGPT
OT  - Drug-disease associations
OT  - Generative AI
OT  - Performance evaluation
OT  - Prompt design
EDAT- 2023/10/19 12:43
MHDA- 2023/10/19 12:43
CRDT- 2023/10/19 11:17
PHST- 2023/09/13 00:00 [received]
PHST- 2023/10/09 00:00 [accepted]
PHST- 2023/10/19 12:43 [medline]
PHST- 2023/10/19 12:43 [pubmed]
PHST- 2023/10/19 11:17 [entrez]
AID - 10.1007/s10439-023-03385-w [pii]
AID - 10.1007/s10439-023-03385-w [doi]
PST - aheadofprint
SO  - Ann Biomed Eng. 2023 Oct 19. doi: 10.1007/s10439-023-03385-w.

PMID- 37499282
OWN - NLM
STAT- MEDLINE
DCOM- 20230816
LR  - 20230821
IS  - 1872-7123 (Electronic)
IS  - 0165-1781 (Print)
IS  - 0165-1781 (Linking)
VI  - 326
DP  - 2023 Aug
TI  - ChatGPT and Bard exhibit spontaneous citation fabrication during psychiatry 
      literature search.
PG  - 115334
LID - S0165-1781(23)00284-6 [pii]
LID - 10.1016/j.psychres.2023.115334 [doi]
AB  - ChatGPT (Generative Pre-Trained Transformer) is a large language model (LLM), 
      which comprises a neural network that has learned information and patterns of 
      language use from large amounts of text on the internet. ChatGPT, introduced by 
      OpenAI, responds to human queries in a conversational manner. Here, we aimed to 
      assess whether ChatGPT could reliably produce accurate references to supplement 
      the literature search process. We describe our March 2023 exchange with ChatGPT, 
      which generated thirty-five citations, two of which were real. 12 citations were 
      similar to actual manuscripts (e.g., titles with incorrect author lists, 
      journals, or publication years) and the remaining 21, while plausible, were in 
      fact a pastiche of multiple existent manuscripts. In June 2023, we re-tested 
      ChatGPT's performance and compared it to that of Google's GPT counterpart, Bard 
      2.0. We investigated performance in English, as well as in Spanish and Italian. 
      Fabrications made by LLMs, including erroneous citations, have been called 
      "hallucinations"; we discuss reasons for which this is a misnomer. Furthermore, 
      we describe potential explanations for citation fabrication by GPTs, as well as 
      measures being taken to remedy this issue, including reinforcement learning. Our 
      results underscore that output from conversational LLMs should be verified.
CI  - Copyright © 2023 Elsevier B.V. All rights reserved.
FAU - McGowan, Alessia
AU  - McGowan A
AD  - Icahn School of Medicine at Mount Sinai, New York, NY, USA.
FAU - Gui, Yunlai
AU  - Gui Y
AD  - Icahn School of Medicine at Mount Sinai, New York, NY, USA.
FAU - Dobbs, Matthew
AU  - Dobbs M
AD  - Icahn School of Medicine at Mount Sinai, New York, NY, USA.
FAU - Shuster, Sophia
AU  - Shuster S
AD  - Icahn School of Medicine at Mount Sinai, New York, NY, USA.
FAU - Cotter, Matthew
AU  - Cotter M
AD  - Icahn School of Medicine at Mount Sinai, New York, NY, USA.
FAU - Selloni, Alexandria
AU  - Selloni A
AD  - Icahn School of Medicine at Mount Sinai, New York, NY, USA.
FAU - Goodman, Marianne
AU  - Goodman M
AD  - Icahn School of Medicine at Mount Sinai, New York, NY, USA; James J. Peters 
      Veterans Administration, Bronx, NY, USA.
FAU - Srivastava, Agrima
AU  - Srivastava A
AD  - Icahn School of Medicine at Mount Sinai, New York, NY, USA.
FAU - Cecchi, Guillermo A
AU  - Cecchi GA
AD  - IBM TJ Watson Research Center, Yorktown Heights, NY, USA.
FAU - Corcoran, Cheryl M
AU  - Corcoran CM
AD  - Icahn School of Medicine at Mount Sinai, New York, NY, USA; James J. Peters 
      Veterans Administration, Bronx, NY, USA. Electronic address: 
      cheryl.corcoran@mssm.edu.
LA  - eng
GR  - R01 MH107558/MH/NIMH NIH HHS/United States
GR  - R01 MH115332/MH/NIMH NIH HHS/United States
GR  - R01 MH132239/MH/NIMH NIH HHS/United States
PT  - Journal Article
DEP - 20230707
PL  - Ireland
TA  - Psychiatry Res
JT  - Psychiatry research
JID - 7911385
SB  - IM
MH  - Humans
MH  - *Communication
MH  - Language
MH  - Dietary Supplements
MH  - Hallucinations
MH  - *Psychiatry
PMC - PMC10424704
MID - NIHMS1918813
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Bard
OT  - ChatGPT
OT  - Citations
OT  - Fabrication
OT  - Large language models
OT  - Linguistic
OT  - Literature search
OT  - Natural language processing
OT  - References
COIS- Declaration of Competing Interest None.
EDAT- 2023/07/27 19:10
MHDA- 2023/08/16 06:43
PMCR- 2024/08/01
CRDT- 2023/07/27 18:00
PHST- 2023/05/09 00:00 [received]
PHST- 2023/07/01 00:00 [revised]
PHST- 2023/07/03 00:00 [accepted]
PHST- 2024/08/01 00:00 [pmc-release]
PHST- 2023/08/16 06:43 [medline]
PHST- 2023/07/27 19:10 [pubmed]
PHST- 2023/07/27 18:00 [entrez]
AID - S0165-1781(23)00284-6 [pii]
AID - 10.1016/j.psychres.2023.115334 [doi]
PST - ppublish
SO  - Psychiatry Res. 2023 Aug;326:115334. doi: 10.1016/j.psychres.2023.115334. Epub 
      2023 Jul 7.

PMID- 36924907
OWN - NLM
STAT- MEDLINE
DCOM- 20230605
LR  - 20231213
IS  - 1097-6868 (Electronic)
IS  - 0002-9378 (Linking)
VI  - 228
IP  - 6
DP  - 2023 Jun
TI  - The exciting potential for ChatGPT in obstetrics and gynecology.
PG  - 696-705
LID - S0002-9378(23)00154-0 [pii]
LID - 10.1016/j.ajog.2023.03.009 [doi]
AB  - Natural language processing-the branch of artificial intelligence concerned with 
      the interaction between computers and human language-has advanced markedly in 
      recent years with the introduction of sophisticated deep-learning models. 
      Improved performance in natural language processing tasks, such as text and 
      speech processing, have fueled impressive demonstrations of these models' 
      capabilities. Perhaps no demonstration has been more impactful to date than the 
      introduction of the publicly available online chatbot ChatGPT in November 2022 by 
      OpenAI, which is based on a natural language processing model known as a 
      Generative Pretrained Transformer. Through a series of questions posed by the 
      authors about obstetrics and gynecology to ChatGPT as prompts, we evaluated the 
      model's ability to handle clinical-related queries. Its answers demonstrated that 
      in its current form, ChatGPT can be valuable for users who want preliminary 
      information about virtually any topic in the field. Because its educational role 
      is still being defined, we must recognize its limitations. Although answers were 
      generally eloquent, informed, and lacked a significant degree of mistakes or 
      misinformation, we also observed evidence of its weaknesses. A significant 
      drawback is that the data on which the model has been trained are apparently not 
      readily updated. The specific model that was assessed here, seems to not reliably 
      (if at all) source data from after 2021. Users of ChatGPT who expect data to be 
      more up to date need to be aware of this drawback. An inability to cite sources 
      or to truly understand what the user is asking suggests that it has the 
      capability to mislead. Responsible use of models like ChatGPT will be important 
      for ensuring that they work to help but not harm users seeking information on 
      obstetrics and gynecology.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Grünebaum, Amos
AU  - Grünebaum A
AD  - Department of Obstetrics and Gynecology, Donald and Barbara Zucker School of 
      Medicine at Hofstra/Northwell, Lenox Hill Hospital, New York, NY. Electronic 
      address: agrunebaum@northwell.edu.
FAU - Chervenak, Joseph
AU  - Chervenak J
AD  - Department of Obstetrics and Gynecology, Albert Einstein College of Medicine, 
      Montefiore Hospital, Bronx, NY.
FAU - Pollet, Susan L
AU  - Pollet SL
AD  - Department of Obstetrics and Gynecology, Donald and Barbara Zucker School of 
      Medicine at Hofstra/Northwell, Lenox Hill Hospital, New York, NY.
FAU - Katz, Adi
AU  - Katz A
AD  - Department of Obstetrics and Gynecology, Donald and Barbara Zucker School of 
      Medicine at Hofstra/Northwell, Lenox Hill Hospital, New York, NY.
FAU - Chervenak, Frank A
AU  - Chervenak FA
AD  - Department of Obstetrics and Gynecology, Donald and Barbara Zucker School of 
      Medicine at Hofstra/Northwell, Lenox Hill Hospital, New York, NY.
LA  - eng
PT  - Journal Article
DEP - 20230315
PL  - United States
TA  - Am J Obstet Gynecol
JT  - American journal of obstetrics and gynecology
JID - 0370476
SB  - IM
CIN - Am J Obstet Gynecol. 2023 Sep;229(3):356-357. PMID: 37031761
CIN - Am J Obstet Gynecol. 2023 Dec;229(6):706. PMID: 37454961
MH  - Female
MH  - Pregnancy
MH  - Humans
MH  - *Gynecology
MH  - Artificial Intelligence
MH  - *Obstetrics
MH  - Awareness
MH  - Educational Status
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - cesarean delivery
OT  - chatbots
OT  - ethics
OT  - gynecology
OT  - home birth
OT  - informed consent
OT  - maternal-fetal medicine
OT  - obstetrics
OT  - oncology
OT  - preeclampsia
OT  - prematurity
OT  - preterm birth
OT  - progesterone
OT  - reproductive medicine
OT  - short cervix
OT  - vaginal progesterone
EDAT- 2023/03/17 06:00
MHDA- 2023/06/05 06:42
CRDT- 2023/03/16 20:33
PHST- 2023/02/10 00:00 [received]
PHST- 2023/03/02 00:00 [revised]
PHST- 2023/03/02 00:00 [accepted]
PHST- 2023/06/05 06:42 [medline]
PHST- 2023/03/17 06:00 [pubmed]
PHST- 2023/03/16 20:33 [entrez]
AID - S0002-9378(23)00154-0 [pii]
AID - 10.1016/j.ajog.2023.03.009 [doi]
PST - ppublish
SO  - Am J Obstet Gynecol. 2023 Jun;228(6):696-705. doi: 10.1016/j.ajog.2023.03.009. 
      Epub 2023 Mar 15.

PMID- 37158147
OWN - NLM
STAT- MEDLINE
DCOM- 20230918
LR  - 20230918
IS  - 1527-330X (Electronic)
IS  - 1090-820X (Linking)
VI  - 43
IP  - 10
DP  - 2023 Sep 14
TI  - Evaluating Chatbot Efficacy for Answering Frequently Asked Questions in Plastic 
      Surgery: A ChatGPT Case Study Focused on Breast Augmentation.
PG  - 1126-1135
LID - 10.1093/asj/sjad140 [doi]
AB  - BACKGROUND: The integration of artificial intelligence (AI) and machine learning 
      (ML) technologies into healthcare is transforming patient-practitioner 
      interaction and could offer an additional platform for patient education and 
      support. OBJECTIVES: This study investigated whether ChatGPT-4 could provide safe 
      and up-to-date medical information about breast augmentation that is comparable 
      to other patient information sources. METHODS: ChatGPT-4 was asked to generate 6 
      commonly asked questions regarding breast augmentation and respond to them. Its 
      responses were qualitatively evaluated by a panel of specialist plastic and 
      reconstructive surgeons and reconciled with a literature search of 2 large 
      medical databases for accuracy, informativeness, and accessibility. RESULTS: 
      ChatGPT-4 provided well-structured, grammatically accurate, and comprehensive 
      responses to the questions posed; however, it was limited in providing 
      personalized advice and sometimes generated inappropriate or outdated references. 
      ChatGPT consistently encouraged engagement with a specialist for specific 
      information. CONCLUSIONS: Although ChatGPT-4 showed promise as an adjunct tool in 
      patient education regarding breast augmentation, there are areas requiring 
      improvement. Additional advancements and software engineering are needed to 
      enhance the reliability and applicability of AI-driven chatbots in patient 
      education and support systems.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of The 
      Aesthetic Society. All rights reserved. For permissions, please e-mail: 
      journals.permissions@oup.com.
FAU - Seth, Ishith
AU  - Seth I
AUID- ORCID: 0000-0001-5444-8925
FAU - Cox, Aram
AU  - Cox A
FAU - Xie, Yi
AU  - Xie Y
FAU - Bulloch, Gabriella
AU  - Bulloch G
FAU - Hunter-Smith, David J
AU  - Hunter-Smith DJ
FAU - Rozen, Warren M
AU  - Rozen WM
FAU - Ross, Richard J
AU  - Ross RJ
LA  - eng
PT  - Journal Article
PL  - England
TA  - Aesthet Surg J
JT  - Aesthetic surgery journal
JID - 9707469
SB  - IM
CIN - Aesthet Surg J. 2023 Sep 14;43(10):1136-1138. PMID: 37287210
MH  - Humans
MH  - *Surgery, Plastic
MH  - Artificial Intelligence
MH  - Reproducibility of Results
MH  - *Mammaplasty
MH  - Software
EDAT- 2023/05/09 06:42
MHDA- 2023/09/18 12:42
CRDT- 2023/05/09 05:33
PHST- 2023/09/18 12:42 [medline]
PHST- 2023/05/09 06:42 [pubmed]
PHST- 2023/05/09 05:33 [entrez]
AID - 7157259 [pii]
AID - 10.1093/asj/sjad140 [doi]
PST - ppublish
SO  - Aesthet Surg J. 2023 Sep 14;43(10):1126-1135. doi: 10.1093/asj/sjad140.

PMID- 38435177
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240305
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 16
IP  - 2
DP  - 2024 Feb
TI  - Generative Artificial Intelligence in Patient Education: ChatGPT Takes on 
      Hypertension Questions.
PG  - e53441
LID - 10.7759/cureus.53441 [doi]
LID - e53441
AB  - Introduction Uncontrolled hypertension significantly contributes to the 
      development and deterioration of various medical conditions, such as myocardial 
      infarction, chronic kidney disease, and cerebrovascular events. Despite being the 
      most common preventable risk factor for all-cause mortality, only a fraction&nbsp;of 
      affected individuals maintain their blood pressure in the desired range. In 
      recent times, there has been a growing reliance on online platforms for medical 
      information. While providing a convenient source of information, differentiating 
      reliable from unreliable information can be daunting for the layperson, and false 
      information can potentially hinder timely diagnosis and management of medical 
      conditions. The surge in accessibility of generative artificial intelligence 
      (GeAI) technology has led to increased use in obtaining health-related 
      information. This has sparked debates among healthcare providers about the 
      potential for misuse and misinformation while recognizing the role of GeAI in 
      improving health literacy. This study aims to investigate the accuracy of 
      AI-generated information specifically related to hypertension. Additionally, it 
      seeks to explore the reproducibility of information provided by GeAI. Method A 
      nonhuman-subject qualitative study was devised to evaluate the accuracy of 
      information provided by ChatGPT regarding hypertension and its secondary 
      complications. Frequently asked questions on hypertension were compiled by three 
      study staff, internal medicine residents at an ACGME-accredited program, and then 
      reviewed by a physician experienced in treating hypertension, resulting in a 
      final set of 100 questions. Each question was posed to ChatGPT three times, once 
      by each study staff, and the majority response was then assessed against the 
      recommended guidelines. A board-certified internal medicine physician with over 
      eight years of experience further reviewed the responses and categorized them 
      into two classes based on their clinical appropriateness: appropriate (in line 
      with clinical recommendations) and inappropriate (containing errors). Descriptive 
      statistical analysis was employed to assess ChatGPT responses for accuracy and 
      reproducibility. Result Initially, a pool of 130 questions was gathered, of which 
      a final set of 100 questions was selected for the purpose of this study. When 
      assessed against acceptable standard responses, ChatGPT responses were found to 
      be appropriate in 92.5% of cases and inappropriate in 7.5%. Furthermore, ChatGPT 
      had a reproducibility score of 93%, meaning that it could consistently reproduce 
      answers that conveyed similar meanings across multiple runs. Conclusion ChatGPT 
      showcased commendable accuracy in addressing commonly asked questions about 
      hypertension. These results underscore the potential of GeAI in providing 
      valuable information to patients. However, continued research and refinement are 
      essential to evaluate further the reliability and broader applicability of 
      ChatGPT within the medical field.
CI  - Copyright © 2024, Almagazzachi et al.
FAU - Almagazzachi, Ahmed
AU  - Almagazzachi A
AD  - Internal Medicine, Capital Health System, Trenton, USA.
FAU - Mustafa, Ahmed
AU  - Mustafa A
AD  - Internal Medicine, Capital Health System, Trenton, USA.
FAU - Eighaei Sedeh, Ashkan
AU  - Eighaei Sedeh A
AD  - Internal Medicine, Capital Health System, Trenton, USA.
FAU - Vazquez Gonzalez, Andres E
AU  - Vazquez Gonzalez AE
AD  - Internal Medicine, Capital Health System, Trenton, USA.
FAU - Polianovskaia, Anastasiia
AU  - Polianovskaia A
AD  - Internal Medicine, Capital Health System, Trenton, USA.
FAU - Abood, Muhanad
AU  - Abood M
AD  - Internal Medicine, Capital Health System, Trenton, USA.
FAU - Abdelrahman, Ameer
AU  - Abdelrahman A
AD  - Internal Medicine, Capital Health System, Trenton, USA.
FAU - Muyolema Arce, Veronica
AU  - Muyolema Arce V
AD  - Internal Medicine, Capital Health System, Trenton, USA.
FAU - Acob, Talar
AU  - Acob T
AD  - Internal Medicine Residency Program, Capital Health Regional Medical Center, 
      Trenton, USA.
FAU - Saleem, Bushra
AU  - Saleem B
AD  - Internal Medicine, Capital Health System, Trenton, USA.
LA  - eng
PT  - Journal Article
DEP - 20240202
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10909311
OTO - NOTNLM
OT  - ai in cardiology
OT  - artificial intelligence in medicine
OT  - cardiology research
OT  - chatgpt
OT  - general internal medicine
OT  - generative ai
OT  - hypertension
OT  - patient education
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/03/04 06:46
MHDA- 2024/03/04 06:47
PMCR- 2024/02/02
CRDT- 2024/03/04 04:57
PHST- 2024/02/01 00:00 [accepted]
PHST- 2024/03/04 06:47 [medline]
PHST- 2024/03/04 06:46 [pubmed]
PHST- 2024/03/04 04:57 [entrez]
PHST- 2024/02/02 00:00 [pmc-release]
AID - 10.7759/cureus.53441 [doi]
PST - epublish
SO  - Cureus. 2024 Feb 2;16(2):e53441. doi: 10.7759/cureus.53441. eCollection 2024 Feb.

PMID- 38313827
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240206
IS  - 1177-889X (Print)
IS  - 1177-889X (Electronic)
IS  - 1177-889X (Linking)
VI  - 18
DP  - 2024
TI  - Assessment of the Reliability and Clinical Applicability of ChatGPT's Responses 
      to Patients' Common Queries About Rosacea.
PG  - 249-253
LID - 10.2147/PPA.S444928 [doi]
AB  - OBJECTIVE: Artificial intelligence chatbot, particularly ChatGPT (Chat Generative 
      Pre-trained Transformer), is capable of analyzing human input and generating 
      human-like responses, which shows its potential application in healthcare. People 
      with rosacea often have questions about alleviating symptoms and daily skin-care, 
      which is suitable for ChatGPT to response. This study aims to assess the 
      reliability and clinical applicability of ChatGPT 3.5 in responding to patients' 
      common queries about rosacea and to evaluate the extent of ChatGPT's coverage in 
      dermatology resources. METHODS: Based on a qualitative analysis of the literature 
      on the queries from rosacea patients, we have extracted 20 questions of patients' 
      greatest concerns, covering four main categories: treatment, triggers and diet, 
      skincare, and special manifestations of rosacea. Each question was inputted into 
      ChatGPT separately for three rounds of question-and-answer conversations. The 
      generated answers will be evaluated by three experienced dermatologists with 
      postgraduate degrees and over five years of clinical experience in dermatology, 
      to assess their reliability and applicability for clinical practice. RESULTS: The 
      analysis results indicate that the reviewers unanimously agreed that ChatGPT 
      achieved a high reliability of 92.22% to 97.78% in responding to patients' common 
      queries about rosacea. Additionally, almost all answers were applicable for 
      supporting rosacea patient education, with a clinical applicability ranging from 
      98.61% to 100.00%. The consistency of the expert ratings was excellent (all 
      significance levels were less than 0.05), with a consistency coefficient of 0.404 
      for content reliability and 0.456 for clinical practicality, indicating 
      significant consistency in the results and a high level of agreement among the 
      expert ratings. CONCLUSION: ChatGPT 3.5 exhibits excellent reliability and 
      clinical applicability in responding to patients' common queries about rosacea. 
      This artificial intelligence tool is applicable for supporting rosacea patient 
      education.
CI  - © 2024 Yan et al.
FAU - Yan, Sihan
AU  - Yan S
AUID- ORCID: 0000-0003-0374-5166
AD  - Department of Dermatology, West China Hospital, Sichuan University, Chengdu, 
      People's Republic of China.
AD  - Laboratory of Dermatology, Clinical Institute of Inflammation and Immunology, 
      Frontiers Science Center for Disease-Related Molecular Network, West China 
      Hospital, Sichuan University, Chengdu, People's Republic of China.
FAU - Du, Dan
AU  - Du D
AUID- ORCID: 0000-0001-9766-9875
AD  - Department of Dermatology, West China Hospital, Sichuan University, Chengdu, 
      People's Republic of China.
AD  - Laboratory of Dermatology, Clinical Institute of Inflammation and Immunology, 
      Frontiers Science Center for Disease-Related Molecular Network, West China 
      Hospital, Sichuan University, Chengdu, People's Republic of China.
FAU - Liu, Xu
AU  - Liu X
AUID- ORCID: 0009-0003-7620-7323
AD  - Department of Dermatology, West China Hospital, Sichuan University, Chengdu, 
      People's Republic of China.
AD  - Laboratory of Dermatology, Clinical Institute of Inflammation and Immunology, 
      Frontiers Science Center for Disease-Related Molecular Network, West China 
      Hospital, Sichuan University, Chengdu, People's Republic of China.
FAU - Dai, Yingying
AU  - Dai Y
AD  - Department of Dermatology, West China Hospital, Sichuan University, Chengdu, 
      People's Republic of China.
AD  - Laboratory of Dermatology, Clinical Institute of Inflammation and Immunology, 
      Frontiers Science Center for Disease-Related Molecular Network, West China 
      Hospital, Sichuan University, Chengdu, People's Republic of China.
FAU - Kim, Min-Kyu
AU  - Kim MK
AUID- ORCID: 0000-0002-7277-0341
AD  - Department of Dermatology, West China Hospital, Sichuan University, Chengdu, 
      People's Republic of China.
AD  - Laboratory of Dermatology, Clinical Institute of Inflammation and Immunology, 
      Frontiers Science Center for Disease-Related Molecular Network, West China 
      Hospital, Sichuan University, Chengdu, People's Republic of China.
FAU - Zhou, Xinyu
AU  - Zhou X
AD  - Department of Dermatology, Nanbu County People's Hospital, Nanbu County, 
      Nanchong, Sichuan, People's Republic of China.
FAU - Wang, Lian
AU  - Wang L
AD  - Department of Dermatology, West China Hospital, Sichuan University, Chengdu, 
      People's Republic of China.
AD  - Laboratory of Dermatology, Clinical Institute of Inflammation and Immunology, 
      Frontiers Science Center for Disease-Related Molecular Network, West China 
      Hospital, Sichuan University, Chengdu, People's Republic of China.
FAU - Zhang, Lu
AU  - Zhang L
AD  - Department of Dermatology, West China Hospital, Sichuan University, Chengdu, 
      People's Republic of China.
AD  - Laboratory of Dermatology, Clinical Institute of Inflammation and Immunology, 
      Frontiers Science Center for Disease-Related Molecular Network, West China 
      Hospital, Sichuan University, Chengdu, People's Republic of China.
FAU - Jiang, Xian
AU  - Jiang X
AUID- ORCID: 0000-0001-5109-6055
AD  - Department of Dermatology, West China Hospital, Sichuan University, Chengdu, 
      People's Republic of China.
AD  - Laboratory of Dermatology, Clinical Institute of Inflammation and Immunology, 
      Frontiers Science Center for Disease-Related Molecular Network, West China 
      Hospital, Sichuan University, Chengdu, People's Republic of China.
LA  - eng
PT  - Journal Article
DEP - 20240131
PL  - New Zealand
TA  - Patient Prefer Adherence
JT  - Patient preference and adherence
JID - 101475748
PMC - PMC10838492
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - patient education
OT  - rosacea
COIS- Sihan Yan and Dan Du are co-first authors for this study. All authors declare no 
      conflicts of interest in this work.
EDAT- 2024/02/05 06:42
MHDA- 2024/02/05 06:43
PMCR- 2024/01/31
CRDT- 2024/02/05 04:51
PHST- 2023/10/16 00:00 [received]
PHST- 2024/01/22 00:00 [accepted]
PHST- 2024/02/05 06:43 [medline]
PHST- 2024/02/05 06:42 [pubmed]
PHST- 2024/02/05 04:51 [entrez]
PHST- 2024/01/31 00:00 [pmc-release]
AID - 444928 [pii]
AID - 10.2147/PPA.S444928 [doi]
PST - epublish
SO  - Patient Prefer Adherence. 2024 Jan 31;18:249-253. doi: 10.2147/PPA.S444928. 
      eCollection 2024.

PMID- 36834073
OWN - NLM
STAT- MEDLINE
DCOM- 20230228
LR  - 20231117
IS  - 1660-4601 (Electronic)
IS  - 1661-7827 (Print)
IS  - 1660-4601 (Linking)
VI  - 20
IP  - 4
DP  - 2023 Feb 15
TI  - Diagnostic Accuracy of Differential-Diagnosis Lists Generated by Generative 
      Pretrained Transformer 3 Chatbot for Clinical Vignettes with Common Chief 
      Complaints: A Pilot Study.
LID - 10.3390/ijerph20043378 [doi]
LID - 3378
AB  - The diagnostic accuracy of differential diagnoses generated by artificial 
      intelligence (AI) chatbots, including the generative pretrained transformer 3 
      (GPT-3) chatbot (ChatGPT-3) is unknown. This study evaluated the accuracy of 
      differential-diagnosis lists generated by ChatGPT-3 for clinical vignettes with 
      common chief complaints. General internal medicine physicians created clinical 
      cases, correct diagnoses, and five differential diagnoses for ten common chief 
      complaints. The rate of correct diagnosis by ChatGPT-3 within the ten 
      differential-diagnosis lists was 28/30 (93.3%). The rate of correct diagnosis by 
      physicians was still superior to that by ChatGPT-3 within the five 
      differential-diagnosis lists (98.3% vs. 83.3%, p = 0.03). The rate of correct 
      diagnosis by physicians was also superior to that by ChatGPT-3 in the top 
      diagnosis (53.3% vs. 93.3%, p &lt; 0.001). The rate of consistent differential 
      diagnoses among physicians within the ten differential-diagnosis lists generated 
      by ChatGPT-3 was 62/88 (70.5%). In summary, this study demonstrates the high 
      diagnostic accuracy of differential-diagnosis lists generated by ChatGPT-3 for 
      clinical cases with common chief complaints. This suggests that AI chatbots such 
      as ChatGPT-3 can generate a well-differentiated diagnosis list for common chief 
      complaints. However, the order of these lists can be improved in the future.
FAU - Hirosawa, Takanobu
AU  - Hirosawa T
AUID- ORCID: 0000-0002-3573-8203
AD  - Department of Diagnostic and Generalist Medicine, Dokkyo Medical University, 
      Tochigi 321-0293, Japan.
FAU - Harada, Yukinori
AU  - Harada Y
AUID- ORCID: 0000-0001-6042-7397
AD  - Department of Diagnostic and Generalist Medicine, Dokkyo Medical University, 
      Tochigi 321-0293, Japan.
FAU - Yokose, Masashi
AU  - Yokose M
AUID- ORCID: 0000-0002-2685-0658
AD  - Department of Diagnostic and Generalist Medicine, Dokkyo Medical University, 
      Tochigi 321-0293, Japan.
FAU - Sakamoto, Tetsu
AU  - Sakamoto T
AD  - Department of Diagnostic and Generalist Medicine, Dokkyo Medical University, 
      Tochigi 321-0293, Japan.
FAU - Kawamura, Ren
AU  - Kawamura R
AD  - Department of Diagnostic and Generalist Medicine, Dokkyo Medical University, 
      Tochigi 321-0293, Japan.
FAU - Shimizu, Taro
AU  - Shimizu T
AD  - Department of Diagnostic and Generalist Medicine, Dokkyo Medical University, 
      Tochigi 321-0293, Japan.
LA  - eng
PT  - Journal Article
DEP - 20230215
PL  - Switzerland
TA  - Int J Environ Res Public Health
JT  - International journal of environmental research and public health
JID - 101238455
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Diagnosis, Differential
MH  - Pilot Projects
MH  - Software
MH  - *General Practitioners
PMC - PMC9967747
OTO - NOTNLM
OT  - AI chatbot
OT  - artificial intelligence
OT  - clinical decision support
OT  - diagnosis
OT  - diagnostic accuracy
OT  - generative pretrained transformers
OT  - natural language processing
COIS- The authors declare no conflict of interest.
EDAT- 2023/02/26 06:00
MHDA- 2023/03/03 06:00
PMCR- 2023/02/15
CRDT- 2023/02/25 02:28
PHST- 2023/01/25 00:00 [received]
PHST- 2023/02/09 00:00 [revised]
PHST- 2023/02/13 00:00 [accepted]
PHST- 2023/02/25 02:28 [entrez]
PHST- 2023/02/26 06:00 [pubmed]
PHST- 2023/03/03 06:00 [medline]
PHST- 2023/02/15 00:00 [pmc-release]
AID - ijerph20043378 [pii]
AID - ijerph-20-03378 [pii]
AID - 10.3390/ijerph20043378 [doi]
PST - epublish
SO  - Int J Environ Res Public Health. 2023 Feb 15;20(4):3378. doi: 
      10.3390/ijerph20043378.

PMID- 37991499
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 1434-4726 (Electronic)
IS  - 0937-4477 (Linking)
VI  - 281
IP  - 3
DP  - 2024 Mar
TI  - How ChatGPT works: a mini review.
PG  - 1565-1569
LID - 10.1007/s00405-023-08337-7 [doi]
AB  - OBJECTIVE: This paper offers a mini-review of OpenAI's language model, ChatGPT, 
      detailing its mechanisms, applications in healthcare, and comparisons with other 
      large language models (LLMs). METHODS: The underlying technology of ChatGPT is 
      outlined, focusing on its neural network architecture, training process, and the 
      role of key elements such as input embedding, encoder, decoder, attention 
      mechanism, and output projection. The advancements in GPT-4, including its 
      capacity for internet connection and the integration of plugins for enhanced 
      functionality are discussed. RESULTS: ChatGPT can generate creative, coherent, 
      and contextually relevant sentences, making it a valuable tool in healthcare for 
      patient engagement, medical education, and clinical decision support. Yet, like 
      other LLMs, it has limitations, including a lack of common sense knowledge, a 
      propensity for hallucination of facts, a restricted context window, and potential 
      privacy concerns. CONCLUSION: Despite the limitations, LLMs like ChatGPT offer 
      transformative possibilities for healthcare. With ongoing research in model 
      interpretability, common-sense reasoning, and handling of longer context windows, 
      their potential is vast. It is crucial for healthcare professionals to remain 
      informed about these technologies and consider their ethical integration into 
      practice.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Briganti, Giovanni
AU  - Briganti G
AUID- ORCID: 0000-0002-4038-3363
AD  - Chair of AI and Digital Medicine, Department of Neuroscience, Faculty of 
      Medicine, University of Mons, Avenue du Champs de Mars 6, B7000, Mons, Belgium. 
      giovanni.briganti@hotmail.com.
AD  - Department of Clinical Science, Faculty of Medicine, University of Liège, 
      Quartier Hôpital, Avenue Hippocrate 13, B4000, Liege, Belgium. 
      giovanni.briganti@hotmail.com.
AD  - Faculty of Medicine, Université Libre de Bruxelles, Route de Lennik 808, B1070, 
      Brussels, Belgium. giovanni.briganti@hotmail.com.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20231122
PL  - Germany
TA  - Eur Arch Otorhinolaryngol
JT  - European archives of oto-rhino-laryngology : official journal of the European 
      Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the 
      German Society for Oto-Rhino-Laryngology - Head and Neck Surgery
JID - 9002937
SB  - IM
MH  - Humans
MH  - *Education, Medical
MH  - Health Personnel
MH  - Internet
MH  - Language
OTO - NOTNLM
OT  - Artificial
OT  - ChatGPT
OT  - Chatbot
OT  - GPT
OT  - Head Neck
OT  - Medicine
OT  - Otolaryngology
OT  - Surgery
EDAT- 2023/11/22 12:45
MHDA- 2024/02/09 18:42
CRDT- 2023/11/22 11:03
PHST- 2023/08/05 00:00 [received]
PHST- 2023/11/06 00:00 [accepted]
PHST- 2024/02/09 18:42 [medline]
PHST- 2023/11/22 12:45 [pubmed]
PHST- 2023/11/22 11:03 [entrez]
AID - 10.1007/s00405-023-08337-7 [pii]
AID - 10.1007/s00405-023-08337-7 [doi]
PST - ppublish
SO  - Eur Arch Otorhinolaryngol. 2024 Mar;281(3):1565-1569. doi: 
      10.1007/s00405-023-08337-7. Epub 2023 Nov 22.

PMID- 37725885
OWN - NLM
STAT- Publisher
LR  - 20231006
IS  - 1873-5150 (Electronic)
IS  - 0887-8994 (Linking)
VI  - 148
DP  - 2023 Nov
TI  - Leveraging ChatGPT in the Pediatric Neurology Clinic: Practical Considerations 
      for Use to Improve Efficiency and Outcomes.
PG  - 157-163
LID - S0887-8994(23)00303-X [pii]
LID - 10.1016/j.pediatrneurol.2023.08.035 [doi]
AB  - BACKGROUND: Artificial intelligence (AI) is progressively influencing healthcare 
      sectors, including pediatric neurology. This paper aims to investigate the 
      potential and limitations of using ChatGPT, a large language model (LLM) 
      developed by OpenAI, in an outpatient pediatric neurology clinic. The analysis 
      focuses on the tool's capabilities in enhancing clinical efficiency, 
      productivity, and patient education. METHOD: This is an opinion-based exploration 
      supplemented with practical examples. We assessed ChatGPT's utility in 
      administrative and educational tasks such as drafting medical necessity letters 
      and creating patient educational materials. RESULTS: ChatGPT showed efficacy in 
      streamlining administrative work, particularly in drafting administrative letters 
      and formulating personalized patient education materials. However, the model has 
      limitations in performing higher-order tasks like formulating nuanced 
      differential diagnoses. Additionally, ethical and legal concerns, including data 
      privacy and the potential dissemination of misinformation, warrant cautious 
      implementation. CONCLUSIONS: The integration of AI tools like ChatGPT in 
      pediatric neurology clinics has demonstrated promising results in boosting 
      efficiency and patient education, despite present limitations and ethical 
      concerns. As technology advances, we anticipate future applications may extend to 
      more complex clinical tasks like precise differential diagnoses and treatment 
      strategy guidance. Careful, patient-centered implementation is essential for 
      leveraging the potential benefits of AI in pediatric neurology effectively.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Karakas, Cemal
AU  - Karakas C
AD  - Division of Pediatric Neurology, Department of Neurology, University of 
      Louisville, Louisville, Kentucky; Norton Neuroscience Institute, Louisville, 
      Kentucky. Electronic address: cemal.karakas@louisville.edu.
FAU - Brock, Dylan
AU  - Brock D
AD  - Division of Pediatric Neurology, Department of Neurology, University of 
      Louisville, Louisville, Kentucky; Norton Neuroscience Institute, Louisville, 
      Kentucky.
FAU - Lakhotia, Arpita
AU  - Lakhotia A
AD  - Division of Pediatric Neurology, Department of Neurology, University of 
      Louisville, Louisville, Kentucky; Norton Neuroscience Institute, Louisville, 
      Kentucky.
LA  - eng
PT  - Journal Article
DEP - 20230829
PL  - United States
TA  - Pediatr Neurol
JT  - Pediatric neurology
JID - 8508183
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - Clinic
OT  - Efficiency
OT  - Letters
OT  - Practical
OT  - Workflow
COIS- Declaration of competing interest All authors declare that they have no conflict 
      of interests.
EDAT- 2023/09/20 00:43
MHDA- 2023/09/20 00:43
CRDT- 2023/09/19 18:02
PHST- 2023/05/15 00:00 [received]
PHST- 2023/08/17 00:00 [revised]
PHST- 2023/08/25 00:00 [accepted]
PHST- 2023/09/20 00:43 [pubmed]
PHST- 2023/09/20 00:43 [medline]
PHST- 2023/09/19 18:02 [entrez]
AID - S0887-8994(23)00303-X [pii]
AID - 10.1016/j.pediatrneurol.2023.08.035 [doi]
PST - ppublish
SO  - Pediatr Neurol. 2023 Nov;148:157-163. doi: 10.1016/j.pediatrneurol.2023.08.035. 
      Epub 2023 Aug 29.

PMID- 37794249
OWN - NLM
STAT- Publisher
LR  - 20231004
IS  - 1432-1084 (Electronic)
IS  - 0938-7994 (Linking)
DP  - 2023 Oct 5
TI  - ChatGPT makes medicine easy to swallow: an exploratory case study on simplified 
      radiology reports.
LID - 10.1007/s00330-023-10213-1 [doi]
AB  - OBJECTIVES: To assess the quality of simplified radiology reports generated with 
      the large language model (LLM) ChatGPT and to discuss challenges and chances of 
      ChatGPT-like LLMs for medical text simplification. METHODS: In this exploratory 
      case study, a radiologist created three fictitious radiology reports which we 
      simplified by prompting ChatGPT with "Explain this medical report to a child 
      using simple language." In a questionnaire, we tasked 15 radiologists to rate the 
      quality of the simplified radiology reports with respect to their factual 
      correctness, completeness, and potential harm for patients. We used Likert scale 
      analysis and inductive free-text categorization to assess the quality of the 
      simplified reports. RESULTS: Most radiologists agreed that the simplified reports 
      were factually correct, complete, and not potentially harmful to the patient. 
      Nevertheless, instances of incorrect statements, missed relevant medical 
      information, and potentially harmful passages were reported. CONCLUSION: While we 
      see a need for further adaption to the medical field, the initial insights of 
      this study indicate a tremendous potential in using LLMs like ChatGPT to improve 
      patient-centered care in radiology and other medical domains. CLINICAL RELEVANCE 
      STATEMENT: Patients have started to use ChatGPT to simplify and explain their 
      medical reports, which is expected to affect patient-doctor interaction. This 
      phenomenon raises several opportunities and challenges for clinical routine. KEY 
      POINTS: • Patients have started to use ChatGPT to simplify their medical reports, 
      but their quality was unknown. • In a questionnaire, most participating 
      radiologists overall asserted good quality to radiology reports simplified with 
      ChatGPT. However, they also highlighted a notable presence of errors, potentially 
      leading patients to draw harmful conclusions. • Large language models such as 
      ChatGPT have vast potential to enhance patient-centered care in radiology and 
      other medical domains. To realize this potential while minimizing harm, they need 
      supervision by medical experts and adaption to the medical field.
CI  - © 2023. The Author(s).
FAU - Jeblick, Katharina
AU  - Jeblick K
AUID- ORCID: 0000-0002-8678-7152
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany. 
      katharina.jeblick@med.uni-muenchen.de.
AD  - Comprehensive Pneumology Center (CPC-M), Member of the German Center for Lung 
      Research (DZL), Munich, Germany. katharina.jeblick@med.uni-muenchen.de.
AD  - Munich Center for Machine Learning (MCML), Munich, Germany. 
      katharina.jeblick@med.uni-muenchen.de.
FAU - Schachtner, Balthasar
AU  - Schachtner B
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany.
AD  - Munich Center for Machine Learning (MCML), Munich, Germany.
FAU - Dexl, Jakob
AU  - Dexl J
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany.
AD  - Munich Center for Machine Learning (MCML), Munich, Germany.
FAU - Mittermeier, Andreas
AU  - Mittermeier A
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany.
AD  - Munich Center for Machine Learning (MCML), Munich, Germany.
FAU - Stüber, Anna Theresa
AU  - Stüber AT
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany.
AD  - Munich Center for Machine Learning (MCML), Munich, Germany.
AD  - Department of Statistics, LMU Munich, Munich, Germany.
FAU - Topalis, Johanna
AU  - Topalis J
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany.
FAU - Weber, Tobias
AU  - Weber T
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany.
AD  - Munich Center for Machine Learning (MCML), Munich, Germany.
AD  - Department of Statistics, LMU Munich, Munich, Germany.
FAU - Wesp, Philipp
AU  - Wesp P
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany.
AD  - Munich Center for Machine Learning (MCML), Munich, Germany.
FAU - Sabel, Bastian Oliver
AU  - Sabel BO
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany.
FAU - Ricke, Jens
AU  - Ricke J
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany.
FAU - Ingrisch, Michael
AU  - Ingrisch M
AD  - Department of Radiology, LMU University Hospital, LMU Munich, Munich, Germany.
AD  - Munich Center for Machine Learning (MCML), Munich, Germany.
LA  - eng
GR  - 460037581/Deutsche Forschungsgemeinschaft/
PT  - Journal Article
DEP - 20231005
PL  - Germany
TA  - Eur Radiol
JT  - European radiology
JID - 9114774
SB  - IM
OTO - NOTNLM
OT  - Natural language processing
OT  - Patient-centered care
OT  - Radiology
EDAT- 2023/10/05 01:00
MHDA- 2023/10/05 01:00
CRDT- 2023/10/04 23:33
PHST- 2023/03/06 00:00 [received]
PHST- 2023/07/07 00:00 [accepted]
PHST- 2023/05/24 00:00 [revised]
PHST- 2023/10/05 01:00 [medline]
PHST- 2023/10/05 01:00 [pubmed]
PHST- 2023/10/04 23:33 [entrez]
AID - 10.1007/s00330-023-10213-1 [pii]
AID - 10.1007/s00330-023-10213-1 [doi]
PST - aheadofprint
SO  - Eur Radiol. 2023 Oct 5. doi: 10.1007/s00330-023-10213-1.

PMID- 37560946
OWN - NLM
STAT- Publisher
LR  - 20231221
IS  - 2192-5682 (Print)
IS  - 2192-5682 (Linking)
DP  - 2023 Aug 10
TI  - ChatGPT and its Role in the Decision-Making for the Diagnosis and Treatment of 
      Lumbar Spinal Stenosis: A Comparative Analysis and Narrative Review.
PG  - 21925682231195783
LID - 10.1177/21925682231195783 [doi]
AB  - STUDY DESIGN: Comparative Analysis and Narrative Review. OBJECTIVE: To assess and 
      compare ChatGPT's responses to the clinical questions and recommendations 
      proposed by The 2011 North American Spine Society (NASS) Clinical Guideline for 
      the Diagnosis and Treatment of Degenerative Lumbar Spinal Stenosis (LSS). We 
      explore the advantages and disadvantages of ChatGPT's responses through an 
      updated literature review on spinal stenosis. METHODS: We prompted ChatGPT with 
      questions from the NASS Evidence-based Clinical Guidelines for LSS and compared 
      its generated responses with the recommendations provided by the guidelines. A 
      review of the literature was performed via PubMed, OVID, and Cochrane on the 
      diagnosis and treatment of lumbar spinal stenosis between January 2012 and April 
      2023. RESULTS: 14 questions proposed by the NASS guidelines for LSS were uploaded 
      into ChatGPT and directly compared to the responses offered by NASS. Three 
      questions were on the definition and history of LSS, one on diagnostic tests, 
      seven on non-surgical interventions and three on surgical interventions. The 
      review process found 40 articles that were selected for inclusion that helped 
      corroborate or contradict the responses that were generated by ChatGPT. 
      CONCLUSIONS: ChatGPT's responses were similar to findings in the current 
      literature on LSS. These results demonstrate the potential for implementing 
      ChatGPT into the spine surgeon's workplace as a means of supporting the 
      decision-making process for LSS diagnosis and treatment. However, our narrative 
      summary only provides a limited literature review and additional research is 
      needed to standardize our findings as means of validating ChatGPT's use in the 
      clinical space.
FAU - Rajjoub, Rami
AU  - Rajjoub R
AUID- ORCID: 0009-0005-2990-7874
AD  - Department of Orthopedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA. RINGGOLD: 5925
FAU - Arroyave, Juan Sebastian
AU  - Arroyave JS
AUID- ORCID: 0009-0003-9480-0657
AD  - Department of Orthopedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA. RINGGOLD: 5925
FAU - Zaidat, Bashar
AU  - Zaidat B
AD  - Department of Orthopedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA. RINGGOLD: 5925
FAU - Ahmed, Wasil
AU  - Ahmed W
AD  - Department of Orthopedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA. RINGGOLD: 5925
FAU - Mejia, Mateo Restrepo
AU  - Mejia MR
AUID- ORCID: 0009-0003-0457-3308
AD  - Department of Orthopedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA. RINGGOLD: 5925
FAU - Tang, Justin
AU  - Tang J
AUID- ORCID: 0000-0003-4544-3262
AD  - Department of Orthopedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA. RINGGOLD: 5925
FAU - Kim, Jun S
AU  - Kim JS
AD  - Department of Orthopedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA. RINGGOLD: 5925
FAU - Cho, Samuel K
AU  - Cho SK
AD  - Department of Orthopedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA. RINGGOLD: 5925
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20230810
PL  - England
TA  - Global Spine J
JT  - Global spine journal
JID - 101596156
OTO - NOTNLM
OT  - artificial intelligence
OT  - clinical guidelines
OT  - decision-making
OT  - large language models
OT  - lumbar stenosis
OT  - spine
COIS- Declaration of Conflicting InterestsThe author(s) declared no potential conflicts 
      of interest with respect to the research, authorship, and/or publication of this 
      article.
EDAT- 2023/08/10 12:41
MHDA- 2023/08/10 12:41
CRDT- 2023/08/10 09:12
PHST- 2023/08/10 12:41 [pubmed]
PHST- 2023/08/10 12:41 [medline]
PHST- 2023/08/10 09:12 [entrez]
AID - 10.1177/21925682231195783 [doi]
PST - aheadofprint
SO  - Global Spine J. 2023 Aug 10:21925682231195783. doi: 10.1177/21925682231195783.

PMID- 37352529
OWN - NLM
STAT- MEDLINE
DCOM- 20231023
LR  - 20231212
IS  - 1743-9159 (Electronic)
IS  - 1743-9191 (Print)
IS  - 1743-9159 (Linking)
VI  - 109
IP  - 10
DP  - 2023 Oct 1
TI  - ChatGPT encounters multiple opportunities and challenges in neurosurgery.
PG  - 2886-2891
LID - 10.1097/JS9.0000000000000571 [doi]
AB  - BACKGROUND: ChatGPT, powered by the GPT model and Transformer architecture, has 
      demonstrated remarkable performance in the domains of medicine and healthcare, 
      providing customized and informative responses. In our study, we investigated the 
      potential of ChatGPT in the field of neurosurgery, focusing on its applications 
      at the patient, neurosurgery student/resident, and neurosurgeon levels. METHOD: 
      The authors conducted inquiries with ChatGPT from the viewpoints of patients, 
      neurosurgery students/residents, and neurosurgeons, covering a range of topics, 
      such as disease diagnosis, treatment options, prognosis, rehabilitation, and 
      patient care. The authors also explored concepts related to neurosurgery, 
      including fundamental principles and clinical aspects, as well as tools and 
      techniques to enhance the skills of neurosurgery students/residents. 
      Additionally, the authors examined disease-specific medical interventions and the 
      decision-making processes involved in clinical practice. RESULTS: The authors 
      received individual responses from ChatGPT, but they tended to be shallow and 
      repetitive, lacking depth and personalization. Furthermore, ChatGPT may struggle 
      to discern a patient's emotional state, hindering the establishment of rapport 
      and the delivery of appropriate care. The language used in the medical field is 
      influenced by technical and cultural factors, and biases in the training data can 
      result in skewed or inaccurate responses. Additionally, ChatGPT's limitations 
      include the inability to conduct physical examinations or interpret diagnostic 
      images, potentially overlooking complex details and individual nuances in each 
      patient's case. Moreover, its absence in the surgical setting limits its 
      practical utility. CONCLUSION: Although ChatGPT is a powerful language model, it 
      cannot substitute for the expertise and experience of trained medical 
      professionals. It lacks the capability to perform physical examinations, make 
      diagnoses, administer treatments, establish trust, provide emotional support, and 
      assist in the recovery process. Moreover, the implementation of Artificial 
      Intelligence in healthcare necessitates careful consideration of legal and 
      ethical concerns. While recognizing the potential of ChatGPT, additional training 
      with comprehensive data is necessary to fully maximize its capabilities.
CI  - Copyright © 2023 The Author(s). Published by Wolters Kluwer Health, Inc.
FAU - Kuang, Yi-Rui
AU  - Kuang YR
AD  - Department of Spine Surgery, The First Affiliated Hospital, Hengyang medical 
      school, University of South China, Hengyang, China.
AD  - Department of Neurosurgery, Xiangya Hospital, Central South University, Changsha, 
      China.
AD  - National Clinical Research Center for Geriatric Disorders, Xiangya Hospital, 
      Central South University, Changsha, China.
FAU - Zou, Ming-Xiang
AU  - Zou MX
AD  - Department of Spine Surgery, The First Affiliated Hospital, Hengyang medical 
      school, University of South China, Hengyang, China.
FAU - Niu, Hua-Qing
AU  - Niu HQ
AD  - Department of Ophthalmology, The Second Affiliated Hospital of Zhengzhou 
      University, Zhengzhou, China.
FAU - Zheng, Bo-Yv
AU  - Zheng BY
AD  - Department of Orthopedics Surgery, General Hospital of the Central Theater 
      Command, Wuhan, China.
FAU - Zhang, Tao-Lan
AU  - Zhang TL
AD  - Department of Spine Surgery, The First Affiliated Hospital, Hengyang medical 
      school, University of South China, Hengyang, China.
AD  - Department of Pharmacy, The First Affiliated Hospital, Hengyang Medical School, 
      University of South China, Hengyang, China.
FAU - Zheng, Bo-Wen
AU  - Zheng BW
AD  - Department of Musculoskeletal Tumor Center, People's Hospital, Peking University, 
      Beijing Key Laboratory of Musculoskeletal Tumor. Beijing, China.
LA  - eng
PT  - Journal Article
DEP - 20231001
PL  - United States
TA  - Int J Surg
JT  - International journal of surgery (London, England)
JID - 101228232
SB  - IM
MH  - Humans
MH  - *Neurosurgery
MH  - Artificial Intelligence
MH  - Neurosurgical Procedures
MH  - Health Facilities
PMC - PMC10583932
COIS- The authors declare that they have no competing interests.
EDAT- 2023/06/23 19:11
MHDA- 2023/10/23 00:44
PMCR- 2023/10/18
CRDT- 2023/06/23 17:03
PHST- 2023/06/06 00:00 [received]
PHST- 2023/06/10 00:00 [accepted]
PHST- 2023/10/23 00:44 [medline]
PHST- 2023/06/23 19:11 [pubmed]
PHST- 2023/06/23 17:03 [entrez]
PHST- 2023/10/18 00:00 [pmc-release]
AID - 01279778-202310000-00001 [pii]
AID - IJS-D-23-01093 [pii]
AID - 10.1097/JS9.0000000000000571 [doi]
PST - epublish
SO  - Int J Surg. 2023 Oct 1;109(10):2886-2891. doi: 10.1097/JS9.0000000000000571.

PMID- 38049285
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20231206
LR  - 20240123
IS  - 1701-2163 (Print)
IS  - 1701-2163 (Linking)
VI  - 45
IP  - 12
DP  - 2023 Dec
TI  - Artificial Intelligence/ChatGPT-Generated Manuscripts and Disclosure: Comment.
PG  - 102192
LID - S1701-2163(23)00514-5 [pii]
LID - 10.1016/j.jogc.2023.102192 [doi]
FAU - Kleebayoon, Amnuay
AU  - Kleebayoon A
AD  - Private Academic Consultant, Samraong, Cambodia. Electronic address: 
      amnuaykleebai@gmail.com.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AD  - Research Center, Chandigarh University, Punjab, India; Department of Biological 
      Science, Joesph Ayobabalola University, Ikeji-Arakeji, Nigeria.
LA  - eng
PT  - Comment
PT  - Letter
PL  - Netherlands
TA  - J Obstet Gynaecol Can
JT  - Journal of obstetrics and gynaecology Canada : JOGC = Journal d'obstetrique et 
      gynecologie du Canada : JOGC
JID - 101126664
SB  - IM
CON - Cell Mol Bioeng. 2023 Feb 17;16(2):173-174. PMID: 37096073
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - disclosure
OT  - manuscript
EDAT- 2023/12/05 00:42
MHDA- 2023/12/06 06:42
CRDT- 2023/12/04 21:01
PHST- 2023/08/11 00:00 [received]
PHST- 2023/08/11 00:00 [accepted]
PHST- 2023/12/06 06:42 [medline]
PHST- 2023/12/05 00:42 [pubmed]
PHST- 2023/12/04 21:01 [entrez]
AID - S1701-2163(23)00514-5 [pii]
AID - 10.1016/j.jogc.2023.102192 [doi]
PST - ppublish
SO  - J Obstet Gynaecol Can. 2023 Dec;45(12):102192. doi: 10.1016/j.jogc.2023.102192.

PMID- 37547515
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230808
IS  - 2468-0249 (Electronic)
IS  - 2468-0249 (Linking)
VI  - 8
IP  - 8
DP  - 2023 Aug
TI  - Assessing the Accuracy of ChatGPT on Core Questions in Glomerular Disease.
PG  - 1657-1659
LID - 10.1016/j.ekir.2023.05.014 [doi]
FAU - Miao, Jing
AU  - Miao J
AD  - Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
      Rochester, Minnesota, USA.
FAU - Thongprayoon, Charat
AU  - Thongprayoon C
AD  - Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
      Rochester, Minnesota, USA.
FAU - Cheungpasitporn, Wisit
AU  - Cheungpasitporn W
AD  - Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
      Rochester, Minnesota, USA.
LA  - eng
PT  - Journal Article
DEP - 20230526
PL  - United States
TA  - Kidney Int Rep
JT  - Kidney international reports
JID - 101684752
PMC - PMC10403654
OTO - NOTNLM
OT  - ChatGPT
OT  - KSAP
OT  - NephSAP
OT  - accuracy
OT  - concordance
OT  - glomerular disease
EDAT- 2023/08/07 06:42
MHDA- 2023/08/07 06:43
PMCR- 2023/05/26
CRDT- 2023/08/07 04:54
PHST- 2023/04/01 00:00 [received]
PHST- 2023/05/09 00:00 [revised]
PHST- 2023/05/15 00:00 [accepted]
PHST- 2023/08/07 06:43 [medline]
PHST- 2023/08/07 06:42 [pubmed]
PHST- 2023/08/07 04:54 [entrez]
PHST- 2023/05/26 00:00 [pmc-release]
AID - S2468-0249(23)01309-8 [pii]
AID - 10.1016/j.ekir.2023.05.014 [doi]
PST - epublish
SO  - Kidney Int Rep. 2023 May 26;8(8):1657-1659. doi: 10.1016/j.ekir.2023.05.014. 
      eCollection 2023 Aug.

PMID- 37926642
OWN - NLM
STAT- MEDLINE
DCOM- 20240122
LR  - 20240201
IS  - 1873-7560 (Electronic)
IS  - 0302-2838 (Linking)
VI  - 85
IP  - 2
DP  - 2024 Feb
TI  - Awareness and Use of ChatGPT and Large Language Models: A Prospective 
      Cross-sectional Global Survey in Urology.
PG  - 146-153
LID - S0302-2838(23)03211-6 [pii]
LID - 10.1016/j.eururo.2023.10.014 [doi]
AB  - BACKGROUND: Since its release in November 2022, ChatGPT has captivated society 
      and shown potential for various aspects of health care. OBJECTIVE: To investigate 
      potential use of ChatGPT, a large language model (LLM), in urology by gathering 
      opinions from urologists worldwide. DESIGN, SETTING, AND PARTICIPANTS: An open 
      web-based survey was distributed via social media and e-mail chains to urologists 
      between April 20, 2023 and May 5, 2023. Participants were asked to answer 
      questions related to their knowledge and experience with artificial intelligence, 
      as well as their opinions of potential use of ChatGPT/LLMs in research and 
      clinical practice. OUTCOME MEASUREMENTS AND STATISTICAL ANALYSIS: Data are 
      reported as the mean and standard deviation for continuous variables, and the 
      frequency and percentage for categorical variables. Charts and tables are used as 
      appropriate, with descriptions of the chart types and the measures used. The data 
      are reported in accordance with the Checklist for Reporting Results of Internet 
      E-Surveys (CHERRIES). RESULTS AND LIMITATIONS: A total of 456 individuals 
      completed the survey (64% completion rate). Nearly half (47.7%) reported that 
      they use ChatGPT/LLMs in their academic practice, with fewer using the technology 
      in clinical practice (19.8%). More than half (62.2%) believe there are potential 
      ethical concerns when using ChatGPT for scientific or academic writing, and 53% 
      reported that they have experienced limitations when using ChatGPT in academic 
      practice. CONCLUSIONS: Urologists recognise the potential of ChatGPT/LLMs in 
      research but have concerns regarding ethics and patient acceptance. There is a 
      desire for regulations and guidelines to ensure appropriate use. In addition, 
      measures should be taken to establish rules and guidelines to maximise safety and 
      efficiency when using this novel technology. PATIENT SUMMARY: A survey asked 456 
      urologists from around the world about using an artificial intelligence tool 
      called ChatGPT in their work. Almost half of them use ChatGPT for research, but 
      not many use it for patients care. The resonders think ChatGPT could be helpful, 
      but they worry about problems like ethics and want rules to make sure it's used 
      safely.
CI  - Copyright © 2023 European Association of Urology. Published by Elsevier B.V. All 
      rights reserved.
FAU - Eppler, Michael
AU  - Eppler M
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, CA, USA; 
      AI Center, USC Institute of Urology, University of Southern California, Los 
      Angeles, CA, USA.
FAU - Ganjavi, Conner
AU  - Ganjavi C
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, CA, USA; 
      AI Center, USC Institute of Urology, University of Southern California, Los 
      Angeles, CA, USA.
FAU - Ramacciotti, Lorenzo Storino
AU  - Ramacciotti LS
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, CA, USA; 
      AI Center, USC Institute of Urology, University of Southern California, Los 
      Angeles, CA, USA.
FAU - Piazza, Pietro
AU  - Piazza P
AD  - Division of Urology, IRCCS Azienda Ospedaliero-Universitaria di Bologna, Bologna, 
      Italy.
FAU - Rodler, Severin
AU  - Rodler S
AD  - Department of Urology, Klinikum der Universität München, Munich, Germany.
FAU - Checcucci, Enrico
AU  - Checcucci E
AD  - Department of Surgery, FPO-IRCCS Candiolo Cancer Institute, Candiolo, Italy.
FAU - Gomez Rivas, Juan
AU  - Gomez Rivas J
AD  - Department of Urology, Clinico San Carlos University Hospital, Madrid, Spain.
FAU - Kowalewski, Karl F
AU  - Kowalewski KF
AD  - Department of Urology, University Medical Center Mannheim, Heidelberg University, 
      Mannheim, Germany.
FAU - Belenchón, Ines Rivero
AU  - Belenchón IR
AD  - Urology and Nephrology Department, Virgen del Rocío University Hospital, Seville, 
      Spain.
FAU - Puliatti, Stefano
AU  - Puliatti S
AD  - Urology Department, University of Modena and Reggio Emilia, Modena, Italy.
FAU - Taratkin, Mark
AU  - Taratkin M
AD  - Institute for Urology and Reproductive Health, Sechenov University, Moscow, 
      Russia.
FAU - Veccia, Alessandro
AU  - Veccia A
AD  - Department of Urology, Azienda Ospedaliera Universitaria Integrata Verona, 
      Verona, Italy.
FAU - Baekelandt, Loïc
AU  - Baekelandt L
AD  - Department of Urology, University Hospitals Leuven, Leuven, Belgium.
FAU - Teoh, Jeremy Y-C
AU  - Teoh JY
AD  - Department of Surgery, S.H. Ho Urology Centre, The Chinese University of Hong 
      Kong, Hong Kong, China.
FAU - Somani, Bhaskar K
AU  - Somani BK
AD  - University Hospital Southampton NHS Foundation Trust, Southampton, UK.
FAU - Wroclawski, Marcelo
AU  - Wroclawski M
AD  - Hospital Israelita Albert Einstein, São Paulo, Brazil; Beneficência Portuguesa de 
      São Paulo, São Paulo, Brazil; Faculdade de Medicina do ABC, Santo Andre, Brazil.
FAU - Abreu, Andre
AU  - Abreu A
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, CA, USA; 
      AI Center, USC Institute of Urology, University of Southern California, Los 
      Angeles, CA, USA.
FAU - Porpiglia, Francesco
AU  - Porpiglia F
AD  - Department of Surgery, FPO-IRCCS Candiolo Cancer Institute, Candiolo, Italy.
FAU - Gill, Inderbir S
AU  - Gill IS
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, CA, USA; 
      AI Center, USC Institute of Urology, University of Southern California, Los 
      Angeles, CA, USA.
FAU - Murphy, Declan G
AU  - Murphy DG
AD  - Division of Cancer Surgery, Peter MacCallum Cancer Centre, University of 
      Melbourne, Melbourne, Australia.
FAU - Canes, David
AU  - Canes D
AD  - Division of Urology, Lahey Hospital &amp; Medical Center, Burlington, MA, USA.
FAU - Cacciamani, Giovanni E
AU  - Cacciamani GE
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, CA, USA; 
      AI Center, USC Institute of Urology, University of Southern California, Los 
      Angeles, CA, USA. Electronic address: giovanni.cacciamani@med.usc.edu.
LA  - eng
PT  - Journal Article
DEP - 20231104
PL  - Switzerland
TA  - Eur Urol
JT  - European urology
JID - 7512719
SB  - IM
MH  - Humans
MH  - *Urology
MH  - Artificial Intelligence
MH  - Cross-Sectional Studies
MH  - Prospective Studies
MH  - Language
OTO - NOTNLM
OT  - ChatGPT
OT  - Ethics
OT  - Global
OT  - Large language models
OT  - OpenAI
OT  - Survey
OT  - Urology
EDAT- 2023/11/06 00:41
MHDA- 2024/01/22 06:42
CRDT- 2023/11/05 21:56
PHST- 2023/05/29 00:00 [received]
PHST- 2023/09/27 00:00 [revised]
PHST- 2023/10/24 00:00 [accepted]
PHST- 2024/01/22 06:42 [medline]
PHST- 2023/11/06 00:41 [pubmed]
PHST- 2023/11/05 21:56 [entrez]
AID - S0302-2838(23)03211-6 [pii]
AID - 10.1016/j.eururo.2023.10.014 [doi]
PST - ppublish
SO  - Eur Urol. 2024 Feb;85(2):146-153. doi: 10.1016/j.eururo.2023.10.014. Epub 2023 
      Nov 4.

PMID- 38020160
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231201
IS  - 2296-858X (Print)
IS  - 2296-858X (Electronic)
IS  - 2296-858X (Linking)
VI  - 10
DP  - 2023
TI  - Artificial intelligence in global health equity: an evaluation and discussion on 
      the application of ChatGPT, in the Chinese National Medical Licensing 
      Examination.
PG  - 1237432
LID - 10.3389/fmed.2023.1237432 [doi]
LID - 1237432
AB  - BACKGROUND: The demand for healthcare is increasing globally, with notable 
      disparities in access to resources, especially in Asia, Africa, and Latin 
      America. The rapid development of Artificial Intelligence (AI) technologies, such 
      as OpenAI's ChatGPT, has shown promise in revolutionizing healthcare. However, 
      potential challenges, including the need for specialized medical training, 
      privacy concerns, and language bias, require attention. METHODS: To assess the 
      applicability and limitations of ChatGPT in Chinese and English settings, we 
      designed an experiment evaluating its performance in the 2022 National Medical 
      Licensing Examination (NMLE) in China. For a standardized evaluation, we used the 
      comprehensive written part of the NMLE, translated into English by a bilingual 
      expert. All questions were input into ChatGPT, which provided answers and reasons 
      for choosing them. Responses were evaluated for "information quality" using the 
      Likert scale. RESULTS: ChatGPT demonstrated a correct response rate of 81.25% for 
      Chinese and 86.25% for English questions. Logistic regression analysis showed 
      that neither the difficulty nor the subject matter of the questions was a 
      significant factor in AI errors. The Brier Scores, indicating predictive 
      accuracy, were 0.19 for Chinese and 0.14 for English, indicating good predictive 
      performance. The average quality score for English responses was excellent (4.43 
      point), slightly higher than for Chinese (4.34 point). CONCLUSION: While AI 
      language models like ChatGPT show promise for global healthcare, language bias is 
      a key challenge. Ensuring that such technologies are robustly trained and 
      sensitive to multiple languages and cultures is vital. Further research into AI's 
      role in healthcare, particularly in areas with limited resources, is warranted.
CI  - Copyright © 2023 Tong, Guan, Chen, Huang, Zhong, Zhang and Zhang.
FAU - Tong, Wenting
AU  - Tong W
AD  - Department of Pharmacy, Gannan Healthcare Vocational College, Ganzhou, Jiangxi, 
      China.
FAU - Guan, Yongfu
AU  - Guan Y
AD  - Department of Rehabilitation and Elderly Care, Gannan Healthcare Vocational 
      College, Ganzhou, Jiangxi, China.
FAU - Chen, Jinping
AU  - Chen J
AD  - Department of Rehabilitation and Elderly Care, Gannan Healthcare Vocational 
      College, Ganzhou, Jiangxi, China.
FAU - Huang, Xixuan
AU  - Huang X
AD  - Department of Mathematics, Xiamen University, Xiamen, Fujian, China.
FAU - Zhong, Yuting
AU  - Zhong Y
AD  - Department of Anesthesiology, Gannan Medical University, Jiangxi, China.
FAU - Zhang, Changrong
AU  - Zhang C
AD  - Department of Chinese Medicine, Affiliated Hospital of Qinghai University, 
      Xining, Qinghai, China.
FAU - Zhang, Hui
AU  - Zhang H
AD  - Department of Rehabilitation and Elderly Care, Gannan Healthcare Vocational 
      College, Ganzhou, Jiangxi, China.
AD  - Chair of Endocrinology and Medical Sexology (ENDOSEX), Department of Experimental 
      Medicine, University of Rome Tor Vergata, Rome, Italy.
LA  - eng
PT  - Journal Article
DEP - 20231019
PL  - Switzerland
TA  - Front Med (Lausanne)
JT  - Frontiers in medicine
JID - 101648047
PMC - PMC10656681
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - equity
OT  - global healthcare
OT  - language bias
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2023/11/29 18:41
MHDA- 2023/11/29 18:42
PMCR- 2023/10/19
CRDT- 2023/11/29 14:40
PHST- 2023/06/09 00:00 [received]
PHST- 2023/10/09 00:00 [accepted]
PHST- 2023/11/29 18:42 [medline]
PHST- 2023/11/29 18:41 [pubmed]
PHST- 2023/11/29 14:40 [entrez]
PHST- 2023/10/19 00:00 [pmc-release]
AID - 10.3389/fmed.2023.1237432 [doi]
PST - epublish
SO  - Front Med (Lausanne). 2023 Oct 19;10:1237432. doi: 10.3389/fmed.2023.1237432. 
      eCollection 2023.

PMID- 37999815
OWN - NLM
STAT- MEDLINE
DCOM- 20231127
LR  - 20240118
IS  - 1435-2451 (Electronic)
IS  - 1435-2443 (Linking)
VI  - 408
IP  - 1
DP  - 2023 Nov 24
TI  - Delving into New Frontiers: assessing ChatGPT's proficiency in revealing 
      uncharted dimensions of general surgery and pinpointing innovations for future 
      advancements.
PG  - 446
LID - 10.1007/s00423-023-03173-z [doi]
AB  - PURPOSE: The advent of artificial intelligence (AI) has significantly influenced 
      various medical domains, including general surgery. This research aims to assess 
      ChatGPT, an AI language model, in its ability to shed light on the historical 
      facets of general surgery and pinpoint opportunities for innovation. METHODS: A 
      series of 7 pertinent questions on field of general surgery was posed to ChatGPT. 
      The AI-generated responses were meticulously examined for their relevance, 
      accuracy, and novelty. Additionally, the study explored the AI's ability to 
      recognize knowledge gaps and propose inventive solutions. Expert general surgeons 
      and general surgical residents possessing comprehensive research experience 
      assessed ChatGPT's answers by comparing them to established guidelines and 
      existing literature. RESULTS: ChatGPT presented information that was relevant and 
      accurate, albeit superficial. However, it exhibited convergent thinking and was 
      unable to produce truly groundbreaking ideas to transform general surgery. 
      Instead, it pointed to current popular trends with significant potential for 
      further development. It failed to provide references when prompted and even 
      created references that could not be verified in exhibiting databases. 
      CONCLUSION: While ChatGPT demonstrated a comprehensive understanding of existing 
      general surgical knowledge and the capacity to generate relevant, evidence-based 
      material, it displayed limitations in producing truly groundbreaking concepts or 
      discoveries beyond current knowledge. These results highlight the necessity of 
      enhancing AI-driven models to facilitate the emergence of new insights and 
      promote synergistic, human-AI partnerships for expediting advancements within the 
      general surgery domain.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Lim, Bryan
AU  - Lim B
AUID- ORCID: 0009-0007-9647-5180
AD  - Department of Surgery, Peninsula Health, Melbourne, VIC, 3199, Australia. 
      lim.bryan58@gmail.com.
AD  - Central Clinical School at Monash University, The Alfred Centre, 99 Commercial 
      Rd, Melbourne, VIC, 3004, Australia. lim.bryan58@gmail.com.
FAU - Seth, Ishith
AU  - Seth I
AD  - Department of Surgery, Peninsula Health, Melbourne, VIC, 3199, Australia.
AD  - Central Clinical School at Monash University, The Alfred Centre, 99 Commercial 
      Rd, Melbourne, VIC, 3004, Australia.
AD  - Department of Surgery, Bendigo Hospital, Victoria, 3550, Australia.
FAU - Dooreemeah, Dilshad
AU  - Dooreemeah D
AD  - Department of Surgery, Bendigo Hospital, Victoria, 3550, Australia.
FAU - Lee, Chun Hin Angus
AU  - Lee CHA
AD  - Department of Surgery, Bendigo Hospital, Victoria, 3550, Australia.
LA  - eng
PT  - Journal Article
DEP - 20231124
PL  - Germany
TA  - Langenbecks Arch Surg
JT  - Langenbeck's archives of surgery
JID - 9808285
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Surgeons
MH  - Databases, Factual
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - General surgery
OT  - Innovation
EDAT- 2023/11/24 12:41
MHDA- 2023/11/27 12:42
CRDT- 2023/11/24 11:08
PHST- 2023/05/05 00:00 [received]
PHST- 2023/11/07 00:00 [accepted]
PHST- 2023/11/27 12:42 [medline]
PHST- 2023/11/24 12:41 [pubmed]
PHST- 2023/11/24 11:08 [entrez]
AID - 10.1007/s00423-023-03173-z [pii]
AID - 10.1007/s00423-023-03173-z [doi]
PST - epublish
SO  - Langenbecks Arch Surg. 2023 Nov 24;408(1):446. doi: 10.1007/s00423-023-03173-z.

PMID- 37699647
OWN - NLM
STAT- MEDLINE
DCOM- 20231207
LR  - 20231217
IS  - 1535-5675 (Electronic)
IS  - 0091-4916 (Linking)
VI  - 51
IP  - 4
DP  - 2023 Dec 5
TI  - ChatGPT and Patient Information in Nuclear Medicine: GPT-3.5 Versus GPT-4.
PG  - 307-313
LID - 10.2967/jnmt.123.266151 [doi]
AB  - The GPT-3.5-powered ChatGPT was released in late November 2022 powered by the 
      generative pretrained transformer (GPT) version 3.5. It has emerged as a readily 
      accessible source of patient information ahead of medical procedures. Although 
      ChatGPT has purported benefits for supporting patient education and information, 
      actual capability has not been evaluated. Moreover, the March 2023 emergence of 
      paid subscription access to GPT-4 promises further enhanced capabilities 
      requiring evaluation. Methods: ChatGPT was used to generate patient information 
      sheets suitable for gaining informed consent for 7 common procedures in nuclear 
      medicine. Responses were generated independently for both GPT-3.5 and GPT-4 
      architectures. Specific procedures were selected that had a long-standing history 
      of use to avoid any bias associated with the September 2021 learning cutoff that 
      constrains both GPT-3.5 and GPT-4 architectures. Each information sheet was 
      independently evaluated by 3 expert assessors and ranked on the basis of 
      accuracy, appropriateness, currency, and fitness for purpose. Results: ChatGPT 
      powered by GPT-3.5 provided patient information that was appropriate in terms of 
      being patient-facing but lacked accuracy and currency and omitted important 
      information. GPT-3.5 produced patient information deemed not fit for the purpose. 
      GPT-4 provided patient information enhanced across appropriateness, accuracy, and 
      currency, despite some omission of information. GPT-4 produced patient 
      information that was largely fit for the purpose. Conclusion: Although ChatGPT 
      powered by GPT-3.5 is accessible and provides plausible patient information, 
      inaccuracies and omissions present a risk to patients and informed consent. 
      Conversely, GPT-4 is more accurate and fit for the purpose but, at the time of 
      writing, was available only through a paid subscription.
CI  - © 2023 by the Society of Nuclear Medicine and Molecular Imaging.
FAU - Currie, Geoff
AU  - Currie G
AD  - School of Dentistry and Medical Sciences, Charles Sturt University, Wagga Wagga, 
      New South Wales, Australia; gcurrie@csu.edu.au.
FAU - Robbie, Stephanie
AU  - Robbie S
AD  - Queensland X-Ray, St. Andrews Hospital, Toowoomba, Queensland, Australia; and.
FAU - Tually, Peter
AU  - Tually P
AD  - School of Dentistry and Medical Sciences, Charles Sturt University, Wagga Wagga, 
      New South Wales, Australia.
AD  - Telemed Health, Kalgoorlie, Western Australia, Australia.
LA  - eng
PT  - Journal Article
DEP - 20231205
PL  - United States
TA  - J Nucl Med Technol
JT  - Journal of nuclear medicine technology
JID - 0430303
SB  - IM
MH  - Humans
MH  - *Nuclear Medicine
MH  - Radionuclide Imaging
MH  - Learning
MH  - Salaries and Fringe Benefits
MH  - Writing
OTO - NOTNLM
OT  - ChatGPT
OT  - GPT-4
OT  - generative AI
OT  - language model
OT  - patient education
EDAT- 2023/09/13 00:41
MHDA- 2023/12/07 12:42
CRDT- 2023/09/12 20:33
PHST- 2023/06/08 00:00 [received]
PHST- 2023/07/13 00:00 [revised]
PHST- 2023/12/07 12:42 [medline]
PHST- 2023/09/13 00:41 [pubmed]
PHST- 2023/09/12 20:33 [entrez]
AID - jnmt.123.266151 [pii]
AID - 10.2967/jnmt.123.266151 [doi]
PST - epublish
SO  - J Nucl Med Technol. 2023 Dec 5;51(4):307-313. doi: 10.2967/jnmt.123.266151.

PMID- 37632558
OWN - NLM
STAT- MEDLINE
DCOM- 20231109
LR  - 20240226
IS  - 1433-8726 (Electronic)
IS  - 0724-4983 (Linking)
VI  - 41
IP  - 11
DP  - 2023 Nov
TI  - ChatGPT and most frequent urological diseases: analysing the quality of 
      information and potential risks for patients.
PG  - 3149-3153
LID - 10.1007/s00345-023-04563-0 [doi]
AB  - PURPOSE: Artificial intelligence (AI) is a set of systems or combinations of 
      algorithms, which mimic human intelligence. ChatGPT is software with artificial 
      intelligence which was recently developed by OpenAI. One of its potential uses 
      could be to consult the information about pathologies and treatments. Our 
      objective was to assess the quality of the information provided by AI like 
      ChatGPT and establish if it is a secure source of information for patients. 
      METHODS: Questions about bladder cancer, prostate cancer, renal cancer, benign 
      prostatic hypertrophy (BPH), and urinary stones were queried through ChatGPT 4.0. 
      Two urologists analysed the responses provided by ChatGPT using DISCERN 
      questionary and a brief instrument for evaluating the quality of informed consent 
      documents. RESULTS: The overall information provided in all pathologies was 
      well-balanced. In each pathology was explained its anatomical location, affected 
      population and a description of the symptoms. It concluded with the established 
      risk factors and possible treatment. All treatment answers had a moderate quality 
      score with DISCERN (3 of 5 points). The answers about surgical options contain 
      the recovery time, type of anaesthesia, and potential complications. After 
      analysing all the responses related to each disease, all pathologies except BPH 
      achieved a DISCERN score of 4. CONCLUSIONS: ChatGPT information should be used 
      with caution since the chatbot does not disclose the sources of information and 
      may contain bias even with simple questions related to the basics of urologic 
      diseases.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Szczesniewski, Juliusz Jan
AU  - Szczesniewski JJ
AUID- ORCID: 0000-0001-8690-5692
AD  - Department of Urology, Getafe University Hospital, Carretera Madrid-Toledo km 
      12,500 Getafe, 28905, Madrid, Spain. juliusz.szcz@gmail.com.
AD  - Faculty of Medicine, University of Salamanca, C. Alfonso X el Sabio, s/n, 37007, 
      Salamanca, Spain. juliusz.szcz@gmail.com.
FAU - Tellez Fouz, Carlos
AU  - Tellez Fouz C
AD  - Department of Urology, Getafe University Hospital, Carretera Madrid-Toledo km 
      12,500 Getafe, 28905, Madrid, Spain.
FAU - Ramos Alba, Alejandra
AU  - Ramos Alba A
AD  - DXC Technology, C. de José Echegaray, Las Rozas, 28232, Madrid, Spain.
AD  - Rey Juan Carlos University, P. de los Artilleros 38, 28032, Madrid, Spain.
FAU - Diaz Goizueta, Francisco Javier
AU  - Diaz Goizueta FJ
AD  - Department of Urology, Getafe University Hospital, Carretera Madrid-Toledo km 
      12,500 Getafe, 28905, Madrid, Spain.
FAU - García Tello, Ana
AU  - García Tello A
AD  - Department of Urology, Getafe University Hospital, Carretera Madrid-Toledo km 
      12,500 Getafe, 28905, Madrid, Spain.
FAU - Llanes González, Luis
AU  - Llanes González L
AD  - Department of Urology, Getafe University Hospital, Carretera Madrid-Toledo km 
      12,500 Getafe, 28905, Madrid, Spain.
AD  - Francisco de Vitoria University, Carretera Pozuelo a Majadahonda, Km 1.800, 
      28223, Madrid, Spain.
LA  - eng
PT  - Journal Article
DEP - 20230826
PL  - Germany
TA  - World J Urol
JT  - World journal of urology
JID - 8307716
SB  - IM
CIN - World J Urol. 2023 Sep 26;:. PMID: 37750962
CIN - World J Urol. 2023 Sep 26;:. PMID: 37752273
CIN - World J Urol. 2024 Feb 26;42(1):102. PMID: 38407629
MH  - Male
MH  - Humans
MH  - Artificial Intelligence
MH  - *Prostatic Hyperplasia
MH  - *Urologic Diseases
MH  - *Urinary Calculi
MH  - *Kidney Neoplasms
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Patient information
OT  - Quality of information
OT  - Urology
EDAT- 2023/08/27 05:42
MHDA- 2023/11/09 06:42
CRDT- 2023/08/26 11:04
PHST- 2023/04/16 00:00 [received]
PHST- 2023/08/01 00:00 [accepted]
PHST- 2023/11/09 06:42 [medline]
PHST- 2023/08/27 05:42 [pubmed]
PHST- 2023/08/26 11:04 [entrez]
AID - 10.1007/s00345-023-04563-0 [pii]
AID - 10.1007/s00345-023-04563-0 [doi]
PST - ppublish
SO  - World J Urol. 2023 Nov;41(11):3149-3153. doi: 10.1007/s00345-023-04563-0. Epub 
      2023 Aug 26.

PMID- 37625267
OWN - NLM
STAT- MEDLINE
DCOM- 20230918
LR  - 20230918
IS  - 2352-3964 (Electronic)
IS  - 2352-3964 (Linking)
VI  - 95
DP  - 2023 Sep
TI  - Benchmarking large language models' performances for myopia care: a comparative 
      analysis of ChatGPT-3.5, ChatGPT-4.0, and Google Bard.
PG  - 104770
LID - S2352-3964(23)00336-5 [pii]
LID - 10.1016/j.ebiom.2023.104770 [doi]
LID - 104770
AB  - BACKGROUND: Large language models (LLMs) are garnering wide interest due to their 
      human-like and contextually relevant responses. However, LLMs' accuracy across 
      specific medical domains has yet been thoroughly evaluated. Myopia is a frequent 
      topic which patients and parents commonly seek information online. Our study 
      evaluated the performance of three LLMs namely ChatGPT-3.5, ChatGPT-4.0, and 
      Google Bard, in delivering accurate responses to common myopia-related queries. 
      METHODS: We curated thirty-one commonly asked myopia care-related questions, 
      which were categorised into six domains-pathogenesis, risk factors, clinical 
      presentation, diagnosis, treatment and prevention, and prognosis. Each question 
      was posed to the LLMs, and their responses were independently graded by three 
      consultant-level paediatric ophthalmologists on a three-point accuracy scale 
      (poor, borderline, good). A majority consensus approach was used to determine the 
      final rating for each response. 'Good' rated responses were further evaluated for 
      comprehensiveness on a five-point scale. Conversely, 'poor' rated responses were 
      further prompted for self-correction and then re-evaluated for accuracy. 
      FINDINGS: ChatGPT-4.0 demonstrated superior accuracy, with 80.6% of responses 
      rated as 'good', compared to 61.3% in ChatGPT-3.5 and 54.8% in Google Bard 
      (Pearson's chi-squared test, all p&nbsp;≤&nbsp;0.009). All three LLM-Chatbots showed high 
      mean comprehensiveness scores (Google Bard: 4.35; ChatGPT-4.0: 4.23; ChatGPT-3.5: 
      4.11, out of a maximum score of 5). All LLM-Chatbots also demonstrated 
      substantial self-correction capabilities: 66.7% (2 in 3) of ChatGPT-4.0's, 40% (2 
      in 5) of ChatGPT-3.5's, and 60% (3 in 5) of Google Bard's responses improved 
      after self-correction. The LLM-Chatbots performed consistently across domains, 
      except for 'treatment and prevention'. However, ChatGPT-4.0 still performed 
      superiorly in this domain, receiving 70% 'good' ratings, compared to 40% in 
      ChatGPT-3.5 and 45% in Google Bard (Pearson's chi-squared test, all p&nbsp;≤&nbsp;0.001). 
      INTERPRETATION: Our findings underscore the potential of LLMs, particularly 
      ChatGPT-4.0, for delivering accurate and comprehensive responses to 
      myopia-related queries. Continuous strategies and evaluations to improve LLMs' 
      accuracy remain crucial. FUNDING: Dr Yih-Chung Tham was supported by the National 
      Medical Research Council of Singapore (NMRC/MOH/HCSAINV21nov-0001).
CI  - Copyright © 2023 The Author(s). Published by Elsevier B.V. All rights reserved.
FAU - Lim, Zhi Wei
AU  - Lim ZW
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore.
FAU - Pushpanathan, Krithi
AU  - Pushpanathan K
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore; 
      Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore and National 
      University Health System, Singapore.
FAU - Yew, Samantha Min Er
AU  - Yew SME
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore; 
      Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore and National 
      University Health System, Singapore.
FAU - Lai, Yien
AU  - Lai Y
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore; 
      Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore and National 
      University Health System, Singapore; Department of Ophthalmology, National 
      University Hospital, Singapore.
FAU - Sun, Chen-Hsin
AU  - Sun CH
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore; 
      Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore and National 
      University Health System, Singapore; Department of Ophthalmology, National 
      University Hospital, Singapore.
FAU - Lam, Janice Sing Harn
AU  - Lam JSH
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore; 
      Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore and National 
      University Health System, Singapore; Department of Ophthalmology, National 
      University Hospital, Singapore.
FAU - Chen, David Ziyou
AU  - Chen DZ
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore; 
      Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore and National 
      University Health System, Singapore; Department of Ophthalmology, National 
      University Hospital, Singapore.
FAU - Goh, Jocelyn Hui Lin
AU  - Goh JHL
AD  - Singapore Eye Research Institute, Singapore National Eye Centre, Singapore.
FAU - Tan, Marcus Chun Jin
AU  - Tan MCJ
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore; 
      Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore and National 
      University Health System, Singapore; Department of Ophthalmology, National 
      University Hospital, Singapore.
FAU - Sheng, Bin
AU  - Sheng B
AD  - Department of Computer Science and Engineering, Shanghai Jiao Tong University, 
      Shanghai, China; Department of Endocrinology and Metabolism, Shanghai Jiao Tong 
      University Affiliated Sixth People's Hospital, Shanghai Diabetes Institute, 
      Shanghai Clinical Center for Diabetes, Shanghai, China; MoE Key Lab of Artificial 
      Intelligence, Artificial Intelligence Institute, Shanghai Jiao Tong University, 
      Shanghai, China.
FAU - Cheng, Ching-Yu
AU  - Cheng CY
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore; 
      Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore and National 
      University Health System, Singapore; Singapore Eye Research Institute, Singapore 
      National Eye Centre, Singapore; Eye Academic Clinical Program (Eye ACP), Duke NUS 
      Medical School, Singapore.
FAU - Koh, Victor Teck Chang
AU  - Koh VTC
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore; 
      Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore and National 
      University Health System, Singapore; Department of Ophthalmology, National 
      University Hospital, Singapore.
FAU - Tham, Yih-Chung
AU  - Tham YC
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore; 
      Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore and National 
      University Health System, Singapore; Singapore Eye Research Institute, Singapore 
      National Eye Centre, Singapore; Eye Academic Clinical Program (Eye ACP), Duke NUS 
      Medical School, Singapore. Electronic address: thamyc@nus.edu.sg.
LA  - eng
PT  - Journal Article
DEP - 20230823
PL  - Netherlands
TA  - EBioMedicine
JT  - EBioMedicine
JID - 101647039
SB  - IM
MH  - Humans
MH  - Child
MH  - *Benchmarking
MH  - Search Engine
MH  - Consensus
MH  - Language
MH  - *Myopia/diagnosis/epidemiology/therapy
PMC - PMC10470220
OTO - NOTNLM
OT  - ChatGPT-3.5
OT  - ChatGPT-4.0
OT  - Chatbot
OT  - Google Bard
OT  - Large language models
OT  - Myopia
COIS- Declaration of interests All authors declare no competing interests.
EDAT- 2023/08/26 05:41
MHDA- 2023/09/18 12:42
PMCR- 2023/08/23
CRDT- 2023/08/25 18:04
PHST- 2023/06/30 00:00 [received]
PHST- 2023/07/21 00:00 [revised]
PHST- 2023/08/08 00:00 [accepted]
PHST- 2023/09/18 12:42 [medline]
PHST- 2023/08/26 05:41 [pubmed]
PHST- 2023/08/25 18:04 [entrez]
PHST- 2023/08/23 00:00 [pmc-release]
AID - S2352-3964(23)00336-5 [pii]
AID - 104770 [pii]
AID - 10.1016/j.ebiom.2023.104770 [doi]
PST - ppublish
SO  - EBioMedicine. 2023 Sep;95:104770. doi: 10.1016/j.ebiom.2023.104770. Epub 2023 Aug 
      23.

PMID- 38016664
OWN - NLM
STAT- Publisher
LR  - 20231227
IS  - 1098-8793 (Electronic)
IS  - 0736-6825 (Linking)
DP  - 2023 Dec 27
TI  - Effectiveness of ChatGPT in Identifying and Accurately Guiding Patients in 
      Rhinoplasty Complications.
LID - 10.1055/a-2218-6984 [doi]
AB  - Postoperative complications in rhinoplasty require prompt intervention for 
      optimal outcomes. ChatGPT, an artificial intelligence (AI) tool, offers potential 
      for assisting in postoperative care.This study aims to assess ChatGPT's 
      effectiveness in providing guidance for various rhinoplasty 
      complications.Different complication scenarios were input into ChatGPT. Responses 
      were categorized into "Contact Surgeon" or "Follow Postoperative 
      Instructions."ChatGPT consistently advised immediate surgeon contact for 
      infection. For other complications, it recommended monitoring and adhering to 
      instructions while suggesting surgeon contact if concerns persisted.ChatGPT shows 
      promise in aiding patients' postoperative care by accurately identifying cases 
      necessitating communication with surgeons or emergency care. This research 
      underscores AI's potential in enhancing patient-centered care and contributes to 
      the evolving landscape of health care practices.
CI  - Thieme. All rights reserved.
FAU - Soto-Galindo, Germán A
AU  - Soto-Galindo GA
AD  - International Fellow of the European Academy of Facial Plastic Surgery, Ege 
      University, Izmir, Türkiye.
FAU - Capelleras, Marta
AU  - Capelleras M
AD  - Clinical Fellow of the European Academy of Facial Plastic Surgery, Ege 
      University, Izmir, Türkiye.
FAU - Cruellas, Marc
AU  - Cruellas M
AD  - Otolaryngology - Head and Neck Surgery Resident, Department of 
      Otorhinolaryngology, Hospital Universtari de Bellvitge, Barcelona, Spain.
FAU - Apaydin, Fazil
AU  - Apaydin F
AD  - Division of Facial Plastic and Reconstructive Surgery, Department of 
      Otorhinolaryngology, Ege University, Izmir, Türkiye.
LA  - eng
PT  - Journal Article
DEP - 20231227
PL  - United States
TA  - Facial Plast Surg
JT  - Facial plastic surgery : FPS
JID - 8405303
COIS- None declared.
EDAT- 2023/11/29 00:42
MHDA- 2023/11/29 00:42
CRDT- 2023/11/28 19:23
PHST- 2023/11/29 00:42 [pubmed]
PHST- 2023/11/29 00:42 [medline]
PHST- 2023/11/28 19:23 [entrez]
AID - 10.1055/a-2218-6984 [doi]
PST - aheadofprint
SO  - Facial Plast Surg. 2023 Dec 27. doi: 10.1055/a-2218-6984.

PMID- 37678271
OWN - NLM
STAT- MEDLINE
DCOM- 20240124
LR  - 20240201
IS  - 1743-9159 (Electronic)
IS  - 1743-9191 (Print)
IS  - 1743-9159 (Linking)
VI  - 109
IP  - 12
DP  - 2023 Dec 1
TI  - Auxiliary use of ChatGPT in surgical diagnosis and treatment.
PG  - 3940-3943
LID - 10.1097/JS9.0000000000000686 [doi]
AB  - ChatGPT can be used as an auxiliary tool in surgical diagnosis and treatment in 
      several ways. One of the most incredible values of using ChatGPT is its ability 
      to quickly process and handle large amounts of data and provide relatively 
      accurate information to healthcare workers. Due to its high accuracy and ability 
      to process big data, ChatGPT has been widely used in the healthcare industry for 
      tasks such as assisting medical diagnosis, giving predictions of some diseases, 
      and analyzing some medical cases. Surgical diagnosis and treatment can serve as 
      an auxiliary tool to help healthcare professionals. Process large amounts of 
      medical data, provide real-time guidance and feedback, and increase healthcare's 
      overall speed and quality. Although it has great acceptance, it still faces 
      issues such as ethics, patient privacy, data security, law, trustworthiness, and 
      accuracy. This study aimed to explore the auxiliary use of ChatGPT in surgical 
      diagnosis and treatment.
CI  - Copyright © 2023 The Author(s). Published by Wolters Kluwer Health, Inc.
FAU - Au, Kahei
AU  - Au K
AD  - School of Medicine, Jinan University.
FAU - Yang, Wah
AU  - Yang W
AD  - Department of Metabolic and Bariatric Surgery, The First Affiliated Hospital of 
      Jinan University, Guangzhou, Guangdong Province, People's Republic of China.
LA  - eng
PT  - Journal Article
DEP - 20231201
PL  - United States
TA  - Int J Surg
JT  - International journal of surgery (London, England)
JID - 101228232
SB  - IM
MH  - Humans
MH  - *Big Data
MH  - *Health Personnel
PMC - PMC10720849
COIS- The authors declared no conflicts of interest.
EDAT- 2023/09/08 00:41
MHDA- 2024/01/24 06:43
PMCR- 2023/12/14
CRDT- 2023/09/07 18:53
PHST- 2023/05/24 00:00 [received]
PHST- 2023/08/09 00:00 [accepted]
PHST- 2024/01/24 06:43 [medline]
PHST- 2023/09/08 00:41 [pubmed]
PHST- 2023/09/07 18:53 [entrez]
PHST- 2023/12/14 00:00 [pmc-release]
AID - 01279778-990000000-00583 [pii]
AID - IJS-D-23-00986 [pii]
AID - 10.1097/JS9.0000000000000686 [doi]
PST - epublish
SO  - Int J Surg. 2023 Dec 1;109(12):3940-3943. doi: 10.1097/JS9.0000000000000686.

PMID- 37948100
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231127
IS  - 2561-326X (Electronic)
IS  - 2561-326X (Linking)
VI  - 7
DP  - 2023 Nov 10
TI  - Strengths and Weaknesses of ChatGPT Models for Scientific Writing About Medical 
      Vitamin B12: Mixed Methods Study.
PG  - e49459
LID - 10.2196/49459 [doi]
LID - e49459
AB  - BACKGROUND: ChatGPT is a large language model developed by OpenAI designed to 
      generate human-like responses to prompts. OBJECTIVE: This study aims to evaluate 
      the ability of GPT-4 to generate scientific content and assist in scientific 
      writing using medical vitamin B12 as the topic. Furthermore, the study will 
      compare the performance of GPT-4 to its predecessor, GPT-3.5. METHODS: The study 
      examined responses from GPT-4 and GPT-3.5 to vitamin B12-related prompts, 
      focusing on their quality and characteristics and comparing them to established 
      scientific literature. RESULTS: The results indicated that GPT-4 can potentially 
      streamline scientific writing through its ability to edit language and write 
      abstracts, keywords, and abbreviation lists. However, significant limitations of 
      ChatGPT were revealed, including its inability to identify and address bias, 
      inability to include recent information, lack of transparency, and inclusion of 
      inaccurate information. Additionally, it cannot check for plagiarism or provide 
      proper references. The accuracy of GPT-4's answers was found to be superior to 
      GPT-3.5. CONCLUSIONS: ChatGPT can be considered a helpful assistant in the 
      writing process but not a replacement for a scientist's expertise. Researchers 
      must remain aware of its limitations and use it appropriately. The improvements 
      in consecutive ChatGPT versions suggest the possibility of overcoming some 
      present limitations in the near future.
CI  - ©Omar Abuyaman. Originally published in JMIR Formative Research 
      (https://formative.jmir.org), 10.11.2023.
FAU - Abuyaman, Omar
AU  - Abuyaman O
AUID- ORCID: 0000-0002-7694-7941
AD  - Department of Medical Laboratory Sciences, Faculty of Applied Medical Sciences, 
      The Hashemite University, Zarqa, 13133, Jordan.
LA  - eng
PT  - Journal Article
DEP - 20231110
PL  - Canada
TA  - JMIR Form Res
JT  - JMIR formative research
JID - 101726394
PMC - PMC10674142
OTO - NOTNLM
OT  - AI
OT  - AI solutions
OT  - ChatGPT
OT  - GPT-3.5
OT  - GPT-4
OT  - artificial intelligence
OT  - language editing
OT  - scientific content
OT  - vitamin B12
OT  - wide range information
COIS- Conflicts of Interest: None declared.
EDAT- 2023/11/10 12:45
MHDA- 2023/11/10 12:46
PMCR- 2023/11/10
CRDT- 2023/11/10 11:53
PHST- 2023/05/30 00:00 [received]
PHST- 2023/10/29 00:00 [accepted]
PHST- 2023/08/17 00:00 [revised]
PHST- 2023/11/10 12:46 [medline]
PHST- 2023/11/10 12:45 [pubmed]
PHST- 2023/11/10 11:53 [entrez]
PHST- 2023/11/10 00:00 [pmc-release]
AID - v7i1e49459 [pii]
AID - 10.2196/49459 [doi]
PST - epublish
SO  - JMIR Form Res. 2023 Nov 10;7:e49459. doi: 10.2196/49459.

PMID- 38371717
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240220
IS  - 1985-207X (Print)
IS  - 1985-2274 (Electronic)
IS  - 1985-2274 (Linking)
VI  - 19
DP  - 2024
TI  - ChatGPT, medical research and scientific writing.
PG  - 3
LID - 10.51866/lte.545 [doi]
FAU - Wiwanitkit, Somsri
AU  - Wiwanitkit S
AD  - PhD, Private Academic and Ediorial Consultant, Bangkok Thailand. Email: 
      somsriwiwan@hotmail.com.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AD  - MD, FRFM, Adjunct professor, Center for global health research, Saveetha medical 
      college, Saveetha Institute of medical and technical sciences, India.
LA  - eng
PT  - Journal Article
DEP - 20240106
PL  - Malaysia
TA  - Malays Fam Physician
JT  - Malaysian family physician : the official journal of the Academy of Family 
      Physicians of Malaysia
JID - 101466855
PMC - PMC10866722
OTO - NOTNLM
OT  - ChatGPT
OT  - Medical
OT  - Research
OT  - Writing
COIS- None.
EDAT- 2024/02/19 06:44
MHDA- 2024/02/19 06:45
PMCR- 2024/01/06
CRDT- 2024/02/19 04:30
PHST- 2024/02/19 06:45 [medline]
PHST- 2024/02/19 06:44 [pubmed]
PHST- 2024/02/19 04:30 [entrez]
PHST- 2024/01/06 00:00 [pmc-release]
AID - 10.51866/lte.545 [doi]
PST - epublish
SO  - Malays Fam Physician. 2024 Jan 6;19:3. doi: 10.51866/lte.545. eCollection 2024.

PMID- 37462242
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230925
LR  - 20230925
IS  - 1469-0756 (Electronic)
IS  - 0032-5473 (Linking)
VI  - 99
IP  - 1176
DP  - 2023 Sep 21
TI  - ChatGPT.
PG  - 1047-1048
LID - 10.1093/postmj/qgad056 [doi]
AB  - An algorithm is a process or set of rules to be followed, especially by a 
      computer.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of 
      Postgraduate Medical Journal. All rights reserved. For permissions, please 
      e-mail: journals.permissions@oup.com.
FAU - Welsby, Philip
AU  - Welsby P
AD  - Faculty of Medicine, University of Hong Kong.
FAU - Cheung, Bernard M Y
AU  - Cheung BMY
AD  - Faculty of Medicine, University of Hong Kong.
LA  - eng
PT  - Journal Article
PL  - England
TA  - Postgrad Med J
JT  - Postgraduate medical journal
JID - 0234135
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - generative pre-trained transformer
EDAT- 2023/07/18 13:10
MHDA- 2023/09/25 06:42
CRDT- 2023/07/18 07:32
PHST- 2023/06/15 00:00 [received]
PHST- 2023/06/22 00:00 [accepted]
PHST- 2023/09/25 06:42 [medline]
PHST- 2023/07/18 13:10 [pubmed]
PHST- 2023/07/18 07:32 [entrez]
AID - 7224397 [pii]
AID - 10.1093/postmj/qgad056 [doi]
PST - ppublish
SO  - Postgrad Med J. 2023 Sep 21;99(1176):1047-1048. doi: 10.1093/postmj/qgad056.

PMID- 36753318
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240227
IS  - 2369-3762 (Print)
IS  - 2369-3762 (Electronic)
IS  - 2369-3762 (Linking)
VI  - 9
DP  - 2023 Feb 8
TI  - How Does ChatGPT Perform on the United States Medical Licensing Examination 
      (USMLE)? The Implications of Large Language Models for Medical Education and 
      Knowledge Assessment.
PG  - e45312
LID - 10.2196/45312 [doi]
LID - e45312
AB  - BACKGROUND: Chat Generative Pre-trained Transformer (ChatGPT) is a 
      175-billion-parameter natural language processing model that can generate 
      conversation-style responses to user input. OBJECTIVE: This study aimed to 
      evaluate the performance of ChatGPT on questions within the scope of the United 
      States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as 
      to analyze responses for user interpretability. METHODS: We used 2 sets of 
      multiple-choice questions to evaluate ChatGPT's performance, each with questions 
      pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a 
      commonly used question bank for medical students, which also provides statistics 
      on question difficulty and the performance on an exam relative to the user base. 
      The second set was the National Board of Medical Examiners (NBME) free 120 
      questions. ChatGPT's performance was compared to 2 other large language models, 
      GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated 
      across 3 qualitative metrics: logical justification of the answer selected, 
      presence of information internal to the question, and presence of information 
      external to the question. RESULTS: Of the 4 data sets, AMBOSS-Step1, 
      AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies 
      of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. 
      ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and 
      GPT-3 performed similarly to random chance. The model demonstrated a significant 
      decrease in performance as question difficulty increased (P=.01) within the 
      AMBOSS-Step1 data set. We found that logical justification for ChatGPT's answer 
      selection was present in 100% of outputs of the NBME data sets. Internal 
      information to the question was present in 96.8% (183/189) of all questions. The 
      presence of information external to the question was 44.5% and 27% lower for 
      incorrect answers relative to correct answers on the NBME-Free-Step1 (P&lt;.001) and 
      NBME-Free-Step2 (P=.001) data sets, respectively. CONCLUSIONS: ChatGPT marks a 
      significant improvement in natural language processing models on the tasks of 
      medical question answering. By performing at a greater than 60% threshold on the 
      NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a 
      passing score for a third-year medical student. Additionally, we highlight 
      ChatGPT's capacity to provide logic and informational context across the majority 
      of answers. These facts taken together make a compelling case for the potential 
      applications of ChatGPT as an interactive medical education tool to support 
      learning.
CI  - ©Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard 
      Andrew Taylor, David Chartash. Originally published in JMIR Medical Education 
      (https://mededu.jmir.org), 08.02.2023.
FAU - Gilson, Aidan
AU  - Gilson A
AUID- ORCID: 0000-0002-4770-4705
AD  - Section for Biomedical Informatics and Data Science, Yale University School of 
      Medicine, New Haven, CT, United States.
AD  - Department of Emergency Medicine, Yale University School of Medicine, New Haven, 
      CT, United States.
FAU - Safranek, Conrad W
AU  - Safranek CW
AUID- ORCID: 0000-0003-1985-9432
AD  - Section for Biomedical Informatics and Data Science, Yale University School of 
      Medicine, New Haven, CT, United States.
FAU - Huang, Thomas
AU  - Huang T
AUID- ORCID: 0000-0001-9056-7016
AD  - Department of Emergency Medicine, Yale University School of Medicine, New Haven, 
      CT, United States.
FAU - Socrates, Vimig
AU  - Socrates V
AUID- ORCID: 0000-0001-7955-9875
AD  - Section for Biomedical Informatics and Data Science, Yale University School of 
      Medicine, New Haven, CT, United States.
AD  - Program of Computational Biology and Bioinformatics, Yale University, New Haven, 
      CT, United States.
FAU - Chi, Ling
AU  - Chi L
AUID- ORCID: 0000-0002-8270-9245
AD  - Section for Biomedical Informatics and Data Science, Yale University School of 
      Medicine, New Haven, CT, United States.
FAU - Taylor, Richard Andrew
AU  - Taylor RA
AUID- ORCID: 0000-0002-9082-6644
AD  - Section for Biomedical Informatics and Data Science, Yale University School of 
      Medicine, New Haven, CT, United States.
AD  - Department of Emergency Medicine, Yale University School of Medicine, New Haven, 
      CT, United States.
FAU - Chartash, David
AU  - Chartash D
AUID- ORCID: 0000-0002-0265-330X
AD  - Section for Biomedical Informatics and Data Science, Yale University School of 
      Medicine, New Haven, CT, United States.
AD  - School of Medicine, University College Dublin, National University of Ireland, 
      Dublin, Dublin, Ireland.
LA  - eng
GR  - T35 DK104689/DK/NIDDK NIH HHS/United States
GR  - T35 HL007649/HL/NHLBI NIH HHS/United States
GR  - UL1 TR001863/TR/NCATS NIH HHS/United States
PT  - Journal Article
DEP - 20230208
PL  - Canada
TA  - JMIR Med Educ
JT  - JMIR medical education
JID - 101684518
CIN - Med Clin (Barc). 2023 Oct 27;161(8):363-364. PMID: 37438191
EIN - JMIR Med Educ. 2024 Feb 27;10:e57594. PMID: 38412478
PMC - PMC9947764
OTO - NOTNLM
OT  - ChatGPT
OT  - GPT
OT  - MedQA
OT  - NLP
OT  - artificial intelligence
OT  - chatbot
OT  - conversational agent
OT  - education technology
OT  - generative pre-trained transformer
OT  - machine learning
OT  - medical education
OT  - natural language processing
OT  - USMLE
COIS- Conflicts of Interest: None declared.
EDAT- 2023/02/09 06:00
MHDA- 2023/02/09 06:01
PMCR- 2023/02/08
CRDT- 2023/02/08 11:53
PHST- 2022/12/23 00:00 [received]
PHST- 2023/01/29 00:00 [accepted]
PHST- 2023/01/27 00:00 [revised]
PHST- 2023/02/08 11:53 [entrez]
PHST- 2023/02/09 06:00 [pubmed]
PHST- 2023/02/09 06:01 [medline]
PHST- 2023/02/08 00:00 [pmc-release]
AID - v9i1e45312 [pii]
AID - 10.2196/45312 [doi]
PST - epublish
SO  - JMIR Med Educ. 2023 Feb 8;9:e45312. doi: 10.2196/45312.

PMID- 38182023
OWN - NLM
STAT- Publisher
LR  - 20240105
IS  - 1532-6500 (Electronic)
IS  - 1058-2746 (Linking)
DP  - 2024 Jan 3
TI  - Do ChatGPT and Google Differ in Answers to Commonly Asked Patient Questions 
      Regarding Total Shoulder and Total Elbow Arthroplasty?
LID - S1058-2746(23)00899-6 [pii]
LID - 10.1016/j.jse.2023.11.014 [doi]
AB  - BACKGROUND: Artificial intelligence (AI) and large language models (LLM) offer a 
      new potential resource for patient education. The answers by ChatGPT, a LLM AI 
      text bot, to frequently asked questions (FAQs) were compared to answers provided 
      by a contemporary Google search to determine the reliability of information 
      provided by these sources for patient education in upper extremity arthroplasty. 
      METHODS: "Total shoulder arthroplasty" (TSA) and "total elbow arthroplasty" (TEA) 
      were entered into Google Search and ChatGPT 3.0 to determine the ten most 
      frequently asked questions (FAQs). On Google, the FAQs were obtained through the 
      "people also ask" section, while ChatGPT was asked to provide the ten most FAQs. 
      Each question, answer, and reference(s) cited were recorded. A modified version 
      of the Rothwell system was used to categorize questions into 10 subtopics: 
      special activities, timeline of recovery, restrictions, technical details, cost, 
      indications/management, risks and complications, pain, longevity, and evaluation 
      of surgery. Each reference was categorized into the following groups: commercial, 
      academic, medical practice, single surgeon personal, or social media. Questions 
      for TSA and TEA were combined for analysis and compared between Google and 
      ChatGPT with a two sample Z-test for proportions. RESULTS: Overall, most 
      questions were related to procedural indications or management (17.5%). There 
      were no significant differences between Google and ChatGPT between question 
      categories. The majority of references were from academic websites (65%). ChatGPT 
      produced a greater number of academic references compared to Google (80% vs 50%; 
      p=0.047), while Google more commonly provided medical practice references (25% vs 
      0%; p=0.017). CONCLUSION: In conjunction with patient-physician discussions, AI 
      LLMs may provide a reliable resource for patients. By providing information based 
      on academic references, these tools have the potential to improve health literacy 
      and improved shared decision making for patients searching for information about 
      TSA and TEA. CLINICAL SIGNIFICANCE: With the rising prevalence of AI programs, it 
      is essential to understand how these applications affect patient education in 
      medicine.
CI  - Copyright © 2024. Published by Elsevier Inc.
FAU - Tharakan, Shebin
AU  - Tharakan S
AD  - New York Institute of Technology - College of Osteopathic Medicine, Old Westbury, 
      NY, USA.
FAU - Klein, Brandon
AU  - Klein B
AD  - Department of Orthopaedic Surgery, Northwell Health, Donald and Barbara Zucker 
      School of Medicine, Huntington, NY, USA. Electronic address: 
      Bklein2417@gmail.com.
FAU - Bartlett, Luke
AU  - Bartlett L
AD  - Department of Orthopaedic Surgery, Northwell Health, Donald and Barbara Zucker 
      School of Medicine, Huntington, NY, USA.
FAU - Atlas, Aaron
AU  - Atlas A
AD  - Department of Orthopaedic Surgery, Northwell Health, Donald and Barbara Zucker 
      School of Medicine, Huntington, NY, USA.
FAU - Parada, Stephen A
AU  - Parada SA
AD  - Department of Orthopaedic Surgery, Augusta University Health, Augusta, GA, USA.
FAU - Cohn, Randy M
AU  - Cohn RM
AD  - Department of Orthopaedic Surgery, Northwell Health, Donald and Barbara Zucker 
      School of Medicine, Huntington, NY, USA.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20240103
PL  - United States
TA  - J Shoulder Elbow Surg
JT  - Journal of shoulder and elbow surgery
JID - 9206499
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - Google
OT  - Internet Analytics
OT  - Patient Education
OT  - Patient Information
OT  - Shoulder Arthroplasty
OT  - Total Elbow Arthroplasty
EDAT- 2024/01/06 10:42
MHDA- 2024/01/06 10:42
CRDT- 2024/01/05 19:21
PHST- 2023/08/29 00:00 [received]
PHST- 2023/11/03 00:00 [revised]
PHST- 2023/11/14 00:00 [accepted]
PHST- 2024/01/06 10:42 [medline]
PHST- 2024/01/06 10:42 [pubmed]
PHST- 2024/01/05 19:21 [entrez]
AID - S1058-2746(23)00899-6 [pii]
AID - 10.1016/j.jse.2023.11.014 [doi]
PST - aheadofprint
SO  - J Shoulder Elbow Surg. 2024 Jan 3:S1058-2746(23)00899-6. doi: 
      10.1016/j.jse.2023.11.014.

PMID- 37707707
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231116
IS  - 2193-8245 (Print)
IS  - 2193-6528 (Electronic)
VI  - 12
IP  - 6
DP  - 2023 Dec
TI  - The Use of ChatGPT to Assist in Diagnosing Glaucoma Based on Clinical Case 
      Reports.
PG  - 3121-3132
LID - 10.1007/s40123-023-00805-x [doi]
AB  - INTRODUCTION: The purpose of this study was to evaluate the capabilities of large 
      language models such as Chat Generative Pretrained Transformer (ChatGPT) to 
      diagnose glaucoma based on specific clinical case descriptions with comparison to 
      the performance of senior ophthalmology resident trainees. METHODS: We selected 
      11 cases with primary and secondary glaucoma from a publicly accessible online 
      database of case reports. A total of four cases had primary glaucoma including 
      open-angle, juvenile, normal-tension, and angle-closure glaucoma, while seven 
      cases had secondary glaucoma including pseudo-exfoliation, pigment dispersion 
      glaucoma, glaucomatocyclitic crisis, aphakic, neovascular, aqueous misdirection, 
      and inflammatory glaucoma. We input the text of each case detail into ChatGPT and 
      asked for provisional and differential diagnoses. We then presented the details 
      of 11 cases to three senior ophthalmology residents and recorded their 
      provisional and differential diagnoses. We finally evaluated the responses based 
      on the correct diagnoses and evaluated agreements. RESULTS: The provisional 
      diagnosis based on ChatGPT was correct in eight out of 11 (72.7%) cases and three 
      ophthalmology residents were correct in six (54.5%), eight (72.7%), and eight 
      (72.7%) cases, respectively. The agreement between ChatGPT and the first, second, 
      and third ophthalmology residents were 9, 7, and 7, respectively. CONCLUSIONS: 
      The accuracy of ChatGPT in diagnosing patients with primary and secondary 
      glaucoma, using specific case examples, was similar or better than senior 
      ophthalmology residents. With further development, ChatGPT may have the potential 
      to be used in clinical care settings, such as primary care offices, for triaging 
      and in eye care clinical practices to provide objective and quick diagnoses of 
      patients with glaucoma.
CI  - © 2023. The Author(s).
FAU - Delsoz, Mohammad
AU  - Delsoz M
AD  - Department of Ophthalmology, Hamilton Eye Institute, University of Tennessee 
      Health Science Center, 930 Madison Ave., Suite 471, Memphis, TN, 38163, USA.
FAU - Raja, Hina
AU  - Raja H
AD  - Department of Ophthalmology, Hamilton Eye Institute, University of Tennessee 
      Health Science Center, 930 Madison Ave., Suite 471, Memphis, TN, 38163, USA.
FAU - Madadi, Yeganeh
AU  - Madadi Y
AD  - Department of Ophthalmology, Hamilton Eye Institute, University of Tennessee 
      Health Science Center, 930 Madison Ave., Suite 471, Memphis, TN, 38163, USA.
FAU - Tang, Anthony A
AU  - Tang AA
AD  - Department of Ophthalmology, Hamilton Eye Institute, University of Tennessee 
      Health Science Center, 930 Madison Ave., Suite 471, Memphis, TN, 38163, USA.
FAU - Wirostko, Barbara M
AU  - Wirostko BM
AD  - John Moran Eye Center, University of Utah, Salt Lake City, UT, USA.
FAU - Kahook, Malik Y
AU  - Kahook MY
AD  - Department of Ophthalmology, University of Colorado School of Medicine, Aurora, 
      CO, USA.
FAU - Yousefi, Siamak
AU  - Yousefi S
AUID- ORCID: 0000-0001-8633-5730
AD  - Department of Ophthalmology, Hamilton Eye Institute, University of Tennessee 
      Health Science Center, 930 Madison Ave., Suite 471, Memphis, TN, 38163, USA. 
      siamak.yousefi@uthsc.edu.
AD  - Department of Genetics, Genomics, and Informatics, University of Tennessee Health 
      Science Center, Memphis, TN, USA. siamak.yousefi@uthsc.edu.
LA  - eng
GR  - R01 EY033005/EY/NEI NIH HHS/United States
GR  - R01EY033005/EY/NEI NIH HHS/United States
GR  - R21EY031725/EY/NEI NIH HHS/United States
PT  - Journal Article
DEP - 20230914
PL  - England
TA  - Ophthalmol Ther
JT  - Ophthalmology and therapy
JID - 101634502
PMC - PMC10640454
OTO - NOTNLM
OT  - Artificial intelligence (AI)
OT  - ChatGPT
OT  - Differential diagnosis
OT  - Glaucoma
OT  - Large language models (LLM)
OT  - Provisional diagnosis
COIS- Mohammad Delsoz, Hina Raja, Yeganeh Madadi, Anthony A. Tang, and Malik Y. Kahook 
      have nothing to disclose. Barbara M. Wirostko: Works for MyEyes LLC, and provides 
      consultation for Qlaris Bio and iCare. Siamak Yousefi: Received prototype 
      instruments from Remidio, M&amp;S Technologies, and Visrtucal Fields. He gives 
      consultations to the InsihgtAEye and Enolink.
EDAT- 2023/09/14 12:42
MHDA- 2023/09/14 12:43
PMCR- 2023/09/14
CRDT- 2023/09/14 11:12
PHST- 2023/08/01 00:00 [received]
PHST- 2023/08/29 00:00 [accepted]
PHST- 2023/09/14 12:43 [medline]
PHST- 2023/09/14 12:42 [pubmed]
PHST- 2023/09/14 11:12 [entrez]
PHST- 2023/09/14 00:00 [pmc-release]
AID - 10.1007/s40123-023-00805-x [pii]
AID - 805 [pii]
AID - 10.1007/s40123-023-00805-x [doi]
PST - ppublish
SO  - Ophthalmol Ther. 2023 Dec;12(6):3121-3132. doi: 10.1007/s40123-023-00805-x. Epub 
      2023 Sep 14.

PMID- 38016014
OWN - NLM
STAT- Publisher
LR  - 20231128
IS  - 1536-4798 (Electronic)
IS  - 0277-3740 (Linking)
DP  - 2023 Nov 28
TI  - Quality and Agreement With Scientific Consensus of ChatGPT Information Regarding 
      Corneal Transplantation and Fuchs Dystrophy.
LID - 10.1097/ICO.0000000000003439 [doi]
AB  - PURPOSE: ChatGPT is a commonly used source of information by patients and 
      clinicians. However, it can be prone to error and requires validation. We sought 
      to assess the quality and accuracy of information regarding corneal 
      transplantation and Fuchs dystrophy from 2 iterations of ChatGPT, and whether its 
      answers improve over time. METHODS: A total of 10 corneal specialists 
      collaborated to assess responses of the algorithm to 10 commonly asked questions 
      related to endothelial keratoplasty and Fuchs dystrophy. These questions were 
      asked from both ChatGPT-3.5 and its newer generation, GPT-4. Assessments tested 
      quality, safety, accuracy, and bias of information. Chi-squared, Fisher exact 
      tests, and regression analyses were conducted. RESULTS: We analyzed 180 valid 
      responses. On a 1 (A+) to 5 (F) scale, the average score given by all specialists 
      across questions was 2.5 for ChatGPT-3.5 and 1.4 for GPT-4, a significant 
      improvement (P &lt; 0.0001). Most responses by both ChatGPT-3.5 (61%) and GPT-4 
      (89%) used correct facts, a proportion that significantly improved across 
      iterations (P &lt; 0.00001). Approximately a third (35%) of responses from 
      ChatGPT-3.5 were considered against the scientific consensus, a notable rate of 
      error that decreased to only 5% of answers from GPT-4 (P &lt; 0.00001). CONCLUSIONS: 
      The quality of responses in ChatGPT significantly improved between versions 3.5 
      and 4, and the odds of providing information against the scientific consensus 
      decreased. However, the technology is still capable of producing inaccurate 
      statements. Corneal specialists are uniquely positioned to assist users to 
      discern the veracity and application of such information.
CI  - Copyright © 2023 Wolters Kluwer Health, Inc. All rights reserved.
FAU - Barclay, Kayson S
AU  - Barclay KS
AD  - Morgan State University, Baltimore, MD.
FAU - You, Jane Y
AU  - You JY
AD  - Harvard Medical School, Boston, MA.
FAU - Coleman, Michael J
AU  - Coleman MJ
AD  - Cataract and Laser Institute, Mishawaka, IN.
FAU - Mathews, Priya M
AU  - Mathews PM
AD  - Center for Sight, Sarasota, FL.
FAU - Ray, Vincent L
AU  - Ray VL
AD  - Fremont Eye Care Physicians, Fremont, CA.
FAU - Riaz, Kamran M
AU  - Riaz KM
AD  - Dean McGee Eye Institute, Oklahoma City, OK.
FAU - De Rojas, Joaquin O
AU  - De Rojas JO
AD  - Center for Sight, Sarasota, FL.
FAU - Wang, Aaron S
AU  - Wang AS
AD  - Glaucoma Cataract Consultants, Pittsburgh, PA.
FAU - Watson, Shelly H
AU  - Watson SH
AD  - Northern Virginia Ophthalmology Associates, Falls Church, VA.
FAU - Koo, Ellen H
AU  - Koo EH
AD  - Bascom Palmer Eye Institute, Miami, FL; and.
FAU - Eghrari, Allen O
AU  - Eghrari AO
AUID- ORCID: 0000-0003-2798-038
AD  - Wilmer Eye Institute at Johns Hopkins, Baltimore, MD.
LA  - eng
PT  - Journal Article
DEP - 20231128
PL  - United States
TA  - Cornea
JT  - Cornea
JID - 8216186
SB  - IM
COIS- The authors have no funding or conflicts of interest to disclose.
EDAT- 2023/11/28 18:42
MHDA- 2023/11/28 18:42
CRDT- 2023/11/28 14:53
PHST- 2023/06/07 00:00 [received]
PHST- 2023/10/30 00:00 [accepted]
PHST- 2023/11/28 18:42 [medline]
PHST- 2023/11/28 18:42 [pubmed]
PHST- 2023/11/28 14:53 [entrez]
AID - 00003226-990000000-00430 [pii]
AID - 10.1097/ICO.0000000000003439 [doi]
PST - aheadofprint
SO  - Cornea. 2023 Nov 28. doi: 10.1097/ICO.0000000000003439.

PMID- 37809155
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231018
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 9
DP  - 2023 Sep
TI  - ChatGPT Surpasses 1000 Publications on PubMed: Envisioning the Road Ahead.
PG  - e44769
LID - 10.7759/cureus.44769 [doi]
LID - e44769
AB  - The exponential growth of ChatGPT in medical literature, amassing over 1000 
      PubMed citations by August 2023, underscores a pivotal juncture in the 
      convergence of artificial intelligence (AI) and healthcare. This remarkable rise 
      not only showcases its potential to revolutionize medical academia but also 
      indicates its impending influence on patient care and healthcare systems. 
      Notwithstanding this enthusiasm, one-third of these citations are editorials or 
      commentaries, stressing a gap in empirical research. Alongside its potential, 
      there are concerns about ChatGPT becoming a "Weapon of Mass Deception" and the 
      need for rigorous evaluations to counter inaccuracies. The World Association of 
      Medical Editors has released guidelines emphasizing that AI tools should not be 
      manuscript co-authors and advocates for clear disclosures in AI-assisted academic 
      works. Interestingly, ChatGPT achieved its citation milestone within nine months, 
      compared to Google's 14 years. As Large Language Models (LLMs), like ChatGPT, 
      become more integral in healthcare, issues surrounding data protection, patient 
      privacy, and ethical implications gain prominence. As the future of LLM&nbsp;research 
      unfolds, key areas of interest include its efficacy in clinical settings, its 
      role in telemedicine, and its potential in medical education. The journey ahead 
      necessitates a harmonious partnership between the medical community and AI 
      developers, emphasizing both technological advancements and ethical 
      considerations.
CI  - Copyright © 2023, Temsah et al.
FAU - Temsah, Mohamad-Hani
AU  - Temsah MH
AD  - Pediatric Intensive Care Unit, King Saud University, Riyadh, SAU.
FAU - Altamimi, Ibraheem
AU  - Altamimi I
AD  - College of Medicine, King Saud University, Riyadh, SAU.
FAU - Jamal, Amr
AU  - Jamal A
AD  - Family and Community Medicine, King Saud University, Riyadh, SAU.
FAU - Alhasan, Khalid
AU  - Alhasan K
AD  - Pediatric Nephrology, King Saud University, Riyadh, SAU.
FAU - Al-Eyadhy, Ayman
AU  - Al-Eyadhy A
AD  - Pediatrics, King Saud University, Riyadh, SAU.
AD  - Pediatric Intensive Care Unit, King Saud University Medical City, Riyadh, SAU.
LA  - eng
PT  - Editorial
DEP - 20230906
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10557088
OTO - NOTNLM
OT  - ai chatbot
OT  - artificial intelligence (ai)
OT  - chatgpt
OT  - ethical implications
OT  - llms in healthcare
OT  - medical education
OT  - published in pubmed journal
OT  - pubmed
OT  - randomized controlled trial (rct)
OT  - telemedicine
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/10/09 06:41
MHDA- 2023/10/09 06:42
PMCR- 2023/09/06
CRDT- 2023/10/09 05:52
PHST- 2023/09/06 00:00 [accepted]
PHST- 2023/10/09 06:42 [medline]
PHST- 2023/10/09 06:41 [pubmed]
PHST- 2023/10/09 05:52 [entrez]
PHST- 2023/09/06 00:00 [pmc-release]
AID - 10.7759/cureus.44769 [doi]
PST - epublish
SO  - Cureus. 2023 Sep 6;15(9):e44769. doi: 10.7759/cureus.44769. eCollection 2023 Sep.

PMID- 36757192
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230314
LR  - 20230912
IS  - 1532-0987 (Electronic)
IS  - 0891-3668 (Linking)
VI  - 42
IP  - 4
DP  - 2023 Apr 1
TI  - To ChatGPT or not to ChatGPT? The Impact of Artificial Intelligence on Academic 
      Publishing.
PG  - 275
LID - 10.1097/INF.0000000000003852 [doi]
FAU - Curtis, Nigel
AU  - Curtis N
AUID- ORCID: 0000-0003-3446-4594
AD  - Department of Paediatrics, The University of Melbourne, Parkville, Victoria, 
      Australia.
AD  - Infectious Diseases Unit, Royal Children's Hospital Melbourne, Parkville, 
      Victoria, Australia.
AD  - Infectious Diseases Group, Murdoch Children's Research Institute, Parkville, 
      Victoria, Australia.
CN  - ChatGPT
LA  - eng
PT  - Journal Article
DEP - 20230209
PL  - United States
TA  - Pediatr Infect Dis J
JT  - The Pediatric infectious disease journal
JID - 8701858
SB  - IM
COIS- The authors have no funding or conflicts of interest to disclose.
EDAT- 2023/02/10 06:00
MHDA- 2023/03/14 06:00
CRDT- 2023/02/09 09:03
PHST- 2023/02/10 06:00 [pubmed]
PHST- 2023/03/14 06:00 [medline]
PHST- 2023/02/09 09:03 [entrez]
AID - 00006454-202304000-00001 [pii]
AID - 10.1097/INF.0000000000003852 [doi]
PST - ppublish
SO  - Pediatr Infect Dis J. 2023 Apr 1;42(4):275. doi: 10.1097/INF.0000000000003852. 
      Epub 2023 Feb 9.

PMID- 38206515
OWN - NLM
STAT- MEDLINE
DCOM- 20240117
LR  - 20240117
IS  - 1481-8043 (Electronic)
IS  - 1481-8035 (Linking)
VI  - 26
IP  - 1
DP  - 2024 Jan
TI  - Repeatability, reproducibility, and diagnostic accuracy of a commercial large 
      language model (ChatGPT) to perform emergency department triage using the 
      Canadian triage and acuity scale.
PG  - 40-46
LID - 10.1007/s43678-023-00616-w [doi]
AB  - PURPOSE: The release of the ChatGPT prototype to the public in November 2022 
      drastically reduced the barrier to using artificial intelligence by allowing easy 
      access to a large language model with only a simple web interface. One situation 
      where ChatGPT could be useful is in triaging patients arriving to the emergency 
      department. This study aimed to address the research problem: "can emergency 
      physicians use ChatGPT to accurately triage patients using the Canadian Triage 
      and Acuity Scale (CTAS)?". METHODS: Six unique prompts were developed 
      independently by five emergency physicians. An automated script was used to query 
      ChatGPT with each of the 6 prompts combined with 61 validated and previously 
      published patient vignettes. Thirty repetitions of each combination were 
      performed for a total of 10,980 simulated triages. RESULTS: In 99.6% of 10,980 
      queries, a CTAS score was returned. However, there was considerable variations in 
      results. Repeatability (use of the same prompt repeatedly) was responsible for 
      21.0% of overall variation. Reproducibility (use of different prompts) was 
      responsible for 4.0% of overall variation. Overall accuracy of ChatGPT to triage 
      simulated patients was 47.5% with a 13.7% under-triage rate and a 38.7% 
      over-triage rate. More extensively detailed text given as a prompt was associated 
      with greater reproducibility, but minimal increase in accuracy. CONCLUSIONS: This 
      study suggests that the current ChatGPT large language model is not sufficient 
      for emergency physicians to triage simulated patients using the Canadian Triage 
      and Acuity Scale due to poor repeatability and accuracy. Medical practitioners 
      should be aware that while ChatGPT can be a valuable tool, it may lack 
      consistency and may frequently provide false information.
CI  - © 2024. The Author(s), under exclusive licence to Canadian Association of 
      Emergency Physicians (CAEP)/ Association Canadienne de Médecine d'Urgence (ACMU).
FAU - Franc, Jeffrey Michael
AU  - Franc JM
AUID- ORCID: 0000-0002-2421-3479
AD  - Department of Emergency Medicine, University of Alberta, Edmonton, AB, Canada. 
      jeffrey.franc@ualberta.ca.
AD  - Faculty of Medicine, University of Alberta, Edmonton, AB, Canada. 
      jeffrey.franc@ualberta.ca.
AD  - Università del Piemonte Orientale, Novara, Italy. jeffrey.franc@ualberta.ca.
FAU - Cheng, Lenard
AU  - Cheng L
AD  - Department of Emergency Medicine, Beth Israel Deaconess Medical Center, Boston, 
      MA, USA.
AD  - Harvard Medical School, Boston, MA, USA.
FAU - Hart, Alexander
AU  - Hart A
AD  - Department of Emergency Medicine, Beth Israel Deaconess Medical Center, Boston, 
      MA, USA.
AD  - Department of Emergency Medicine, Hartford Hospital, Hartford, CT, USA.
AD  - University of Connecticut School of Medicine, Farmington, CT, USA.
FAU - Hata, Ryan
AU  - Hata R
AD  - Department of Emergency Medicine, Beth Israel Deaconess Medical Center, Boston, 
      MA, USA.
AD  - Harvard Medical School, Boston, MA, USA.
FAU - Hertelendy, Atilla
AU  - Hertelendy A
AD  - Department of Emergency Medicine, Beth Israel Deaconess Medical Center, Boston, 
      MA, USA.
AD  - Department of Information Systems and Business Analytics, College of Business, 
      Florida International University, Miami, FL, USA.
LA  - eng
PT  - Journal Article
DEP - 20240111
PL  - England
TA  - CJEM
JT  - CJEM
JID - 100893237
SB  - IM
MH  - Humans
MH  - *Triage/methods
MH  - Reproducibility of Results
MH  - *Artificial Intelligence
MH  - Canada
MH  - Emergency Service, Hospital
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Canadian triage and acuity scale
OT  - Emergency medicine
OT  - Large language models
OT  - Triage
EDAT- 2024/01/11 12:43
MHDA- 2024/01/17 06:42
CRDT- 2024/01/11 11:12
PHST- 2023/04/24 00:00 [received]
PHST- 2023/10/29 00:00 [accepted]
PHST- 2024/01/17 06:42 [medline]
PHST- 2024/01/11 12:43 [pubmed]
PHST- 2024/01/11 11:12 [entrez]
AID - 10.1007/s43678-023-00616-w [pii]
AID - 10.1007/s43678-023-00616-w [doi]
PST - ppublish
SO  - CJEM. 2024 Jan;26(1):40-46. doi: 10.1007/s43678-023-00616-w. Epub 2024 Jan 11.

PMID- 38458774
OWN - NLM
STAT- Publisher
LR  - 20240308
IS  - 1521-0111 (Electronic)
IS  - 0026-895X (Linking)
DP  - 2024 Mar 8
TI  - A conversation with ChatGPT on contentious issues in senescence and cancer 
      research.
LID - MOLPHARM-EMC-2024-000871 [pii]
LID - 10.1124/molpharm.124.000871 [doi]
AB  - Artificial Intelligence (AI) platforms such as Generative Pre-trained Transformer 
      (ChatGPT) have achieved a high degree of popularity amongst the scientific 
      community due to their util-ity in providing evidence-based reviews of the 
      literature. However, the accuracy and reliability of the information output and 
      the ability to provide critical analysis of the literature, espe-cially with 
      respect to highly controversial issues, has generally not been evaluated. In this 
      work, we arranged a Q/A session with ChatGPT regarding several unresolved 
      questions in the field of cancer research relating to Therapy-Induced Senescence 
      (TIS) including the topics of senescence reversibility, its connection to tumor 
      dormancy, and the pharmacology of the newly emerging drug class of senolytics. 
      ChatGPT generally provided responses consistent with the available literature, 
      while occasionally overlooking essential components of the cur-rent understanding 
      of the role of TIS in cancer biology and treatment. While ChatGPT, and similar AI 
      platforms, have utility in providing an accurate evidence-based review of the 
      liter-ature, their outputs should still be considered carefully, especially with 
      respect to unresolved issues in tumor biology. Significance Statement Artificial 
      Intelligence platforms have provided great utility for researchers to investigate 
      the biomedical literature in a prompt manner. However, several issues arise when 
      it comes to certain unresolved biological questions, especially in the cancer 
      field. This work provided a discussion with ChatGPT regarding some of the yet to 
      be fully elucidated conundrums of the role of Therapy-Induced Senescence in 
      cancer treatment and highlights the strengths and weaknesses in utilizing such 
      platforms for analyzing the scientific literature on this topic.
CI  - Copyright © 2024 American Society for Pharmacology and Experimental Therapeutics.
FAU - Elshazly, Ahmed M
AU  - Elshazly AM
AUID- ORCID: 0000-0003-0353-1643
AD  - Pharmacology and Toxicology, Virginia Commonwealth University, United States.
FAU - Shahin, Uruk
AU  - Shahin U
AD  - Pharmacology and Public Health, The Hashemite University, Jordan.
FAU - Al Shboul, Sofian
AU  - Al Shboul S
AD  - Pharmacology and Public Health, The Hashemite University, Jordan.
FAU - Gewirtz, David A
AU  - Gewirtz DA
AD  - Dept. of Medicine, Virginia Commonwealth University, United States 
      tareq@hu.edu.jo.
FAU - Saleh, Tareq
AU  - Saleh T
AUID- ORCID: 0000-0002-2878-1107
AD  - Pharmacology and Public Health, The Hashemite University, Jordan tareq@hu.edu.jo.
LA  - eng
PT  - Journal Article
DEP - 20240308
PL  - United States
TA  - Mol Pharmacol
JT  - Molecular pharmacology
JID - 0035623
SB  - IM
OTO - NOTNLM
OT  - Cell death
OT  - anticancer
OT  - cancer
OT  - cancer chemotherapy
OT  - cell injury/cell death
EDAT- 2024/03/09 10:42
MHDA- 2024/03/09 10:42
CRDT- 2024/03/08 21:23
PHST- 2024/02/26 00:00 [accepted]
PHST- 2024/01/03 00:00 [received]
PHST- 2024/02/19 00:00 [revised]
PHST- 2024/03/09 10:42 [medline]
PHST- 2024/03/09 10:42 [pubmed]
PHST- 2024/03/08 21:23 [entrez]
AID - molpharm.124.000871 [pii]
AID - 10.1124/molpharm.124.000871 [doi]
PST - aheadofprint
SO  - Mol Pharmacol. 2024 Mar 8:MOLPHARM-EMC-2024-000871. doi: 
      10.1124/molpharm.124.000871.

PMID- 38310063
OWN - NLM
STAT- Publisher
LR  - 20240203
IS  - 1097-6841 (Electronic)
IS  - 0022-3913 (Linking)
DP  - 2024 Feb 2
TI  - ChatGPT performance in prosthodontics: Assessment of accuracy and repeatability 
      in answer generation.
LID - S0022-3913(24)00049-0 [pii]
LID - 10.1016/j.prosdent.2024.01.018 [doi]
AB  - STATEMENT OF PROBLEM: The artificial intelligence (AI) software program ChatGPT 
      is based on large language models (LLMs) and is widely accessible. However, in 
      prosthodontics, little is known about its performance in generating answers. 
      PURPOSE: The purpose of this study was to determine the performance of ChatGPT in 
      generating answers about removable dental prostheses (RDPs) and tooth-supported 
      fixed dental prostheses (FDPs). MATERIAL AND METHODS: Thirty short questions were 
      designed about RDPs and tooth-supported FDP, and 30 answers were generated for 
      each of the questions using ChatGPT-4 in October 2023. The 900 generated answers 
      were independently graded by experts using a 3-point Likert scale. The relative 
      frequency and absolute percentage of answers were described. Accuracy was 
      assessed using the Wald binomial method, while repeatability was evaluated using 
      percentage agreement, Brennan and Prediger coefficient, Conger generalized Cohen 
      kappa, Fleiss kappa, Gwet AC, and Krippendorff alpha methods. Confidence 
      intervals were set at 95%. Statistical analysis was performed using the STATA 
      software program. RESULTS: The performance of ChatGPT in generating answers 
      related to RDP and tooth-supported FDP was limited. The answers showed a 
      reliability of 25.6%, with a confidence range between 22.9% and 28.6%. The 
      repeatability ranged from substantial to moderate. CONCLUSIONS: The results show 
      that currently ChatGPT has limited ability to generate answers related to RDPs 
      and tooth-supported FDPs. Therefore, ChatGPT cannot replace a dentist, and, if 
      professionals were to use it, they should be aware of its limitations.
CI  - Copyright © 2024 The Authors. Published by Elsevier Inc. All rights reserved.
FAU - Freire, Yolanda
AU  - Freire Y
AD  - Assistant Professor, Department of Pre-Clinic Dentistry, Faculty of Biomedical 
      and Health Sciences, European University of Madrid (UEM), Madrid, Spain.
FAU - Santamaría Laorden, Andrea
AU  - Santamaría Laorden A
AD  - Assistant Professor, Department of Pre-Clinic Dentistry, Faculty of Biomedical 
      and Health Sciences, European University of Madrid (UEM), Madrid, Spain.
FAU - Orejas Pérez, Jaime
AU  - Orejas Pérez J
AD  - Assistant Professor, Department of Pre-Clinic Dentistry, Faculty of Biomedical 
      and Health Sciences, European University of Madrid (UEM), Madrid, Spain.
FAU - Gómez Sánchez, Margarita
AU  - Gómez Sánchez M
AD  - Assistant Professor, Vice Dean of Dentistry, Department of Pre-Clinic Dentistry 
      and Clinical Dentistry, Faculty of Biomedical and Health Sciences, European 
      University of Madrid (UEM), Madrid, Spain.
FAU - Díaz-Flores García, Víctor
AU  - Díaz-Flores García V
AD  - Assistant Professor, Department of Pre-Clinic Dentistry, Faculty of Biomedical 
      and Health Sciences, European University of Madrid (UEM), Madrid, Spain. 
      Electronic address: victor.diaz-flores@universidadeuropea.es.
FAU - Suárez, Ana
AU  - Suárez A
AD  - Associate Professor, Department of Pre-Clinic Dentistry, Faculty of Biomedical 
      and Health Sciences, European University of Madrid (UEM), Madrid, Spain.
LA  - eng
PT  - Journal Article
DEP - 20240202
PL  - United States
TA  - J Prosthet Dent
JT  - The Journal of prosthetic dentistry
JID - 0376364
SB  - IM
EDAT- 2024/02/04 00:42
MHDA- 2024/02/04 00:42
CRDT- 2024/02/03 22:03
PHST- 2023/10/31 00:00 [received]
PHST- 2024/01/17 00:00 [revised]
PHST- 2024/01/18 00:00 [accepted]
PHST- 2024/02/04 00:42 [medline]
PHST- 2024/02/04 00:42 [pubmed]
PHST- 2024/02/03 22:03 [entrez]
AID - S0022-3913(24)00049-0 [pii]
AID - 10.1016/j.prosdent.2024.01.018 [doi]
PST - aheadofprint
SO  - J Prosthet Dent. 2024 Feb 2:S0022-3913(24)00049-0. doi: 
      10.1016/j.prosdent.2024.01.018.

PMID- 37840252
OWN - NLM
STAT- MEDLINE
DCOM- 20231101
LR  - 20231214
IS  - 1975-5937 (Electronic)
IS  - 1975-5937 (Linking)
VI  - 20
DP  - 2023
TI  - Efficacy and limitations of ChatGPT as a biostatistical problem-solving tool in 
      medical education in Serbia: a descriptive study.
PG  - 28
LID - 10.3352/jeehp.2023.20.28 [doi]
LID - 28
AB  - PURPOSE: This study aimed to assess the performance of ChatGPT (GPT-3.5 and 
      GPT-4) as a study tool in solving biostatistical problems and to identify any 
      potential drawbacks that might arise from using ChatGPT in medical education, 
      particularly in solving practical biostatistical problems. METHODS: ChatGPT was 
      tested to evaluate its ability to solve biostatistical problems from the Handbook 
      of Medical Statistics by Peacock and Peacock in this descriptive study. Tables 
      from the problems were transformed into textual questions. Ten biostatistical 
      problems were randomly chosen and used as text-based input for conversation with 
      ChatGPT (versions 3.5 and 4). RESULTS: GPT-3.5 solved 5 practical problems in the 
      first attempt, related to categorical data, cross-sectional study, measuring 
      reliability, probability properties, and the t-test. GPT-3.5 failed to provide 
      correct answers regarding analysis of variance, the chi-square test, and sample 
      size within 3 attempts. GPT-4 also solved a task related to the confidence 
      interval in the first attempt and solved all questions within 3 attempts, with 
      precise guidance and monitoring. CONCLUSION: The assessment of both versions of 
      ChatGPT performance in 10 biostatistical problems revealed that GPT-3.5 and 4’s 
      performance was below average, with correct response rates of 5 and 6 out of 10 
      on the first attempt. GPT-4 succeeded in providing all correct answers within 3 
      attempts. These findings indicate that students must be aware that this tool, 
      even when providing and calculating different statistical analyses, can be wrong, 
      and they should be aware of ChatGPT’s limitations and be careful when 
      incorporating this model into medical education.
FAU - Ignjatović, Aleksandra
AU  - Ignjatović A
AD  - Department of Medical Statistics and Informatics, Faculty of Medicine, University 
      of Niš, Niš, Serbia.
FAU - Stevanović, Lazar
AU  - Stevanović L
AD  - Faculty of Medicine, University of Niš, Niš, Serbia.
LA  - eng
PT  - Journal Article
DEP - 20231016
PL  - Korea (South)
TA  - J Educ Eval Health Prof
JT  - Journal of educational evaluation for health professions
JID - 101490061
SB  - IM
MH  - Humans
MH  - Cross-Sectional Studies
MH  - Reproducibility of Results
MH  - *Education, Medical
MH  - Awareness
MH  - Communication
PMC - PMC10646144
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Biostatistics
OT  - Medical education
OT  - Reproducibility of results
COIS- Conflict of interest No potential conflict of interest relevant to this article 
      was reported.
EDAT- 2023/10/16 06:48
MHDA- 2023/11/01 12:45
PMCR- 2023/10/16
CRDT- 2023/10/16 03:20
PHST- 2023/09/21 00:00 [received]
PHST- 2023/10/10 00:00 [accepted]
PHST- 2023/11/01 12:45 [medline]
PHST- 2023/10/16 06:48 [pubmed]
PHST- 2023/10/16 03:20 [entrez]
PHST- 2023/10/16 00:00 [pmc-release]
AID - jeehp.2023.20.28 [pii]
AID - jeehp-20-28 [pii]
AID - 10.3352/jeehp.2023.20.28 [doi]
PST - ppublish
SO  - J Educ Eval Health Prof. 2023;20:28. doi: 10.3352/jeehp.2023.20.28. Epub 2023 Oct 
      16.

PMID- 37714915
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231121
IS  - 2045-2322 (Electronic)
IS  - 2045-2322 (Linking)
VI  - 13
IP  - 1
DP  - 2023 Sep 15
TI  - ChatGPT in education: global reactions to AI innovations.
PG  - 15310
LID - 10.1038/s41598-023-42227-6 [doi]
LID - 15310
AB  - The release and rapid diffusion of ChatGPT have caught the attention of educators 
      worldwide. Some educators are enthusiastic about its potential to support 
      learning. Others are concerned about how it might circumvent learning 
      opportunities or contribute to misinformation. To better understand reactions 
      about ChatGPT concerning education, we analyzed Twitter data (16,830,997 tweets 
      from 5,541,457 users). Based on topic modeling and sentiment analysis, we provide 
      an overview of global perceptions and reactions to ChatGPT regarding education. 
      ChatGPT triggered a massive response on Twitter, with education being the most 
      tweeted content topic. Topics ranged from specific (e.g., cheating) to broad 
      (e.g., opportunities), which were discussed with mixed sentiment. We traced that 
      authority decisions may influence public opinions. We discussed that the average 
      reaction on Twitter (e.g., using ChatGPT to cheat in exams) differs from 
      discussions in which education and teaching-learning researchers are likely to be 
      more interested (e.g., ChatGPT as an intelligent learning partner). This study 
      provides insights into people's reactions when new groundbreaking technology is 
      released and implications for scientific and policy communication in rapidly 
      changing circumstances.
CI  - © 2023. Springer Nature Limited.
FAU - Fütterer, Tim
AU  - Fütterer T
AUID- ORCID: 0000-0001-5399-9557
AD  - Hector Research Institute of Education Sciences and Psychology, University of 
      Tübingen, Europastraße 6, 72072, Tübingen, Germany. 
      tim.fuetterer@uni-tuebingen.de.
FAU - Fischer, Christian
AU  - Fischer C
AUID- ORCID: 0000-0002-8809-2776
AD  - Hector Research Institute of Education Sciences and Psychology, University of 
      Tübingen, Europastraße 6, 72072, Tübingen, Germany.
FAU - Alekseeva, Anastasiia
AU  - Alekseeva A
AUID- ORCID: 0000-0002-7395-6180
AD  - Hector Research Institute of Education Sciences and Psychology, University of 
      Tübingen, Europastraße 6, 72072, Tübingen, Germany.
FAU - Chen, Xiaobin
AU  - Chen X
AUID- ORCID: 0000-0003-3158-0899
AD  - Hector Research Institute of Education Sciences and Psychology, University of 
      Tübingen, Europastraße 6, 72072, Tübingen, Germany.
FAU - Tate, Tamara
AU  - Tate T
AUID- ORCID: 0000-0002-1753-8435
AD  - University of California, Irvine, USA.
FAU - Warschauer, Mark
AU  - Warschauer M
AUID- ORCID: 0000-0002-6817-4416
AD  - University of California, Irvine, USA.
FAU - Gerjets, Peter
AU  - Gerjets P
AUID- ORCID: 0000-0003-1358-6779
AD  - Leibniz-Institut für Wissensmedien, Tübingen, Germany.
LA  - eng
PT  - Journal Article
DEP - 20230915
PL  - England
TA  - Sci Rep
JT  - Scientific reports
JID - 101563288
SB  - IM
PMC - PMC10504368
COIS- The authors declare no competing interests.
EDAT- 2023/09/16 05:42
MHDA- 2023/09/16 05:43
PMCR- 2023/09/15
CRDT- 2023/09/15 23:23
PHST- 2023/04/20 00:00 [received]
PHST- 2023/09/07 00:00 [accepted]
PHST- 2023/09/16 05:43 [medline]
PHST- 2023/09/16 05:42 [pubmed]
PHST- 2023/09/15 23:23 [entrez]
PHST- 2023/09/15 00:00 [pmc-release]
AID - 10.1038/s41598-023-42227-6 [pii]
AID - 42227 [pii]
AID - 10.1038/s41598-023-42227-6 [doi]
PST - epublish
SO  - Sci Rep. 2023 Sep 15;13(1):15310. doi: 10.1038/s41598-023-42227-6.

PMID- 38206257
OWN - NLM
STAT- MEDLINE
DCOM- 20240308
LR  - 20240308
IS  - 1943-4723 (Electronic)
IS  - 0002-8177 (Linking)
VI  - 155
IP  - 3
DP  - 2024 Mar
TI  - Is ChatGPT a reliable source of scientific information regarding third-molar 
      surgery?
PG  - 227-232.e6
LID - S0002-8177(23)00681-5 [pii]
LID - 10.1016/j.adaj.2023.11.004 [doi]
AB  - BACKGROUND: ChatGPT (OpenAI) is a large language model. This model uses 
      artificial intelligence and machine learning techniques to generate humanlike 
      language and responses, even to complex questions. The authors aimed to assess 
      the reliability of responses provided via ChatGPT and evaluate its 
      trustworthiness as a means of obtaining information about third-molar surgery. 
      METHODS: The authors assessed the 10 most frequently asked questions about 
      mandibular third-molar extraction. A validated questionnaire (Chatbot Usability 
      Questionnaire) was used and 2 oral and maxillofacial surgeons compared the 
      answers provided with the literature. RESULTS: Most of the responses (90.63%) 
      provided via the ChatGPT platform were considered safe and accurate and followed 
      what was the stated in the English-language literature. CONCLUSIONS: The ChatGPT 
      platform offers accurate and scientifically backed answers to inquiries about 
      third-molar surgical extraction, making it a dependable and easy-to-use resource 
      for both patients and the general public. However, the platform should provide 
      references with the responses to validate the information. PRACTICAL 
      IMPLICATIONS: Patients worldwide are exposed to reliable information sources. 
      Oral surgeons and health care providers should always advise patients to be aware 
      of the information source and that the ChatGPT platform offers a reliable 
      solution.
CI  - Copyright © 2024 American Dental Association. Published by Elsevier Inc. All 
      rights reserved.
FAU - Aguiar de Sousa, Rafael
AU  - Aguiar de Sousa R
FAU - Costa, Samuel Macedo
AU  - Costa SM
FAU - Almeida Figueiredo, Pedro Henrique
AU  - Almeida Figueiredo PH
FAU - Camargos, Caroline Rabelo
AU  - Camargos CR
FAU - Ribeiro, Bruna Campos
AU  - Ribeiro BC
FAU - Alves E Silva, Micena Roberta Miranda
AU  - Alves E Silva MRM
LA  - eng
PT  - Journal Article
DEP - 20240108
PL  - England
TA  - J Am Dent Assoc
JT  - Journal of the American Dental Association (1939)
JID - 7503060
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Reproducibility of Results
MH  - *Molar, Third
MH  - Molar
MH  - Health Personnel
OTO - NOTNLM
OT  - Artificial intelligence
OT  - oral surgery
OT  - third molar
COIS- Disclosure None of the authors reported any disclosures.
EDAT- 2024/01/11 12:42
MHDA- 2024/03/08 06:42
CRDT- 2024/01/11 10:31
PHST- 2023/05/17 00:00 [received]
PHST- 2023/11/04 00:00 [revised]
PHST- 2023/11/13 00:00 [accepted]
PHST- 2024/03/08 06:42 [medline]
PHST- 2024/01/11 12:42 [pubmed]
PHST- 2024/01/11 10:31 [entrez]
AID - S0002-8177(23)00681-5 [pii]
AID - 10.1016/j.adaj.2023.11.004 [doi]
PST - ppublish
SO  - J Am Dent Assoc. 2024 Mar;155(3):227-232.e6. doi: 10.1016/j.adaj.2023.11.004. 
      Epub 2024 Jan 8.

PMID- 37709536
OWN - NLM
STAT- MEDLINE
DCOM- 20231204
LR  - 20240227
IS  - 1535-5667 (Electronic)
IS  - 0161-5505 (Print)
IS  - 0161-5505 (Linking)
VI  - 64
IP  - 12
DP  - 2023 Dec 1
TI  - ChatGPT: Can You Prepare My Patients for [(18)F]FDG PET/CT and Explain My 
      Reports?
PG  - 1876-1879
LID - 10.2967/jnumed.123.266114 [doi]
AB  - We evaluated whether the artificial intelligence chatbot ChatGPT can adequately 
      answer patient questions related to [(18)F]FDG PET/CT in common clinical 
      indications before and after scanning. Methods: Thirteen questions regarding 
      [(18)F]FDG PET/CT were submitted to ChatGPT. ChatGPT was also asked to explain 6 
      PET/CT reports (lung cancer, Hodgkin lymphoma) and answer 6 follow-up questions 
      (e.g., on tumor stage or recommended treatment). To be rated "useful" or 
      "appropriate," a response had to be adequate by the standards of the nuclear 
      medicine staff. Inconsistency was assessed by regenerating responses. Results: 
      Responses were rated "appropriate" for 92% of 25 tasks and "useful" for 96%. 
      Considerable inconsistencies were found between regenerated responses for 16% of 
      tasks. Responses to 83% of sensitive questions (e.g., staging/treatment options) 
      were rated "empathetic." Conclusion: ChatGPT might adequately substitute for 
      advice given to patients by nuclear medicine staff in the investigated settings. 
      Improving the consistency of ChatGPT would further increase reliability.
CI  - © 2023 by the Society of Nuclear Medicine and Molecular Imaging.
FAU - Rogasch, Julian M M
AU  - Rogasch JMM
AD  - Department of Nuclear Medicine, Charité-Universitätsmedizin Berlin, Berlin, 
      Germany; julian.rogasch@charite.de.
AD  - Berlin Institute of Health, Charité-Universitätsmedizin Berlin, Berlin, Germany.
FAU - Metzger, Giulia
AU  - Metzger G
AD  - Department of Nuclear Medicine, Charité-Universitätsmedizin Berlin, Berlin, 
      Germany.
FAU - Preisler, Martina
AU  - Preisler M
AD  - Charité Comprehensive Cancer Center, Charité-Universitätsmedizin Berlin, Berlin, 
      Germany; and.
FAU - Galler, Markus
AU  - Galler M
AD  - Department of Nuclear Medicine, Charité-Universitätsmedizin Berlin, Berlin, 
      Germany.
FAU - Thiele, Felix
AU  - Thiele F
AD  - Department of Nuclear Medicine, Charité-Universitätsmedizin Berlin, Berlin, 
      Germany.
FAU - Brenner, Winfried
AU  - Brenner W
AD  - Department of Nuclear Medicine, Charité-Universitätsmedizin Berlin, Berlin, 
      Germany.
FAU - Feldhaus, Felix
AU  - Feldhaus F
AD  - Department of Radiology, Charité-Universitätsmedizin Berlin, Berlin, Germany.
FAU - Wetz, Christoph
AU  - Wetz C
AD  - Department of Nuclear Medicine, Charité-Universitätsmedizin Berlin, Berlin, 
      Germany.
FAU - Amthauer, Holger
AU  - Amthauer H
AD  - Department of Nuclear Medicine, Charité-Universitätsmedizin Berlin, Berlin, 
      Germany.
FAU - Furth, Christian
AU  - Furth C
AD  - Department of Nuclear Medicine, Charité-Universitätsmedizin Berlin, Berlin, 
      Germany.
FAU - Schatka, Imke
AU  - Schatka I
AD  - Department of Nuclear Medicine, Charité-Universitätsmedizin Berlin, Berlin, 
      Germany.
LA  - eng
PT  - Journal Article
DEP - 20231201
PL  - United States
TA  - J Nucl Med
JT  - Journal of nuclear medicine : official publication, Society of Nuclear Medicine
JID - 0217410
RN  - 0Z5B2CJX4D (Fluorodeoxyglucose F18)
RN  - 0 (Radiopharmaceuticals)
SB  - IM
MH  - Humans
MH  - *Positron Emission Tomography Computed Tomography
MH  - *Fluorodeoxyglucose F18
MH  - Radiopharmaceuticals
MH  - Artificial Intelligence
MH  - Reproducibility of Results
PMC - PMC10690125
OTO - NOTNLM
OT  - FDG PET/CT
OT  - GPT-4
OT  - artificial intelligence
OT  - chatbot
OT  - patient communication
EDAT- 2023/09/15 00:41
MHDA- 2023/12/04 12:43
PMCR- 2023/12/01
CRDT- 2023/09/14 21:23
PHST- 2023/06/02 00:00 [received]
PHST- 2023/08/22 00:00 [revised]
PHST- 2023/12/04 12:43 [medline]
PHST- 2023/09/15 00:41 [pubmed]
PHST- 2023/09/14 21:23 [entrez]
PHST- 2023/12/01 00:00 [pmc-release]
AID - jnumed.123.266114 [pii]
AID - 266114 [pii]
AID - 10.2967/jnumed.123.266114 [doi]
PST - epublish
SO  - J Nucl Med. 2023 Dec 1;64(12):1876-1879. doi: 10.2967/jnumed.123.266114.

PMID- 38152714
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231229
IS  - 2169-7574 (Print)
IS  - 2169-7574 (Electronic)
IS  - 2169-7574 (Linking)
VI  - 11
IP  - 9
DP  - 2023 Sep
TI  - Navigating the Ethical Landmines of ChatGPT: Implications of Intelligent Chatbots 
      in Plastic Surgery Clinical Practice.
PG  - e5290
LID - 10.1097/GOX.0000000000005290 [doi]
LID - e5290
AB  - ChatGPT is a cutting-edge language model developed by OpenAI with the potential 
      to impact all facets of plastic surgery from research to clinical practice. New 
      applications for ChatGPT are emerging at a rapid pace in both the scientific 
      literature and popular media. It is important for clinicians to understand the 
      capabilities and limitations of these tools before patient-facing implementation. 
      In this article, the authors explore some of the technical details behind 
      ChatGPT: what it is, and what it is not. As with any emerging technology, 
      attention should be given to the ethical and health equity implications of this 
      technology on our plastic surgery patients. The authors explore these concerns 
      within the framework of the foundational principles of biomedical ethics: patient 
      autonomy, nonmaleficence, beneficence, and justice. ChatGPT and similar 
      intelligent conversation agents have incredible promise in the field of plastic 
      surgery but should be used cautiously and sparingly in their current form. To 
      protect patients, it is imperative that societal guidelines for the safe use of 
      this rapidly developing technology are developed.
CI  - Copyright © 2023 The Authors. Published by Wolters Kluwer Health, Inc. on behalf 
      of The American Society of Plastic Surgeons.
FAU - Oleck, Nicholas C
AU  - Oleck NC
AD  - From the Division of Plastic, Maxillofacial and Oral Surgery, Duke University 
      Medical Center, Durham, N.C.
FAU - Naga, Hani I
AU  - Naga HI
AD  - From the Division of Plastic, Maxillofacial and Oral Surgery, Duke University 
      Medical Center, Durham, N.C.
FAU - Nichols, D Spencer
AU  - Nichols DS
AD  - From the Division of Plastic, Maxillofacial and Oral Surgery, Duke University 
      Medical Center, Durham, N.C.
FAU - Morris, Miranda X
AU  - Morris MX
AD  - From the Division of Plastic, Maxillofacial and Oral Surgery, Duke University 
      Medical Center, Durham, N.C.
FAU - Dhingra, Bhuwan
AU  - Dhingra B
AD  - Department of Computer Science, Duke University, Durham, N.C.
FAU - Patel, Ash
AU  - Patel A
AD  - From the Division of Plastic, Maxillofacial and Oral Surgery, Duke University 
      Medical Center, Durham, N.C.
LA  - eng
PT  - Journal Article
DEP - 20230915
PL  - United States
TA  - Plast Reconstr Surg Glob Open
JT  - Plastic and reconstructive surgery. Global open
JID - 101622231
PMC - PMC10752483
COIS- The authors have no financial interest to declare in relation to the content of 
      this article. Disclosure statements are at the end of this article, following the 
      correspondence information.
EDAT- 2023/12/28 06:42
MHDA- 2023/12/28 06:43
PMCR- 2023/09/15
CRDT- 2023/12/28 04:17
PHST- 2023/07/27 00:00 [received]
PHST- 2023/08/09 00:00 [accepted]
PHST- 2023/12/28 06:43 [medline]
PHST- 2023/12/28 06:42 [pubmed]
PHST- 2023/12/28 04:17 [entrez]
PHST- 2023/09/15 00:00 [pmc-release]
AID - 10.1097/GOX.0000000000005290 [doi]
PST - epublish
SO  - Plast Reconstr Surg Glob Open. 2023 Sep 15;11(9):e5290. doi: 
      10.1097/GOX.0000000000005290. eCollection 2023 Sep.

PMID- 37229893
OWN - NLM
STAT- MEDLINE
DCOM- 20230726
LR  - 20230726
IS  - 1532-8171 (Electronic)
IS  - 0735-6757 (Linking)
VI  - 70
DP  - 2023 Aug
TI  - ChatGPT and conversational artificial intelligence: Friend, foe, or future of 
      research?
PG  - 81-83
LID - S0735-6757(23)00262-0 [pii]
LID - 10.1016/j.ajem.2023.05.018 [doi]
AB  - Artificial intelligence (AI) and machine learning are increasingly utilized 
      across healthcare. More recently, there has been a rise in the use AI within 
      research, particularly through novel conversational AI platforms, such as 
      ChatGPT. In this Controversies paper, we discuss the advantages, limitations, and 
      future directions for ChatGPT and other forms of conversational AI in research 
      and scholarly dissemination.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Gottlieb, Michael
AU  - Gottlieb M
AD  - Department of Emergency Medicine, Rush University Medical Center, Chicago, IL, 
      United States of America. Electronic address: MichaelGottliebMD@gmail.com.
FAU - Kline, Jeffrey A
AU  - Kline JA
AD  - Department of Emergency Medicine, Wayne State University School of Medicine, 
      Detroit, MI, United States of America. Electronic address: jkline@wayne.edu.
FAU - Schneider, Alexander J
AU  - Schneider AJ
FAU - Coates, Wendy C
AU  - Coates WC
AD  - Department of Emergency Medicine, University of California, David Geffen School 
      of Medicine, Los Angeles, CA, United States of America.
LA  - eng
PT  - Journal Article
DEP - 20230518
PL  - United States
TA  - Am J Emerg Med
JT  - The American journal of emergency medicine
JID - 8309942
SB  - IM
CIN - Am J Emerg Med. 2023 Aug;70:190. PMID: 37349234
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Machine Learning
MH  - Communication
MH  - Health Facilities
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Ethics
OT  - Publication
OT  - Research
COIS- Declaration of Competing Interest We have no conflicts of interest to declare and 
      this manuscript has not been submitted elsewhere.
EDAT- 2023/05/26 01:05
MHDA- 2023/07/26 06:43
CRDT- 2023/05/25 18:03
PHST- 2023/03/30 00:00 [received]
PHST- 2023/05/01 00:00 [revised]
PHST- 2023/05/12 00:00 [accepted]
PHST- 2023/07/26 06:43 [medline]
PHST- 2023/05/26 01:05 [pubmed]
PHST- 2023/05/25 18:03 [entrez]
AID - S0735-6757(23)00262-0 [pii]
AID - 10.1016/j.ajem.2023.05.018 [doi]
PST - ppublish
SO  - Am J Emerg Med. 2023 Aug;70:81-83. doi: 10.1016/j.ajem.2023.05.018. Epub 2023 May 
      18.

PMID- 36701446
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230127
LR  - 20230202
IS  - 1095-9203 (Electronic)
IS  - 0036-8075 (Linking)
VI  - 379
IP  - 6630
DP  - 2023 Jan 27
TI  - ChatGPT is fun, but not an author.
PG  - 313
LID - 10.1126/science.adg7879 [doi]
AB  - In less than 2 months, the artificial intelligence (AI) program ChatGPT has 
      become a cultural sensation. It is freely accessible through a web portal created 
      by the tool's developer, OpenAI. The program-which automatically creates text 
      based on written prompts-is so popular that it's likely to be "at capacity right 
      now" if you attempt to use it. When you do get through, ChatGPT provides endless 
      entertainment. I asked it to rewrite the first scene of the classic American play 
      Death of a Salesman, but to feature Princess Elsa from the animated movie Frozen 
      as the main character instead of Willy Loman. The output was an amusing 
      conversation in which Elsa-who has come home from a tough day of selling-is told 
      by her son Happy, "Come on, Mom. You're Elsa from Frozen. You have ice powers and 
      you're a queen. You're unstoppable." Mash-ups like this are certainly fun, but 
      there are serious implications for generative AI programs like ChatGPT in science 
      and academia.
FAU - Thorp, H Holden
AU  - Thorp HH
AD  - H. Holden Thorp Editor-in-Chief, Science journals.
LA  - eng
PT  - Editorial
DEP - 20230126
PL  - United States
TA  - Science
JT  - Science (New York, N.Y.)
JID - 0404511
SB  - IM
EDAT- 2023/01/27 06:00
MHDA- 2023/01/27 06:01
CRDT- 2023/01/26 14:03
PHST- 2023/01/26 14:03 [entrez]
PHST- 2023/01/27 06:00 [pubmed]
PHST- 2023/01/27 06:01 [medline]
AID - 10.1126/science.adg7879 [doi]
PST - ppublish
SO  - Science. 2023 Jan 27;379(6630):313. doi: 10.1126/science.adg7879. Epub 2023 Jan 
      26.

PMID- 38409178
OWN - NLM
STAT- MEDLINE
DCOM- 20240228
LR  - 20240229
IS  - 1749-8090 (Electronic)
IS  - 1749-8090 (Linking)
VI  - 19
IP  - 1
DP  - 2024 Feb 26
TI  - Can ChatGPT transform cardiac surgery and heart transplantation?
PG  - 108
LID - 10.1186/s13019-024-02541-0 [doi]
LID - 108
AB  - Artificial intelligence (AI) is a transformative technology with many benefits, 
      but also risks when applied to healthcare and cardiac surgery in particular. 
      Surgeons must be aware of AI and its application through generative pre-trained 
      transformers (GPT/ChatGPT) to fully understand what this offers to clinical care, 
      decision making, training, research and education. Clinicians must appreciate 
      that the advantages and potential for transformative change in practice is 
      balanced by risks typified by validation, ethical challenges and medicolegal 
      concerns. ChatGPT should be seen as a tool to support and enhance the skills of 
      surgeons, rather than a replacement for their experience and judgment. Human 
      oversight and intervention will always be necessary to ensure patient safety and 
      to make complex decisions that may require a refined understanding of individual 
      patient circumstances.
CI  - © 2024. The Author(s).
FAU - Clark, S C
AU  - Clark SC
AD  - Cardiothoracic Surgery and Transplantation Freeman Hospital, Newcastle upon Tyne, 
      NE7 7DN, UK. Stephen.clark18@nhs.net.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20240226
PL  - England
TA  - J Cardiothorac Surg
JT  - Journal of cardiothoracic surgery
JID - 101265113
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Cardiac Surgical Procedures
MH  - *Heart Transplantation
MH  - Educational Status
MH  - Patient Safety
PMC - PMC10898059
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Generative pre-training transformer
OT  - Natural language processing
COIS- The authors declare no competing interests.
EDAT- 2024/02/27 00:42
MHDA- 2024/02/28 06:44
PMCR- 2024/02/26
CRDT- 2024/02/26 23:49
PHST- 2023/06/08 00:00 [received]
PHST- 2024/01/28 00:00 [accepted]
PHST- 2024/02/28 06:44 [medline]
PHST- 2024/02/27 00:42 [pubmed]
PHST- 2024/02/26 23:49 [entrez]
PHST- 2024/02/26 00:00 [pmc-release]
AID - 10.1186/s13019-024-02541-0 [pii]
AID - 2541 [pii]
AID - 10.1186/s13019-024-02541-0 [doi]
PST - epublish
SO  - J Cardiothorac Surg. 2024 Feb 26;19(1):108. doi: 10.1186/s13019-024-02541-0.

PMID- 38283995
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240131
IS  - 2624-8212 (Electronic)
IS  - 2624-8212 (Linking)
VI  - 7
DP  - 2024
TI  - Rare and complex diseases in focus: ChatGPT's role in improving diagnosis and 
      treatment.
PG  - 1338433
LID - 10.3389/frai.2024.1338433 [doi]
LID - 1338433
AB  - Rare and complex diseases pose significant challenges to both patients and 
      healthcare providers. These conditions often present with atypical symptoms, 
      making diagnosis and treatment a formidable task. In recent years, artificial 
      intelligence and natural language processing technologies have shown great 
      promise in assisting medical professionals in diagnosing and managing such 
      conditions. This paper explores the role of ChatGPT, an advanced artificial 
      intelligence model, in improving the diagnosis and treatment of rare and complex 
      diseases. By analyzing its potential applications, limitations, and ethical 
      considerations, we demonstrate how ChatGPT can contribute to better patient 
      outcomes and enhance the healthcare system's overall effectiveness.
CI  - Copyright © 2024 Zheng, Sun, Feng, Kang, Yang, Zhao and Wu.
FAU - Zheng, Yue
AU  - Zheng Y
AD  - Cancer Center, West China Hospital, Sichuan University, Chengdu, Sichuan, China.
FAU - Sun, Xu
AU  - Sun X
AD  - Department of Hematology, West China Hospital, Sichuan University, Chengdu, 
      Sichuan, China.
FAU - Feng, Baijie
AU  - Feng B
AD  - West China School of Medicine, Sichuan University, Chengdu, Sichuan, China.
FAU - Kang, Kai
AU  - Kang K
AD  - Cancer Center, West China Hospital, Sichuan University, Chengdu, Sichuan, China.
FAU - Yang, Yuqi
AU  - Yang Y
AD  - West China School of Medicine, Sichuan University, Chengdu, Sichuan, China.
FAU - Zhao, Ailin
AU  - Zhao A
AD  - Department of Hematology, West China Hospital, Sichuan University, Chengdu, 
      Sichuan, China.
FAU - Wu, Yijun
AU  - Wu Y
AD  - Cancer Center, West China Hospital, Sichuan University, Chengdu, Sichuan, China.
LA  - eng
PT  - Journal Article
DEP - 20240111
PL  - Switzerland
TA  - Front Artif Intell
JT  - Frontiers in artificial intelligence
JID - 101770551
PMC - PMC10808657
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - diagnosis
OT  - rare and complex diseases
OT  - treatment
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2024/01/29 06:44
MHDA- 2024/01/29 06:45
PMCR- 2024/01/11
CRDT- 2024/01/29 04:50
PHST- 2023/11/14 00:00 [received]
PHST- 2024/01/02 00:00 [accepted]
PHST- 2024/01/29 06:45 [medline]
PHST- 2024/01/29 06:44 [pubmed]
PHST- 2024/01/29 04:50 [entrez]
PHST- 2024/01/11 00:00 [pmc-release]
AID - 10.3389/frai.2024.1338433 [doi]
PST - epublish
SO  - Front Artif Intell. 2024 Jan 11;7:1338433. doi: 10.3389/frai.2024.1338433. 
      eCollection 2024.

PMID- 38000671
OWN - NLM
STAT- MEDLINE
DCOM- 20240118
LR  - 20240118
IS  - 1878-8769 (Electronic)
IS  - 1878-8750 (Linking)
VI  - 181
DP  - 2024 Jan
TI  - Information Quality and Readability: ChatGPT's Responses to the Most Common 
      Questions About Spinal Cord Injury.
PG  - e1138-e1144
LID - S1878-8750(23)01625-X [pii]
LID - 10.1016/j.wneu.2023.11.062 [doi]
AB  - OBJECTIVE: This study aimed to assess the quality, readability, and comprehension 
      of texts generated by ChatGPT in response to commonly asked questions about 
      spinal cord injury (SCI). METHODS: The study utilized Google Trends to identify 
      the most frequently searched keywords related to SCI. The identified keywords 
      were sequentially inputted into ChatGPT, and the resulting responses were 
      assessed for quality using the Ensuring Quality Information for Patients (EQIP) 
      tool. The readability of the texts was analyzed using the Flesch-Kincaid grade 
      level and the Flesch-Kincaid reading ease parameters. RESULTS: The mean EQIP 
      score of the texts was determined to be 43.02&nbsp;±&nbsp;6.37, the Flesch-Kincaid reading 
      ease score to be 26.24&nbsp;±&nbsp;13.81, and the Flesch-Kincaid grade level was determined 
      to be 14.84&nbsp;±&nbsp;1.79. The analysis revealed significant concerns regarding the 
      quality of texts generated by ChatGPT, indicating serious problems with 
      readability and comprehension. The mean EQIP score was low, suggesting a need for 
      improvement in the accuracy and reliability of the information provided. The 
      Flesch-Kincaid grade level indicated a high linguistic complexity, requiring a 
      level of education equivalent to approximately 14 to 15&nbsp;years of formal education 
      for comprehension. CONCLUSIONS: The results of this study show heightened 
      complexity in ChatGPT-generated SCI texts, surpassing optimal health 
      communication readability. ChatGPT currently cannot substitute comprehensive 
      medical consultations. Enhancing text quality could be attainable through 
      dependence on credible sources, the establishment of a scientific board, and 
      collaboration with expert teams. Addressing these concerns could improve text 
      accessibility, empowering patients and facilitating informed decision-making in 
      SCI.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Temel, Mustafa Hüseyin
AU  - Temel MH
AD  - Physical Medicine and Rehabilitation Clinic, Üsküdar State Hospital, İstanbul, 
      Turkey. Electronic address: mhuseyintemel@gmail.com.
FAU - Erden, Yakup
AU  - Erden Y
AD  - Physical Medicine and Rehabilitation Clinic, İzzet Baysal Physical Medicine and 
      Rehabilitation Training and Research Hospital, Bolu, Turkey.
FAU - Bağcıer, Fatih
AU  - Bağcıer F
AD  - Physical Medicine and Rehabilitation Clinic, Başakşehir Çam and Sakura City 
      Hospital, İstanbul, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20231122
PL  - United States
TA  - World Neurosurg
JT  - World neurosurgery
JID - 101528275
SB  - IM
MH  - Humans
MH  - *Comprehension
MH  - Reproducibility of Results
MH  - *Health Literacy
MH  - Educational Status
MH  - Reading
MH  - Internet
OTO - NOTNLM
OT  - ChatGPT
OT  - Comprehension
OT  - Quality assessment
OT  - Readability
OT  - Spinal cord injury
EDAT- 2023/11/25 07:42
MHDA- 2024/01/18 06:43
CRDT- 2023/11/24 19:29
PHST- 2023/10/29 00:00 [received]
PHST- 2023/11/14 00:00 [revised]
PHST- 2023/11/15 00:00 [accepted]
PHST- 2024/01/18 06:43 [medline]
PHST- 2023/11/25 07:42 [pubmed]
PHST- 2023/11/24 19:29 [entrez]
AID - S1878-8750(23)01625-X [pii]
AID - 10.1016/j.wneu.2023.11.062 [doi]
PST - ppublish
SO  - World Neurosurg. 2024 Jan;181:e1138-e1144. doi: 10.1016/j.wneu.2023.11.062. Epub 
      2023 Nov 22.

PMID- 37833847
OWN - NLM
STAT- MEDLINE
DCOM- 20231205
LR  - 20231216
IS  - 1460-2350 (Electronic)
IS  - 0268-1161 (Linking)
VI  - 38
IP  - 12
DP  - 2023 Dec 4
TI  - AI language models in human reproduction research: exploring ChatGPT's potential 
      to assist academic writing.
PG  - 2281-2288
LID - 10.1093/humrep/dead207 [doi]
AB  - Artificial intelligence (AI)-driven language models have the potential to serve 
      as an educational tool, facilitate clinical decision-making, and support research 
      and academic writing. The benefits of their use are yet to be evaluated and 
      concerns have been raised regarding the accuracy, transparency, and ethical 
      implications of using this AI technology in academic publishing. At the moment, 
      Chat Generative Pre-trained Transformer (ChatGPT) is one of the most powerful and 
      widely debated AI language models. Here, we discuss its feasibility to answer 
      scientific questions, identify relevant literature, and assist writing in the 
      field of human reproduction. With consideration of the scarcity of data on this 
      topic, we assessed the feasibility of ChatGPT in academic writing, using data 
      from six meta-analyses published in a leading journal of human reproduction. The 
      text generated by ChatGPT was evaluated and compared to the original text by 
      blinded reviewers. While ChatGPT can produce high-quality text and summarize 
      information efficiently, its current ability to interpret data and answer 
      scientific questions is limited, and it cannot be relied upon for a literature 
      search or accurate source citation due to the potential spread of incomplete or 
      false information. We advocate for open discussions within the reproductive 
      medicine research community to explore the advantages and disadvantages of 
      implementing this AI technology. Researchers and reviewers should be informed 
      about AI language models, and we encourage authors to transparently disclose 
      their use.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of European 
      Society of Human Reproduction and Embryology. All rights reserved. For 
      permissions, please email: journals.permissions@oup.com.
FAU - Semrl, N
AU  - Semrl N
AUID- ORCID: 0000-0003-0199-605X
AD  - Department of Obstetrics and Gynecology, Medical University of Graz, Graz, 
      Austria.
FAU - Feigl, S
AU  - Feigl S
AD  - Department of Obstetrics and Gynecology, Medical University of Graz, Graz, 
      Austria.
FAU - Taumberger, N
AU  - Taumberger N
AUID- ORCID: 0000-0002-8969-7064
AD  - Department of Obstetrics and Gynecology, Medical University of Graz, Graz, 
      Austria.
FAU - Bracic, T
AU  - Bracic T
AD  - Department of Obstetrics and Gynecology, Medical University of Graz, Graz, 
      Austria.
FAU - Fluhr, H
AU  - Fluhr H
AD  - Department of Obstetrics and Gynecology, Medical University of Graz, Graz, 
      Austria.
FAU - Blockeel, C
AU  - Blockeel C
AD  - Centre for Reproductive Medicine, Universitair Ziekenhuis Brussel (UZ Brussel), 
      Brussels, Belgium.
FAU - Kollmann, M
AU  - Kollmann M
AUID- ORCID: 0000-0001-8528-4759
AD  - Department of Obstetrics and Gynecology, Medical University of Graz, Graz, 
      Austria.
LA  - eng
PT  - Journal Article
PL  - England
TA  - Hum Reprod
JT  - Human reproduction (Oxford, England)
JID - 8701199
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Language
MH  - Clinical Decision-Making
MH  - Reproduction
MH  - Writing
OTO - NOTNLM
OT  - ChatGPT
OT  - academic writing
OT  - artificial intelligence
OT  - language models
OT  - reproductive medicine
EDAT- 2023/10/14 10:46
MHDA- 2023/12/05 12:42
CRDT- 2023/10/14 00:52
PHST- 2023/05/10 00:00 [received]
PHST- 2023/09/06 00:00 [revised]
PHST- 2023/12/05 12:42 [medline]
PHST- 2023/10/14 10:46 [pubmed]
PHST- 2023/10/14 00:52 [entrez]
AID - 7313666 [pii]
AID - 10.1093/humrep/dead207 [doi]
PST - ppublish
SO  - Hum Reprod. 2023 Dec 4;38(12):2281-2288. doi: 10.1093/humrep/dead207.

PMID- 37634667
OWN - NLM
STAT- MEDLINE
DCOM- 20231216
LR  - 20231216
IS  - 1878-8769 (Electronic)
IS  - 1878-8750 (Linking)
VI  - 179
DP  - 2023 Nov
TI  - Chat GPT as a Neuro-Score Calculator: Analysis of a Large Language Model's 
      Performance on Various Neurological Exam Grading Scales.
PG  - e342-e347
LID - S1878-8750(23)01201-9 [pii]
LID - 10.1016/j.wneu.2023.08.088 [doi]
AB  - BACKGROUND: ChatGPT is a large language model artificial intelligence chatbot 
      that has been applied to different aspects of the medical field. Our study aims 
      to assess the quality of chatGPT to evaluate patients based on their exams for 
      different scores including Glasgow Coma Scale (GCS), intracranial hemorrhage 
      score (ICH), and Hunt &amp; Hess (H&amp;H) classification. METHODS: We created batches of 
      patient test cases with detailed neurological exams, totaling 20 cases and 
      created variants of increasing complex phrasing of the test cases. Using ChatGPT, 
      we assessed repeatability and quantified the errors, including the average error 
      rate (AER) and magnitude of errors (AME). We repeated this process for the H&amp;H 
      and the ICH score using base cases. Specific prompts were created for each 
      calculator. RESULTS: The GCS calculator on 10 base test cases had an AER/AME of 
      10%/0.150. The accuracy of ChatGPT decreased with increasing complexity; for 
      example, in a variation where crucial information was missing, the AER was 45% 
      for 20 cases. For H&amp;H, AER/AME was 13%/0.13 and for ICH, AER/AME was 27.5%/0.325. 
      Using a simple prompt resulted in a significantly higher error rate of 70%. 
      CONCLUSIONS: ChatGPT demonstrates ability in this proof-of-concept experiment in 
      evaluating neuroexams using established assessment scales including GCS, ICH, and 
      H&amp;H. However, it has limitations in accuracy and may "hallucinate" with complex 
      or vague descriptions. Nonetheless, ChatGPT, has promising potential in medicine.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Chen, Tse Chiang
AU  - Chen TC
AD  - Department of Neurology, Tulane University School of Medicine, New Orleans, 
      Louisiana, USA.
FAU - Kaminski, Emily
AU  - Kaminski E
AD  - Tulane University School of Medicine, New Orleans, Louisiana, USA.
FAU - Koduri, Laila
AU  - Koduri L
AD  - Tulane University School of Medicine, New Orleans, Louisiana, USA.
FAU - Singer, Alyssa
AU  - Singer A
AD  - Tulane University School of Medicine, New Orleans, Louisiana, USA.
FAU - Singer, Jorie
AU  - Singer J
AD  - Tulane University School of Medicine, New Orleans, Louisiana, USA.
FAU - Couldwell, Mitch
AU  - Couldwell M
AD  - Tulane University School of Medicine, New Orleans, Louisiana, USA.
FAU - Delashaw, Johnny
AU  - Delashaw J
AD  - Department of Neurological Surgery, Tulane University School of Medicine, New 
      Orleans, Louisiana, USA.
FAU - Dumont, Aaron
AU  - Dumont A
AD  - Department of Neurological Surgery, Tulane University School of Medicine, New 
      Orleans, Louisiana, USA.
FAU - Wang, Arthur
AU  - Wang A
AD  - Department of Neurological Surgery, Tulane University School of Medicine, New 
      Orleans, Louisiana, USA. Electronic address: awang15@tulane.edu.
LA  - eng
PT  - Journal Article
DEP - 20230826
PL  - United States
TA  - World Neurosurg
JT  - World neurosurgery
JID - 101528275
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Neurology
MH  - *Medicine
MH  - Glasgow Coma Scale
MH  - Intracranial Hemorrhages
MH  - Language
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Calculator
OT  - ChatGPT
OT  - Grading scale
OT  - Large language model
OT  - Neurological exam
EDAT- 2023/08/28 00:41
MHDA- 2023/12/17 09:43
CRDT- 2023/08/27 19:25
PHST- 2023/08/20 00:00 [received]
PHST- 2023/08/22 00:00 [accepted]
PHST- 2023/12/17 09:43 [medline]
PHST- 2023/08/28 00:41 [pubmed]
PHST- 2023/08/27 19:25 [entrez]
AID - S1878-8750(23)01201-9 [pii]
AID - 10.1016/j.wneu.2023.08.088 [doi]
PST - ppublish
SO  - World Neurosurg. 2023 Nov;179:e342-e347. doi: 10.1016/j.wneu.2023.08.088. Epub 
      2023 Aug 26.

PMID- 36798998
OWN - NLM
STAT- MEDLINE
DCOM- 20230222
LR  - 20230222
IS  - 2047-2986 (Electronic)
IS  - 2047-2978 (Print)
IS  - 2047-2978 (Linking)
VI  - 13
DP  - 2023 Feb 17
TI  - Can ChatGPT draft a research article? An example of population-level vaccine 
      effectiveness analysis.
PG  - 01003
LID - 10.7189/jogh.13.01003 [doi]
LID - 01003
AB  - We reflect on our experiences of using Generative Pre-trained Transformer 
      ChatGPT, a chatbot launched by OpenAI in November 2022, to draft a research 
      article. We aim to demonstrate how ChatGPT could help researchers to accelerate 
      drafting their papers. We created a simulated data set of 100 000 health care 
      workers with varying ages, Body Mass Index (BMI), and risk profiles. Simulation 
      data allow analysts to test statistical analysis techniques, such as 
      machine-learning based approaches, without compromising patient privacy. 
      Infections were simulated with a randomized probability of hospitalisation. A 
      subset of these fictitious people was vaccinated with a fictional vaccine that 
      reduced this probability of hospitalisation after infection. We then used ChatGPT 
      to help us decide how to handle the simulated data in order to determine vaccine 
      effectiveness and draft a related research paper. AI-based language models in 
      data analysis and scientific writing are an area of growing interest, and this 
      exemplar analysis aims to contribute to the understanding of how ChatGPT can be 
      used to facilitate these tasks.
CI  - Copyright © 2023 by the Journal of Global Health. All rights reserved.
FAU - Macdonald, Calum
AU  - Macdonald C
FAU - Adeloye, Davies
AU  - Adeloye D
FAU - Sheikh, Aziz
AU  - Sheikh A
FAU - Rudan, Igor
AU  - Rudan I
LA  - eng
PT  - Editorial
DEP - 20230217
PL  - Scotland
TA  - J Glob Health
JT  - Journal of global health
JID - 101578780
SB  - IM
MH  - Humans
MH  - *Vaccine Efficacy
MH  - Computer Simulation
MH  - *Software
MH  - Confidentiality
MH  - Health Personnel
PMC - PMC9936200
EDAT- 2023/02/18 06:00
MHDA- 2023/02/22 06:00
PMCR- 2023/02/17
CRDT- 2023/02/17 02:42
PHST- 2023/02/17 02:42 [entrez]
PHST- 2023/02/18 06:00 [pubmed]
PHST- 2023/02/22 06:00 [medline]
PHST- 2023/02/17 00:00 [pmc-release]
AID - jogh-13-01003 [pii]
AID - 10.7189/jogh.13.01003 [doi]
PST - epublish
SO  - J Glob Health. 2023 Feb 17;13:01003. doi: 10.7189/jogh.13.01003.

PMID- 37984563
OWN - NLM
STAT- Publisher
LR  - 20231230
IS  - 1542-7714 (Electronic)
IS  - 1542-3565 (Linking)
DP  - 2023 Nov 19
TI  - Accuracy of ChatGPT in Common Gastrointestinal Diseases: Impact for Patients and 
      Providers.
LID - S1542-3565(23)00946-1 [pii]
LID - 10.1016/j.cgh.2023.11.008 [doi]
AB  - Since its release in 2022, Chat Generative Pre-Trained Transformer (ChatGPT) 
      became the most rapidly expanding consumer software application in history,(1) 
      and its role in medicine is underscored by its potential to enhance patient 
      education and physician-patient communication. Previous studies in 
      gastroenterology and hepatology have focused primarily on the earlier Generative 
      Pre-Trained Transformer 3 (GPT-3) model, with none investigating ChatGPT's 
      ability to generate supportive references for its responses, or its applicability 
      as a physician educational tool.(2-6) Our study evaluated the accuracy of the 
      more recent ChatGPT, powered by GPT-4, in addressing frequently asked questions 
      by patients on irritable bowel syndrome (IBS), inflammatory bowel disease (IBD), 
      colonoscopy and colorectal cancer (CRC) screening, questions on CRC screening 
      from a physician perspective, and reference generation and suitability.
CI  - Copyright © 2023 AGA Institute. Published by Elsevier Inc. All rights reserved.
FAU - Kerbage, Anthony
AU  - Kerbage A
AD  - Department of Internal Medicine, Cleveland Clinic, Cleveland, Ohio.
FAU - Kassab, Joseph
AU  - Kassab J
AD  - Research Institute, Cleveland Clinic, Cleveland, Ohio.
FAU - El Dahdah, Joseph
AU  - El Dahdah J
AD  - Research Institute, Cleveland Clinic, Cleveland, Ohio.
FAU - Burke, Carol A
AU  - Burke CA
AD  - Department of Gastroenterology, Hepatology and Nutrition, Cleveland Clinic, 
      Cleveland, Ohio.
FAU - Achkar, Jean-Paul
AU  - Achkar JP
AD  - Department of Gastroenterology, Hepatology and Nutrition, Cleveland Clinic, 
      Cleveland, Ohio.
FAU - Rouphael, Carol
AU  - Rouphael C
AD  - Department of Gastroenterology, Hepatology and Nutrition, Cleveland Clinic, 
      Cleveland, Ohio. Electronic address: rouphac@ccf.org.
LA  - eng
PT  - Journal Article
DEP - 20231119
PL  - United States
TA  - Clin Gastroenterol Hepatol
JT  - Clinical gastroenterology and hepatology : the official clinical practice journal 
      of the American Gastroenterological Association
JID - 101160775
SB  - IM
EDAT- 2023/11/21 01:07
MHDA- 2023/11/21 01:07
CRDT- 2023/11/20 19:37
PHST- 2023/08/04 00:00 [received]
PHST- 2023/10/22 00:00 [revised]
PHST- 2023/11/06 00:00 [accepted]
PHST- 2023/11/21 01:07 [pubmed]
PHST- 2023/11/21 01:07 [medline]
PHST- 2023/11/20 19:37 [entrez]
AID - S1542-3565(23)00946-1 [pii]
AID - 10.1016/j.cgh.2023.11.008 [doi]
PST - aheadofprint
SO  - Clin Gastroenterol Hepatol. 2023 Nov 19:S1542-3565(23)00946-1. doi: 
      10.1016/j.cgh.2023.11.008.

PMID- 38194819
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 1873-1244 (Electronic)
IS  - 0899-9007 (Linking)
VI  - 119
DP  - 2024 Mar
TI  - AI dietician: Unveiling the accuracy of ChatGPT's nutritional estimations.
PG  - 112325
LID - S0899-9007(23)00353-2 [pii]
LID - 10.1016/j.nut.2023.112325 [doi]
AB  - We investigate the accuracy and reliability of ChatGPT, an artificial 
      intelligence model developed by OpenAI, in providing nutritional information for 
      dietary planning and weight management. The results have a reasonable level of 
      accuracy, with energy values having the highest level of conformity: 97% of the 
      artificial intelligence values fall within a 40% difference from United States 
      Department of Agriculture data. Additionally, ChatGPT displayed consistency in 
      its provision of nutritional data, as indicated by relatively low coefficient of 
      variation values for each nutrient. The artificial intelligence model also proved 
      efficient in generating a daily meal plan within a specified caloric limit, with 
      all the meals falling within a 30% bound of the United States Department of 
      Agriculture's caloric values. These findings suggest that ChatGPT can provide 
      reasonably accurate and consistent nutritional information. Further research is 
      recommended to assess the model's performance across a broader range of foods and 
      meals.&lt;END ABSTRACT&gt;.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Haman, Michael
AU  - Haman M
AD  - Department of Humanities, Faculty of Economics and Management, Czech University 
      of Life Sciences Prague, Prague, Czech Republic. Electronic address: 
      haman@pef.czu.cz.
FAU - Školník, Milan
AU  - Školník M
AD  - Department of Humanities, Faculty of Economics and Management, Czech University 
      of Life Sciences Prague, Prague, Czech Republic.
FAU - Lošťák, Michal
AU  - Lošťák M
AD  - Department of Humanities, Faculty of Economics and Management, Czech University 
      of Life Sciences Prague, Prague, Czech Republic.
LA  - eng
PT  - Journal Article
DEP - 20231212
PL  - United States
TA  - Nutrition
JT  - Nutrition (Burbank, Los Angeles County, Calif.)
JID - 8802712
SB  - IM
MH  - United States
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Nutritionists
MH  - Reproducibility of Results
MH  - Meals
MH  - Nutrients
OTO - NOTNLM
OT  - AI
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Dietary planning
OT  - Nutritional data
OT  - Weight management
COIS- Declaration of competing interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2024/01/10 00:42
MHDA- 2024/02/11 07:42
CRDT- 2024/01/09 18:13
PHST- 2023/08/14 00:00 [received]
PHST- 2023/11/02 00:00 [revised]
PHST- 2023/12/04 00:00 [accepted]
PHST- 2024/02/11 07:42 [medline]
PHST- 2024/01/10 00:42 [pubmed]
PHST- 2024/01/09 18:13 [entrez]
AID - S0899-9007(23)00353-2 [pii]
AID - 10.1016/j.nut.2023.112325 [doi]
PST - ppublish
SO  - Nutrition. 2024 Mar;119:112325. doi: 10.1016/j.nut.2023.112325. Epub 2023 Dec 12.

PMID- 38015597
OWN - NLM
STAT- MEDLINE
DCOM- 20231129
LR  - 20240110
IS  - 1438-8871 (Electronic)
IS  - 1439-4456 (Print)
IS  - 1438-8871 (Linking)
VI  - 25
DP  - 2023 Nov 28
TI  - Security Implications of AI Chatbots in Health Care.
PG  - e47551
LID - 10.2196/47551 [doi]
LID - e47551
AB  - Artificial intelligence (AI) chatbots like ChatGPT and Google Bard are computer 
      programs that use AI and natural language processing to understand customer 
      questions and generate natural, fluid, dialogue-like responses to their inputs. 
      ChatGPT, an AI chatbot created by OpenAI, has rapidly become a widely used tool 
      on the internet. AI chatbots have the potential to improve patient care and 
      public health. However, they are trained on massive amounts of people's data, 
      which may include sensitive patient data and business information. The increased 
      use of chatbots introduces data security issues, which should be handled yet 
      remain understudied. This paper aims to identify the most important security 
      problems of AI chatbots and propose guidelines for protecting sensitive health 
      information. It explores the impact of using ChatGPT in health care. It also 
      identifies the principal security risks of ChatGPT and suggests key 
      considerations for security risk mitigation. It concludes by discussing the 
      policy implications of using AI chatbots in health care.
CI  - ©Jingquan Li. Originally published in the Journal of Medical Internet Research 
      (https://www.jmir.org), 28.11.2023.
FAU - Li, Jingquan
AU  - Li J
AUID- ORCID: 0000-0002-9654-1770
AD  - Hofstra University, Hempstead, NY, United States.
LA  - eng
PT  - Journal Article
DEP - 20231128
PL  - Canada
TA  - J Med Internet Res
JT  - Journal of medical Internet research
JID - 100959882
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Software
MH  - Natural Language Processing
MH  - Commerce
MH  - Delivery of Health Care
PMC - PMC10716748
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - HIPAA
OT  - artificial intelligence
OT  - care
OT  - chatbot
OT  - computer program
OT  - data security
OT  - guidelines
OT  - health information
OT  - improvement
OT  - natural language processing
OT  - patient care
OT  - policy
OT  - privacy
OT  - risk
OT  - security
OT  - tool
COIS- Conflicts of Interest: None declared.
EDAT- 2023/11/28 12:42
MHDA- 2023/11/29 06:42
PMCR- 2023/11/28
CRDT- 2023/11/28 11:53
PHST- 2023/03/24 00:00 [received]
PHST- 2023/11/20 00:00 [accepted]
PHST- 2023/08/30 00:00 [revised]
PHST- 2023/11/29 06:42 [medline]
PHST- 2023/11/28 12:42 [pubmed]
PHST- 2023/11/28 11:53 [entrez]
PHST- 2023/11/28 00:00 [pmc-release]
AID - v25i1e47551 [pii]
AID - 10.2196/47551 [doi]
PST - epublish
SO  - J Med Internet Res. 2023 Nov 28;25:e47551. doi: 10.2196/47551.

PMID- 37462773
OWN - NLM
STAT- Publisher
LR  - 20231019
IS  - 1432-1335 (Electronic)
IS  - 0171-5216 (Linking)
VI  - 149
IP  - 14
DP  - 2023 Nov
TI  - Letter to the Editor: ChatGPT's contribution to cancer study.
PG  - 13495-13500
LID - 10.1007/s00432-023-05183-2 [doi]
AB  - The role of large language models (LLM) in medical and biomedical sciences is 
      remarkable, and chat generative pre-trained transformer (ChatGPT) as an AI model 
      has the potential to assist in research and clinical practice. While it is 
      essential to recognize that AI models like ChatGPT are tools that should be used 
      with human expertise and judgment, they should argue, rather than replace, the 
      knowledge and experience of healthcare professionals and researchers in the 
      medical and biomedical fields.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Faraji, Niloofar
AU  - Faraji N
AUID- ORCID: 0000-0001-5796-7157
AD  - Gastrointestinal and Liver Diseases Research Center, Razi Hospital, Guilan 
      University of Medical Sciences, Rasht, Iran.
FAU - Aali, Shahab
AU  - Aali S
AUID- ORCID: 0009-0004-1717-2690
AD  - Department of Urology, School of Medicine, Urology Research Center, Razi 
      Hospital, Guilan University of Medical Sciences, Rasht, Iran.
FAU - Motiei, Mahsa
AU  - Motiei M
AUID- ORCID: 0000-0002-3543-9975
AD  - School of Medicine, Razi Hospital, Guilan University of Medical Sciences, Rasht, 
      Iran.
FAU - Sadat Mansouri, Sahand
AU  - Sadat Mansouri S
AUID- ORCID: 0009-0003-1536-9150
AD  - Pediatric Diseases Research Center, Guilan University of Medical Sciences, Rasht, 
      Iran. ssadatmansoory@gmail.com.
LA  - eng
PT  - Letter
DEP - 20230718
PL  - Germany
TA  - J Cancer Res Clin Oncol
JT  - Journal of cancer research and clinical oncology
JID - 7902060
SB  - IM
OTO - NOTNLM
OT  - Cancer
OT  - ChatGPT
OT  - Medical
OT  - Oncology
OT  - Research
EDAT- 2023/07/18 13:10
MHDA- 2023/07/18 13:10
CRDT- 2023/07/18 11:07
PHST- 2023/07/09 00:00 [received]
PHST- 2023/07/13 00:00 [accepted]
PHST- 2023/07/18 13:10 [pubmed]
PHST- 2023/07/18 13:10 [medline]
PHST- 2023/07/18 11:07 [entrez]
AID - 10.1007/s00432-023-05183-2 [pii]
AID - 10.1007/s00432-023-05183-2 [doi]
PST - ppublish
SO  - J Cancer Res Clin Oncol. 2023 Nov;149(14):13495-13500. doi: 
      10.1007/s00432-023-05183-2. Epub 2023 Jul 18.

PMID- 37519406
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231020
IS  - 2052-2975 (Print)
IS  - 2052-2975 (Electronic)
IS  - 2052-2975 (Linking)
VI  - 54
DP  - 2023 Sep
TI  - ChatGPT in infectious diseases: A practical evaluation and future considerations.
PG  - 101166
LID - 10.1016/j.nmni.2023.101166 [doi]
LID - 101166
FAU - Kaneda, Yudai
AU  - Kaneda Y
AD  - School of Medicine, Hokkaido University, Hokkaido, Japan.
LA  - eng
PT  - Journal Article
DEP - 20230720
PL  - England
TA  - New Microbes New Infect
JT  - New microbes and new infections
JID - 101624750
PMC - PMC10382662
OTO - NOTNLM
OT  - ChatGPT
OT  - Infectious diseases
OT  - The Japanese association for infectious diseases
COIS- None.
EDAT- 2023/07/31 06:42
MHDA- 2023/07/31 06:43
PMCR- 2023/07/20
CRDT- 2023/07/31 04:36
PHST- 2023/06/26 00:00 [received]
PHST- 2023/07/13 00:00 [revised]
PHST- 2023/07/18 00:00 [accepted]
PHST- 2023/07/31 06:43 [medline]
PHST- 2023/07/31 06:42 [pubmed]
PHST- 2023/07/31 04:36 [entrez]
PHST- 2023/07/20 00:00 [pmc-release]
AID - S2052-2975(23)00085-9 [pii]
AID - 101166 [pii]
AID - 10.1016/j.nmni.2023.101166 [doi]
PST - epublish
SO  - New Microbes New Infect. 2023 Jul 20;54:101166. doi: 10.1016/j.nmni.2023.101166. 
      eCollection 2023 Sep.

PMID- 37210281
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230724
LR  - 20230724
IS  - 1471-6771 (Electronic)
IS  - 0007-0912 (Linking)
VI  - 131
IP  - 2
DP  - 2023 Aug
TI  - Performance of ChatGPT on a primary FRCA multiple choice question bank.
PG  - e34-e35
LID - S0007-0912(23)00200-3 [pii]
LID - 10.1016/j.bja.2023.04.025 [doi]
FAU - Birkett, Liam
AU  - Birkett L
AD  - Royal Free Hospital, London, UK. Electronic address: liam.birkett@nhs.net.
FAU - Fowler, Thomas
AU  - Fowler T
AD  - Queen's Hospital, London, UK.
FAU - Pullen, Simon
AU  - Pullen S
AD  - Princess Alexandra Hospital, Essex, UK.
LA  - eng
PT  - Letter
DEP - 20230518
PL  - England
TA  - Br J Anaesth
JT  - British journal of anaesthesia
JID - 0372541
SB  - IM
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - artificial intelligence
OT  - assessments
OT  - postgraduate examinations
OT  - primary FRCA
EDAT- 2023/05/21 01:05
MHDA- 2023/05/21 01:06
CRDT- 2023/05/20 21:59
PHST- 2023/04/08 00:00 [received]
PHST- 2023/04/19 00:00 [revised]
PHST- 2023/04/21 00:00 [accepted]
PHST- 2023/05/21 01:06 [medline]
PHST- 2023/05/21 01:05 [pubmed]
PHST- 2023/05/20 21:59 [entrez]
AID - S0007-0912(23)00200-3 [pii]
AID - 10.1016/j.bja.2023.04.025 [doi]
PST - ppublish
SO  - Br J Anaesth. 2023 Aug;131(2):e34-e35. doi: 10.1016/j.bja.2023.04.025. Epub 2023 
      May 18.

PMID- 37792149
OWN - NLM
STAT- MEDLINE
DCOM- 20240126
LR  - 20240206
IS  - 1867-108X (Electronic)
IS  - 1867-1071 (Print)
IS  - 1867-1071 (Linking)
VI  - 42
IP  - 2
DP  - 2024 Feb
TI  - Performance evaluation of ChatGPT, GPT-4, and Bard on the official board 
      examination of the Japan Radiology Society.
PG  - 201-207
LID - 10.1007/s11604-023-01491-2 [doi]
AB  - PURPOSE: Herein, we assessed the accuracy of large language models (LLMs) in 
      generating responses to questions in clinical radiology practice. We compared the 
      performance of ChatGPT, GPT-4, and Google Bard using questions from the Japan 
      Radiology Board Examination (JRBE). MATERIALS AND METHODS: In total, 103 
      questions from the JRBE 2022 were used with permission from the Japan 
      Radiological Society. These questions were categorized by pattern, required level 
      of thinking, and topic. McNemar's test was used to compare the proportion of 
      correct responses between the LLMs. Fisher's exact test was used to assess the 
      performance of GPT-4 for each topic category. RESULTS: ChatGPT, GPT-4, and Google 
      Bard correctly answered 40.8% (42 of 103), 65.0% (67 of 103), and 38.8% (40 of 
      103) of the questions, respectively. GPT-4 significantly outperformed ChatGPT by 
      24.2% (p &lt; 0.001) and Google Bard by 26.2% (p &lt; 0.001). In the categorical 
      analysis by level of thinking, GPT-4 correctly answered 79.7% of the lower-order 
      questions, which was significantly higher than ChatGPT or Google Bard 
      (p &lt; 0.001). The categorical analysis by question pattern revealed GPT-4's 
      superiority over ChatGPT (67.4% vs. 46.5%, p = 0.004) and Google Bard (39.5%, 
      p &lt; 0.001) in the single-answer questions. The categorical analysis by topic 
      revealed that GPT-4 outperformed ChatGPT (40%, p = 0.013) and Google Bard (26.7%, 
      p = 0.004). No significant differences were observed between the LLMs in the 
      categories not mentioned above. The performance of GPT-4 was significantly better 
      in nuclear medicine (93.3%) than in diagnostic radiology (55.8%; p &lt; 0.001). 
      GPT-4 also performed better on lower-order questions than on higher-order 
      questions (79.7% vs. 45.5%, p &lt; 0.001). CONCLUSION: ChatGPTplus based on GPT-4 
      scored 65% when answering Japanese questions from the JRBE, outperforming ChatGPT 
      and Google Bard. This highlights the potential of using LLMs to address advanced 
      clinical questions in the field of radiology in Japan.
CI  - © 2023. The Author(s).
FAU - Toyama, Yoshitaka
AU  - Toyama Y
AUID- ORCID: 0000-0003-0027-9681
AD  - Department of Diagnostic Radiology, Tohoku University Hospital, 1-1 Seiryo-Machi, 
      Aoba-Ku, Sendai, 980-8575, Japan. ytoyama0818@gmail.com.
FAU - Harigai, Ayaka
AU  - Harigai A
AD  - Department of Diagnostic Radiology, Tohoku University Hospital, 1-1 Seiryo-Machi, 
      Aoba-Ku, Sendai, 980-8575, Japan.
AD  - Department of Radiology, Tohoku Medical and Pharmaceutical University, Sendai, 
      Japan.
AD  - Department of Diagnostic Radiology, Tohoku University Graduate School of 
      Medicine, Sendai, Japan.
FAU - Abe, Mirei
AU  - Abe M
AD  - Department of Diagnostic Radiology, Tohoku University Hospital, 1-1 Seiryo-Machi, 
      Aoba-Ku, Sendai, 980-8575, Japan.
FAU - Nagano, Mitsutoshi
AU  - Nagano M
AD  - School of Medicine, Tohoku University, Sendai, Japan.
FAU - Kawabata, Masahiro
AU  - Kawabata M
AD  - Department of Diagnostic Radiology, Tohoku University Hospital, 1-1 Seiryo-Machi, 
      Aoba-Ku, Sendai, 980-8575, Japan.
FAU - Seki, Yasuhiro
AU  - Seki Y
AD  - Department of Radiation Oncology, Tohoku University Hospital, Sendai, Japan.
FAU - Takase, Kei
AU  - Takase K
AD  - Department of Diagnostic Radiology, Tohoku University Graduate School of 
      Medicine, Sendai, Japan.
LA  - eng
PT  - Journal Article
DEP - 20231004
PL  - Japan
TA  - Jpn J Radiol
JT  - Japanese journal of radiology
JID - 101490689
SB  - IM
MH  - Humans
MH  - Japan
MH  - Radiography
MH  - *Nuclear Medicine
PMC - PMC10811006
OTO - NOTNLM
OT  - Bard
OT  - ChatGPT
OT  - GPT-4
OT  - Japan Radiology Society
COIS- The authors declare no conflicts of interest.
EDAT- 2023/10/04 12:42
MHDA- 2024/01/26 06:44
PMCR- 2023/10/04
CRDT- 2023/10/04 11:10
PHST- 2023/08/25 00:00 [received]
PHST- 2023/09/12 00:00 [accepted]
PHST- 2024/01/26 06:44 [medline]
PHST- 2023/10/04 12:42 [pubmed]
PHST- 2023/10/04 11:10 [entrez]
PHST- 2023/10/04 00:00 [pmc-release]
AID - 10.1007/s11604-023-01491-2 [pii]
AID - 1491 [pii]
AID - 10.1007/s11604-023-01491-2 [doi]
PST - ppublish
SO  - Jpn J Radiol. 2024 Feb;42(2):201-207. doi: 10.1007/s11604-023-01491-2. Epub 2023 
      Oct 4.

PMID- 38279999
OWN - NLM
STAT- MEDLINE
DCOM- 20240328
LR  - 20240330
IS  - 1432-1459 (Electronic)
IS  - 0340-5354 (Print)
IS  - 0340-5354 (Linking)
VI  - 271
IP  - 4
DP  - 2024 Apr
TI  - ChatGPT fails challenging the recent ESCMID brain abscess guideline.
PG  - 2086-2101
LID - 10.1007/s00415-023-12168-1 [doi]
AB  - BACKGROUND: With artificial intelligence (AI) on the rise, it remains unclear if 
      AI is able to professionally evaluate medical research and give scientifically 
      valid recommendations. AIM: This study aimed to assess the accuracy of ChatGPT's 
      responses to ten key questions on brain abscess diagnostics and treatment in 
      comparison to the guideline recently published by the European Society for 
      Clinical Microbiology and Infectious Diseases (ESCMID). METHODS: All ten PECO 
      (Population, Exposure, Comparator, Outcome) questions which had been developed 
      during the guideline process were presented directly to ChatGPT. Next, ChatGPT 
      was additionally fed with data from studies selected for each PECO question by 
      the ESCMID committee. AI's responses were subsequently compared with the 
      recommendations of the ESCMID guideline. RESULTS: For 17 out of 20 challenges, 
      ChatGPT was able to give recommendations on the management of patients with brain 
      abscess, including grade of evidence and strength of recommendation. Without data 
      prompting, 70% of questions were answered very similar to the guideline 
      recommendation. In the answers that differed from the guideline recommendations, 
      no patient hazard was present. Data input slightly improved the clarity of 
      ChatGPT's recommendations, but, however, led to less correct answers including 
      two recommendations that directly contradicted the guideline, being associated 
      with the possibility of a hazard to the patient. CONCLUSION: ChatGPT seems to be 
      able to rapidly gather information on brain abscesses and give recommendations on 
      key questions about their management in most cases. Nevertheless, single 
      responses could possibly harm the patients. Thus, the expertise of an expert 
      committee remains inevitable.
CI  - © 2024. The Author(s).
FAU - Dyckhoff-Shen, Susanne
AU  - Dyckhoff-Shen S
AUID- ORCID: 0000-0001-9247-4884
AD  - Department of Neurology with Friedrich-Baur-Institute, LMU University Hospital, 
      LMU Munich (en.), Klinikum Grosshadern of the Ludwig Maximilians University of 
      Munich, Marchioninistr. 15, 81377, Munich, Germany. 
      susanne.dyckhoff@med.uni-muenchen.de.
FAU - Koedel, Uwe
AU  - Koedel U
AD  - Department of Neurology with Friedrich-Baur-Institute, LMU University Hospital, 
      LMU Munich (en.), Klinikum Grosshadern of the Ludwig Maximilians University of 
      Munich, Marchioninistr. 15, 81377, Munich, Germany.
FAU - Brouwer, Matthijs C
AU  - Brouwer MC
AD  - Department of Neurology, Amsterdam UMC, University of Amsterdam, Amsterdam 
      Neuroscience, Amsterdam, The Netherlands.
AD  - European Society for Clinical Microbiology and Infectious Diseases (ESCMID) Study 
      Group for Infections of the Brain (ESGIB), Basel, Switzerland.
FAU - Bodilsen, Jacob
AU  - Bodilsen J
AD  - Department of Infectious Diseases, Aalborg University Hospital, Aalborg, Denmark.
AD  - European Society for Clinical Microbiology and Infectious Diseases (ESCMID) Study 
      Group for Infections of the Brain (ESGIB), Basel, Switzerland.
FAU - Klein, Matthias
AU  - Klein M
AD  - Department of Neurology with Friedrich-Baur-Institute, LMU University Hospital, 
      LMU Munich (en.), Klinikum Grosshadern of the Ludwig Maximilians University of 
      Munich, Marchioninistr. 15, 81377, Munich, Germany.
AD  - Emergency Department, LMU University Hospital, LMU Munich (en.), Munich, Germany.
AD  - European Society for Clinical Microbiology and Infectious Diseases (ESCMID) Study 
      Group for Infections of the Brain (ESGIB), Basel, Switzerland.
LA  - eng
PT  - Journal Article
DEP - 20240127
PL  - Germany
TA  - J Neurol
JT  - Journal of neurology
JID - 0423161
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Brain Abscess
MH  - *Central Nervous System Infections
MH  - *Brain Diseases
MH  - *Biomedical Research
PMC - PMC10972965
OTO - NOTNLM
OT  - AI
OT  - Brain abscess
OT  - ChatGPT
OT  - Guideline
COIS- The authors declare that they have no conflict of interest.
EDAT- 2024/01/28 07:42
MHDA- 2024/03/28 06:45
PMCR- 2024/01/27
CRDT- 2024/01/27 11:13
PHST- 2023/11/13 00:00 [received]
PHST- 2023/12/21 00:00 [accepted]
PHST- 2023/12/20 00:00 [revised]
PHST- 2024/03/28 06:45 [medline]
PHST- 2024/01/28 07:42 [pubmed]
PHST- 2024/01/27 11:13 [entrez]
PHST- 2024/01/27 00:00 [pmc-release]
AID - 10.1007/s00415-023-12168-1 [pii]
AID - 12168 [pii]
AID - 10.1007/s00415-023-12168-1 [doi]
PST - ppublish
SO  - J Neurol. 2024 Apr;271(4):2086-2101. doi: 10.1007/s00415-023-12168-1. Epub 2024 
      Jan 27.

PMID- 38334288
OWN - NLM
STAT- MEDLINE
DCOM- 20240325
LR  - 20240325
IS  - 1531-7021 (Electronic)
IS  - 1040-8738 (Linking)
VI  - 35
IP  - 3
DP  - 2024 May 1
TI  - ChatGPT enters the room: what it means for patient counseling, physician 
      education, academics, and disease management.
PG  - 205-209
LID - 10.1097/ICU.0000000000001036 [doi]
AB  - PURPOSE OF REVIEW: This review seeks to provide a summary of the most recent 
      research findings regarding the utilization of ChatGPT, an artificial 
      intelligence (AI)-powered chatbot, in the field of ophthalmology in addition to 
      exploring the limitations and ethical considerations associated with its 
      application. RECENT FINDINGS: ChatGPT has gained widespread recognition and 
      demonstrated potential in enhancing patient and physician education, boosting 
      research productivity, and streamlining administrative tasks. In various studies 
      examining its utility in ophthalmology, ChatGPT has exhibited fair to good 
      accuracy, with its most recent iteration showcasing superior performance in 
      providing ophthalmic recommendations across various ophthalmic disorders such as 
      corneal diseases, orbital disorders, vitreoretinal diseases, uveitis, 
      neuro-ophthalmology, and glaucoma. This proves beneficial for patients in 
      accessing information and aids physicians in triaging as well as formulating 
      differential diagnoses. Despite such benefits, ChatGPT has limitations that 
      require acknowledgment including the potential risk of offering inaccurate or 
      harmful information, dependence on outdated data, the necessity for a high level 
      of education for data comprehension, and concerns regarding patient privacy and 
      ethical considerations within the research domain. SUMMARY: ChatGPT is a 
      promising new tool that could contribute to ophthalmic healthcare education and 
      research, potentially reducing work burdens. However, its current limitations 
      necessitate a complementary role with human expert oversight.
CI  - Copyright © 2024 Wolters Kluwer Health, Inc. All rights reserved.
FAU - Momenaei, Bita
AU  - Momenaei B
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Mansour, Hana A
AU  - Mansour HA
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Kuriyan, Ajay E
AU  - Kuriyan AE
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Xu, David
AU  - Xu D
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Sridhar, Jayanth
AU  - Sridhar J
AD  - University of California Los Angeles, Los Angeles, California, USA.
FAU - Ting, Daniel S W
AU  - Ting DSW
AD  - Singapore National Eye Center, Singapore, Singapore.
FAU - Yonekawa, Yoshihiro
AU  - Yonekawa Y
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20240207
PL  - United States
TA  - Curr Opin Ophthalmol
JT  - Current opinion in ophthalmology
JID - 9011108
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Educational Status
MH  - Disease Management
MH  - *Physicians
MH  - Counseling
EDAT- 2024/02/09 12:44
MHDA- 2024/03/25 06:43
CRDT- 2024/02/09 08:12
PHST- 2024/03/25 06:43 [medline]
PHST- 2024/02/09 12:44 [pubmed]
PHST- 2024/02/09 08:12 [entrez]
AID - 00055735-990000000-00152 [pii]
AID - 10.1097/ICU.0000000000001036 [doi]
PST - ppublish
SO  - Curr Opin Ophthalmol. 2024 May 1;35(3):205-209. doi: 
      10.1097/ICU.0000000000001036. Epub 2024 Feb 7.

PMID- 38088393
OWN - NLM
STAT- MEDLINE
DCOM- 20231216
LR  - 20231216
IS  - 2384-8553 (Electronic)
IS  - 0021-2571 (Linking)
VI  - 59
IP  - 4
DP  - 2023 Oct-Dec
TI  - Exploring the potential of ChatGPT for clinical reasoning and decision-making: a 
      cross-sectional study on the Italian Medical Residency Exam.
PG  - 267-270
LID - 10.4415/ANN_23_04_05 [doi]
AB  - BACKGROUND: This study aimed to assess the performance of ChatGPT, a large 
      language model (LLM), on the Italian State Exam for Medical Residency (SSM) test 
      to determine its potential as a tool for medical education and clinical 
      decision-making support. MATERIALS AND METHODS: A total of 136 questions were 
      obtained from the official SSM test. ChatGPT responses were analyzed and compared 
      to the performance of medical doctors who took the test in 2022. Questions were 
      classified into clinical cases (CC) and notional questions (NQ). RESULTS: ChatGPT 
      achieved an overall accuracy of 90.44%, with higher performance on clinical cases 
      (92.45%) than on notional questions (89.15%). Compared to medical doctors' 
      scores, ChatGPT performance was higher than 99.6% of the participants. 
      CONCLUSIONS: These results suggest that ChatGPT holds promise as a valuable tool 
      in clinical decision-making, particularly in the context of clinical reasoning. 
      Further research is needed to explore the potential applications and 
      implementation of large language models (LLMs) in medical education and medical 
      practice.
FAU - Scaioli, Giacomo
AU  - Scaioli G
AD  - Dipartimento di Scienze della Sanità Pubblica e Pediatriche, Università degli 
      Studi di Torino, Turin, Italy.
FAU - Lo Moro, Giuseppina
AU  - Lo Moro G
AD  - Dipartimento di Scienze della Sanità Pubblica e Pediatriche, Università degli 
      Studi di Torino, Turin, Italy.
FAU - Conrado, Francesco
AU  - Conrado F
AD  - Dipartimento di Scienze della Sanità Pubblica e Pediatriche, Università degli 
      Studi di Torino, Turin, Italy.
FAU - Rosset, Lorenzo
AU  - Rosset L
AD  - Dipartimento di Scienze della Sanità Pubblica e Pediatriche, Università degli 
      Studi di Torino, Turin, Italy.
FAU - Bert, Fabrizio
AU  - Bert F
AD  - Dipartimento di Scienze della Sanità Pubblica e Pediatriche, Università degli 
      Studi di Torino, Turin, Italy.
FAU - Siliquini, Roberta
AU  - Siliquini R
AD  - Dipartimento di Scienze della Sanità Pubblica e Pediatriche, Università degli 
      Studi di Torino, Turin, Italy.
LA  - eng
PT  - Journal Article
PL  - Italy
TA  - Ann Ist Super Sanita
JT  - Annali dell'Istituto superiore di sanita
JID - 7502520
SB  - IM
MH  - Humans
MH  - Cross-Sectional Studies
MH  - *Internship and Residency
MH  - Clinical Reasoning
MH  - Language
MH  - Italy
EDAT- 2023/12/13 12:43
MHDA- 2023/12/17 09:44
CRDT- 2023/12/13 08:51
PHST- 2023/12/17 09:44 [medline]
PHST- 2023/12/13 12:43 [pubmed]
PHST- 2023/12/13 08:51 [entrez]
AID - 10.4415/ANN_23_04_05 [doi]
PST - ppublish
SO  - Ann Ist Super Sanita. 2023 Oct-Dec;59(4):267-270. doi: 10.4415/ANN_23_04_05.

PMID- 38443499
OWN - NLM
STAT- Publisher
LR  - 20240305
IS  - 1432-2218 (Electronic)
IS  - 0930-2794 (Linking)
DP  - 2024 Mar 5
TI  - Dr. Google to Dr. ChatGPT: assessing the content and quality of artificial 
      intelligence-generated medical information on appendicitis.
LID - 10.1007/s00464-024-10739-5 [doi]
AB  - INTRODUCTION: Generative artificial intelligence (AI) chatbots have recently been 
      posited as potential sources of online medical information for patients making 
      medical decisions. Existing online patient-oriented medical information has 
      repeatedly been shown to be of variable quality and difficult readability. 
      Therefore, we sought to evaluate the content and quality of AI-generated medical 
      information on acute appendicitis. METHODS: A modified DISCERN assessment tool, 
      comprising 16 distinct criteria each scored on a 5-point Likert scale (score 
      range 16-80), was used to assess AI-generated content. Readability was determined 
      using the Flesch Reading Ease (FRE) and Flesch-Kincaid Grade Level (FKGL) scores. 
      Four popular chatbots, ChatGPT-3.5 and ChatGPT-4, Bard, and Claude-2, were 
      prompted to generate medical information about appendicitis. Three investigators 
      independently scored the generated texts blinded to the identity of the AI 
      platforms. RESULTS: ChatGPT-3.5, ChatGPT-4, Bard, and Claude-2 had overall mean 
      (SD) quality scores of 60.7 (1.2), 62.0 (1.0), 62.3 (1.2), and 51.3 (2.3), 
      respectively, on a scale of 16-80. Inter-rater reliability was 0.81, 0.75, 0.81, 
      and 0.72, respectively, indicating substantial agreement. Claude-2 demonstrated a 
      significantly lower mean quality score compared to ChatGPT-4 (p = 0.001), 
      ChatGPT-3.5 (p = 0.005), and Bard (p = 0.001). Bard was the only AI platform that 
      listed verifiable sources, while Claude-2 provided fabricated sources. All 
      chatbots except for Claude-2 advised readers to consult a physician if 
      experiencing symptoms. Regarding readability, FKGL and FRE scores of ChatGPT-3.5, 
      ChatGPT-4, Bard, and Claude-2 were 14.6 and 23.8, 11.9 and 33.9, 8.6 and 52.8, 
      11.0 and 36.6, respectively, indicating difficulty readability at a college 
      reading skill level. CONCLUSION: AI-generated medical information on appendicitis 
      scored favorably upon quality assessment, but most either fabricated sources or 
      did not provide any altogether. Additionally, overall readability far exceeded 
      recommended levels for the public. Generative AI platforms demonstrate measured 
      potential for patient education and engagement about appendicitis.
CI  - © 2024. The Author(s).
FAU - Ghanem, Yazid K
AU  - Ghanem YK
AUID- ORCID: 0000-0002-0113-9632
AD  - Department of Surgery, Cooper University Hospital, 3 Cooper Plaza, Suite 411, 
      Camden, NJ, 08103, USA. ghanem-yazidkhalilmo@cooperhealth.edu.
AD  - Cooper Medical School of Rowan University, Camden, NJ, USA. 
      ghanem-yazidkhalilmo@cooperhealth.edu.
FAU - Rouhi, Armaun D
AU  - Rouhi AD
AD  - Department of Surgery, Perelman School of Medicine, University of Pennsylvania, 
      Philadelphia, PA, USA.
FAU - Al-Houssan, Ammr
AU  - Al-Houssan A
AD  - Department of Surgery, University of Connecticut, Hartford, CT, USA.
FAU - Saleh, Zena
AU  - Saleh Z
AD  - Department of Surgery, Cooper University Hospital, 3 Cooper Plaza, Suite 411, 
      Camden, NJ, 08103, USA.
FAU - Moccia, Matthew C
AU  - Moccia MC
AD  - Department of Surgery, Cooper University Hospital, 3 Cooper Plaza, Suite 411, 
      Camden, NJ, 08103, USA.
FAU - Joshi, Hansa
AU  - Joshi H
AD  - Department of Surgery, Cooper University Hospital, 3 Cooper Plaza, Suite 411, 
      Camden, NJ, 08103, USA.
FAU - Dumon, Kristoffel R
AU  - Dumon KR
AD  - Department of Surgery, Perelman School of Medicine, University of Pennsylvania, 
      Philadelphia, PA, USA.
FAU - Hong, Young
AU  - Hong Y
AD  - Department of Surgery, Cooper University Hospital, 3 Cooper Plaza, Suite 411, 
      Camden, NJ, 08103, USA.
AD  - Cooper Medical School of Rowan University, Camden, NJ, USA.
FAU - Spitz, Francis
AU  - Spitz F
AD  - Department of Surgery, Cooper University Hospital, 3 Cooper Plaza, Suite 411, 
      Camden, NJ, 08103, USA.
AD  - Cooper Medical School of Rowan University, Camden, NJ, USA.
FAU - Joshi, Amit R
AU  - Joshi AR
AD  - Department of Surgery, Cooper University Hospital, 3 Cooper Plaza, Suite 411, 
      Camden, NJ, 08103, USA.
AD  - Cooper Medical School of Rowan University, Camden, NJ, USA.
FAU - Kwiatt, Michael
AU  - Kwiatt M
AD  - Department of Surgery, Cooper University Hospital, 3 Cooper Plaza, Suite 411, 
      Camden, NJ, 08103, USA.
AD  - Cooper Medical School of Rowan University, Camden, NJ, USA.
LA  - eng
PT  - Journal Article
DEP - 20240305
PL  - Germany
TA  - Surg Endosc
JT  - Surgical endoscopy
JID - 8806653
SB  - IM
OTO - NOTNLM
OT  - Appendicitis
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Health literacy
OT  - Large language models
OT  - Online medical information
EDAT- 2024/03/06 00:42
MHDA- 2024/03/06 00:42
CRDT- 2024/03/05 23:24
PHST- 2023/12/26 00:00 [received]
PHST- 2024/01/28 00:00 [accepted]
PHST- 2024/03/06 00:42 [medline]
PHST- 2024/03/06 00:42 [pubmed]
PHST- 2024/03/05 23:24 [entrez]
AID - 10.1007/s00464-024-10739-5 [pii]
AID - 10.1007/s00464-024-10739-5 [doi]
PST - aheadofprint
SO  - Surg Endosc. 2024 Mar 5. doi: 10.1007/s00464-024-10739-5.

PMID- 38523565
OWN - NLM
STAT- Publisher
LR  - 20240325
IS  - 1879-3479 (Electronic)
IS  - 0020-7292 (Linking)
DP  - 2024 Mar 25
TI  - Evaluating the validity of ChatGPT responses on common obstetric issues: 
      Potential clinical applications and implications.
LID - 10.1002/ijgo.15501 [doi]
AB  - OBJECTIVE: To evaluate the quality of ChatGPT responses to common issues in 
      obstetrics and assess its ability to provide reliable responses to pregnant 
      individuals. The study aimed to examine the responses based on expert opinions 
      using predetermined criteria, including "accuracy," "completeness," and "safety." 
      METHODS: We curated 15 common and potentially clinically significant questions 
      that pregnant women are asking. Two native English-speaking women were asked to 
      reframe the questions in their own words, and we employed the ChatGPT language 
      model to generate responses to the questions. To evaluate the accuracy, 
      completeness, and safety of the ChatGPT's generated responses, we developed a 
      questionnaire with a scale of 1 to 5 that obstetrics and gynecology experts from 
      different countries were invited to rate accordingly. The ratings were analyzed 
      to evaluate the average level of agreement and percentage of positive ratings 
      (≥4) for each criterion. RESULTS: Of the 42 experts invited, 20 responded to the 
      questionnaire. The combined score for all responses yielded a mean rating of 4, 
      with 75% of responses receiving a positive rating (≥4). While examining specific 
      criteria, the ChatGPT responses were better for the accuracy criterion, with a 
      mean rating of 4.2 and 80% of the questions received a positive rating. The 
      responses scored less for the completeness criterion, with a mean rating of 3.8 
      and 46.7% of questions received a positive rating. For safety, the mean rating 
      was 3.9 and 53.3% of questions received a positive rating. There was no response 
      with an average negative rating below three. CONCLUSION: This study demonstrates 
      promising results regarding potential use of ChatGPT's in providing accurate 
      responses to obstetric clinical questions posed by pregnant women. However, it is 
      crucial to exercise caution when addressing inquiries concerning the safety of 
      the fetus or the mother.
CI  - © 2024 The Authors. International Journal of Gynecology &amp; Obstetrics published by 
      John Wiley &amp; Sons Ltd on behalf of International Federation of Gynecology and 
      Obstetrics.
FAU - Peled, Tzuria
AU  - Peled T
AD  - Department of Obstetrics and Gynecology, Shaare Zedek Medical Center, Affiliated 
      with the Hebrew University School of Medicine, Jerusalem, Israel.
FAU - Sela, Hen Y
AU  - Sela HY
AD  - Department of Obstetrics and Gynecology, Shaare Zedek Medical Center, Affiliated 
      with the Hebrew University School of Medicine, Jerusalem, Israel.
FAU - Weiss, Ari
AU  - Weiss A
AD  - Department of Obstetrics and Gynecology, Shaare Zedek Medical Center, Affiliated 
      with the Hebrew University School of Medicine, Jerusalem, Israel.
FAU - Grisaru-Granovsky, Sorina
AU  - Grisaru-Granovsky S
AD  - Department of Obstetrics and Gynecology, Shaare Zedek Medical Center, Affiliated 
      with the Hebrew University School of Medicine, Jerusalem, Israel.
FAU - Agrawal, Swati
AU  - Agrawal S
AD  - Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, 
      Hamilton Health Sciences, McMaster University, Hamilton, Ontario, Canada.
FAU - Rottenstreich, Misgav
AU  - Rottenstreich M
AUID- ORCID: 0000-0003-4875-1455
AD  - Department of Obstetrics and Gynecology, Shaare Zedek Medical Center, Affiliated 
      with the Hebrew University School of Medicine, Jerusalem, Israel.
AD  - Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, 
      Hamilton Health Sciences, McMaster University, Hamilton, Ontario, Canada.
AD  - Department of Nursing, Jerusalem College of Technology, Jerusalem, Israel.
LA  - eng
PT  - Journal Article
DEP - 20240325
PL  - United States
TA  - Int J Gynaecol Obstet
JT  - International journal of gynaecology and obstetrics: the official organ of the 
      International Federation of Gynaecology and Obstetrics
JID - 0210174
SB  - IM
OTO - NOTNLM
OT  - AI‐generated responses
OT  - ChatGPT
OT  - expert opinions
OT  - obstetrics
OT  - pregnant
OT  - quality evaluation
EDAT- 2024/03/25 06:43
MHDA- 2024/03/25 06:43
CRDT- 2024/03/25 03:53
PHST- 2024/02/29 00:00 [revised]
PHST- 2023/08/25 00:00 [received]
PHST- 2024/03/10 00:00 [accepted]
PHST- 2024/03/25 06:43 [medline]
PHST- 2024/03/25 06:43 [pubmed]
PHST- 2024/03/25 03:53 [entrez]
AID - 10.1002/ijgo.15501 [doi]
PST - aheadofprint
SO  - Int J Gynaecol Obstet. 2024 Mar 25. doi: 10.1002/ijgo.15501.

PMID- 38268099
OWN - NLM
STAT- Publisher
LR  - 20240125
IS  - 2042-6984 (Electronic)
IS  - 2042-6976 (Linking)
DP  - 2024 Jan 24
TI  - ChatGPT-4 performance in rhinology: A clinical case series.
LID - 10.1002/alr.23323 [doi]
AB  - Chatbot Generative Pre-trained Transformer (ChatGPT)-4 indicated more than twice 
      additional examinations than practitioners in the management of clinical cases in 
      rhinology. The consistency between ChatGPT-4 and practitioner in the indication 
      of additional examinations may significantly vary from one examination to 
      another. The ChatGPT-4 proposed a plausible and correct primary diagnosis in 
      62.5% cases, while pertinent and necessary additional examinations and 
      therapeutic regimen were indicated in 7.5%-30.0% and 7.5%-32.5% of cases, 
      respectively. The stability of ChatGPT-4 responses is moderate-to-high. The 
      performance of ChatGPT-4 was not influenced by the human-reported level of 
      difficulty of clinical cases.
CI  - © 2024 ARS-AAOA, LLC.
FAU - Radulesco, Thomas
AU  - Radulesco T
AUID- ORCID: 0000-0002-5939-5372
AD  - Research Committee of Young Otolaryngologists of the International Federation of 
      Otorhinolaryngological Societies, Paris, France.
AD  - Aix Marseille University, APHM, CNRS, IUSTI, La Conception University Hospital, 
      ENT-HNS Department, Marseille, France.
FAU - Saibene, Alberto Maria
AU  - Saibene AM
AUID- ORCID: 0000-0003-1457-6871
AD  - Research Committee of Young Otolaryngologists of the International Federation of 
      Otorhinolaryngological Societies, Paris, France.
AD  - Otolaryngology Unit, ASST Santi Paolo E Carlo, Department of Health Sciences, 
      Università Degli Studi Di Milano, Milan, Italy.
FAU - Michel, Justin
AU  - Michel J
AD  - Aix Marseille University, APHM, CNRS, IUSTI, La Conception University Hospital, 
      ENT-HNS Department, Marseille, France.
FAU - Vaira, Luigi Angelo
AU  - Vaira LA
AUID- ORCID: 0000-0002-7789-145X
AD  - Research Committee of Young Otolaryngologists of the International Federation of 
      Otorhinolaryngological Societies, Paris, France.
AD  - Maxillofacial Surgery Operative Unit, Department of Medicine, Surgery and 
      Pharmacy, University of Sassari, Sassari, Italy.
AD  - PhD School of Biomedical Sciences, Department of Biomedical Sciences, University 
      of Sassari, Sassari, Italy.
FAU - Lechien, Jérôme R
AU  - Lechien JR
AUID- ORCID: 0000-0002-0845-0845
AD  - Research Committee of Young Otolaryngologists of the International Federation of 
      Otorhinolaryngological Societies, Paris, France.
AD  - Division of Laryngology and Broncho-esophagology, Department of 
      Otolaryngology-Head Neck Surgery, EpiCURA Hospital, UMONS Research Institute for 
      Health Sciences and Technology, University of Mons (UMons), Mons, Belgium.
AD  - Department of Otorhinolaryngology and Head and Neck Surgery, Foch Hospital, Paris 
      Saclay University, Phonetics and Phonology Laboratory (UMR 7018 CNRS, Université 
      Sorbonne Nouvelle/Paris 3), Paris, France.
AD  - Department of Otorhinolaryngology and Head and Neck Surgery, CHU Saint-Pierre, 
      Brussels, Belgium.
LA  - eng
PT  - Journal Article
DEP - 20240124
PL  - United States
TA  - Int Forum Allergy Rhinol
JT  - International forum of allergy &amp; rhinology
JID - 101550261
SB  - IM
OTO - NOTNLM
OT  - ChatGPT-4
OT  - artificial intelligence
OT  - head neck surgery
OT  - otolaryngology
OT  - performance
OT  - rhinology
EDAT- 2024/01/25 06:43
MHDA- 2024/01/25 06:43
CRDT- 2024/01/25 00:13
PHST- 2023/12/06 00:00 [revised]
PHST- 2023/08/17 00:00 [received]
PHST- 2023/12/28 00:00 [accepted]
PHST- 2024/01/25 06:43 [medline]
PHST- 2024/01/25 06:43 [pubmed]
PHST- 2024/01/25 00:13 [entrez]
AID - 10.1002/alr.23323 [doi]
PST - aheadofprint
SO  - Int Forum Allergy Rhinol. 2024 Jan 24. doi: 10.1002/alr.23323.

PMID- 37659658
OWN - NLM
STAT- Publisher
LR  - 20231022
IS  - 1879-0887 (Electronic)
IS  - 0167-8140 (Linking)
VI  - 188
DP  - 2023 Nov
TI  - Potential of ChatGPT in facilitating research in radiation oncology?
PG  - 109894
LID - S0167-8140(23)89788-2 [pii]
LID - 10.1016/j.radonc.2023.109894 [doi]
AB  - PURPOSE: To evaluate the potential of the artificial intelligence (AI) chatbot 
      ChatGPT in supporting young clinical scientists with scientific tasks in radio 
      oncological research. MATERIALS AND METHODS: Seven scientific tasks were to be 
      completed in 3&nbsp;h by 8 radiation oncologists with different scientific experience 
      working at a university hospital: creation of a scientific synopsis, creation of 
      a research question and corresponding clinical trial hypotheses, writing of the 
      first paragraph of a manuscript introduction, clinical trial sample size 
      calculation, and clinical data analyses (multivariate analysis, boxplot and 
      survival curve). No participant had prior experience with an AI chatbot. All 
      participants were instructed in ChatGPT v3.5 and its use was provided for all 
      tasks. Answers were scored independently by two blinded experts. The subjective 
      value of ChatGPT was rated by each participant. Data were analyzed with 
      regression-, t-test and Spearman correlation (p&nbsp;&lt;&nbsp;0.05). RESULTS: Participants 
      completed tasks 1-3 with an average score of 50% and 4-7 with 56%. Scientific 
      experience, number of original publications and of first/last authorships showed 
      a positive correlation with overall scoring (p&nbsp;=&nbsp;0.01-0.04). Participants with 
      little to moderate scientific experience scored ChatGPT to be more helpful in 
      solving tasks 4-7 compared to more experienced participants (p&nbsp;=&nbsp;0.04), with 
      simultaneously presenting lower scorings (p&nbsp;=&nbsp;0.03). CONCLUSIONS: ChatGPT did not 
      compensate for differences in scientific experience of young clinical scientists, 
      with less experienced researchers believing false AI-generated scientific 
      results.
CI  - Copyright © 2023 The Authors. Published by Elsevier B.V. All rights reserved.
FAU - Guckenberger, Matthias
AU  - Guckenberger M
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Andratschke, Nicolaus
AU  - Andratschke N
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Ahmadsei, Maiwand
AU  - Ahmadsei M
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Christ, Sebastian Matthias
AU  - Christ SM
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Heusel, Astrid Elisabeth
AU  - Heusel AE
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Kamal, Sandeep
AU  - Kamal S
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Kroese, Tiuri Ewout
AU  - Kroese TE
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Looman, Esmée Lauren
AU  - Looman EL
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Reichl, Sabrina
AU  - Reichl S
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Vlaskou Badra, Eugenia
AU  - Vlaskou Badra E
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - von der Grün, Jens
AU  - von der Grün J
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Willmann, Jonas
AU  - Willmann J
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Tanadini-Lang, Stephanie
AU  - Tanadini-Lang S
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland.
FAU - Mayinger, Michael
AU  - Mayinger M
AD  - Department of Radiation Oncology, University Hospital of Zurich, University of 
      Zurich, Zurich, Switzerland. Electronic address: michael.mayinger@usz.ch.
LA  - eng
PT  - Journal Article
DEP - 20230901
PL  - Ireland
TA  - Radiother Oncol
JT  - Radiotherapy and oncology : journal of the European Society for Therapeutic 
      Radiology and Oncology
JID - 8407192
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Radiation oncology
COIS- Declaration of Competing Interest The authors declare the following financial 
      interests/personal relationships which may be considered as potential competing 
      interests: [Dr. Andratschke reports personal fees from AstraZeneca, personal fees 
      from Debiopharm, grants, personal fees and nonfinancial support from ViewRay, 
      grants and personal fees from Brainlab, outside the submitted work. Dr. 
      Tanadini-Lang reports that her husband works at Varian Medical Systems. The other 
      authors report no conflicts of interest.].
EDAT- 2023/09/03 00:41
MHDA- 2023/09/03 00:41
CRDT- 2023/09/02 19:27
PHST- 2023/05/14 00:00 [received]
PHST- 2023/08/20 00:00 [revised]
PHST- 2023/08/25 00:00 [accepted]
PHST- 2023/09/03 00:41 [pubmed]
PHST- 2023/09/03 00:41 [medline]
PHST- 2023/09/02 19:27 [entrez]
AID - S0167-8140(23)89788-2 [pii]
AID - 10.1016/j.radonc.2023.109894 [doi]
PST - ppublish
SO  - Radiother Oncol. 2023 Nov;188:109894. doi: 10.1016/j.radonc.2023.109894. Epub 
      2023 Sep 1.

PMID- 38056130
OWN - NLM
STAT- MEDLINE
DCOM- 20240102
LR  - 20240106
IS  - 1872-7123 (Electronic)
IS  - 0165-1781 (Linking)
VI  - 331
DP  - 2024 Jan
TI  - Assessing the potential of ChatGPT for psychodynamic formulations in psychiatry: 
      An exploratory study.
PG  - 115655
LID - S0165-1781(23)00605-4 [pii]
LID - 10.1016/j.psychres.2023.115655 [doi]
AB  - Although there were several attempts to apply ChatGPT (Generative Pre-Trained 
      Transformer) to medicine, little is known about therapeutic applications in 
      psychiatry. In this exploratory study, we aimed to evaluate the characteristics 
      and appropriateness of the psychodynamic formulations created by ChatGPT. Along 
      with a case selected from the psychoanalytic literature, input prompts were 
      designed to include different levels of background knowledge. These included 
      naïve prompts, keywords created by ChatGPT, keywords created by psychiatrists, 
      and psychodynamic concepts from the literature. The psychodynamic formulations 
      generated from the different prompts were evaluated by five psychiatrists from 
      different institutions. We next conducted further tests in which instructions on 
      the use of different psychodynamic models were added to the input prompts. The 
      models used were ego psychology, self-psychology, and object relations. The 
      results from naïve prompts and psychodynamic concepts were rated as appropriate 
      by most raters. The psychodynamic concept prompt output was rated the highest. 
      Interrater agreement was statistically significant. The results from the tests 
      using instructions in different psychoanalytic theories were also rated as 
      appropriate by most raters. They included key elements of the psychodynamic 
      formulation and suggested interpretations similar to the literature. These 
      findings suggest potential of ChatGPT for use in psychiatry.
CI  - Copyright © 2023. Published by Elsevier B.V.
FAU - Hwang, Gyubeom
AU  - Hwang G
AD  - Department of Biomedical Informatics, Ajou University School of Medicine, Suwon, 
      Republic of Korea; Department of Medical Sciences, Graduate School of Ajou 
      University, Suwon, Republic of Korea.
FAU - Lee, Dong Yun
AU  - Lee DY
AD  - Department of Biomedical Informatics, Ajou University School of Medicine, Suwon, 
      Republic of Korea; Department of Medical Sciences, Graduate School of Ajou 
      University, Suwon, Republic of Korea.
FAU - Seol, Soobeen
AU  - Seol S
AD  - Department of Biomedical Sciences, Ajou University Graduate School of Medicine, 
      Suwon, Republic of Korea.
FAU - Jung, Jaeoh
AU  - Jung J
AD  - Department of Child and Adolescent Psychiatry, Seoul Metropolitan Eunpyeong 
      Hospital, Seoul, Republic of Korea.
FAU - Choi, Yeonkyu
AU  - Choi Y
AD  - Armed Forces Yangju Hospital, Yang-ju, Republic of Korea.
FAU - Her, Eun Sil
AU  - Her ES
AD  - Ajou Big Tree Psychiatric Clinic, Suwon, Republic of Korea.
FAU - An, Min Ho
AU  - An MH
AD  - Department of Biomedical Informatics, Ajou University School of Medicine, Suwon, 
      Republic of Korea; Department of Medical Sciences, Graduate School of Ajou 
      University, Suwon, Republic of Korea.
FAU - Park, Rae Woong
AU  - Park RW
AD  - Department of Biomedical Informatics, Ajou University School of Medicine, Suwon, 
      Republic of Korea; Department of Medical Sciences, Graduate School of Ajou 
      University, Suwon, Republic of Korea; Department of Biomedical Sciences, Ajou 
      University Graduate School of Medicine, Suwon, Republic of Korea. Electronic 
      address: veritas@ajou.ac.kr.
LA  - eng
PT  - Journal Article
DEP - 20231202
PL  - Ireland
TA  - Psychiatry Res
JT  - Psychiatry research
JID - 7911385
SB  - IM
MH  - Humans
MH  - *Psychiatry
MH  - *Psychoanalysis
OTO - NOTNLM
OT  - ChatGPT
OT  - Psychiatry
OT  - Psychoanalysis
OT  - Psychodynamic formulations
COIS- Declaration of Competing Interest All authors declare no competing interests.
EDAT- 2023/12/07 00:42
MHDA- 2024/01/02 11:43
CRDT- 2023/12/06 18:07
PHST- 2023/07/21 00:00 [received]
PHST- 2023/11/27 00:00 [revised]
PHST- 2023/12/01 00:00 [accepted]
PHST- 2024/01/02 11:43 [medline]
PHST- 2023/12/07 00:42 [pubmed]
PHST- 2023/12/06 18:07 [entrez]
AID - S0165-1781(23)00605-4 [pii]
AID - 10.1016/j.psychres.2023.115655 [doi]
PST - ppublish
SO  - Psychiatry Res. 2024 Jan;331:115655. doi: 10.1016/j.psychres.2023.115655. Epub 
      2023 Dec 2.

PMID- 38058223
OWN - NLM
STAT- MEDLINE
DCOM- 20240205
LR  - 20240322
IS  - 2042-7670 (Electronic)
IS  - 0042-4900 (Print)
IS  - 0042-4900 (Linking)
VI  - 194
IP  - 3
DP  - 2024 Feb 3
TI  - Evaluating ChatGPT text mining of clinical records for companion animal obesity 
      monitoring.
PG  - e3669
LID - 10.1002/vetr.3669 [doi]
LID - e3669
AB  - BACKGROUND: Veterinary clinical narratives remain a largely untapped resource for 
      addressing complex diseases. Here we compare the ability of a large language 
      model (ChatGPT) and a previously developed regular expression (RegexT) to 
      identify overweight body condition scores (BCS) in veterinary narratives 
      pertaining to companion animals. METHODS: BCS values were extracted from 4415 
      anonymised clinical narratives using either RegexT or by appending the narrative 
      to a prompt sent to ChatGPT, prompting the model to return the BCS information. 
      Data were manually reviewed for comparison. RESULTS: The precision of RegexT was 
      higher (100%, 95% confidence interval [CI] 94.81%-100%) than that of ChatGPT 
      (89.3%, 95% CI 82.75%-93.64%). However, the recall of ChatGPT (100%, 95% CI 
      96.18%-100%) was considerably higher than that of RegexT (72.6%, 95% CI 
      63.92%-79.94%). LIMITATIONS: Prior anonymisation and subtle prompt engineering 
      are needed to improve ChatGPT output. CONCLUSIONS: Large language models create 
      diverse opportunities and, while complex, present an intuitive interface to 
      information. However, they require careful implementation to avoid unpredictable 
      errors.
CI  - © 2023 The Authors. Veterinary Record published by John Wiley &amp; Sons Ltd on 
      behalf of British Veterinary Association.
FAU - Fins, Ivo S
AU  - Fins IS
AUID- ORCID: 0000-0002-1519-7550
AD  - Small Animal Veterinary Surveillance Network, Institute of Infection, Veterinary 
      and Ecological Sciences, University of Liverpool, Liverpool, UK.
FAU - Davies, Heather
AU  - Davies H
AUID- ORCID: 0000-0001-6905-4718
AD  - Small Animal Veterinary Surveillance Network, Institute of Infection, Veterinary 
      and Ecological Sciences, University of Liverpool, Liverpool, UK.
FAU - Farrell, Sean
AU  - Farrell S
AD  - Department of Computer Science, Durham University, Durham, UK.
FAU - Torres, Jose R
AU  - Torres JR
AD  - Institute for Animal Health and Food Safety, University of Las Palmas de Gran 
      Canaria, Las Palmas, Gran Canaria, Spain.
FAU - Pinchbeck, Gina
AU  - Pinchbeck G
AD  - Small Animal Veterinary Surveillance Network, Institute of Infection, Veterinary 
      and Ecological Sciences, University of Liverpool, Liverpool, UK.
FAU - Radford, Alan D
AU  - Radford AD
AD  - Small Animal Veterinary Surveillance Network, Institute of Infection, Veterinary 
      and Ecological Sciences, University of Liverpool, Liverpool, UK.
FAU - Noble, Peter-John
AU  - Noble PJ
AD  - Small Animal Veterinary Surveillance Network, Institute of Infection, Veterinary 
      and Ecological Sciences, University of Liverpool, Liverpool, UK.
LA  - eng
GR  - BB_/Biotechnology and Biological Sciences Research Council/United Kingdom
PT  - Journal Article
DEP - 20231206
PL  - England
TA  - Vet Rec
JT  - The Veterinary record
JID - 0031164
SB  - IM
MH  - Animals
MH  - *Pets
MH  - *Data Mining
MH  - Language
MH  - Narration
MH  - Obesity/veterinary
PMC - PMC10952314
COIS- The authors declare they have no conflicts of interest.
EDAT- 2023/12/07 06:42
MHDA- 2024/02/05 06:44
PMCR- 2024/03/20
CRDT- 2023/12/07 03:44
PHST- 2023/09/25 00:00 [revised]
PHST- 2023/07/31 00:00 [received]
PHST- 2023/11/07 00:00 [accepted]
PHST- 2024/02/05 06:44 [medline]
PHST- 2023/12/07 06:42 [pubmed]
PHST- 2023/12/07 03:44 [entrez]
PHST- 2024/03/20 00:00 [pmc-release]
AID - VETR3669 [pii]
AID - 10.1002/vetr.3669 [doi]
PST - ppublish
SO  - Vet Rec. 2024 Feb 3;194(3):e3669. doi: 10.1002/vetr.3669. Epub 2023 Dec 6.

PMID- 36972383
OWN - NLM
STAT- MEDLINE
DCOM- 20230329
LR  - 20240330
IS  - 2471-254X (Electronic)
IS  - 2471-254X (Linking)
VI  - 7
IP  - 4
DP  - 2023 Apr 1
TI  - Artificial intelligence-based text generators in hepatology: ChatGPT is just the 
      beginning.
LID - 10.1097/HC9.0000000000000097 [doi]
LID - e0097
AB  - Since its release as a "research preview" in November 2022, ChatGPT, the 
      conversational interface to the Generative Pretrained Transformer 3 large 
      language model built by OpenAI, has garnered significant publicity for its 
      ability to generate detailed responses to a variety of questions. ChatGPT and 
      other large language models generate sentences and paragraphs in response to word 
      patterns in training data that they have previously seen. By allowing users to 
      communicate with an artificial intelligence model in a human-like way, however, 
      ChatGPT has crossed the technological adoption barrier into the mainstream. 
      Existing examples of ChatGPT use-cases, such as negotiating bills, debugging 
      programing code, and writing essays, indicate that ChatGPT and similar models 
      have the potential to have profound (and yet unknown) impacts on clinical 
      research and practice in hepatology. In this special article, we discuss the 
      general background and potential pitfalls of ChatGPT and associated 
      technologies-and then we explore its uses in hepatology with specific examples.
CI  - Copyright © 2023 The Author(s). Published by Wolters Kluwer Health, Inc. on 
      behalf of the American Association for the Study of Liver Diseases.
FAU - Ge, Jin
AU  - Ge J
AUID- ORCID: 0000-0003-1574-1525
AD  - Division of Gastroenterology and Hepatology, Department of Medicine, University 
      of California-San Francisco, San Francisco, California, USA.
FAU - Lai, Jennifer C
AU  - Lai JC
AUID- ORCID: 0000-0003-2092-6380
LA  - eng
GR  - P30 DK026743/DK/NIDDK NIH HHS/United States
GR  - R01 AG059183/AG/NIA NIH HHS/United States
GR  - KL2 TR001870/TR/NCATS NIH HHS/United States
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
PT  - Research Support, Non-U.S. Gov't
DEP - 20230324
PL  - United States
TA  - Hepatol Commun
JT  - Hepatology communications
JID - 101695860
SB  - IM
MH  - Humans
MH  - *Gastroenterology
MH  - Artificial Intelligence
PMC - PMC10043591
COIS- Jin Ge—research support: Merck and Co. Jennifer C. Lai—consultant: GenFit Corp; 
      Advisory Board: Novo Nordisk; research support: Gore Therapeutics; Site 
      investigator: Lipocine.
EDAT- 2023/03/28 06:00
MHDA- 2023/03/29 06:05
PMCR- 2023/03/24
CRDT- 2023/03/27 14:32
PHST- 2023/01/11 00:00 [received]
PHST- 2023/01/31 00:00 [accepted]
PHST- 2023/03/29 06:05 [medline]
PHST- 2023/03/27 14:32 [entrez]
PHST- 2023/03/28 06:00 [pubmed]
PHST- 2023/03/24 00:00 [pmc-release]
AID - 02009842-202304010-00011 [pii]
AID - 10.1097/HC9.0000000000000097 [doi]
PST - epublish
SO  - Hepatol Commun. 2023 Mar 24;7(4):e0097. doi: 10.1097/HC9.0000000000000097. 
      eCollection 2023 Apr 1.

PMID- 38108178
OWN - NLM
STAT- MEDLINE
DCOM- 20231219
LR  - 20231225
IS  - 2687-4792 (Electronic)
IS  - 2687-4784 (Print)
IS  - 2687-4792 (Linking)
VI  - 35
IP  - 1
DP  - 2024 Jan 1
TI  - ChatGPT's potential to support home care for patients in the early period after 
      orthopedic interventions and enhance public health.
PG  - 169-176
LID - jdrs.2023.1402 [pii]
LID - 10.52312/jdrs.2023.1402 [doi]
AB  - OBJECTIVES: This study presents the first investigation into the potential of 
      ChatGPT to provide medical consultation for patients undergoing orthopedic 
      interventions, with the primary objective of evaluating ChatGPT's effectiveness 
      in supporting patient self-management during the essential early recovery phase 
      at home. MATERIALS AND METHODS: Seven scenarios, representative of common 
      situations in orthopedics and traumatology, were presented to ChatGPT version 4.0 
      to obtain advice. These scenarios and ChatGPT̓s responses were then evaluated by 
      68 expert orthopedists (67 males, 1 female; mean age: 37.9±5.9 years; range, 30 
      to 59 years), 40 of whom had at least four years of orthopedic experience, while 
      28 were associate or full professors. Expert orthopedists used a rubric on a 
      scale of 1 to 5 to evaluate ChatGPT's advice based on accuracy, applicability, 
      comprehensiveness, and clarity. Those who gave ChatGPT a score of 4 or higher 
      considered its performance as above average or excellent. RESULTS: In all 
      scenarios, the median evaluation scores were at least 4 across accuracy, 
      applicability, comprehensiveness, and communication. As for mean scores, accuracy 
      was the highest-rated dimension at 4.2±0.8, while mean comprehensiveness was 
      slightly lower at 3.9±0.8. Orthopedist characteristics, such as academic title 
      and prior use of ChatGPT, did not influence their evaluation (all p&gt;0.05). Across 
      all scenarios, ChatGPT demonstrated an accuracy of 79.8%, with applicability at 
      75.2%, comprehensiveness at 70.6%, and a 75.6% rating for communication clarity. 
      CONCLUSION: This study emphasizes ChatGPT̓s strengths in accuracy and 
      applicability for home care after orthopedic intervention but underscores a need 
      for improved comprehensiveness. This focused evaluation not only sheds light on 
      ChatGPT̓s potential in specialized medical advice but also suggests its potential 
      to play a broader role in the advancement of public health.
FAU - Yapar, Dilek
AU  - Yapar D
FAU - Demir Avcı, Yasemin
AU  - Demir Avcı Y
FAU - Tokur Sonuvar, Esra
AU  - Tokur Sonuvar E
FAU - Eğerci, Ömer Faruk
AU  - Eğerci ÖF
FAU - Yapar, Aliekber
AU  - Yapar A
AD  - Antalya Eğitim ve Araştırma Hastanesi, Ortopedi ve Travmatoloji Kliniği, 07100 
      Muratpaşa, Antalya, Türkiye. aliekberyapar@hotmail.com.
LA  - eng
PT  - Journal Article
DEP - 20231130
PL  - Turkey
TA  - Jt Dis Relat Surg
JT  - Joint diseases and related surgery
JID - 101764223
SB  - IM
MH  - Male
MH  - Humans
MH  - Female
MH  - Adult
MH  - Public Health
MH  - *Orthopedic Procedures
MH  - *Orthopedics
MH  - *Home Care Services
MH  - *Orthopedic Surgeons
PMC - PMC10746912
COIS- Conflict of Interest: The authors declared no conflicts of interest with respect 
      to the authorship and/or publication of this article.
EDAT- 2023/12/18 06:41
MHDA- 2023/12/19 06:42
PMCR- 2023/11/30
CRDT- 2023/12/18 05:28
PHST- 2023/08/27 00:00 [received]
PHST- 2023/11/06 00:00 [accepted]
PHST- 2023/12/19 06:42 [medline]
PHST- 2023/12/18 06:41 [pubmed]
PHST- 2023/12/18 05:28 [entrez]
PHST- 2023/11/30 00:00 [pmc-release]
AID - jdrs.2023.1402 [pii]
AID - 10.52312/jdrs.2023.1402 [doi]
PST - ppublish
SO  - Jt Dis Relat Surg. 2024 Jan 1;35(1):169-176. doi: 10.52312/jdrs.2023.1402. Epub 
      2023 Nov 30.

PMID- 37457604
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230718
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 7
DP  - 2023 Jul
TI  - Advancing Artificial Intelligence for Clinical Knowledge Retrieval: A Case Study 
      Using ChatGPT-4 and Link Retrieval Plug-In to Analyze Diabetic Ketoacidosis 
      Guidelines.
PG  - e41916
LID - 10.7759/cureus.41916 [doi]
LID - e41916
AB  - Introduction This case study aimed to enhance the traceability and retrieval 
      accuracy of ChatGPT-4 in medical text by employing a step-by-step systematic 
      approach. The focus was on retrieving clinical answers from three international 
      guidelines on diabetic ketoacidosis (DKA). Methods A systematic methodology was 
      developed to guide the retrieval process. One question was asked per guideline to 
      ensure accuracy and maintain referencing. ChatGPT-4 was utilized to retrieve 
      answers, and the 'Link Reader' plug-in was integrated to facilitate direct access 
      to webpages containing the guidelines. Subsequently, ChatGPT-4 was employed to 
      compile answers while providing citations to the sources. This process was 
      iterated 30 times per question to ensure consistency. In this report, we present 
      our observations regarding the retrieval accuracy, consistency of responses, and 
      the challenges encountered during the process. Results Integrating ChatGPT-4 with 
      the 'Link Reader' plug-in demonstrated notable traceability and retrieval 
      accuracy benefits. The AI model successfully provided relevant and accurate 
      clinical answers based on the analyzed guidelines. Despite occasional challenges 
      with webpage access and minor memory drift, the overall performance of the 
      integrated system was promising. The compilation of the answers was also 
      impressive and held significant promise for further trials. Conclusion The 
      findings of this case study contribute to the utilization of AI text-generation 
      models as valuable tools for medical professionals and researchers. The 
      systematic approach employed in this case study and the integration of the 'Link 
      Reader' plug-in offer a framework for automating medical text synthesis, asking 
      one question at a time before compilation from different sources, which has led 
      to improving AI models' traceability and retrieval accuracy. Further advancements 
      and refinement of AI models and integration with other software utilities hold 
      promise for enhancing the utility and applicability of AI-generated 
      recommendations in medicine and scientific academia. These advancements have the 
      potential to drive significant improvements in everyday medical practice.
CI  - Copyright © 2023, Hamed et al.
FAU - Hamed, Ehab
AU  - Hamed E
AD  - Family Medicine, Qatar University Health Centre, Primary Health Care Corporation, 
      Doha, QAT.
FAU - Sharif, Anna
AU  - Sharif A
AD  - Family Medicine, Primary Health Care Corporation, Doha, QAT.
FAU - Eid, Ahmad
AU  - Eid A
AD  - Family Medicine, Primary Health Care Corporation, Doha, QAT.
FAU - Alfehaidi, Alanoud
AU  - Alfehaidi A
AD  - Family Medicine, Primary Health Care Corporation, Doha, QAT.
FAU - Alberry, Medhat
AU  - Alberry M
AD  - Obstetrics and Gynecology, Weill Cornell Medicine - Qatar, Doha, QAT.
AD  - Fetal and Maternal Medicine, Sidra Medicine, Doha, QAT.
LA  - eng
PT  - Journal Article
DEP - 20230715
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10349539
OTO - NOTNLM
OT  - artificial intelligence in medicine
OT  - chatgpt
OT  - chatgpt-4
OT  - clinical decision support
OT  - clinical decision support system
OT  - clinical decision tool
OT  - generative ai
OT  - large language model
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/07/17 06:42
MHDA- 2023/07/17 06:43
PMCR- 2023/07/15
CRDT- 2023/07/17 04:41
PHST- 2023/07/15 00:00 [accepted]
PHST- 2023/07/17 06:43 [medline]
PHST- 2023/07/17 06:42 [pubmed]
PHST- 2023/07/17 04:41 [entrez]
PHST- 2023/07/15 00:00 [pmc-release]
AID - 10.7759/cureus.41916 [doi]
PST - epublish
SO  - Cureus. 2023 Jul 15;15(7):e41916. doi: 10.7759/cureus.41916. eCollection 2023 
      Jul.

PMID- 38394625
OWN - NLM
STAT- Publisher
LR  - 20240223
IS  - 1744-5078 (Electronic)
IS  - 0927-3948 (Linking)
DP  - 2024 Feb 23
TI  - Investigating the Accuracy and Completeness of an Artificial Intelligence Large 
      Language Model About Uveitis: An Evaluation of ChatGPT.
PG  - 1-4
LID - 10.1080/09273948.2024.2317417 [doi]
AB  - PURPOSE: To assess the accuracy and completeness of ChatGPT-generated answers 
      regarding uveitis description, prevention, treatment, and prognosis. METHODS: 
      Thirty-two uveitis-related questions were generated by a uveitis specialist and 
      inputted into ChatGPT 3.5. Answers were compiled into a survey and were reviewed 
      by five uveitis specialists using standardized Likert scales of accuracy and 
      completeness. RESULTS: In total, the median accuracy score for all the uveitis 
      questions (n = 32) was 4.00 (between "more correct than incorrect" and "nearly 
      all correct"), and the median completeness score was 2.00 ("adequate, addresses 
      all aspects of the question and provides the minimum amount of information 
      required to be considered complete"). The interrater variability assessment had a 
      total kappa value of 0.0278 for accuracy and 0.0847 for completeness. CONCLUSION: 
      ChatGPT can provide relatively high accuracy responses for various questions 
      related to uveitis; however, the answers it provides are incomplete, with some 
      inaccuracies. Its utility in providing medical information requires further 
      validation and development prior to serving as a source of uveitis information 
      for patients.
FAU - Marshall, Rayna F
AU  - Marshall RF
AD  - The Drexel University College of Medicine, Philadelphia, Pennsylvania, USA.
FAU - Mallem, Krishna
AU  - Mallem K
AD  - The Drexel University College of Medicine, Philadelphia, Pennsylvania, USA.
FAU - Xu, Hannah
AU  - Xu H
AD  - University of California San Diego, San Diego, California, USA.
FAU - Thorne, Jennifer
AU  - Thorne J
AD  - The Wilmer Eye Institute, Division of Ocular Immunology, The Johns Hopkins 
      University School of Medicine, Baltimore, Maryland, USA.
FAU - Burkholder, Bryn
AU  - Burkholder B
AD  - The Wilmer Eye Institute, Division of Ocular Immunology, The Johns Hopkins 
      University School of Medicine, Baltimore, Maryland, USA.
FAU - Chaon, Benjamin
AU  - Chaon B
AD  - The Wilmer Eye Institute, Division of Ocular Immunology, The Johns Hopkins 
      University School of Medicine, Baltimore, Maryland, USA.
FAU - Liberman, Paulina
AU  - Liberman P
AUID- ORCID: 0000-0002-1053-8651
AD  - The Wilmer Eye Institute, Division of Ocular Immunology, The Johns Hopkins 
      University School of Medicine, Baltimore, Maryland, USA.
FAU - Berkenstock, Meghan
AU  - Berkenstock M
AD  - The Wilmer Eye Institute, Division of Ocular Immunology, The Johns Hopkins 
      University School of Medicine, Baltimore, Maryland, USA.
LA  - eng
PT  - Journal Article
DEP - 20240223
PL  - England
TA  - Ocul Immunol Inflamm
JT  - Ocular immunology and inflammation
JID - 9312169
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - large language model
OT  - ophthalmology
OT  - uveitis
EDAT- 2024/02/23 18:42
MHDA- 2024/02/23 18:42
CRDT- 2024/02/23 16:23
PHST- 2024/02/23 18:42 [medline]
PHST- 2024/02/23 18:42 [pubmed]
PHST- 2024/02/23 16:23 [entrez]
AID - 10.1080/09273948.2024.2317417 [doi]
PST - aheadofprint
SO  - Ocul Immunol Inflamm. 2024 Feb 23:1-4. doi: 10.1080/09273948.2024.2317417.

PMID- 37493985
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230729
IS  - 2197-1153 (Print)
IS  - 2197-1153 (Electronic)
IS  - 2197-1153 (Linking)
VI  - 10
IP  - 1
DP  - 2023 Jul 26
TI  - Artificial intelligence and ChatGPT in Orthopaedics and sports medicine.
PG  - 74
LID - 10.1186/s40634-023-00642-8 [doi]
LID - 74
AB  - Artificial intelligence (AI) is looked upon nowadays as the potential major 
      catalyst for the fourth industrial revolution. In the last decade, AI use in 
      Orthopaedics increased approximately tenfold. Artificial intelligence helps with 
      tracking activities, evaluating diagnostic images, predicting injury risk, and 
      several other uses. Chat Generated Pre-trained Transformer (ChatGPT), which is an 
      AI-chatbot, represents an extremely controversial topic in the academic 
      community. The aim of this review article is to simplify the concept of AI and 
      study the extent of AI use in Orthopaedics and sports medicine literature. 
      Additionally, the article will also evaluate the role of ChatGPT in scientific 
      research and publications.Level of evidence:&nbsp;Level V, letter to review.
CI  - © 2023. This is a U.S. Government work and not under copyright protection in the 
      US; foreign copyright protection may apply.
FAU - Fayed, Aly M
AU  - Fayed AM
AUID- ORCID: 0000-0002-3905-9662
AD  - Department of Orthopaedics and Rehabilitation, University of Iowa Hospitals and 
      Clinics, Iowa City, IA, USA. a.m.alyfayed@gmail.com.
FAU - Mansur, Nacime Salomao Barbachan
AU  - Mansur NSB
AD  - Department of Orthopaedics and Rehabilitation, University of Iowa Hospitals and 
      Clinics, Iowa City, IA, USA.
FAU - de Carvalho, Kepler Alencar
AU  - de Carvalho KA
AD  - Department of Orthopaedics and Rehabilitation, University of Iowa Hospitals and 
      Clinics, Iowa City, IA, USA.
FAU - Behrens, Andrew
AU  - Behrens A
AD  - Department of Orthopaedics and Rehabilitation, University of Iowa Hospitals and 
      Clinics, Iowa City, IA, USA.
FAU - D'Hooghe, Pieter
AU  - D'Hooghe P
AD  - Aspetar Orthopedic and Sports Medicine Hospital, Doha, Qatar.
FAU - de Cesar Netto, Cesar
AU  - de Cesar Netto C
AD  - Department of Orthopaedic Surgery, Duke University, Durham, NC, USA.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20230726
PL  - United States
TA  - J Exp Orthop
JT  - Journal of experimental orthopaedics
JID - 101653750
PMC - PMC10371934
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Distance mapping
OT  - Machine learning
OT  - Statistical algorithm
COIS- The authors declare that they have no competing interests.
EDAT- 2023/07/26 13:08
MHDA- 2023/07/26 13:09
PMCR- 2023/07/26
CRDT- 2023/07/26 11:22
PHST- 2023/06/13 00:00 [received]
PHST- 2023/07/18 00:00 [accepted]
PHST- 2023/07/26 13:09 [medline]
PHST- 2023/07/26 13:08 [pubmed]
PHST- 2023/07/26 11:22 [entrez]
PHST- 2023/07/26 00:00 [pmc-release]
AID - 10.1186/s40634-023-00642-8 [pii]
AID - 642 [pii]
AID - 10.1186/s40634-023-00642-8 [doi]
PST - epublish
SO  - J Exp Orthop. 2023 Jul 26;10(1):74. doi: 10.1186/s40634-023-00642-8.

PMID- 38353558
OWN - NLM
STAT- Publisher
LR  - 20240214
IS  - 1524-4040 (Electronic)
IS  - 0148-396X (Linking)
DP  - 2024 Feb 14
TI  - Usefulness and Accuracy of Artificial Intelligence Chatbot Responses to Patient 
      Questions for Neurosurgical Procedures.
LID - 10.1227/neu.0000000000002856 [doi]
AB  - BACKGROUND AND OBJECTIVES: The Internet has become a primary source of health 
      information, leading patients to seek answers online before consulting health 
      care providers. This study aims to evaluate the implementation of Chat Generative 
      Pre-Trained Transformer (ChatGPT) in neurosurgery by assessing the accuracy and 
      helpfulness of artificial intelligence (AI)-generated responses to common 
      postsurgical questions. METHODS: A list of 60 commonly asked questions regarding 
      neurosurgical procedures was developed. ChatGPT-3.0, ChatGPT-3.5, and ChatGPT-4.0 
      responses to these questions were recorded and graded by numerous practitioners 
      for accuracy and helpfulness. The understandability and actionability of the 
      answers were assessed using the Patient Education Materials Assessment Tool. 
      Readability analysis was conducted using established scales. RESULTS: A total of 
      1080 responses were evaluated, equally divided among ChatGPT-3.0, 3.5, and 4.0, 
      each contributing 360 responses. The mean helpfulness score across the 3 
      subsections was 3.511 ± 0.647 while the accuracy score was 4.165 ± 0.567. The 
      Patient Education Materials Assessment Tool analysis revealed that the 
      AI-generated responses had higher actionability scores than understandability. 
      This indicates that the answers provided practical guidance and recommendations 
      that patients could apply effectively. On the other hand, the mean Flesch Reading 
      Ease score was 33.5, suggesting that the readability level of the responses was 
      relatively complex. The Raygor Readability Estimate scores ranged within the 
      graduate level, with an average score of the 15th grade. CONCLUSION: The 
      artificial intelligence chatbot's responses, although factually accurate, were 
      not rated highly beneficial, with only marginal differences in perceived 
      helpfulness and accuracy between ChatGPT-3.0 and ChatGPT-3.5 versions. Despite 
      this, the responses from ChatGPT-4.0 showed a notable improvement in 
      understandability, indicating enhanced readability over earlier versions.
CI  - Copyright © Congress of Neurological Surgeons 2024. All rights reserved.
FAU - Gajjar, Avi A
AU  - Gajjar AA
AUID- ORCID: 0000-0003-3686-9413
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
AD  - Department of Neurological Surgery, Albany Medical College, Albany, New York, 
      USA.
FAU - Kumar, Rohit Prem
AU  - Kumar RP
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
FAU - Paliwoda, Ethan D
AU  - Paliwoda ED
AUID- ORCID: 0009-0005-5069-5829
AD  - Albany Medical College, Albany, New York, USA.
FAU - Kuo, Cathleen C
AU  - Kuo CC
AD  - Department of Neurological Surgery, Jacobs School of Medicine and Biomedical 
      Sciences at University at Buffalo, New York, New York, USA.
FAU - Adida, Samuel
AU  - Adida S
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
AD  - University of Pittsburgh School of Medicine, Pittsburgh, Pennsylvania, USA.
FAU - Legarreta, Andrew D
AU  - Legarreta AD
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
FAU - Deng, Hansen
AU  - Deng H
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
FAU - Anand, Sharath Kumar
AU  - Anand SK
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
FAU - Hamilton, D Kojo
AU  - Hamilton DK
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
FAU - Buell, Thomas J
AU  - Buell TJ
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
FAU - Agarwal, Nitin
AU  - Agarwal N
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
FAU - Gerszten, Peter C
AU  - Gerszten PC
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
FAU - Hudson, Joseph S
AU  - Hudson JS
AD  - Department of Neurological Surgery, University of Pittsburgh Medical Center, 
      Pittsburgh, Pennsylvania, USA.
LA  - eng
PT  - Journal Article
DEP - 20240214
PL  - United States
TA  - Neurosurgery
JT  - Neurosurgery
JID - 7802914
SB  - IM
EDAT- 2024/02/14 12:42
MHDA- 2024/02/14 12:42
CRDT- 2024/02/14 09:04
PHST- 2023/08/30 00:00 [received]
PHST- 2023/12/17 00:00 [accepted]
PHST- 2024/02/14 12:42 [medline]
PHST- 2024/02/14 12:42 [pubmed]
PHST- 2024/02/14 09:04 [entrez]
AID - 00006123-990000000-01053 [pii]
AID - 10.1227/neu.0000000000002856 [doi]
PST - aheadofprint
SO  - Neurosurgery. 2024 Feb 14. doi: 10.1227/neu.0000000000002856.

PMID- 38501898
OWN - NLM
STAT- Publisher
LR  - 20240319
IS  - 2561-326X (Electronic)
IS  - 2561-326X (Linking)
DP  - 2024 Mar 14
TI  - Assessing the Accuracy of Generative Conversational Artificial Intelligence in 
      Debunking Sleep Health Myths: Mixed-Methods Comparative Study with Expert 
      Analysis.
LID - 10.2196/55762 [doi]
AB  - BACKGROUND: Adequate sleep is essential for maintaining both individual and 
      public health, positively affecting cognition and well-being, and reducing 
      chronic disease risks. It also plays a significant role in driving the economy, 
      public safety, and managing healthcare costs. Digital tools, including websites, 
      sleep trackers, and apps, are key in promoting sleep health education. 
      Conversational Artificial Intelligence (AI) like ChatGPT offers accessible, 
      personalized advice on sleep health but raises concerns about potential 
      misinformation. This underscores the importance of ensuring AI-driven sleep 
      health information is accurate, given its significant impact on individual and 
      public health, and the spread of sleep-related myths. OBJECTIVE: The study aims 
      to examine ChatGPT's capability to debunk sleep-related disbeliefs. METHODS: A 
      mixed-methods design was leveraged. ChatGPT was asked to categorize twenty 
      sleep-related false myths identified by ten sleep experts and to rate them in 
      terms of falseness and public health significance, on a 5-point Likert scale. 
      Sensitivity, positive predictive value, and inter-rater agreement were also 
      calculated. A qualitative comparative analysis was also conducted. RESULTS: 
      ChatGPT labeled a significant portion (85%, n=17) of the statements as "false" 
      (45%, n=9) or "generally false" (40%, n=8), with varying accuracy across 
      different domains. For instance, it correctly identified most myths about "sleep 
      timing", "sleep duration", and "behaviors during sleep", while it had varying 
      degrees of success with other categories like "pre-sleep behaviors" and "brain 
      function and sleep". ChatGPT's assessment of the degree of falseness and public 
      health significance, on the 5-point Likert scale, showed an average score of 3.45 
      (SD=0.85) and 3.15 (SD=0.96), respectively, indicating a good level of accuracy 
      in identifying the falseness of statements and a good understanding of their 
      impact on public health. The AI-based tool showed a sensitivity of 85% and a 
      perfect positive predictive value of 100%. Overall, this indicates that when 
      ChatGPT labels a statement as false, it is highly reliable, but it may miss 
      identifying some false statements. When comparing with expert ratings, high 
      intra-class correlation coefficients (ICCs) between ChatGPT's appraisals and 
      expert opinions could be found, suggesting that the AI's ratings were generally 
      aligned with expert views on falseness (ICC=.83, P&lt;.0001) and public health 
      significance (ICC=.79, P=.001) of sleep-related myths. From a qualitative 
      standpoint, both ChatGPT and sleep experts refuted sleep-related misconceptions. 
      However, ChatGPT adopted a more accessible style and provided a more generalized, 
      focusing on broad concepts, while experts sometimes used technical jargon, 
      providing evidence-based explanations. CONCLUSIONS: ChatGPT-4 can accurately 
      address sleep-related queries and debunk sleep-related myths, with a performance 
      comparable to sleep experts, even if, given its limitations, the AI cannot 
      completely replace expert opinions, especially in nuanced and complex fields like 
      sleep health, but can be a valuable complement in the dissemination of updated 
      information and promotion of healthy behaviors.
FAU - Bragazzi, Nicola Luigi
AU  - Bragazzi NL
AD  - University of Parma, Via Volturno 39, Parma, IT.
FAU - Garbarino, Sergio
AU  - Garbarino S
AD  - University of Genoa, Genoa, IT.
LA  - eng
PT  - Journal Article
DEP - 20240314
PL  - Canada
TA  - JMIR Form Res
JT  - JMIR formative research
JID - 101726394
EDAT- 2024/03/19 12:43
MHDA- 2024/03/19 12:43
CRDT- 2024/03/19 09:22
PHST- 2024/03/19 12:43 [medline]
PHST- 2024/03/19 12:43 [pubmed]
PHST- 2024/03/19 09:22 [entrez]
AID - 10.2196/55762 [doi]
PST - aheadofprint
SO  - JMIR Form Res. 2024 Mar 14. doi: 10.2196/55762.

PMID- 38313589
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240206
IS  - 2169-7574 (Print)
IS  - 2169-7574 (Electronic)
IS  - 2169-7574 (Linking)
VI  - 12
IP  - 2
DP  - 2024 Feb
TI  - Validation of ChatGPT 3.5 as a Tool to Optimize Readability of Patient-facing 
      Craniofacial Education Materials.
PG  - e5575
LID - 10.1097/GOX.0000000000005575 [doi]
LID - e5575
AB  - BACKGROUND: To address patient health literacy, the American Medical Association 
      recommends that readability of patient education materials should not exceed a 
      sixth grade reading level; the National Institutes of Health recommend no greater 
      than an eigth-grade reading level. However, patient-facing materials in plastic 
      surgery often remain at an above-recommended average reading level. The purpose 
      of this study was to evaluate ChatGPT 3.5 as a tool for optimizing patient-facing 
      craniofacial education materials. METHODS: Eighteen patient-facing craniofacial 
      education materials were evaluated for readability by a traditional calculator 
      and ChatGPT 3.5. The resulting scores were compared. The original excerpts were 
      then inputted to ChatGPT 3.5 and simplified by the artificial intelligence tool. 
      The simplified excerpts were scored by the calculators. RESULTS: The difference 
      in scores for the original excerpts between the online calculator and ChatGPT 3.5 
      were not significant (P = 0.441). Additionally, the simplified excerpts' scores 
      were significantly lower than the originals (P &lt; 0.001), and the mean of the 
      simplified excerpts was 7.78, less than the maximum recommended 8. CONCLUSIONS: 
      The use of ChatGPT 3.5 for simplification and readability analysis of 
      patient-facing craniofacial materials is efficient and may help facilitate the 
      conveyance of important health information. ChatGPT 3.5 rendered readability 
      scores comparable to traditional readability calculators, in addition to 
      excerpt-specific feedback. It was also able to simplify materials to the 
      recommended grade levels. With human oversight, we validate this tool for 
      readability analysis and simplification.
CI  - Copyright © 2024 The Authors. Published by Wolters Kluwer Health, Inc. on behalf 
      of The American Society of Plastic Surgeons.
FAU - Vallurupalli, Medha
AU  - Vallurupalli M
AD  - From the Keck School of Medicine of USC, Los Angeles, Calif.
AD  - Department of Plastic Surgery, University of California, Irvine, Calif.
FAU - Shah, Nikhil D
AU  - Shah ND
AD  - Department of Plastic Surgery, University of California, Irvine, Calif.
FAU - Vyas, Raj M
AU  - Vyas RM
AD  - Department of Plastic Surgery, University of California, Irvine, Calif.
AD  - CHOC Children's Hospital of Orange County, Orange, Calif.
LA  - eng
PT  - Journal Article
DEP - 20240202
PL  - United States
TA  - Plast Reconstr Surg Glob Open
JT  - Plastic and reconstructive surgery. Global open
JID - 101622231
PMC - PMC10836906
COIS- The authors have no financial interest to declare in relation to the content of 
      this article.
EDAT- 2024/02/05 06:43
MHDA- 2024/02/05 06:44
PMCR- 2024/02/02
CRDT- 2024/02/05 04:46
PHST- 2023/10/02 00:00 [received]
PHST- 2023/12/11 00:00 [accepted]
PHST- 2024/02/05 06:44 [medline]
PHST- 2024/02/05 06:43 [pubmed]
PHST- 2024/02/05 04:46 [entrez]
PHST- 2024/02/02 00:00 [pmc-release]
AID - 10.1097/GOX.0000000000005575 [doi]
PST - epublish
SO  - Plast Reconstr Surg Glob Open. 2024 Feb 2;12(2):e5575. doi: 
      10.1097/GOX.0000000000005575. eCollection 2024 Feb.

PMID- 38357084
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240216
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 16
IP  - 1
DP  - 2024 Jan
TI  - Pioneering the Integration of Artificial Intelligence in Medical Oral Board 
      Examinations.
PG  - e52318
LID - 10.7759/cureus.52318 [doi]
LID - e52318
AB  - We evaluated the use of ChatGPT-4, an advanced artificial intelligence (AI) 
      language model, in medical oral examinations, specifically in anesthesiology. 
      Initially proven adept in written examinations, ChatGPT-4's performance was 
      tested against oral board sample sessions of the American Board of 
      Anesthesiology. Modifications were made to ensure responses were concise and 
      conversationally natural, simulating real patient consultations or oral 
      examinations. The results demonstrate ChatGPT-4's impressive adaptability and 
      potential in oral board examinations as a training and assessment tool in medical 
      education, indicating new avenues for AI application in this field.
CI  - Copyright © 2024, Hanada et al.
FAU - Hanada, Satoshi
AU  - Hanada S
AD  - Anesthesia, University of Iowa Hospitals and Clinics, Iowa City, USA.
FAU - Hayashi, Yuri
AU  - Hayashi Y
AD  - Anesthesia, University of Iowa Hospitals and Clinics, Iowa City, USA.
AD  - Department of Anesthesiology and Intensive Care Medicine, Osaka University 
      Graduate School of Medicine, Suita, JPN.
FAU - Subramani, Sudhakar
AU  - Subramani S
AD  - Anesthesia, University of Iowa Hospitals and Clinics, Iowa City, USA.
FAU - Thenuwara, Kokila
AU  - Thenuwara K
AD  - Anesthesia, University of Iowa Hospitals and Clinics, Iowa City, USA.
LA  - eng
PT  - Editorial
DEP - 20240115
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10866608
OTO - NOTNLM
OT  - anesthesiology
OT  - artificial intelligence (ai)
OT  - chatgpt-4
OT  - medical education technology
OT  - oral board examination
COIS- The authors have declared financial relationships, which are detailed in the next 
      section.
EDAT- 2024/02/15 06:43
MHDA- 2024/02/15 06:44
PMCR- 2024/01/15
CRDT- 2024/02/15 04:15
PHST- 2024/01/11 00:00 [accepted]
PHST- 2024/02/15 06:44 [medline]
PHST- 2024/02/15 06:43 [pubmed]
PHST- 2024/02/15 04:15 [entrez]
PHST- 2024/01/15 00:00 [pmc-release]
AID - 10.7759/cureus.52318 [doi]
PST - epublish
SO  - Cureus. 2024 Jan 15;16(1):e52318. doi: 10.7759/cureus.52318. eCollection 2024 
      Jan.

PMID- 37368125
OWN - NLM
STAT- MEDLINE
DCOM- 20240125
LR  - 20240204
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 52
IP  - 2
DP  - 2024 Feb
TI  - Using AI Tools in Writing Peer Review Reports: Should Academic Journals Embrace 
      the Use of ChatGPT?
PG  - 139-140
LID - 10.1007/s10439-023-03299-7 [doi]
AB  - This letter highlights a pressing issue regarding the absence of established 
      editorial policies for the utilization of AI tools (e.g., ChatGPT) in the peer 
      review process. The increasing adoption of AI tools in academic publishing 
      necessitates the formulation of standardized guidelines to ensure fairness, 
      transparency, and accountability. Without clear editorial policies, there is a 
      threat of compromising the integrity of the peer review process and undermining 
      the credibility of academic publications. Urgent attention is needed to address 
      this gap and establish robust protocols that govern the use of AI tools in peer 
      review.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Garcia, Manuel B
AU  - Garcia MB
AUID- ORCID: 0000-0003-2615-422X
AD  - Educational Innovation and Technology Hub, FEU Institute of Technology, Manila, 
      Philippines. mbgarcia@feutech.edu.ph.
AD  - College of Education, University of the Philippines Diliman, Quezon City, 
      Philippines. mbgarcia@feutech.edu.ph.
LA  - eng
PT  - Letter
DEP - 20230627
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - *Biomedical Research
MH  - *Periodicals as Topic
MH  - Publishing
MH  - Peer Review
MH  - Writing
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Editorial policies
OT  - Peer review
EDAT- 2023/06/27 13:10
MHDA- 2024/01/25 06:44
CRDT- 2023/06/27 11:10
PHST- 2023/06/18 00:00 [received]
PHST- 2023/06/22 00:00 [accepted]
PHST- 2024/01/25 06:44 [medline]
PHST- 2023/06/27 13:10 [pubmed]
PHST- 2023/06/27 11:10 [entrez]
AID - 10.1007/s10439-023-03299-7 [pii]
AID - 10.1007/s10439-023-03299-7 [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2024 Feb;52(2):139-140. doi: 10.1007/s10439-023-03299-7. Epub 
      2023 Jun 27.

PMID- 38379960
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240222
IS  - 2405-8440 (Print)
IS  - 2405-8440 (Electronic)
IS  - 2405-8440 (Linking)
VI  - 10
IP  - 4
DP  - 2024 Feb 29
TI  - Is AI changing learning and assessment as we know it? Evidence from a ChatGPT 
      experiment and a conceptual framework.
PG  - e25953
LID - 10.1016/j.heliyon.2024.e25953 [doi]
LID - e25953
AB  - ChatGPT, a state-of-the-art chatbot built upon Open AI's generative pre-trained 
      transformer, has generated a major public interest and caused quite a stir in the 
      higher education sector, where reactions have ranged from excitement to 
      consternation. This paper therefore examines the potential impact of ChatGPT on 
      learning and assessment, using the example of academic essays, being a major form 
      of assessment with widespread applications of ChatGPT. This provides an 
      opportunity to unpack broader insights on the challenge of generative AI's to the 
      relevance, quality and credibility of higher education learning in a rapidly 
      changing 21st century knowledge economy. We conducted a quasi-experiment in which 
      we deployed ChatGPT to generate academic essays in response to a typical 
      assessment brief, and then subjected the essays to plagiarism checks and 
      independent grading. The results indicate that ChatGPT is able to generate highly 
      original, and high quality, contents from distinct individual accounts in 
      response to the same assessment brief. However, it is unable to generate multiple 
      original contents from the same account, and it struggled with referencing. The 
      discussion highlights the need for higher education providers to rethink their 
      approach to assessment, in response to disruption precipitated by artificial 
      intelligence. Thus, following the discussion of empirical data, we propose a new 
      conceptual framework for AI-assisted assessment for lifelong learning, in which 
      the parameters of assessment extend beyond knowledge (know what) testing, to 
      competence (know how) assessment and performance (show how) evaluation.
CI  - © 2024 The Authors.
FAU - Kolade, Oluwaseun
AU  - Kolade O
AD  - Sheffield Business School, Sheffield Hallam University, Howard Street, Sheffield, 
      S1 1WB, UK.
FAU - Owoseni, Adebowale
AU  - Owoseni A
AD  - School of Computer Science and Informatics, Faculty of Computing Engineering and 
      Media, De Montfort University, Leicester, UK.
FAU - Egbetokun, Abiodun
AU  - Egbetokun A
AD  - Leicester Castle Business School, De Montfort University, Leicester, LE1 9BH, UK.
LA  - eng
PT  - Journal Article
DEP - 20240210
PL  - England
TA  - Heliyon
JT  - Heliyon
JID - 101672560
PMC - PMC10877295
COIS- The authors declare that they have no known competing financial interests or 
      personal relationships that could have appeared to influence the work reported in 
      this paper.
EDAT- 2024/02/21 11:16
MHDA- 2024/02/21 11:17
PMCR- 2024/02/10
CRDT- 2024/02/21 04:03
PHST- 2023/08/29 00:00 [received]
PHST- 2024/01/27 00:00 [revised]
PHST- 2024/02/05 00:00 [accepted]
PHST- 2024/02/21 11:17 [medline]
PHST- 2024/02/21 11:16 [pubmed]
PHST- 2024/02/21 04:03 [entrez]
PHST- 2024/02/10 00:00 [pmc-release]
AID - S2405-8440(24)01984-4 [pii]
AID - e25953 [pii]
AID - 10.1016/j.heliyon.2024.e25953 [doi]
PST - epublish
SO  - Heliyon. 2024 Feb 10;10(4):e25953. doi: 10.1016/j.heliyon.2024.e25953. 
      eCollection 2024 Feb 29.

PMID- 38247132
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240320
IS  - 2287-8572 (Print)
IS  - 2287-8580 (Electronic)
IS  - 2287-8572 (Linking)
VI  - 67
IP  - 2
DP  - 2024 Mar
TI  - Potential applications of ChatGPT in obstetrics and gynecology in Korea: a review 
      article.
PG  - 153-159
LID - 10.5468/ogs.23231 [doi]
AB  - The use of chatbot technology, particularly chat generative pre-trained 
      transformer (ChatGPT) with an impressive 175 billion parameters, has garnered 
      significant attention across various domains, including Obstetrics and Gynecology 
      (OBGYN). This comprehensive review delves into the transformative potential of 
      chatbots with a special focus on ChatGPT as a leading artificial intelligence 
      (AI) technology. Moreover, ChatGPT harnesses the power of deep learning 
      algorithms to generate responses that closely mimic human language, opening up 
      myriad applications in medicine, research, and education. In the field of 
      medicine, ChatGPT plays a pivotal role in diagnosis, treatment, and personalized 
      patient education. Notably, the technology has demonstrated remarkable 
      capabilities, surpassing human performance in OBGYN examinations, and delivering 
      highly accurate diagnoses. However, challenges remain, including the need to 
      verify the accuracy of the information and address the ethical considerations and 
      limitations. In the wide scope of chatbot technology, AI systems play a vital 
      role in healthcare processes, including documentation, diagnosis, research, and 
      education. Although promising, the limitations and occasional inaccuracies 
      require validation by healthcare professionals. This review also examined global 
      chatbot adoption in healthcare, emphasizing the need for user awareness to ensure 
      patient safety. Chatbot technology holds great promise in OBGYN and medicine, 
      offering innovative solutions while necessitating responsible integration to 
      ensure patient care and safety.
FAU - Lee, YooKyung
AU  - Lee Y
AD  - Division of Maternal Fetal Medicine, Department of Obstetrics and Gynecology, 
      MizMedi Hospital, Seoul, Korea.
FAU - Kim, So Yun
AU  - Kim SY
AD  - Division of Maternal Fetal Medicine, Department of Obstetrics and Gynecology, 
      MizMedi Hospital, Seoul, Korea.
LA  - eng
PT  - Journal Article
DEP - 20240122
PL  - Korea (South)
TA  - Obstet Gynecol Sci
JT  - Obstetrics &amp; gynecology science
JID - 101602614
PMC - PMC10948210
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Chatbots
OT  - Deep learning
OT  - Natural language processing
OT  - Obstetrics
COIS- Conflict of interest No potential conflict of interest relevant to this article 
      was reported.
EDAT- 2024/01/22 06:42
MHDA- 2024/01/22 06:43
PMCR- 2024/03/01
CRDT- 2024/01/22 01:02
PHST- 2023/09/21 00:00 [received]
PHST- 2023/11/29 00:00 [accepted]
PHST- 2024/01/22 06:43 [medline]
PHST- 2024/01/22 06:42 [pubmed]
PHST- 2024/01/22 01:02 [entrez]
PHST- 2024/03/01 00:00 [pmc-release]
AID - ogs.23231 [pii]
AID - ogs-23231 [pii]
AID - 10.5468/ogs.23231 [doi]
PST - ppublish
SO  - Obstet Gynecol Sci. 2024 Mar;67(2):153-159. doi: 10.5468/ogs.23231. Epub 2024 Jan 
      22.

PMID- 38167988
OWN - NLM
STAT- MEDLINE
DCOM- 20240105
LR  - 20240106
IS  - 2045-2322 (Electronic)
IS  - 2045-2322 (Linking)
VI  - 14
IP  - 1
DP  - 2024 Jan 2
TI  - Evaluation of the reliability and readability of ChatGPT-4 responses regarding 
      hypothyroidism during pregnancy.
PG  - 243
LID - 10.1038/s41598-023-50884-w [doi]
LID - 243
AB  - Hypothyroidism is characterized by thyroid hormone deficiency and has adverse 
      effects on both pregnancy and fetal health. Chat Generative Pre-trained 
      Transformer (ChatGPT) is a large language model trained with a very large 
      database from many sources. Our study was aimed to evaluate the reliability and 
      readability of ChatGPT-4 answers about hypothyroidism in pregnancy. A total of 19 
      questions were created in line with the recommendations in the latest guideline 
      of the American Thyroid Association (ATA) on hypothyroidism in pregnancy and were 
      asked to ChatGPT-4. The reliability and quality of the responses were scored by 
      two independent researchers using the global quality scale (GQS) and modified 
      DISCERN tools. The readability of ChatGPT was assessed used Flesch Reading Ease 
      (FRE) Score, Flesch-Kincaid grade level (FKGL), Gunning Fog Index (GFI), 
      Coleman-Liau Index (CLI), and Simple Measure of Gobbledygook (SMOG) tools. No 
      misleading information was found in any of the answers. The mean mDISCERN score 
      of the responses was 30.26 ± 3.14; the median GQS score was 4 (2-4). In terms of 
      reliability, most of the answers showed moderate (78.9%) followed by good (21.1%) 
      reliability. In the readability analysis, the median FRE was 32.20 (13.00-37.10). 
      The years of education required to read the answers were mostly found at the 
      university level [9 (47.3%)]. Although ChatGPT-4 has significant potential, it 
      can be used as an auxiliary information source for counseling by creating a 
      bridge between patients and clinicians about hypothyroidism in pregnancy. Efforts 
      should be made to improve the reliability and readability of ChatGPT.
CI  - © 2024. The Author(s).
FAU - Onder, C E
AU  - Onder CE
AUID- ORCID: 0000-0002-0293-2309
AD  - Department of Endocrinology and Metabolic Diseases, Ankara Training and Research 
      Hospital, Ankara, Turkey. drcagatayonder@gmail.com.
FAU - Koc, G
AU  - Koc G
AUID- ORCID: 0000-0002-8512-4816
AD  - Department of Endocrinology and Metabolic Diseases, Ankara Training and Research 
      Hospital, Ankara, Turkey.
FAU - Gokbulut, P
AU  - Gokbulut P
AUID- ORCID: 0000-0002-6287-7655
AD  - Department of Endocrinology and Metabolic Diseases, Ankara Training and Research 
      Hospital, Ankara, Turkey.
FAU - Taskaldiran, I
AU  - Taskaldiran I
AUID- ORCID: 0000-0002-1390-7571
AD  - Department of Endocrinology and Metabolic Diseases, Ankara Training and Research 
      Hospital, Ankara, Turkey.
FAU - Kuskonmaz, S M
AU  - Kuskonmaz SM
AUID- ORCID: 0000-0002-2602-1657
AD  - Department of Endocrinology and Metabolic Diseases, Ankara Training and Research 
      Hospital, Ankara, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20240102
PL  - England
TA  - Sci Rep
JT  - Scientific reports
JID - 101563288
RN  - EC 2.3.1.6 (Choline O-Acetyltransferase)
SB  - IM
MH  - Humans
MH  - United States
MH  - Pregnancy
MH  - Female
MH  - *Health Literacy
MH  - Comprehension
MH  - Reproducibility of Results
MH  - Reading
MH  - Choline O-Acetyltransferase
MH  - *Hypothyroidism
MH  - Internet
PMC - PMC10761760
COIS- The authors declare no competing interests.
EDAT- 2024/01/04 01:18
MHDA- 2024/01/05 06:43
PMCR- 2024/01/02
CRDT- 2024/01/03 10:23
PHST- 2023/09/21 00:00 [received]
PHST- 2023/12/27 00:00 [accepted]
PHST- 2024/01/05 06:43 [medline]
PHST- 2024/01/04 01:18 [pubmed]
PHST- 2024/01/03 10:23 [entrez]
PHST- 2024/01/02 00:00 [pmc-release]
AID - 10.1038/s41598-023-50884-w [pii]
AID - 50884 [pii]
AID - 10.1038/s41598-023-50884-w [doi]
PST - epublish
SO  - Sci Rep. 2024 Jan 2;14(1):243. doi: 10.1038/s41598-023-50884-w.

PMID- 37705958
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230916
IS  - 2589-0042 (Electronic)
IS  - 2589-0042 (Linking)
VI  - 26
IP  - 9
DP  - 2023 Sep 15
TI  - A descriptive study based on the comparison of ChatGPT and evidence-based 
      neurosurgeons.
PG  - 107590
LID - 10.1016/j.isci.2023.107590 [doi]
LID - 107590
AB  - ChatGPT is an artificial intelligence product developed by OpenAI. This study 
      aims to investigate whether ChatGPT can respond in accordance with evidence-based 
      medicine in neurosurgery. We generated 50 neurosurgical questions covering 
      neurosurgical diseases. Each question was posed three times to GPT-3.5 and 
      GPT-4.0. We also recruited three neurosurgeons with high, middle, and low 
      seniority to respond to questions. The results were analyzed regarding ChatGPT's 
      overall performance score, mean scores by the items' specialty classification, 
      and question type. In conclusion, GPT-3.5's ability to respond in accordance with 
      evidence-based medicine was comparable to that of neurosurgeons with low 
      seniority, and GPT-4.0's ability was comparable to that of neurosurgeons with 
      high seniority. Although ChatGPT is yet to be comparable to a neurosurgeon with 
      high seniority, future upgrades could enhance its performance and abilities.
CI  - © 2023 The Authors.
FAU - Liu, Jiayu
AU  - Liu J
AD  - Department of Neurosurgery, the First Medical Centre, Chinese PLA General 
      Hospital, Beijing 100853, China.
FAU - Zheng, Jiqi
AU  - Zheng J
AD  - School of Health Humanities, Peking University, Beijing 100191, China.
FAU - Cai, Xintian
AU  - Cai X
AD  - Department of Graduate School, Xinjiang Medical University, Urumqi 830001, China.
FAU - Wu, Dongdong
AU  - Wu D
AD  - Department of Information, Daping Hospital, Army Medical University, Chongqing 
      400042, China.
FAU - Yin, Chengliang
AU  - Yin C
AD  - Faculty of Medicine, Macau University of Science and Technology, Macau 999078, 
      China.
LA  - eng
PT  - Journal Article
DEP - 20230809
PL  - United States
TA  - iScience
JT  - iScience
JID - 101724038
PMC - PMC10495632
OTO - NOTNLM
OT  - Artificial intelligence applications
OT  - Health informatics
OT  - Neurology
OT  - Neurosurgery
COIS- The authors declare no competing interests.
EDAT- 2023/09/14 06:42
MHDA- 2023/09/14 06:43
PMCR- 2023/08/09
CRDT- 2023/09/14 04:10
PHST- 2023/05/31 00:00 [received]
PHST- 2023/06/21 00:00 [revised]
PHST- 2023/08/04 00:00 [accepted]
PHST- 2023/09/14 06:43 [medline]
PHST- 2023/09/14 06:42 [pubmed]
PHST- 2023/09/14 04:10 [entrez]
PHST- 2023/08/09 00:00 [pmc-release]
AID - S2589-0042(23)01667-X [pii]
AID - 107590 [pii]
AID - 10.1016/j.isci.2023.107590 [doi]
PST - epublish
SO  - iScience. 2023 Aug 9;26(9):107590. doi: 10.1016/j.isci.2023.107590. eCollection 
      2023 Sep 15.

PMID- 37193434
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230519
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 4
DP  - 2023 Apr
TI  - Synchronous Bilateral Breast Cancer: A Case Report Piloting and Evaluating the 
      Implementation of the AI-Powered Large Language Model (LLM) ChatGPT.
PG  - e37587
LID - 10.7759/cureus.37587 [doi]
LID - e37587
AB  - Primary breast carcinoma is the most common cancer type in women, and although 
      bilateral synchronous breast cancers (s-BBC) remain quite rare, the reported 
      incidence may increase with the adoption of more sensitive imaging 
      modalities.&nbsp;Here, we present a case of histomorphological&nbsp;and clinically distinct 
      s-BBC, together with a discussion of clinical management decisions, prognosis, 
      and treatment standards&nbsp;and how these relate to outcomes vis-à-vis more 
      established standards in unifocal breast carcinoma.&nbsp;The case report also 
      constitutes a pilot and formal evaluation of a large language model (LLM) of 
      ChatGPT as a tool to aid in generating a single patient case report.
CI  - Copyright © 2023, Naik et al.
FAU - Naik, Himani R
AU  - Naik HR
AD  - Gundersen Medical Foundation, Gundersen Lutheran Medical Center, La Crosse, USA.
FAU - Prather, Andrew D
AU  - Prather AD
AD  - Radiology, Gundersen Lutheran Medical Center, La Crosse, USA.
FAU - Gurda, Grzegorz T
AU  - Gurda GT
AD  - Biology, University of Wisconsin-La Crosse, La Crosse, USA.
AD  - Pathology, Gundersen Lutheran Medical Center, La Crosse, USA.
LA  - eng
PT  - Case Reports
DEP - 20230414
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10183235
OTO - NOTNLM
OT  - bilateral
OT  - breast cancer
OT  - breast cancer risk
OT  - chatgpt
OT  - chatgpt aided case report
OT  - ductal carcinoma
OT  - multifocal
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/05/17 01:07
MHDA- 2023/05/17 01:08
PMCR- 2023/04/14
CRDT- 2023/05/16 21:24
PHST- 2023/04/07 00:00 [accepted]
PHST- 2023/05/17 01:08 [medline]
PHST- 2023/05/17 01:07 [pubmed]
PHST- 2023/05/16 21:24 [entrez]
PHST- 2023/04/14 00:00 [pmc-release]
AID - 10.7759/cureus.37587 [doi]
PST - epublish
SO  - Cureus. 2023 Apr 14;15(4):e37587. doi: 10.7759/cureus.37587. eCollection 2023 
      Apr.

PMID- 38059514
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20240209
LR  - 20240307
IS  - 1619-3997 (Electronic)
IS  - 0300-5577 (Linking)
VI  - 52
IP  - 2
DP  - 2024 Feb 26
TI  - ChatGPT, artificial intelligence and the Journal of Perinatal Medicine: 
      correspondence.
PG  - 246
LID - 10.1515/jpm-2023-0314 [doi]
FAU - Kleebayoon, Amnuay
AU  - Kleebayoon A
AUID- ORCID: 0000-0002-1976-2393
AD  - Private Academic Consultant, Samraong, Cambodia.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AD  - Dr DY Patil Vidhyapeeth, Pune, India.
AD  - Joesph Ayobabalola University, Ikeji-Arakeji, Nigeria.
LA  - eng
PT  - Letter
DEP - 20231208
PL  - Germany
TA  - J Perinat Med
JT  - Journal of perinatal medicine
JID - 0361031
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - ethics
OT  - intelligence
EDAT- 2023/12/07 12:42
MHDA- 2024/02/09 06:42
CRDT- 2023/12/07 07:50
PHST- 2023/08/03 00:00 [received]
PHST- 2023/10/05 00:00 [accepted]
PHST- 2024/02/09 06:42 [medline]
PHST- 2023/12/07 12:42 [pubmed]
PHST- 2023/12/07 07:50 [entrez]
AID - jpm-2023-0314 [pii]
AID - 10.1515/jpm-2023-0314 [doi]
PST - epublish
SO  - J Perinat Med. 2023 Dec 8;52(2):246. doi: 10.1515/jpm-2023-0314. Print 2024 Feb 
      26.
</pre>
      
    </div>
  </main>


  
  


  


</body></html>