<!DOCTYPE html>
<!-- saved from url=(0075)https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&page=5&format=pubmed&size=200 -->
<html lang="en"><head itemscope="" itemtype="http://schema.org/WebPage" prefix="og: http://ogp.me/ns#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Mobile properties -->
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov/">
  <link rel="preconnect" href="https://www.ncbi.nlm.nih.gov/">
  <link rel="preconnect" href="https://www.google-analytics.com/">

  
  
    <link rel="stylesheet" href="./page5_PubMed_files/output.5ecf62baa0fa.css" type="text/css">
  

  <link rel="stylesheet" href="./page5_PubMed_files/output.452c70ce66f7.css" type="text/css">

  
    
  

  
    <link rel="stylesheet" href="./page5_PubMed_files/output.97c300a159d1.css" type="text/css">
  

  


    <title>chatGPT - Search Results - PubMed</title>

  
  
  <!-- Favicons -->
  <link rel="shortcut icon" type="image/ico" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico">
  <link rel="icon" type="image/png" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.png">

  <!-- 192x192, as recommended for Android
  http://updates.html5rocks.com/2014/11/Support-for-theme-color-in-Chrome-39-for-Android
  -->
  <link rel="icon" type="image/png" sizes="192x192" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-192.png">

  <!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
  <link rel="apple-touch-icon-precomposed" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-57.png">
  <!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-72.png">
  <!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-114.png">
  <!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-144.png">


  <!-- For Pinger + Google Optimize integration (NS-820) -->
  <meta name="ncbi_sg_optimize_id" content="">

  <!-- Mobile browser address bar color -->
  <meta name="theme-color" content="#20558a">

  <!-- Preserve the Referrer when going from HTTPS to HTTP -->
  <meta name="referrer" content="origin-when-cross-origin">

  <meta name="ncbi_pinger_gtm_track" content="true">
<!-- Logging params: Pinger defaults -->

  
    <meta name="ncbi_app" content="pubmed">
  

  
    <meta name="ncbi_db" content="pubmed">
  

  
    <meta name="ncbi_phid" content="3E2900011AAE9EB500002E009F6CCE8E.1.m_7">
  

  
    <meta name="ncbi_pinger_stat_url" content="https://www.ncbi.nlm.nih.gov/stat">
  

  
    <meta name="log_category" content="literature">
  

  
    <meta name="ncbi_cost_center" content="pubmed">
  



  <!-- Logging params: Pinger custom -->
  
    <meta name="log_op" content="search">
  
    <meta name="log_query" content="chatGPT">
  
    <meta name="ncbi_pdid" content="searchresult">
  
    <meta name="ncbi_pageno" content="5">
  
    <meta name="log_resultcount" content="2844">
  
    <meta name="log_userterm" content="chatGPT">
  
    <meta name="log_processedquery" content="&quot;chatGPT&quot;[All Fields]">
  
    <meta name="log_filtersactive" content="False">
  
    <meta name="log_filters" content="">
  
    <meta name="ncbi_log_query" content="chatGPT">
  
    <meta name="log_proximity_search_active" content="False">
  
    <meta name="log_format" content="pubmed">
  
    <meta name="log_sortorder" content="relevance">
  
    <meta name="log_pagesize" content="200">
  
    <meta name="log_displayeduids" content="37967485,37795257,37276372,36865204,37811040,37289368,38530460,37902835,37625909,38143345,38495601,37540292,38421392,37828840,37823044,37271213,38507847,38091727,38027458,37945414,38188865,37432530,38500300,37994939,38050835,37645039,38354748,38382637,38526538,38103973,37973369,37476960,38352437,37263804,37802673,37855875,38371667,37567084,38565486,38506966,38283041,38111835,37727444,37207953,37879994,37394880,37410015,37073200,38049136,37833131,37332004,38145370,38457428,37428337,38192545,37356806,38277743,38242810,37945282,37535043,38237025,37538021,37575206,38499197,38197146,37061037,38483582,37537126,37315798,37267643,37151080,38029791,37809772,37790062,36812645,37650428,38499716,37888068,37073184,37059815,37263382,38436756,38379623,38236632,37143631,38199604,38115345,37295794,37444647,38198321,37071282,36950398,38531015,37944140,37458761,37122982,37932006,37434733,37746411,37720897,37609022,38419470,38028967,37634646,37183102,37155932,37124601,37091303,37377631,37283095,37254022,37567487,38373482,38231545,38187653,37218530,38359704,38362238,37886525,38524820,38354135,37501529,38015298,37679503,38493370,37986970,37707884,37515957,38510401,38319707,38446541,37573231,38044929,38196696,37604703,37500572,37024324,36808255,38523987,38452222,38162955,38024047,37394309,38559461,37356702,38375341,37407364,37806782,37062612,38433764,38304112,37796786,38433238,38545872,37843604,38318564,37857839,38482764,36926868,37485215,38248771,37349973,37305993,38238101,37399881,36907556,38450060,38466153,38353690,37823416,37589944,38103488,37660470,37467871,37931432,37277096,37758604,37099373,38354991,38043471,37017291,38448291,36943179,37578819,38235988,37903836,38184777,37901112,37025464,38290286,38146711,37603843,38441078,37811189,37767452,38235987,37937071,37485676,38064244,37181985">
  
    <meta name="ncbi_search_id" content="R3Cs2GC7H0x0KE8U7UASSA:deef7cbfe7e81e7152f8e0e261e5ab1a">
  
    <meta name="ncbi_adj_nav_search_id" content="4s-DWD3awOJd5fLbaadbGg:fbd847f89c96214ac23b7c62d7cb721a">
  



  <!-- Social meta tags for unfurling urls -->
  
<meta name="description" content="chatGPT - Search Results - PubMed"><meta name="robots" content="noindex,follow,noarchive"><meta property="og:title" content="chatGPT - Search Results - PubMed"><meta property="og:url" content="https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&amp;page=5&amp;format=pubmed&amp;size=200"><meta property="og:description" content="chatGPT - Search Results - PubMed"><meta property="og:image" content="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/pubmed-meta-image-v2.jpg"><meta property="og:image:secure_url" content="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/pubmed-meta-image-v2.jpg"><meta property="og:type" content="website"><meta property="og:site_name" content="PubMed"><meta name="twitter:domain" content="pubmed.ncbi.nlm.nih.gov"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="chatGPT - Search Results - PubMed"><meta name="twitter:url" content="https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&amp;page=5&amp;format=pubmed&amp;size=200"><meta name="twitter:description" content="chatGPT - Search Results - PubMed"><meta name="twitter:image" content="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/pubmed-meta-image-v2.jpg">


  <!-- OpenSearch XML -->
  <link rel="search" type="application/opensearchdescription+xml" href="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/opensearch.xml" title="PubMed search">

  <!-- Disables severely broken elements when no JS -->
  <noscript>
    <link rel="stylesheet" type="text/css" href="https://cdn.ncbi.nlm.nih.gov/pubmed/09ad9aad-98d9-47ec-b2ea-fb4dba3d550d/core/no-script.css">
  </noscript>

  
    <link rel="canonical" href="https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&amp;page=5&amp;format=pubmed&amp;size=200">
  


</head>
<body>

  
  <main class="search-page" id="search-page">
    <div class="search-results" id="search-results">
      
        <pre class="search-results-chunk">PMID- 37967485
OWN - NLM
STAT- MEDLINE
DCOM- 20231201
LR  - 20231216
IS  - 1532-8171 (Electronic)
IS  - 0735-6757 (Linking)
VI  - 75
DP  - 2024 Jan
TI  - Performance of Google bard and ChatGPT in mass casualty incidents triage.
PG  - 72-78
LID - S0735-6757(23)00576-4 [pii]
LID - 10.1016/j.ajem.2023.10.034 [doi]
AB  - AIM: The objective of our research is to evaluate and compare the performance of 
      ChatGPT, Google Bard, and medical students in performing START triage during mass 
      casualty situations. METHOD: We conducted a cross-sectional analysis to compare 
      ChatGPT, Google Bard, and medical students in mass casualty incident (MCI) triage 
      using the Simple Triage And Rapid Treatment (START) method. A validated 
      questionnaire with 15 diverse MCI scenarios was used to assess triage accuracy 
      and content analysis in four categories: "Walking wounded," "Respiration," 
      "Perfusion," and "Mental Status." Statistical analysis compared the results. 
      RESULT: Google Bard demonstrated a notably higher accuracy of 60%, while ChatGPT 
      achieved an accuracy of 26.67% (p&nbsp;=&nbsp;0.002). Comparatively, medical students 
      performed at an accuracy rate of 64.3% in a previous study. However, there was no 
      significant difference observed between Google Bard and medical students 
      (p&nbsp;=&nbsp;0.211). Qualitative content analysis of 'walking-wounded', 'respiration', 
      'perfusion', and 'mental status' indicated that Google Bard outperformed ChatGPT. 
      CONCLUSION: Google Bard was found to be superior to ChatGPT in correctly 
      performing mass casualty incident triage. Google Bard achieved an accuracy of 
      60%, while chatGPT only achieved an accuracy of 26.67%. This difference was 
      statistically significant (p&nbsp;=&nbsp;0.002).
CI  - Copyright © 2023 The Authors. Published by Elsevier Inc. All rights reserved.
FAU - Gan, Rick Kye
AU  - Gan RK
AD  - Unit for Research in Emergency and Disaster, Faculty of Medicine and Health 
      Sciences, University of Oviedo, Oviedo 33006, Spain. Electronic address: 
      ganrick@uniovi.es.
FAU - Ogbodo, Jude Chukwuebuka
AU  - Ogbodo JC
AD  - Unit for Research in Emergency and Disaster, Faculty of Medicine and Health 
      Sciences, University of Oviedo, Oviedo 33006, Spain; Department of Primary Care 
      and Population Health, Medical School, University of Nicosia, Nicosia 2408, 
      Cyprus.
FAU - Wee, Yong Zheng
AU  - Wee YZ
AD  - Faculty of Computing &amp; Informatics, Multimedia University, 63100 Cyberjaya, 
      Selangor, Malaysia.
FAU - Gan, Ann Zee
AU  - Gan AZ
AD  - Tenghilan Health Clinic, Tuaran 89208, Sabah, Malaysia; Hospital Universiti Sains 
      Malaysia, 16150 Kota Bharu, Malaysia.
FAU - González, Pedro Arcos
AU  - González PA
AD  - Unit for Research in Emergency and Disaster, Faculty of Medicine and Health 
      Sciences, University of Oviedo, Oviedo 33006, Spain.
LA  - eng
PT  - Journal Article
DEP - 20231029
PL  - United States
TA  - Am J Emerg Med
JT  - The American journal of emergency medicine
JID - 8309942
SB  - IM
MH  - Humans
MH  - *Triage/methods
MH  - *Mass Casualty Incidents
MH  - Cross-Sectional Studies
MH  - Search Engine
MH  - Computer Simulation
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Disaster medicine
OT  - Mass casualty incident
OT  - Triage
COIS- Declaration of Competing Interest All authors declared no conflict of interest.
EDAT- 2023/11/15 18:42
MHDA- 2023/12/01 06:43
CRDT- 2023/11/15 18:00
PHST- 2023/08/20 00:00 [received]
PHST- 2023/10/03 00:00 [revised]
PHST- 2023/10/24 00:00 [accepted]
PHST- 2023/12/01 06:43 [medline]
PHST- 2023/11/15 18:42 [pubmed]
PHST- 2023/11/15 18:00 [entrez]
AID - S0735-6757(23)00576-4 [pii]
AID - 10.1016/j.ajem.2023.10.034 [doi]
PST - ppublish
SO  - Am J Emerg Med. 2024 Jan;75:72-78. doi: 10.1016/j.ajem.2023.10.034. Epub 2023 Oct 
      29.

PMID- 37795257
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231006
IS  - 2631-4797 (Electronic)
IS  - 2631-4797 (Linking)
VI  - 5
DP  - 2023
TI  - Comparing the Efficacy of Large Language Models ChatGPT, BARD, and Bing AI in 
      Providing Information on Rhinoplasty: An Observational Study.
PG  - ojad084
LID - 10.1093/asjof/ojad084 [doi]
LID - ojad084
AB  - BACKGROUND: Large language models (LLMs) are emerging artificial intelligence 
      (AI) technologies refining research and healthcare. However, the impact of these 
      models on presurgical planning and education remains under-explored. OBJECTIVES: 
      This study aims to assess 3 prominent LLMs-Google's AI BARD (Mountain View, CA), 
      Bing AI (Microsoft, Redmond, WA), and ChatGPT-3.5 (Open AI, San Francisco, CA) in 
      providing safe medical information for rhinoplasty. METHODS: Six questions 
      regarding rhinoplasty were prompted to ChatGPT, BARD, and Bing AI. A Likert scale 
      was used to evaluate these responses by a panel of Specialist Plastic and 
      Reconstructive Surgeons with extensive experience in rhinoplasty. To measure 
      reliability, the Flesch Reading Ease Score, the Flesch-Kincaid Grade Level, and 
      the Coleman-Liau Index were used. The modified DISCERN score was chosen as the 
      criterion for assessing suitability and reliability. A t test was performed to 
      calculate the difference between the LLMs, and a double-sided P-value &lt;.05 was 
      considered statistically significant. RESULTS: In terms of reliability, BARD and 
      ChatGPT demonstrated a significantly (P &lt; .05) greater Flesch Reading Ease Score 
      of 47.47 (±15.32) and 37.68 (±12.96), Flesch-Kincaid Grade Level of 9.7 (±3.12) 
      and 10.15 (±1.84), and a Coleman-Liau Index of 10.83 (±2.14) and 12.17 (±1.17) 
      than Bing AI. In terms of suitability, BARD (46.3 ± 2.8) demonstrated a 
      significantly greater DISCERN score than ChatGPT and Bing AI. In terms of Likert 
      score, ChatGPT and BARD demonstrated similar scores and were greater than Bing 
      AI. CONCLUSIONS: BARD delivered the most succinct and comprehensible information, 
      followed by ChatGPT and Bing AI. Although these models demonstrate potential, 
      challenges regarding their depth and specificity remain. Therefore, future 
      research should aim to augment LLM performance through the integration of 
      specialized databases and expert knowledge, while also refining their algorithms.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of The 
      Aesthetic Society.
FAU - Seth, Ishith
AU  - Seth I
AUID- ORCID: 0000-0001-5444-8925
FAU - Lim, Bryan
AU  - Lim B
FAU - Xie, Yi
AU  - Xie Y
FAU - Cevik, Jevan
AU  - Cevik J
FAU - Rozen, Warren M
AU  - Rozen WM
FAU - Ross, Richard J
AU  - Ross RJ
FAU - Lee, Mathew
AU  - Lee M
LA  - eng
PT  - Journal Article
DEP - 20230914
PL  - England
TA  - Aesthet Surg J Open Forum
JT  - Aesthetic surgery journal. Open forum
JID - 101771713
PMC - PMC10547367
EDAT- 2023/10/05 06:44
MHDA- 2023/10/05 06:45
PMCR- 2023/09/14
CRDT- 2023/10/05 04:08
PHST- 2023/10/05 06:45 [medline]
PHST- 2023/10/05 06:44 [pubmed]
PHST- 2023/10/05 04:08 [entrez]
PHST- 2023/09/14 00:00 [pmc-release]
AID - ojad084 [pii]
AID - 10.1093/asjof/ojad084 [doi]
PST - epublish
SO  - Aesthet Surg J Open Forum. 2023 Sep 14;5:ojad084. doi: 10.1093/asjof/ojad084. 
      eCollection 2023.

PMID- 37276372
OWN - NLM
STAT- MEDLINE
DCOM- 20230622
LR  - 20230625
IS  - 2352-0787 (Electronic)
IS  - 2352-0779 (Linking)
VI  - 10
IP  - 4
DP  - 2023 Jul
TI  - New Artificial Intelligence ChatGPT Performs Poorly on the 2022 Self-assessment 
      Study Program for Urology.
PG  - 409-415
LID - 10.1097/UPJ.0000000000000406 [doi]
AB  - INTRODUCTION: Large language models have demonstrated impressive capabilities, 
      but application to medicine remains unclear. We seek to evaluate the use of 
      ChatGPT on the American Urological Association Self-assessment Study Program as 
      an educational adjunct for urology trainees and practicing physicians. METHODS: 
      One hundred fifty questions from the 2022 Self-assessment Study Program exam were 
      screened, and those containing visual assets (n=15) were removed. The remaining 
      items were encoded as open ended or multiple choice. ChatGPT's output was coded 
      as correct, incorrect, or indeterminate; if indeterminate, responses were 
      regenerated up to 2 times. Concordance, quality, and accuracy were ascertained by 
      3 independent researchers and reviewed by 2 physician adjudicators. A new session 
      was started for each entry to avoid crossover learning. RESULTS: ChatGPT was 
      correct on 36/135 (26.7%) open-ended and 38/135 (28.2%) multiple-choice 
      questions. Indeterminate responses were generated in 40 (29.6%) and 4 (3.0%), 
      respectively. Of the correct responses, 24/36 (66.7%) and 36/38 (94.7%) were on 
      initial output, 8 (22.2%) and 1 (2.6%) on second output, and 4 (11.1%) and 1 
      (2.6%) on final output, respectively. Although regeneration decreased 
      indeterminate responses, proportion of correct responses did not increase. For 
      open-ended and multiple-choice questions, ChatGPT provided consistent 
      justifications for incorrect answers and remained concordant between correct and 
      incorrect answers. CONCLUSIONS: ChatGPT previously demonstrated promise on 
      medical licensing exams; however, application to the 2022 Self-assessment Study 
      Program was not demonstrated. Performance improved with multiple-choice over 
      open-ended questions. More importantly were the persistent justifications for 
      incorrect responses-left unchecked, utilization of ChatGPT in medicine may 
      facilitate medical misinformation.
FAU - Huynh, Linda My
AU  - Huynh LM
AUID- ORCID: 0000-0002-8655-8587
AD  - MD/PhD Scholars Program, University of Nebraska Medical Center, Omaha, Nebraska.
FAU - Bonebrake, Benjamin T
AU  - Bonebrake BT
AD  - College of Medicine, University of Nebraska Medical Center, Omaha, Nebraska.
FAU - Schultis, Kaitlyn
AU  - Schultis K
AD  - College of Medicine, University of Nebraska Medical Center, Omaha, Nebraska.
FAU - Quach, Alan
AU  - Quach A
AD  - Division of Urology, University of Nebraska Medical Center, Omaha, Nebraska.
FAU - Deibert, Christopher M
AU  - Deibert CM
AD  - Division of Urology, University of Nebraska Medical Center, Omaha, Nebraska.
LA  - eng
PT  - Journal Article
DEP - 20230605
PL  - United States
TA  - Urol Pract
JT  - Urology practice
JID - 101635343
SB  - IM
MH  - *Urology
MH  - Artificial Intelligence
MH  - Self-Assessment
MH  - *Medicine
MH  - Educational Status
OTO - NOTNLM
OT  - artificial intelligence
OT  - medical informatics applications
OT  - urology
EDAT- 2023/06/05 19:10
MHDA- 2023/06/22 06:42
CRDT- 2023/06/05 15:03
PHST- 2023/06/22 06:42 [medline]
PHST- 2023/06/05 19:10 [pubmed]
PHST- 2023/06/05 15:03 [entrez]
AID - 10.1097/UPJ.0000000000000406 [doi]
PST - ppublish
SO  - Urol Pract. 2023 Jul;10(4):409-415. doi: 10.1097/UPJ.0000000000000406. Epub 2023 
      Jun 5.

PMID- 36865204
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230904
DP  - 2023 Feb 26
TI  - Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow.
LID - 2023.02.21.23285886 [pii]
LID - 10.1101/2023.02.21.23285886 [doi]
AB  - IMPORTANCE: Large language model (LLM) artificial intelligence (AI) chatbots 
      direct the power of large training datasets towards successive, related tasks, as 
      opposed to single-ask tasks, for which AI already achieves impressive 
      performance. The capacity of LLMs to assist in the full scope of iterative 
      clinical reasoning via successive prompting, in effect acting as virtual 
      physicians, has not yet been evaluated. OBJECTIVE: To evaluate ChatGPT's capacity 
      for ongoing clinical decision support via its performance on standardized 
      clinical vignettes. DESIGN: We inputted all 36 published clinical vignettes from 
      the Merck Sharpe &amp; Dohme (MSD) Clinical Manual into ChatGPT and compared accuracy 
      on differential diagnoses, diagnostic testing, final diagnosis, and management 
      based on patient age, gender, and case acuity. SETTING: ChatGPT, a publicly 
      available LLM. PARTICIPANTS: Clinical vignettes featured hypothetical patients 
      with a variety of age and gender identities, and a range of Emergency Severity 
      Indices (ESIs) based on initial clinical presentation. EXPOSURES: MSD Clinical 
      Manual vignettes. MAIN OUTCOMES AND MEASURES: We measured the proportion of 
      correct responses to the questions posed within the clinical vignettes tested. 
      RESULTS: ChatGPT achieved 71.7% (95% CI, 69.3% to 74.1%) accuracy overall across 
      all 36 clinical vignettes. The LLM demonstrated the highest performance in making 
      a final diagnosis with an accuracy of 76.9% (95% CI, 67.8% to 86.1%), and the 
      lowest performance in generating an initial differential diagnosis with an 
      accuracy of 60.3% (95% CI, 54.2% to 66.6%). Compared to answering questions about 
      general medical knowledge, ChatGPT demonstrated inferior performance on 
      differential diagnosis (β=-15.8%, p&lt;0.001) and clinical management (β=-7.4%, 
      p=0.02) type questions. CONCLUSIONS AND RELEVANCE: ChatGPT achieves impressive 
      accuracy in clinical decision making, with particular strengths emerging as it 
      has more clinical information at its disposal.
FAU - Rao, Arya
AU  - Rao A
AUID- ORCID: 0000-0003-3007-4812
FAU - Pang, Michael
AU  - Pang M
FAU - Kim, John
AU  - Kim J
FAU - Kamineni, Meghana
AU  - Kamineni M
FAU - Lie, Winston
AU  - Lie W
FAU - Prasad, Anoop K
AU  - Prasad AK
FAU - Landman, Adam
AU  - Landman A
FAU - Dreyer, Keith J
AU  - Dreyer KJ
FAU - Succi, Marc D
AU  - Succi MD
AUID- ORCID: 0000-0002-1518-3984
LA  - eng
PT  - Preprint
DEP - 20230226
PL  - United States
TA  - medRxiv
JT  - medRxiv : the preprint server for health sciences
JID - 101767986
UIN - J Med Internet Res. 2023 Aug 22;25:e48659. PMID: 37606976
PMC - PMC9980239
EDAT- 2023/03/04 06:00
MHDA- 2023/03/04 06:01
PMCR- 2023/03/02
CRDT- 2023/03/03 02:28
PHST- 2023/03/03 02:28 [entrez]
PHST- 2023/03/04 06:00 [pubmed]
PHST- 2023/03/04 06:01 [medline]
PHST- 2023/03/02 00:00 [pmc-release]
AID - 2023.02.21.23285886 [pii]
AID - 10.1101/2023.02.21.23285886 [doi]
PST - epublish
SO  - medRxiv [Preprint]. 2023 Feb 26:2023.02.21.23285886. doi: 
      10.1101/2023.02.21.23285886.

PMID- 37811040
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231018
IS  - 2049-0801 (Print)
IS  - 2049-0801 (Electronic)
IS  - 2049-0801 (Linking)
VI  - 85
IP  - 10
DP  - 2023 Oct
TI  - ChatGPT and artificial hallucinations in stem cell research: assessing the 
      accuracy of generated references - a preliminary study.
PG  - 5275-5278
LID - 10.1097/MS9.0000000000001228 [doi]
AB  - Stem cell research has the transformative potential to revolutionize medicine. 
      Language models like ChatGPT, which use artificial intelligence (AI) and natural 
      language processing, generate human-like text that can aid researchers. However, 
      it is vital to ensure the accuracy and reliability of AI-generated references. 
      This study assesses Chat Generative Pre-Trained Transformer (ChatGPT)'s utility 
      in stem cell research and evaluates the accuracy of its references. Of the 86 
      references analyzed, 15.12% were fabricated and 9.30% were erroneous. These 
      errors were due to limitations such as no real-time internet access and reliance 
      on preexisting data. Artificial hallucinations were also observed, where the text 
      seems plausible but deviates from fact. Monitoring, diverse training, and 
      expanding knowledge cut-off can help to reduce fabricated references and 
      hallucinations. Researchers must verify references and consider the limitations 
      of AI models. Further research is needed to enhance the accuracy of such language 
      models. Despite these challenges, ChatGPT has the potential to be a valuable tool 
      for stem cell research. It can help researchers to stay up-to-date on the latest 
      developments in the field and to find relevant information.
CI  - Copyright © 2023 The Author(s). Published by Wolters Kluwer Health, Inc.
FAU - Sharun, Khan
AU  - Sharun K
AD  - Division of Surgery.
FAU - Banu, S Amitha
AU  - Banu SA
AD  - Division of Surgery.
FAU - Pawde, Abhijit M
AU  - Pawde AM
AD  - Division of Surgery.
FAU - Kumar, Rohit
AU  - Kumar R
AD  - Division of Surgery.
FAU - Akash, Shopnil
AU  - Akash S
AD  - Department of Pharmacy, Faculty of Allied Health Science, Daffodil International 
      University, Daffodil Smart City, Ashulia, Savar, Dhaka, Bangladesh.
FAU - Dhama, Kuldeep
AU  - Dhama K
AD  - Division of Pathology, ICAR-Indian Veterinary Research Institute, Izatnagar, 
      Bareilly, Uttar Pradesh, India.
FAU - Pal, Amar
AU  - Pal A
AD  - Division of Surgery.
LA  - eng
PT  - Journal Article
DEP - 20230901
PL  - England
TA  - Ann Med Surg (Lond)
JT  - Annals of medicine and surgery (2012)
JID - 101616869
PMC - PMC10553015
OTO - NOTNLM
OT  - artificial intelligence
OT  - erroneous references
OT  - fabricated references
OT  - limitations
OT  - natural language processing
OT  - reliable knowledge
COIS- The authors declare that they have no conflicts of interest.Sponsorships or 
      competing interests that may be relevant to content are disclosed at the end of 
      this article.
EDAT- 2023/10/09 12:42
MHDA- 2023/10/09 12:43
PMCR- 2023/09/01
CRDT- 2023/10/09 06:12
PHST- 2023/06/26 00:00 [received]
PHST- 2023/08/12 00:00 [accepted]
PHST- 2023/10/09 12:43 [medline]
PHST- 2023/10/09 12:42 [pubmed]
PHST- 2023/10/09 06:12 [entrez]
PHST- 2023/09/01 00:00 [pmc-release]
AID - AMSU-D-23-01385 [pii]
AID - 10.1097/MS9.0000000000001228 [doi]
PST - epublish
SO  - Ann Med Surg (Lond). 2023 Sep 1;85(10):5275-5278. doi: 
      10.1097/MS9.0000000000001228. eCollection 2023 Oct.

PMID- 37289368
OWN - NLM
STAT- Publisher
LR  - 20231024
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 51
IP  - 11
DP  - 2023 Nov
TI  - Leveraging ChatGPT for Human Behavior Assessment: Potential Implications for 
      Mental Health Care.
PG  - 2362-2364
LID - 10.1007/s10439-023-03269-z [doi]
AB  - This letter explores the capability of AI, specifically OpenAI's ChatGPT, in 
      interpreting human behavior and its potential implications for mental health 
      care. Data were collected from the Reddit forum "AmItheAsshole" (AITA) to assess 
      the congruence between AI's verdict and the collective human opinion on this 
      platform. AITA, with its vast range of interpersonal situations, provides rich 
      insights into human behavioral evaluation and perception. Two key research 
      questions were addressed: the degree of alignment between ChatGPT's judgment and 
      collective verdicts of Redditors, and the consistency of ChatGPT in evaluating 
      the same AITA post repeatedly. The results exhibited a promising level of 
      agreement between ChatGPT and human verdicts. It also demonstrated high 
      consistency across repeated evaluations of the same posts. These findings hint at 
      the significant potential of AI in mental health care provision, underscoring the 
      importance of continued research and development in this field.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Haman, Michael
AU  - Haman M
AUID- ORCID: 0000-0001-5772-2045
AD  - Department of Humanities, Faculty of Economics and Management, Czech University 
      of Life Sciences Prague, Kamýcká 129, 165 00, Praha-Suchdol, Czech Republic. 
      haman@pef.czu.cz.
FAU - Školník, Milan
AU  - Školník M
AUID- ORCID: 0000-0002-0672-219X
AD  - Department of Humanities, Faculty of Economics and Management, Czech University 
      of Life Sciences Prague, Kamýcká 129, 165 00, Praha-Suchdol, Czech Republic.
FAU - Šubrt, Tomáš
AU  - Šubrt T
AUID- ORCID: 0000-0002-4437-1431
AD  - Department of Systems Engineering, Faculty of Economics and Management, Czech 
      University of Life Sciences Prague, Kamýcká 129, 165 00, Praha-Suchdol, Czech 
      Republic.
LA  - eng
PT  - Letter
DEP - 20230608
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
EDAT- 2023/06/08 13:07
MHDA- 2023/06/08 13:07
CRDT- 2023/06/08 11:12
PHST- 2023/05/27 00:00 [received]
PHST- 2023/05/31 00:00 [accepted]
PHST- 2023/06/08 13:07 [pubmed]
PHST- 2023/06/08 13:07 [medline]
PHST- 2023/06/08 11:12 [entrez]
AID - 10.1007/s10439-023-03269-z [pii]
AID - 10.1007/s10439-023-03269-z [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2023 Nov;51(11):2362-2364. doi: 10.1007/s10439-023-03269-z. Epub 
      2023 Jun 8.

PMID- 38530460
OWN - NLM
STAT- Publisher
LR  - 20240326
IS  - 1434-4726 (Electronic)
IS  - 0937-4477 (Linking)
DP  - 2024 Mar 26
TI  - The quality and readability of patient information provided by ChatGPT: can AI 
      reliably explain common ENT operations?
LID - 10.1007/s00405-024-08598-w [doi]
AB  - PURPOSE: Access to high-quality and comprehensible patient information is 
      crucial. However, information provided by increasingly prevalent Artificial 
      Intelligence tools has not been thoroughly investigated. This study assesses the 
      quality and readability of information from ChatGPT regarding three index ENT 
      operations: tonsillectomy, adenoidectomy, and grommets. METHODS: We asked ChatGPT 
      standard and simplified questions. Readability was calculated using 
      Flesch-Kincaid Reading Ease Score (FRES), Flesch-Kincaid Grade Level (FKGL), 
      Gunning Fog Index (GFI) and Simple Measure of Gobbledygook (SMOG) scores. We 
      assessed quality using the DISCERN instrument and compared these with ENT UK 
      patient leaflets. RESULTS: ChatGPT readability was poor, with mean FRES of 38.9 
      and 55.1 pre- and post-simplification, respectively. Simplified information from 
      ChatGPT was 43.6% more readable (FRES) but scored 11.6% lower for quality. ENT UK 
      patient information readability and quality was consistently higher. CONCLUSIONS: 
      ChatGPT can simplify information at the expense of quality, resulting in shorter 
      answers with important omissions. Limitations in knowledge and insight curb its 
      reliability for healthcare information. Patients should use reputable sources 
      from professional organisations alongside clear communication with their 
      clinicians for well-informed consent and making decisions.
CI  - © 2024. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Abou-Abdallah, Michel
AU  - Abou-Abdallah M
AUID- ORCID: 0000-0001-9782-5209
AD  - Ear, Nose and Throat Department, Luton and Dunstable University Hospital, Lewsey 
      Rd, Luton, LU4 0DZ, UK. michel.abouabdallah@nhs.net.
FAU - Dar, Talib
AU  - Dar T
AUID- ORCID: 0009-0001-5789-4709
AD  - Ear, Nose and Throat Department, Luton and Dunstable University Hospital, Lewsey 
      Rd, Luton, LU4 0DZ, UK.
FAU - Mahmudzade, Yasamin
AU  - Mahmudzade Y
AUID- ORCID: 0000-0003-3602-1025
AD  - Foundation Programme, East and North Hertfordshire NHS Trust, Stevenage, UK.
FAU - Michaels, Joshua
AU  - Michaels J
AUID- ORCID: 0000-0001-8598-3891
AD  - Ear, Nose and Throat Department, Luton and Dunstable University Hospital, Lewsey 
      Rd, Luton, LU4 0DZ, UK.
FAU - Talwar, Rishi
AU  - Talwar R
AUID- ORCID: 0000-0002-1364-3377
AD  - Ear, Nose and Throat Department, Luton and Dunstable University Hospital, Lewsey 
      Rd, Luton, LU4 0DZ, UK.
FAU - Tornari, Chrysostomos
AU  - Tornari C
AUID- ORCID: 0000-0002-1212-0590
AD  - Ear, Nose and Throat Department, Luton and Dunstable University Hospital, Lewsey 
      Rd, Luton, LU4 0DZ, UK.
LA  - eng
PT  - Journal Article
DEP - 20240326
PL  - Germany
TA  - Eur Arch Otorhinolaryngol
JT  - European archives of oto-rhino-laryngology : official journal of the European 
      Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the 
      German Society for Oto-Rhino-Laryngology - Head and Neck Surgery
JID - 9002937
SB  - IM
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - Comprehensible
OT  - ENT
OT  - Informed consent
OT  - Readability
EDAT- 2024/03/26 18:44
MHDA- 2024/03/26 18:44
CRDT- 2024/03/26 12:17
PHST- 2023/10/03 00:00 [received]
PHST- 2024/03/04 00:00 [accepted]
PHST- 2024/03/26 18:44 [medline]
PHST- 2024/03/26 18:44 [pubmed]
PHST- 2024/03/26 12:17 [entrez]
AID - 10.1007/s00405-024-08598-w [pii]
AID - 10.1007/s00405-024-08598-w [doi]
PST - aheadofprint
SO  - Eur Arch Otorhinolaryngol. 2024 Mar 26. doi: 10.1007/s00405-024-08598-w.

PMID- 37902835
OWN - NLM
STAT- MEDLINE
DCOM- 20240321
LR  - 20240321
IS  - 1438-8359 (Electronic)
IS  - 0913-8668 (Linking)
VI  - 38
IP  - 2
DP  - 2024 Apr
TI  - ChatGPT's performance on JSA-certified anesthesiologist exam.
PG  - 282-283
LID - 10.1007/s00540-023-03275-4 [doi]
FAU - Kinoshita, Michiko
AU  - Kinoshita M
AUID- ORCID: 0000-0001-8680-7654
AD  - Department of Anesthesiology, Tokushima University Hospital, 2-50-1 Kuramoto-cho, 
      Tokushima-shi, Tokushima, 770-8503, Japan. michiko-kinoshita@tokushima-u.ac.jp.
FAU - Komasaka, Mizuki
AU  - Komasaka M
AD  - Department of Anesthesiology, Tokushima University Hospital, 2-50-1 Kuramoto-cho, 
      Tokushima-shi, Tokushima, 770-8503, Japan.
FAU - Tanaka, Katsuya
AU  - Tanaka K
AD  - Department of Anesthesiology, Tokushima University Hospital, 2-50-1 Kuramoto-cho, 
      Tokushima-shi, Tokushima, 770-8503, Japan.
LA  - eng
PT  - Letter
DEP - 20231030
PL  - Japan
TA  - J Anesth
JT  - Journal of anesthesia
JID - 8905667
SB  - IM
MH  - Humans
MH  - *Anesthesiologists
OTO - NOTNLM
OT  - Anesthesiology
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Large language model
EDAT- 2023/10/30 18:42
MHDA- 2024/03/21 12:47
CRDT- 2023/10/30 12:02
PHST- 2023/05/24 00:00 [received]
PHST- 2023/10/12 00:00 [accepted]
PHST- 2024/03/21 12:47 [medline]
PHST- 2023/10/30 18:42 [pubmed]
PHST- 2023/10/30 12:02 [entrez]
AID - 10.1007/s00540-023-03275-4 [pii]
AID - 10.1007/s00540-023-03275-4 [doi]
PST - ppublish
SO  - J Anesth. 2024 Apr;38(2):282-283. doi: 10.1007/s00540-023-03275-4. Epub 2023 Oct 
      30.

PMID- 37625909
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20231103
LR  - 20231103
IS  - 1471-6771 (Electronic)
IS  - 0007-0912 (Linking)
VI  - 131
IP  - 5
DP  - 2023 Nov
TI  - ChatGPT risk of fabrication in literature searches. Comment on Br J Anaesth 2023; 
      131: e29-e30.
PG  - e172-e173
LID - S0007-0912(23)00424-5 [pii]
LID - 10.1016/j.bja.2023.07.024 [doi]
FAU - Arrivé, Lionel
AU  - Arrivé L
AD  - Department of Radiology, Saint-Antoine Hospital, Assistance Publique - Hôpitaux 
      de Paris (APHP) and Sorbonne University, Paris, France. Electronic address: 
      lionel.arrive@aphp.fr.
FAU - Minssen, Lise
AU  - Minssen L
AD  - Department of Radiology, Saint-Antoine Hospital, Assistance Publique - Hôpitaux 
      de Paris (APHP) and Sorbonne University, Paris, France.
FAU - Ali, Amal
AU  - Ali A
AD  - Department of Radiology, Saint-Antoine Hospital, Assistance Publique - Hôpitaux 
      de Paris (APHP) and Sorbonne University, Paris, France.
LA  - eng
PT  - Comment
PT  - Letter
DEP - 20230823
PL  - England
TA  - Br J Anaesth
JT  - British journal of anaesthesia
JID - 0372541
SB  - IM
CON - Br J Anaesth. 2023 Jul;131(1):e29-e30. PMID: 37183102
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - bibliography
OT  - citations
OT  - generative pretrained transformer
OT  - large language models
EDAT- 2023/08/26 05:42
MHDA- 2023/10/27 06:43
CRDT- 2023/08/25 21:56
PHST- 2023/06/17 00:00 [received]
PHST- 2023/07/14 00:00 [revised]
PHST- 2023/07/30 00:00 [accepted]
PHST- 2023/10/27 06:43 [medline]
PHST- 2023/08/26 05:42 [pubmed]
PHST- 2023/08/25 21:56 [entrez]
AID - S0007-0912(23)00424-5 [pii]
AID - 10.1016/j.bja.2023.07.024 [doi]
PST - ppublish
SO  - Br J Anaesth. 2023 Nov;131(5):e172-e173. doi: 10.1016/j.bja.2023.07.024. Epub 
      2023 Aug 23.

PMID- 38143345
OWN - NLM
STAT- MEDLINE
DCOM- 20240123
LR  - 20240221
IS  - 1875-8908 (Electronic)
IS  - 1387-2877 (Print)
IS  - 1387-2877 (Linking)
VI  - 97
IP  - 2
DP  - 2024
TI  - What Does ChatGPT Know About Dementia? A Comparative Analysis of Information 
      Quality.
PG  - 559-565
LID - 10.3233/JAD-230573 [doi]
AB  - The quality of information about dementia retrieved using ChatGPT is unknown. 
      Content was evaluated for length, readability, and quality using the QUEST, a 
      validated tool, and compared against online material from three North American 
      organizations. Both sources of information avoided conflicts of interest, 
      supported the patient-physician relationship, and used a balanced tone. Official 
      bodies but not ChatGPT referenced identifiable research and pointed to local 
      resources. Users of ChatGPT are likely to encounter accurate but shallow 
      information about dementia. Recommendations are made for information creators and 
      providers who counsel patients around digital health practices.
FAU - Dosso, Jill A
AU  - Dosso JA
AD  - Department of Medicine, Division of Neurology, The University of British 
      Columbia, Vancouver, British Columbia, Canada.
AD  - BC Children's and Women's Hospitals, Vancouver, British Columbia, Canada.
FAU - Kailley, Jaya N
AU  - Kailley JN
AD  - Department of Medicine, Division of Neurology, The University of British 
      Columbia, Vancouver, British Columbia, Canada.
AD  - BC Children's and Women's Hospitals, Vancouver, British Columbia, Canada.
FAU - Robillard, Julie M
AU  - Robillard JM
AD  - Department of Medicine, Division of Neurology, The University of British 
      Columbia, Vancouver, British Columbia, Canada.
AD  - BC Children's and Women's Hospitals, Vancouver, British Columbia, Canada.
LA  - eng
PT  - Letter
PT  - Research Support, Non-U.S. Gov't
PL  - Netherlands
TA  - J Alzheimers Dis
JT  - Journal of Alzheimer's disease : JAD
JID - 9814863
RN  - 3G6A5W338E (Caffeine)
SB  - IM
MH  - Humans
MH  - *Caffeine
MH  - Digital Health
MH  - Physician-Patient Relations
MH  - *Dementia/diagnosis
PMC - PMC10836539
OTO - NOTNLM
OT  - Alzheimer’s disease
OT  - artificial intelligence
OT  - dementia
OT  - health
COIS- JMR received grant funding from the Alzheimer’s Association, one of the three 
      national organizations studied in this manuscript, for a project on the topic of 
      social media and dementia research.
EDAT- 2023/12/25 06:43
MHDA- 2024/01/23 06:43
PMCR- 2024/02/02
CRDT- 2023/12/25 03:06
PHST- 2024/01/23 06:43 [medline]
PHST- 2023/12/25 06:43 [pubmed]
PHST- 2023/12/25 03:06 [entrez]
PHST- 2024/02/02 00:00 [pmc-release]
AID - JAD230573 [pii]
AID - 10.3233/JAD-230573 [doi]
PST - ppublish
SO  - J Alzheimers Dis. 2024;97(2):559-565. doi: 10.3233/JAD-230573.

PMID- 38495601
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240319
IS  - 2158-8333 (Print)
IS  - 2158-8341 (Electronic)
IS  - 2158-8333 (Linking)
VI  - 21
IP  - 1-3
DP  - 2024 Jan-Mar
TI  - ChatGPT and Transcranial Direct Current Stimulation for Chronic Pain.
PG  - 61-62
AB  - OBJECTIVE: We aimed to open a discussion about the integration of artificial 
      intelligence (AI) in science and clinical practice, specifically with regard to 
      the use of transcranial direct current stimulation (tDCS) as a technique for 
      managing chronic pain. MAIN POINTS OF DISCUSSION: To analyze the responses 
      generated by ChatGPT and the best literature about tDCS, we formulated three 
      questions. The answers from ChatGPT, compared to the guidelines and Cochrane 
      review, showed that AI can be a potential strategy to help clinicians and 
      researchers. AI such as ChatGPT is revolutionizing the academic field and 
      clinical practice. However, there is still an unmet scientific and clinical 
      discussion about the insertion of AI to help researchers and clinicians in the 
      neuromodulation area treat chronic pain. CONCLUSION: We need to know the limits 
      of the use of AI. Even though ChatGPT might be helpful, it should be used with 
      caution in the academic field and clinical practice.
CI  - Copyright © 2024. Matrix Medical Communications. All rights reserved.
FAU - Silva-Filho, Edson
AU  - Silva-Filho E
AD  - Both authors are with Graduate Program in Heath Science, Graduate Program in 
      Physical Therapy, Federal University of Rio Grande do Norte in Rio Grande do 
      Norte, Brazil.
FAU - Pegado, Rodrigo
AU  - Pegado R
AD  - Both authors are with Graduate Program in Heath Science, Graduate Program in 
      Physical Therapy, Federal University of Rio Grande do Norte in Rio Grande do 
      Norte, Brazil.
AD  - Dr. Pegado is additionally with Graduate Program in Rehabilitation Science, 
      Federal University of Rio Grande do Norte in Rio Grande do Norte, Brazil.
LA  - eng
PT  - Editorial
DEP - 20240301
PL  - United States
TA  - Innov Clin Neurosci
JT  - Innovations in clinical neuroscience
JID - 101549695
PMC - PMC10941863
OTO - NOTNLM
OT  - Artificial intelligence
OT  - neuromodulation
OT  - pain
OT  - transcranial direct current stimulation
COIS- DISCLOSURES: The authors have no conflicts of interest relevant to the contents 
      of this article.
EDAT- 2024/03/18 06:43
MHDA- 2024/03/18 06:44
PMCR- 2024/03/01
CRDT- 2024/03/18 04:23
PHST- 2024/03/18 06:44 [medline]
PHST- 2024/03/18 06:43 [pubmed]
PHST- 2024/03/18 04:23 [entrez]
PHST- 2024/03/01 00:00 [pmc-release]
PST - epublish
SO  - Innov Clin Neurosci. 2024 Mar 1;21(1-3):61-62. eCollection 2024 Jan-Mar.

PMID- 37540292
OWN - NLM
STAT- Publisher
LR  - 20230804
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
DP  - 2023 Aug 4
TI  - A Promising Start and Not a Panacea: ChatGPT's Early Impact and Potential in 
      Medical Science and Biomedical Engineering Research.
LID - 10.1007/s10439-023-03335-6 [doi]
AB  - The advent of artificial intelligence (AI) has catalyzed a revolutionary 
      transformation across various industries, including healthcare. Medical 
      applications of ChatGPT, a powerful language model based on the generative 
      pre-trained transformer (GPT) architecture, encompass the creation of 
      conversational agents capable of accessing and generating medical information 
      from multiple sources and formats. This study investigates the research trends of 
      large language models such as ChatGPT, GPT 4, and Google Bard, comparing their 
      publication trends with early COVID-19 research. The findings underscore the 
      current prominence of AI research and its potential implications in biomedical 
      engineering. A search of the Scopus database on July 23, 2023, yielded 1,096 
      articles related to ChatGPT, with approximately 26% being medical 
      science-related. Keywords related to artificial intelligence, natural language 
      processing (NLP), LLM, and generative AI dominate ChatGPT research, while a 
      focused representation of medical science research emerges, with emphasis on 
      biomedical research and engineering. This analysis serves as a call to action for 
      researchers, healthcare professionals, and policymakers to recognize and harness 
      AI's potential in healthcare, particularly in the realm of biomedical research.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Sohail, Shahab Saquib
AU  - Sohail SS
AUID- ORCID: 0000-0002-5944-7371
AD  - Department of Computer Science and Engineering, School of Engineering Sciences 
      and Technology, Jamia Hamdard, New Delhi, 110062, India. 
      shahabssohail@jamiahamdard.ac.in.
LA  - eng
PT  - Letter
DEP - 20230804
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
OTO - NOTNLM
OT  - Biomedical engineering
OT  - Chat GPT
OT  - Generative AI
OT  - Google Bard
OT  - LLM
EDAT- 2023/08/04 13:11
MHDA- 2023/08/04 13:11
CRDT- 2023/08/04 11:06
PHST- 2023/07/23 00:00 [received]
PHST- 2023/07/26 00:00 [accepted]
PHST- 2023/08/04 13:11 [medline]
PHST- 2023/08/04 13:11 [pubmed]
PHST- 2023/08/04 11:06 [entrez]
AID - 10.1007/s10439-023-03335-6 [pii]
AID - 10.1007/s10439-023-03335-6 [doi]
PST - aheadofprint
SO  - Ann Biomed Eng. 2023 Aug 4. doi: 10.1007/s10439-023-03335-6.

PMID- 38421392
OWN - NLM
STAT- MEDLINE
DCOM- 20240318
LR  - 20240402
IS  - 1434-4726 (Electronic)
IS  - 0937-4477 (Linking)
VI  - 281
IP  - 4
DP  - 2024 Apr
TI  - Exploring the landscape of AI-assisted decision-making in head and neck cancer 
      treatment: a comparative analysis of NCCN guidelines and ChatGPT responses.
PG  - 2123-2136
LID - 10.1007/s00405-024-08525-z [doi]
AB  - PURPOSE: Recent breakthroughs in natural language processing and machine 
      learning, exemplified by ChatGPT, have spurred a paradigm shift in healthcare. 
      Released by OpenAI in November 2022, ChatGPT rapidly gained global attention. 
      Trained on massive text datasets, this large language model holds immense 
      potential to revolutionize healthcare. However, existing literature often 
      overlooks the need for rigorous validation and real-world applicability. METHODS: 
      This head-to-head comparative study assesses ChatGPT's capabilities in providing 
      therapeutic recommendations for head and neck cancers. Simulating every NCCN 
      Guidelines scenarios. ChatGPT is queried on primary treatments, adjuvant 
      treatment, and follow-up, with responses compared to the NCCN Guidelines. 
      Performance metrics, including sensitivity, specificity, and F1 score, are 
      employed for assessment. RESULTS: The study includes 68 hypothetical cases and 
      204 clinical scenarios. ChatGPT exhibits promising capabilities in addressing 
      NCCN-related queries, achieving high sensitivity and overall accuracy across 
      primary treatment, adjuvant treatment, and follow-up. The study's metrics 
      showcase robustness in providing relevant suggestions. However, a few 
      inaccuracies are noted, especially in primary treatment scenarios. CONCLUSION: 
      Our study highlights the proficiency of ChatGPT in providing treatment 
      suggestions. The model's alignment with the NCCN Guidelines sets the stage for a 
      nuanced exploration of AI's evolving role in oncological decision support. 
      However, challenges related to the interpretability of AI in clinical 
      decision-making and the importance of clinicians understanding the underlying 
      principles of AI models remain unexplored. As AI continues to advance, 
      collaborative efforts between models and medical experts are deemed essential for 
      unlocking new frontiers in personalized cancer care.
CI  - © 2024. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Marchi, Filippo
AU  - Marchi F
AD  - Unit of Otorhinolaryngology-Head and Neck Surgery, IRCCS Ospedale Policlinico San 
      Martino, Largo Rosanna Benzi, 10, 16132, Genoa, Italy.
AD  - Department of Surgical Sciences and Integrated Diagnostics (DISC), University of 
      Genoa, 16132, Genoa, Italy.
FAU - Bellini, Elisa
AU  - Bellini E
AUID- ORCID: 0000-0003-0957-8354
AD  - Unit of Otorhinolaryngology-Head and Neck Surgery, IRCCS Ospedale Policlinico San 
      Martino, Largo Rosanna Benzi, 10, 16132, Genoa, Italy. 
      e.e.elisabellini@gmail.com.
AD  - Department of Surgical Sciences and Integrated Diagnostics (DISC), University of 
      Genoa, 16132, Genoa, Italy. e.e.elisabellini@gmail.com.
FAU - Iandelli, Andrea
AU  - Iandelli A
AD  - Unit of Otorhinolaryngology-Head and Neck Surgery, IRCCS Ospedale Policlinico San 
      Martino, Largo Rosanna Benzi, 10, 16132, Genoa, Italy.
FAU - Sampieri, Claudio
AU  - Sampieri C
AD  - Department of Experimental Medicine (DIMES), University of Genoa, Genoa, Italy.
AD  - Department of Otolaryngology-Hospital Cliníc, Barcelona, Spain.
AD  - Functional Unit of Head and Neck Tumors-Hospital Cliníc, Barcelona, Spain.
FAU - Peretti, Giorgio
AU  - Peretti G
AD  - Unit of Otorhinolaryngology-Head and Neck Surgery, IRCCS Ospedale Policlinico San 
      Martino, Largo Rosanna Benzi, 10, 16132, Genoa, Italy.
AD  - Department of Surgical Sciences and Integrated Diagnostics (DISC), University of 
      Genoa, 16132, Genoa, Italy.
LA  - eng
PT  - Journal Article
DEP - 20240229
PL  - Germany
TA  - Eur Arch Otorhinolaryngol
JT  - European archives of oto-rhino-laryngology : official journal of the European 
      Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the 
      German Society for Oto-Rhino-Laryngology - Head and Neck Surgery
JID - 9002937
RN  - 0 (Adjuvants, Immunologic)
SB  - IM
MH  - Humans
MH  - *Adjuvants, Immunologic
MH  - Benchmarking
MH  - Clinical Decision-Making
MH  - *Head and Neck Neoplasms/therapy
MH  - Artificial Intelligence
OTO - NOTNLM
OT  - Artificial intelligence (AI) models
OT  - Cancer care
OT  - ChatGPT
OT  - Head and neck cancers
OT  - Machine learning
OT  - National Comprehensive Cancer Network (NCCN) Guidelines
EDAT- 2024/02/29 12:42
MHDA- 2024/03/18 06:43
CRDT- 2024/02/29 11:05
PHST- 2023/12/14 00:00 [received]
PHST- 2024/02/02 00:00 [accepted]
PHST- 2024/03/18 06:43 [medline]
PHST- 2024/02/29 12:42 [pubmed]
PHST- 2024/02/29 11:05 [entrez]
AID - 10.1007/s00405-024-08525-z [pii]
AID - 10.1007/s00405-024-08525-z [doi]
PST - ppublish
SO  - Eur Arch Otorhinolaryngol. 2024 Apr;281(4):2123-2136. doi: 
      10.1007/s00405-024-08525-z. Epub 2024 Feb 29.

PMID- 37828840
OWN - NLM
STAT- MEDLINE
DCOM- 20240111
LR  - 20240116
IS  - 2287-285X (Electronic)
IS  - 2287-2728 (Print)
IS  - 2287-2728 (Linking)
VI  - 30
IP  - 1
DP  - 2024 Jan
TI  - Letter 1 regarding "Assessing the performance of ChatGPT in answering questions 
      regarding cirrhosis and hepatocellular carcinoma".
PG  - 111-112
LID - 10.3350/cmh.2023.0394 [doi]
FAU - Daungsupawong, Hinpetch
AU  - Daungsupawong H
AD  - Private Academic Consultant, Phonhong, Lao People's Democratic Republic.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AD  - Research Center, Chandigarh University, Mohali, India.
AD  - Department of Biological Science, Joseph Ayobabalola University, Ikeji-Arakeji, 
      Nigeria.
LA  - eng
PT  - Comment
PT  - Journal Article
DEP - 20231013
PL  - Korea (South)
TA  - Clin Mol Hepatol
JT  - Clinical and molecular hepatology
JID - 101586730
SB  - IM
CON - Clin Mol Hepatol. 2023 Jul;29(3):721-732. PMID: 36946005
MH  - Humans
MH  - *Carcinoma, Hepatocellular
MH  - *Liver Neoplasms
MH  - Liver Cirrhosis
PMC - PMC10776299
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - Cirrhosis
OT  - Hetocellular carcinoma
OT  - Liver
COIS- Conflicts of Interest The authors declare no conflicts of interest.
EDAT- 2023/10/13 06:45
MHDA- 2024/01/11 07:42
PMCR- 2024/01/01
CRDT- 2023/10/13 03:25
PHST- 2023/10/05 00:00 [received]
PHST- 2023/10/11 00:00 [accepted]
PHST- 2024/01/11 07:42 [medline]
PHST- 2023/10/13 06:45 [pubmed]
PHST- 2023/10/13 03:25 [entrez]
PHST- 2024/01/01 00:00 [pmc-release]
AID - cmh.2023.0394 [pii]
AID - cmh-2023-0394 [pii]
AID - 10.3350/cmh.2023.0394 [doi]
PST - ppublish
SO  - Clin Mol Hepatol. 2024 Jan;30(1):111-112. doi: 10.3350/cmh.2023.0394. Epub 2023 
      Oct 13.

PMID- 37823044
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231030
IS  - 2666-3287 (Electronic)
IS  - 2666-3287 (Linking)
VI  - 13
DP  - 2023 Dec
TI  - ChatGPT, large language models, and artificial intelligence in medicine and 
      health care: A primer for clinicians and researchers.
PG  - 168-169
LID - 10.1016/j.jdin.2023.07.011 [doi]
FAU - Kantor, Jonathan
AU  - Kantor J
AD  - Department of Dermatology, Center for Global Health, and Center for Clinical 
      Epidemiology and Biostatistics, Perelman School of Medicine, University of 
      Pennsylvania, Philadelphia, Pennsylvania; Florida Center for Dermatology, St 
      Augustine, Florida; and Alchemy Labs, Oxford, UK.
LA  - eng
PT  - Editorial
DEP - 20230729
PL  - United States
TA  - JAAD Int
JT  - JAAD international
JID - 101774762
PMC - PMC10562174
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - Google Bard
OT  - Microsoft Bing
OT  - annotation
OT  - artificial intelligence
OT  - large language models
OT  - medical education
OT  - practice management
COIS- None disclosed.
EDAT- 2023/10/12 06:43
MHDA- 2023/10/12 06:44
PMCR- 2023/07/29
CRDT- 2023/10/12 04:19
PHST- 2023/10/12 06:44 [medline]
PHST- 2023/10/12 06:43 [pubmed]
PHST- 2023/10/12 04:19 [entrez]
PHST- 2023/07/29 00:00 [pmc-release]
AID - S2666-3287(23)00118-9 [pii]
AID - 10.1016/j.jdin.2023.07.011 [doi]
PST - epublish
SO  - JAAD Int. 2023 Jul 29;13:168-169. doi: 10.1016/j.jdin.2023.07.011. eCollection 
      2023 Dec.

PMID- 37271213
OWN - NLM
STAT- MEDLINE
DCOM- 20230606
LR  - 20230610
IS  - 2005-8330 (Electronic)
IS  - 1229-6929 (Print)
IS  - 1229-6929 (Linking)
VI  - 24
IP  - 6
DP  - 2023 Jun
TI  - Authorship Policy and ChatGPT.
PG  - 599
LID - 10.3348/kjr.2023.0383 [doi]
FAU - Kleebayoon, Amnuay
AU  - Kleebayoon A
AUID- ORCID: 0000-0002-1976-2393
AD  - Private Academic Consultant, Samraong, Cambodia. amnuaykleebai@gmail.com.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AUID- ORCID: 0000-0003-1039-3728
AD  - Research Center, Chandigarh University, Punjab, India.
AD  - Department of Biological Science, Joesph Ayobabalola University, Ikeji-Arakeji, 
      Nigeria.
LA  - eng
PT  - Comment
PT  - Letter
PL  - Korea (South)
TA  - Korean J Radiol
JT  - Korean journal of radiology
JID - 100956096
SB  - IM
CON - Korean J Radiol. 2023 Mar;24(3):171-172. PMID: 36788775
MH  - Humans
MH  - *Artificial Intelligence
MH  - Authorship
MH  - *Radiology
MH  - Language
MH  - Republic of Korea
PMC - PMC10248358
OTO - NOTNLM
OT  - Authorship
OT  - ChatGPT
OT  - Policy
COIS- The authors have no potential conflicts of interest to disclose.
EDAT- 2023/06/05 00:42
MHDA- 2023/06/06 06:42
PMCR- 2023/06/01
CRDT- 2023/06/04 19:23
PHST- 2023/04/27 00:00 [received]
PHST- 2023/04/30 00:00 [accepted]
PHST- 2023/06/06 06:42 [medline]
PHST- 2023/06/05 00:42 [pubmed]
PHST- 2023/06/04 19:23 [entrez]
PHST- 2023/06/01 00:00 [pmc-release]
AID - 24.599 [pii]
AID - 10.3348/kjr.2023.0383 [doi]
PST - ppublish
SO  - Korean J Radiol. 2023 Jun;24(6):599. doi: 10.3348/kjr.2023.0383.

PMID- 38507847
OWN - NLM
STAT- Publisher
LR  - 20240320
IS  - 1532-8171 (Electronic)
IS  - 0735-6757 (Linking)
VI  - 80
DP  - 2024 Mar 15
TI  - Comparison of emergency medicine specialist, cardiologist, and chat-GPT in 
      electrocardiography assessment.
PG  - 51-60
LID - S0735-6757(24)00127-X [pii]
LID - 10.1016/j.ajem.2024.03.017 [doi]
AB  - INTRODUCTION: ChatGPT, developed by OpenAI, represents the cutting-edge in its 
      field with its latest model, GPT-4. Extensive research is currently being 
      conducted in various domains, including cardiovascular diseases, using ChatGPT. 
      Nevertheless, there is a lack of studies addressing the proficiency of GPT-4 in 
      diagnosing conditions based on Electrocardiography (ECG) data. The goal of this 
      study is to evaluate the diagnostic accuracy of GPT-4 when provided with ECG 
      data, and to compare its performance with that of emergency medicine specialists 
      and cardiologists. METHODS: This study has received approval from the Clinical 
      Research Ethics Committee of Hitit University Medical Faculty on August 21, 2023 
      (decision no: 2023-91). Drawing on cases from the "150 ECG Cases" book, a total 
      of 40 ECG cases were crafted into multiple-choice questions (comprising 20 
      everyday and 20 more challenging ECG questions). The participant pool included 12 
      emergency medicine specialists and 12 cardiology specialists. GPT-4 was 
      administered the questions in a total of 12 separate sessions. The responses from 
      the cardiology physicians, emergency medicine physicians, and GPT-4 were 
      evaluated separately for each of the three groups. RESULTS: In the everyday ECG 
      questions, GPT-4 demonstrated superior performance compared to both the emergency 
      medicine specialists and the cardiology specialists (p&nbsp;&lt;&nbsp;0.001, p&nbsp;=&nbsp;0.001). In 
      the more challenging ECG questions, while Chat-GPT outperformed the emergency 
      medicine specialists (p&nbsp;&lt;&nbsp;0.001), no significant statistical difference was found 
      between Chat-GPT and the cardiology specialists (p&nbsp;=&nbsp;0.190). Upon examining the 
      accuracy of the total ECG questions, Chat-GPT was found to be more successful 
      compared to both the Emergency Medicine Specialists and the cardiologists 
      (p&nbsp;&lt;&nbsp;0.001, p&nbsp;=&nbsp;0.001). CONCLUSION: Our study has shown that GPT-4 is more 
      successful than emergency medicine specialists in evaluating both everyday and 
      more challenging ECG questions. It performed better compared to cardiologists on 
      everyday questions, but its performance aligned closely with that of the 
      cardiologists as the difficulty of the questions increased.
CI  - Copyright © 2024 Elsevier Inc. All rights reserved.
FAU - Günay, Serkan
AU  - Günay S
AD  - Department of Emergency Medicine, Hitit University Erol Olçok Education and 
      Research Hospital, Çorum, Turkey. Electronic address: drsrkngny@gmail.com.
FAU - Öztürk, Ahmet
AU  - Öztürk A
AD  - Department of Emergency Medicine, Hitit University Erol Olçok Education and 
      Research Hospital, Çorum, Turkey.
FAU - Özerol, Hakan
AU  - Özerol H
AD  - Department of Emergency Medicine, Gaziantep City Hospital, Gaziantep, Turkey.
FAU - Yiğit, Yavuz
AU  - Yiğit Y
AD  - Department of Emergency Medicine, Hamad Medical Corporation, Hamad General 
      Hospital, Doha, Qatar. Electronic address: yyigit@hamad.qa.
FAU - Erenler, Ali Kemal
AU  - Erenler AK
AD  - Department of Emergency Medicine, Hitit University Erol Olçok Education and 
      Research Hospital, Çorum, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20240315
PL  - United States
TA  - Am J Emerg Med
JT  - The American journal of emergency medicine
JID - 8309942
SB  - IM
OTO - NOTNLM
OT  - Cardiology
OT  - ChatGPT
OT  - Electrocardiography
OT  - Emergency medicine
COIS- Declaration of competing interest None to declare.
EDAT- 2024/03/21 00:44
MHDA- 2024/03/21 00:44
CRDT- 2024/03/20 19:02
PHST- 2023/11/01 00:00 [received]
PHST- 2024/02/25 00:00 [revised]
PHST- 2024/03/12 00:00 [accepted]
PHST- 2024/03/21 00:44 [medline]
PHST- 2024/03/21 00:44 [pubmed]
PHST- 2024/03/20 19:02 [entrez]
AID - S0735-6757(24)00127-X [pii]
AID - 10.1016/j.ajem.2024.03.017 [doi]
PST - aheadofprint
SO  - Am J Emerg Med. 2024 Mar 15;80:51-60. doi: 10.1016/j.ajem.2024.03.017.

PMID- 38091727
OWN - NLM
STAT- MEDLINE
DCOM- 20240208
LR  - 20240308
IS  - 1879-0534 (Electronic)
IS  - 0010-4825 (Linking)
VI  - 169
DP  - 2024 Feb
TI  - Beyond human in neurosurgical exams: ChatGPT's success in the Turkish 
      neurosurgical society proficiency board exams.
PG  - 107807
LID - S0010-4825(23)01272-6 [pii]
LID - 10.1016/j.compbiomed.2023.107807 [doi]
AB  - Chat Generative Pre-Trained Transformer (ChatGPT) is a sophisticated natural 
      language model that employs advanced deep learning techniques and is trained on 
      extensive datasets to produce responses akin to human conversation for user 
      inputs. In this study, ChatGPT's success in the Turkish Neurosurgical Society 
      Proficiency Board Exams (TNSPBE) is compared to the actual candidates who took 
      the exam, along with identifying the types of questions it answered incorrectly, 
      assessing the quality of its responses, and evaluating its performance based on 
      the difficulty level of the questions. Scores of all 260 candidates were 
      recalculated according to the exams they took and included questions in those 
      exams for ranking purposes of this study. The average score of the candidates for 
      a total of 523 questions is 62.02&nbsp;±&nbsp;0.61 compared to ChatGPT, which was 78.77. We 
      have concluded that in addition to ChatGPT's higher response rate, there was also 
      a correlation with the increase in clarity regardless of the difficulty level of 
      the questions with Clarity 1.5, 2.0, 2.5, and 3.0. In the participants, however, 
      there is no such increase in parallel with the increase in clarity.
CI  - Copyright © 2023 Elsevier Ltd. All rights reserved.
FAU - Sahin, Mustafa Caglar
AU  - Sahin MC
AD  - Gazi University Faculty of Medicine, Department of Neurosurgery, Ankara, Turkey. 
      Electronic address: mcaglarsahin@gazi.edu.tr.
FAU - Sozer, Alperen
AU  - Sozer A
AD  - Gazi University Faculty of Medicine, Department of Neurosurgery, Ankara, Turkey. 
      Electronic address: alperen.sozer@gazi.edu.tr.
FAU - Kuzucu, Pelin
AU  - Kuzucu P
AD  - Gazi University Faculty of Medicine, Department of Neurosurgery, Ankara, Turkey. 
      Electronic address: drpelinkuzucu@gmail.com.
FAU - Turkmen, Tolga
AU  - Turkmen T
AD  - Ministry of Health Dortyol State Hospital, Department of Neurosurgery, Hatay, 
      Turkey. Electronic address: tlgturkmen@gmail.com.
FAU - Sahin, Merve Buke
AU  - Sahin MB
AD  - Ministry of Health Etimesgut District Health Directorate, Department of Public 
      Health, Ankara, Turkey. Electronic address: merve.buke@hacettepe.edu.tr.
FAU - Sozer, Ekin
AU  - Sozer E
AD  - Gazi University, Directorate of Health Culture and Sports, Ankara, Turkey. 
      Electronic address: ekin.aktas@gazi.edu.tr.
FAU - Tufek, Ozan Yavuz
AU  - Tufek OY
AD  - Gazi University Faculty of Medicine, Department of Neurosurgery, Ankara, Turkey. 
      Electronic address: ozanyavuztufek@gazi.edu.tr.
FAU - Nernekli, Kerem
AU  - Nernekli K
AD  - Stanford University Medical School, Department of Radiology, Stanford, CA, USA. 
      Electronic address: kerem.nernekli@stanford.edu.tr.
FAU - Emmez, Hakan
AU  - Emmez H
AD  - Gazi University Faculty of Medicine, Department of Neurosurgery, Ankara, Turkey. 
      Electronic address: hemmez@gazi.edu.tr.
FAU - Celtikci, Emrah
AU  - Celtikci E
AD  - Gazi University Faculty of Medicine, Department of Neurosurgery, Ankara, Turkey; 
      Gazi University Artificial Intelligence Center, Ankara, Turkey. Electronic 
      address: emrahceltikci@gazi.edu.tr.
LA  - eng
PT  - Journal Article
DEP - 20231210
PL  - United States
TA  - Comput Biol Med
JT  - Computers in biology and medicine
JID - 1250250
SB  - IM
MH  - *Language
MH  - *Educational Measurement
MH  - *Artificial Intelligence
MH  - *Neurosurgery/education
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Board
OT  - ChatGPT
OT  - Education
OT  - Exam
OT  - Large language model
OT  - Machine learning
COIS- Declaration of competing interest This research did not receive any specific 
      grant from funding agencies in the public, commercial, or not-for-profit sectors. 
      The authors of this submission declare no conflict of interest.
EDAT- 2023/12/14 00:42
MHDA- 2024/02/08 06:42
CRDT- 2023/12/13 18:06
PHST- 2023/09/19 00:00 [received]
PHST- 2023/11/29 00:00 [revised]
PHST- 2023/12/01 00:00 [accepted]
PHST- 2024/02/08 06:42 [medline]
PHST- 2023/12/14 00:42 [pubmed]
PHST- 2023/12/13 18:06 [entrez]
AID - S0010-4825(23)01272-6 [pii]
AID - 10.1016/j.compbiomed.2023.107807 [doi]
PST - ppublish
SO  - Comput Biol Med. 2024 Feb;169:107807. doi: 10.1016/j.compbiomed.2023.107807. Epub 
      2023 Dec 10.

PMID- 38027458
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231201
IS  - 2473-0114 (Electronic)
IS  - 2473-0114 (Linking)
VI  - 8
IP  - 4
DP  - 2023 Oct
TI  - Evaluating the Quality and Usability of Artificial Intelligence-Generated 
      Responses to Common Patient Questions in Foot and Ankle Surgery.
PG  - 24730114231209919
LID - 10.1177/24730114231209919 [doi]
LID - 24730114231209919
AB  - BACKGROUND: Artificial intelligence (AI) platforms, such as ChatGPT, have become 
      increasingly popular outlets for the consumption and distribution of health 
      care-related advice. Because of a lack of regulation and oversight, the 
      reliability of health care-related responses has become a topic of controversy in 
      the medical community. To date, no study has explored the quality of AI-derived 
      information as it relates to common foot and ankle pathologies. This study aims 
      to assess the quality and educational benefit of ChatGPT responses to common foot 
      and ankle-related questions. METHODS: ChatGPT was asked a series of 5 questions, 
      including "What is the optimal treatment for ankle arthritis?" "How should I 
      decide on ankle arthroplasty versus ankle arthrodesis?" "Do I need surgery for 
      Jones fracture?" "How can I prevent Charcot arthropathy?" and "Do I need to see a 
      doctor for my ankle sprain?" Five responses (1 per each question) were included 
      after applying the exclusion criteria. The content was graded using DISCERN (a 
      well-validated informational analysis tool) and AIRM (a self-designed tool for 
      exercise evaluation). RESULTS: Health care professionals graded the 
      ChatGPT-generated responses as bottom tier 4.5% of the time, middle tier 27.3% of 
      the time, and top tier 68.2% of the time. CONCLUSION: Although ChatGPT and other 
      related AI platforms have become a popular means for medical information 
      distribution, the educational value of the AI-generated responses related to foot 
      and ankle pathologies was variable. With 4.5% of responses receiving a 
      bottom-tier rating, 27.3% of responses receiving a middle-tier rating, and 68.2% 
      of responses receiving a top-tier rating, health care professionals should be 
      aware of the high viewership of variable-quality content easily accessible on 
      ChatGPT. LEVEL OF EVIDENCE: Level III, cross sectional study.
CI  - © The Author(s) 2023.
FAU - Anastasio, Albert Thomas
AU  - Anastasio AT
AUID- ORCID: 0000-0001-5817-3826
AD  - Department of Orthopaedic Surgery, Duke University Medical Center, Durham, NC, 
      USA.
FAU - Mills, Frederic Baker 4th
AU  - Mills FB 4th
AD  - Department of Orthopaedic Surgery, Duke University Medical Center, Durham, NC, 
      USA.
FAU - Karavan, Mark P Jr
AU  - Karavan MP Jr
AUID- ORCID: 0000-0002-8954-5861
AD  - Department of Orthopaedic Surgery, Duke University Medical Center, Durham, NC, 
      USA.
FAU - Adams, Samuel B Jr
AU  - Adams SB Jr
AD  - Department of Orthopaedic Surgery, Duke University Medical Center, Durham, NC, 
      USA.
LA  - eng
PT  - Journal Article
DEP - 20231122
PL  - United States
TA  - Foot Ankle Orthop
JT  - Foot &amp; ankle orthopaedics
JID - 101752333
PMC - PMC10666700
OTO - NOTNLM
OT  - Charcot arthropathy
OT  - ChatGPT
OT  - Jones fracture
OT  - ankle arthritis
OT  - ankle arthroplasty
OT  - ankle sprain
OT  - artificial intelligence
OT  - education
COIS- The author(s) declared no potential conflicts of interest with respect to the 
      research, authorship, and/or publication of this article. ICMJE forms for all 
      authors are available online.
EDAT- 2023/11/29 18:42
MHDA- 2023/11/29 18:43
PMCR- 2023/11/22
CRDT- 2023/11/29 17:05
PHST- 2023/11/29 18:43 [medline]
PHST- 2023/11/29 18:42 [pubmed]
PHST- 2023/11/29 17:05 [entrez]
PHST- 2023/11/22 00:00 [pmc-release]
AID - 10.1177_24730114231209919 [pii]
AID - 10.1177/24730114231209919 [doi]
PST - epublish
SO  - Foot Ankle Orthop. 2023 Nov 22;8(4):24730114231209919. doi: 
      10.1177/24730114231209919. eCollection 2023 Oct.

PMID- 37945414
OWN - NLM
STAT- MEDLINE
DCOM- 20240105
LR  - 20240105
IS  - 1471-6771 (Electronic)
IS  - 0007-0912 (Linking)
VI  - 132
IP  - 1
DP  - 2024 Jan
TI  - Quantitative evaluation of ChatGPT versus Bard responses to anaesthesia-related 
      queries.
PG  - 169-171
LID - S0007-0912(23)00550-0 [pii]
LID - 10.1016/j.bja.2023.09.030 [doi]
FAU - Patnaik, Sourav S
AU  - Patnaik SS
AD  - Department of Anesthesiology and Pain Management, The University of Texas 
      Southwestern Medical Center, Dallas, TX, USA. Electronic address: 
      sourav.patnaik@utsouthwestern.edu.
FAU - Hoffmann, Ulrike
AU  - Hoffmann U
AD  - Department of Anesthesiology and Pain Management, The University of Texas 
      Southwestern Medical Center, Dallas, TX, USA.
LA  - eng
PT  - Letter
DEP - 20231107
PL  - England
TA  - Br J Anaesth
JT  - British journal of anaesthesia
JID - 0372541
SB  - IM
MH  - Humans
MH  - *Anesthesia
MH  - *Anesthesiology
OTO - NOTNLM
OT  - Bard
OT  - ChatGPT
OT  - analytics
OT  - artificial intelligence
OT  - communication
OT  - perioperative
OT  - preoperative evaluation
EDAT- 2023/11/10 00:44
MHDA- 2024/01/05 06:43
CRDT- 2023/11/09 21:54
PHST- 2023/08/18 00:00 [received]
PHST- 2023/09/28 00:00 [revised]
PHST- 2023/09/29 00:00 [accepted]
PHST- 2024/01/05 06:43 [medline]
PHST- 2023/11/10 00:44 [pubmed]
PHST- 2023/11/09 21:54 [entrez]
AID - S0007-0912(23)00550-0 [pii]
AID - 10.1016/j.bja.2023.09.030 [doi]
PST - ppublish
SO  - Br J Anaesth. 2024 Jan;132(1):169-171. doi: 10.1016/j.bja.2023.09.030. Epub 2023 
      Nov 7.

PMID- 38188865
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240109
IS  - 2055-2076 (Print)
IS  - 2055-2076 (Electronic)
IS  - 2055-2076 (Linking)
VI  - 10
DP  - 2024 Jan-Dec
TI  - Evaluation of information provided to patients by ChatGPT about chronic diseases 
      in Spanish language.
PG  - 20552076231224603
LID - 10.1177/20552076231224603 [doi]
LID - 20552076231224603
AB  - INTRODUCTION: Artificial intelligence has presented exponential growth in 
      medicine. The ChatGPT language model has been highlighted as a possible source of 
      patient information. This study evaluates the reliability and readability of 
      ChatGPT-generated patient information on chronic diseases in Spanish. METHODS: 
      Questions frequently asked by patients on the internet about diabetes mellitus, 
      heart failure, rheumatoid arthritis (RA), chronic kidney disease (CKD), and 
      systemic lupus erythematosus (SLE) were submitted to ChatGPT. Reliability was 
      assessed by rating responses as (1) comprehensive, (2) correct but inadequate, 
      (3) some correct and some incorrect, (4) completely incorrect, and divided 
      between "good" (1 and 2) and "bad" (3 and 4). Readability was evaluated with the 
      adapted Flesch and Szigriszt formulas. RESULTS: And 71.67% of the answers were 
      "good," with none qualified as "completely incorrect." Better reliability was 
      observed in questions on diabetes and RA versus heart failure (p = 0.02). In 
      readability, responses were "moderately difficult" (54.73, interquartile range 
      (IQR) 51.59-58.58), with better results for CKD (median 56.1, IQR 53.5-59.1) and 
      RA (56.4, IQR 53.7-60.7), than for heart failure responses (median 50.6, IQR 
      46.3-53.8). CONCLUSION: Our study suggests that the ChatGPT tool can be a 
      reliable source of information in spanish for patients with chronic diseases with 
      different reliability for some of them, however, it needs to improve the 
      readability of its answers to be recommended as a useful tool for patients.
CI  - © The Author(s) 2024.
FAU - Soto-Chávez, María Juliana
AU  - Soto-Chávez MJ
AUID- ORCID: 0000-0003-4946-8774
AD  - Department of Internal Medicine, Pontificia Universidad Javeriana, Bogotá, 
      Colombia. RINGGOLD: 27964
FAU - Bustos, Marlon Mauricio
AU  - Bustos MM
AD  - Department of Internal Medicine, Pontificia Universidad Javeriana, Bogotá, 
      Colombia. RINGGOLD: 27964
AD  - Department of Internal Medicine, Hospital Universitario San Ignacio, Bogotá, 
      Colombia. RINGGOLD: 173049
FAU - Fernández-Ávila, Daniel G
AU  - Fernández-Ávila DG
AD  - Department of Internal Medicine, Pontificia Universidad Javeriana, Bogotá, 
      Colombia. RINGGOLD: 27964
AD  - Rheumatology Unit, Hospital Universitario San Ignacio, Bogotá, Colombia. 
      RINGGOLD: 173049
FAU - Muñoz, Oscar Mauricio
AU  - Muñoz OM
AUID- ORCID: 0000-0001-5401-0018
AD  - Department of Internal Medicine, Pontificia Universidad Javeriana, Bogotá, 
      Colombia. RINGGOLD: 27964
AD  - Department of Internal Medicine, Hospital Universitario San Ignacio, Bogotá, 
      Colombia. RINGGOLD: 173049
LA  - eng
PT  - Journal Article
DEP - 20240102
PL  - United States
TA  - Digit Health
JT  - Digital health
JID - 101690863
PMC - PMC10768597
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - chronic diseases
OT  - readability
OT  - reliability
COIS- The authors declared no potential conflicts of interest with respect to the 
      research, authorship, and/or publication of this article.
EDAT- 2024/01/08 06:42
MHDA- 2024/01/08 06:43
PMCR- 2024/01/02
CRDT- 2024/01/08 05:24
PHST- 2023/07/19 00:00 [received]
PHST- 2023/12/18 00:00 [accepted]
PHST- 2024/01/08 06:43 [medline]
PHST- 2024/01/08 06:42 [pubmed]
PHST- 2024/01/08 05:24 [entrez]
PHST- 2024/01/02 00:00 [pmc-release]
AID - 10.1177_20552076231224603 [pii]
AID - 10.1177/20552076231224603 [doi]
PST - epublish
SO  - Digit Health. 2024 Jan 2;10:20552076231224603. doi: 10.1177/20552076231224603. 
      eCollection 2024 Jan-Dec.

PMID- 37432530
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 52
IP  - 3
DP  - 2024 Mar
TI  - Use of ChatGPT on Taiwan's Examination for Medical Doctors.
PG  - 455-457
LID - 10.1007/s10439-023-03308-9 [doi]
AB  - The study evaluates the performance of OpenAI's GPT-3 model on answering medical 
      exam questions from Staged Senior Professional and Technical Examinations 
      Regulations for Medical Doctors in the field of internal medicine. The study used 
      the official API to connect the questionnaire with the ChatGPT model, and the 
      results showed that the AI model performed reasonably well, with the highest 
      score of 8/13 in chest medicine. However, the overall performance of the AI model 
      was limited, with only chest medicine scoring more than 60. ChatGPT scored 
      relatively high in Chest medicine, Gastroenterology, and general medicine. One of 
      the limitations of the study is the use of non-English text, which may affect the 
      model's performance as the model is primarily trained on English text.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Kao, Yung-Shuo
AU  - Kao YS
AD  - Department of Radiation Oncology, Taoyuan General Hospital, Ministry of Health 
      and Welfare, Taoyüan, Taiwan.
FAU - Chuang, Wei-Kai
AU  - Chuang WK
AD  - Department of Radiation Oncology, Shuang Ho Hospital, Taipei Medical University, 
      New Taipei, Taiwan.
FAU - Yang, Jen
AU  - Yang J
AUID- ORCID: 0000-0002-7165-2140
AD  - Department of Medical Imaging, National Cheng Kung University Hospital, College 
      of Medicine, National Cheng Kung University, Tainan, Taiwan. 
      aldosteroneyang@gmail.com.
LA  - eng
PT  - Letter
DEP - 20230711
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - Humans
MH  - Taiwan
MH  - *Physical Examination
OTO - NOTNLM
OT  - ChatGPT
OT  - Deep learning
OT  - Medical exam
EDAT- 2023/07/11 13:10
MHDA- 2024/02/12 15:42
CRDT- 2023/07/11 11:09
PHST- 2023/06/24 00:00 [received]
PHST- 2023/07/03 00:00 [accepted]
PHST- 2024/02/12 15:42 [medline]
PHST- 2023/07/11 13:10 [pubmed]
PHST- 2023/07/11 11:09 [entrez]
AID - 10.1007/s10439-023-03308-9 [pii]
AID - 10.1007/s10439-023-03308-9 [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2024 Mar;52(3):455-457. doi: 10.1007/s10439-023-03308-9. Epub 
      2023 Jul 11.

PMID- 38500300
OWN - NLM
STAT- Publisher
LR  - 20240319
IS  - 1708-539X (Electronic)
IS  - 1708-5381 (Linking)
DP  - 2024 Mar 18
TI  - Generative artificial intelligence chatbots may provide appropriate informational 
      responses to common vascular surgery questions by patients.
PG  - 17085381241240550
LID - 10.1177/17085381241240550 [doi]
AB  - OBJECTIVES: Generative artificial intelligence (AI) has emerged as a promising 
      tool to engage with patients. The objective of this study was to assess the 
      quality of AI responses to common patient questions regarding vascular surgery 
      disease processes. METHODS: OpenAI's ChatGPT-3.5 and Google Bard were queried 
      with 24 mock patient questions spanning seven vascular surgery disease domains. 
      Six experienced vascular surgery faculty at a tertiary academic center 
      independently graded AI responses on their accuracy (rated 1-4 from completely 
      inaccurate to completely accurate), completeness (rated 1-4 from totally 
      incomplete to totally complete), and appropriateness (binary). Responses were 
      also evaluated with three readability scales. RESULTS: ChatGPT responses were 
      rated, on average, more accurate than Bard responses (3.08 ± 0.33 vs 2.82 ± 0.40, 
      p &lt; .01). ChatGPT responses were scored, on average, more complete than Bard 
      responses (2.98 ± 0.34 vs 2.62 ± 0.36, p &lt; .01). Most ChatGPT responses (75.0%, n 
      = 18) and almost half of Bard responses (45.8%, n = 11) were unanimously deemed 
      appropriate. Almost one-third of Bard responses (29.2%, n = 7) were deemed 
      inappropriate by at least two reviewers (29.2%), and two Bard responses (8.4%) 
      were considered inappropriate by the majority. The mean Flesch Reading Ease, 
      Flesch-Kincaid Grade Level, and Gunning Fog Index of ChatGPT responses were 29.4 
      ± 10.8, 14.5 ± 2.2, and 17.7 ± 3.1, respectively, indicating that responses were 
      readable with a post-secondary education. Bard's mean readability scores were 
      58.9 ± 10.5, 8.2 ± 1.7, and 11.0 ± 2.0, respectively, indicating that responses 
      were readable with a high-school education (p &lt; .0001 for three metrics). 
      ChatGPT's mean response length (332 ± 79 words) was higher than Bard's mean 
      response length (183 ± 53 words, p &lt; .001). There was no difference in the 
      accuracy, completeness, readability, or response length of ChatGPT or Bard 
      between disease domains (p &gt; .05 for all analyses). CONCLUSIONS: AI offers a 
      novel means of educating patients that avoids the inundation of information from 
      "Dr Google" and the time barriers of physician-patient encounters. ChatGPT 
      provides largely valid, though imperfect, responses to myriad patient questions 
      at the expense of readability. While Bard responses are more readable and 
      concise, their quality is poorer. Further research is warranted to better 
      understand failure points for large language models in vascular surgery patient 
      education.
FAU - Chervonski, Ethan
AU  - Chervonski E
AUID- ORCID: 0000-0002-4404-0499
AD  - New York University Grossman School of Medicine, New York, NY, USA. RINGGOLD: 
      12296
FAU - Harish, Keerthi B
AU  - Harish KB
AD  - New York University Grossman School of Medicine, New York, NY, USA. RINGGOLD: 
      12296
FAU - Rockman, Caron B
AU  - Rockman CB
AD  - Division of Vascular &amp; Endovascular Surgery, Department of Surgery, New York 
      University Langone Health, New York, NY, USA. RINGGOLD: 12297
FAU - Sadek, Mikel
AU  - Sadek M
AD  - Division of Vascular &amp; Endovascular Surgery, Department of Surgery, New York 
      University Langone Health, New York, NY, USA. RINGGOLD: 12297
FAU - Teter, Katherine A
AU  - Teter KA
AUID- ORCID: 0000-0002-8807-1553
AD  - Division of Vascular &amp; Endovascular Surgery, Department of Surgery, New York 
      University Langone Health, New York, NY, USA. RINGGOLD: 12297
FAU - Jacobowitz, Glenn R
AU  - Jacobowitz GR
AD  - Division of Vascular &amp; Endovascular Surgery, Department of Surgery, New York 
      University Langone Health, New York, NY, USA. RINGGOLD: 12297
FAU - Berland, Todd L
AU  - Berland TL
AD  - Division of Vascular &amp; Endovascular Surgery, Department of Surgery, New York 
      University Langone Health, New York, NY, USA. RINGGOLD: 12297
FAU - Lohr, Joann
AU  - Lohr J
AD  - Dorn Veterans Affairs Medical Center, Columbia, SC, USA. RINGGOLD: 20102
FAU - Moore, Colleen
AU  - Moore C
AD  - InVein Clinic, Cape Girardeau, MO, USA.
FAU - Maldonado, Thomas S
AU  - Maldonado TS
AD  - Division of Vascular &amp; Endovascular Surgery, Department of Surgery, New York 
      University Langone Health, New York, NY, USA. RINGGOLD: 12297
LA  - eng
PT  - Journal Article
DEP - 20240318
PL  - England
TA  - Vascular
JT  - Vascular
JID - 101196722
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - Vascular surgery
OT  - artificial intelligence
OT  - google bard
OT  - patient education
OT  - readability
COIS- Declaration of conflicting interestsThe author(s) declared no potential conflicts 
      of interest with respect to the research, authorship, and/or publication of this 
      article.
EDAT- 2024/03/19 06:43
MHDA- 2024/03/19 06:43
CRDT- 2024/03/19 01:52
PHST- 2024/03/19 06:43 [medline]
PHST- 2024/03/19 06:43 [pubmed]
PHST- 2024/03/19 01:52 [entrez]
AID - 10.1177/17085381241240550 [doi]
PST - aheadofprint
SO  - Vascular. 2024 Mar 18:17085381241240550. doi: 10.1177/17085381241240550.

PMID- 37994939
OWN - NLM
STAT- MEDLINE
DCOM- 20240103
LR  - 20240328
IS  - 1432-1920 (Electronic)
IS  - 0028-3940 (Linking)
VI  - 66
IP  - 1
DP  - 2024 Jan
TI  - Accuracy of ChatGPT generated diagnosis from patient's medical history and 
      imaging findings in neuroradiology cases.
PG  - 73-79
LID - 10.1007/s00234-023-03252-4 [doi]
AB  - PURPOSE: The noteworthy performance of Chat Generative Pre-trained Transformer 
      (ChatGPT), an artificial intelligence text generation model based on the GPT-4 
      architecture, has been demonstrated in various fields; however, its potential 
      applications in neuroradiology remain unexplored. This study aimed to evaluate 
      the diagnostic performance of GPT-4 based ChatGPT in neuroradiology. METHODS: We 
      collected 100 consecutive "Case of the Week" cases from the American Journal of 
      Neuroradiology between October 2021 and September 2023. ChatGPT generated a 
      diagnosis from patient's medical history and imaging findings for each case. Then 
      the diagnostic accuracy rate was determined using the published ground truth. 
      Each case was categorized by anatomical location (brain, spine, and head &amp; neck), 
      and brain cases were further divided into central nervous system (CNS) tumor and 
      non-CNS tumor groups. Fisher's exact test was conducted to compare the accuracy 
      rates among the three anatomical locations, as well as between the CNS tumor and 
      non-CNS tumor groups. RESULTS: ChatGPT achieved a diagnostic accuracy rate of 50% 
      (50/100 cases). There were no significant differences between the accuracy rates 
      of the three anatomical locations (p = 0.89). The accuracy rate was significantly 
      lower for the CNS tumor group compared to the non-CNS tumor group in the brain 
      cases (16% [3/19] vs. 62% [36/58], p &lt; 0.001). CONCLUSION: This study 
      demonstrated the diagnostic performance of ChatGPT in neuroradiology. ChatGPT's 
      diagnostic accuracy varied depending on disease etiologies, and its diagnostic 
      accuracy was significantly lower in CNS tumors compared to non-CNS tumors.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Horiuchi, Daisuke
AU  - Horiuchi D
AUID- ORCID: 0000-0002-8929-8098
AD  - Department of Diagnostic and Interventional Radiology, Graduate School of 
      Medicine, Osaka Metropolitan University, Osaka, Japan.
FAU - Tatekawa, Hiroyuki
AU  - Tatekawa H
AUID- ORCID: 0000-0002-8050-4895
AD  - Department of Diagnostic and Interventional Radiology, Graduate School of 
      Medicine, Osaka Metropolitan University, Osaka, Japan.
FAU - Shimono, Taro
AU  - Shimono T
AUID- ORCID: 0000-0001-5847-6587
AD  - Department of Diagnostic and Interventional Radiology, Graduate School of 
      Medicine, Osaka Metropolitan University, Osaka, Japan.
FAU - Walston, Shannon L
AU  - Walston SL
AUID- ORCID: 0000-0002-7268-8313
AD  - Department of Diagnostic and Interventional Radiology, Graduate School of 
      Medicine, Osaka Metropolitan University, Osaka, Japan.
FAU - Takita, Hirotaka
AU  - Takita H
AUID- ORCID: 0000-0003-2860-4503
AD  - Department of Diagnostic and Interventional Radiology, Graduate School of 
      Medicine, Osaka Metropolitan University, Osaka, Japan.
FAU - Matsushita, Shu
AU  - Matsushita S
AUID- ORCID: 0000-0003-1132-9736
AD  - Department of Diagnostic and Interventional Radiology, Graduate School of 
      Medicine, Osaka Metropolitan University, Osaka, Japan.
FAU - Oura, Tatsushi
AU  - Oura T
AUID- ORCID: 0000-0002-0930-1032
AD  - Department of Diagnostic and Interventional Radiology, Graduate School of 
      Medicine, Osaka Metropolitan University, Osaka, Japan.
FAU - Mitsuyama, Yasuhito
AU  - Mitsuyama Y
AUID- ORCID: 0000-0001-7979-2300
AD  - Department of Diagnostic and Interventional Radiology, Graduate School of 
      Medicine, Osaka Metropolitan University, Osaka, Japan.
FAU - Miki, Yukio
AU  - Miki Y
AUID- ORCID: 0000-0003-0621-0044
AD  - Department of Diagnostic and Interventional Radiology, Graduate School of 
      Medicine, Osaka Metropolitan University, Osaka, Japan.
FAU - Ueda, Daiju
AU  - Ueda D
AUID- ORCID: 0000-0002-3878-3616
AD  - Department of Diagnostic and Interventional Radiology, Graduate School of 
      Medicine, Osaka Metropolitan University, Osaka, Japan. ai.labo.ocu@gmail.com.
AD  - Smart Life Science Lab, Center for Health Science Innovation, Osaka Metropolitan 
      University, Osaka, Japan. ai.labo.ocu@gmail.com.
LA  - eng
PT  - Journal Article
DEP - 20231123
PL  - Germany
TA  - Neuroradiology
JT  - Neuroradiology
JID - 1302751
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Head
MH  - Brain
MH  - Neck
MH  - *Neoplasms
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Chat Generative Pre-trained Transformer (ChatGPT)
OT  - Generative Pre-trained Transformer (GPT)-4
OT  - Large language models
EDAT- 2023/11/23 12:43
MHDA- 2024/01/03 09:44
CRDT- 2023/11/23 11:07
PHST- 2023/10/02 00:00 [received]
PHST- 2023/11/13 00:00 [accepted]
PHST- 2024/01/03 09:44 [medline]
PHST- 2023/11/23 12:43 [pubmed]
PHST- 2023/11/23 11:07 [entrez]
AID - 10.1007/s00234-023-03252-4 [pii]
AID - 10.1007/s00234-023-03252-4 [doi]
PST - ppublish
SO  - Neuroradiology. 2024 Jan;66(1):73-79. doi: 10.1007/s00234-023-03252-4. Epub 2023 
      Nov 23.

PMID- 38050835
OWN - NLM
STAT- MEDLINE
DCOM- 20240308
LR  - 20240308
IS  - 1545-0813 (Electronic)
IS  - 1059-924X (Linking)
VI  - 29
IP  - 2
DP  - 2024 Apr
TI  - The Potential of AI and ChatGPT in Improving Agricultural Injury and Illness 
      Surveillance Programming and Dissemination.
PG  - 150-154
LID - 10.1080/1059924X.2023.2284959 [doi]
AB  - Generative Artificial Intelligence (AI) provides unprecedented opportunities to 
      improve injury surveillance systems in many ways, including the curation and 
      publication of information related to agricultural injuries and illnesses. This 
      editorial explores the feasibility and implication of ChatGPT integration in an 
      international sentinel agricultural injury surveillance system, AgInjuryNews, 
      highlighting that AI integration may enhance workflows by reducing human and 
      financial resources and increasing outputs. In the coming years, text intensive 
      natural language reports in AgInjuryNews and similar systems could be a rich 
      source for data for ChatGPT or other more customized and fine-tuned LLMs. By 
      harnessing the capabilities of AI and NLP, teams could potentially streamline the 
      process of data analysis, report generation, and public dissemination, ultimately 
      contributing to improved agricultural injury prevention efforts, well beyond any 
      manually driven efforts.
FAU - Weichelt, Bryan P
AU  - Weichelt BP
AUID- ORCID: 0000-0001-7444-4374
AD  - National Children's Center for Rural and Agricultural Health and Safety; National 
      Farm Medicine Center, Marshfield Clinic Research Institute, Marshfield, WI, USA.
FAU - Pilz, Matthew
AU  - Pilz M
AD  - National Children's Center for Rural and Agricultural Health and Safety; National 
      Farm Medicine Center, Marshfield Clinic Research Institute, Marshfield, WI, USA.
FAU - Burke, Richard
AU  - Burke R
AD  - National Children's Center for Rural and Agricultural Health and Safety; National 
      Farm Medicine Center, Marshfield Clinic Research Institute, Marshfield, WI, USA.
FAU - Puthoff, David
AU  - Puthoff D
AD  - Office of Research and Sponsored Programs, Marshfield Clinic Research Institute, 
      Marshfield, WI, USA.
FAU - Namkoong, Kang
AU  - Namkoong K
AD  - Department of Communication, University of Maryland, College Park, MD, USA.
LA  - eng
PT  - Journal Article
DEP - 20231205
PL  - England
TA  - J Agromedicine
JT  - Journal of agromedicine
JID - 9421530
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Agriculture
MH  - Language
OTO - NOTNLM
OT  - Agricultural injury
OT  - ChatGPT
OT  - artificial intelligence
OT  - surveillance
EDAT- 2023/12/05 17:44
MHDA- 2024/03/08 06:43
CRDT- 2023/12/05 06:36
PHST- 2024/03/08 06:43 [medline]
PHST- 2023/12/05 17:44 [pubmed]
PHST- 2023/12/05 06:36 [entrez]
AID - 10.1080/1059924X.2023.2284959 [doi]
PST - ppublish
SO  - J Agromedicine. 2024 Apr;29(2):150-154. doi: 10.1080/1059924X.2023.2284959. Epub 
      2023 Dec 5.

PMID- 37645039
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240129
IS  - 2331-8422 (Electronic)
IS  - 2331-8422 (Linking)
DP  - 2023 Oct 19
TI  - ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug 
      Development with Chatbots.
LID - arXiv:2308.06920v2
AB  - The birth of ChatGPT, a cutting-edge language model-based chatbot developed by 
      OpenAI, ushered in a new era in AI. However, due to potential pitfalls, its role 
      in rigorous scientific research is not clear yet. This paper vividly showcases 
      its innovative application within the field of drug discovery. Focused 
      specifically on developing anti-cocaine addiction drugs, the study employs GPT-4 
      as a virtual guide, offering strategic and methodological insights to researchers 
      working on generative models for drug candidates. The primary objective is to 
      generate optimal drug-like molecules with desired properties. By leveraging the 
      capabilities of ChatGPT, the study introduces a novel approach to the drug 
      discovery process. This symbiotic partnership between AI and researchers 
      transforms how drug development is approached. Chatbots become facilitators, 
      steering researchers towards innovative methodologies and productive paths for 
      creating effective drug candidates. This research sheds light on the 
      collaborative synergy between human expertise and AI assistance, wherein 
      ChatGPT's cognitive abilities enhance the design and development of potential 
      pharmaceutical solutions. This paper not only explores the integration of 
      advanced AI in drug discovery but also reimagines the landscape by advocating for 
      AI-powered chatbots as trailblazers in revolutionizing therapeutic innovation.
FAU - Wang, Rui
AU  - Wang R
AD  - Department of Mathematics,Michigan State University, MI 48824, USA.
FAU - Feng, Hongsong
AU  - Feng H
AD  - Department of Mathematics,Michigan State University, MI 48824, USA.
FAU - Wei, Guo-Wei
AU  - Wei GW
AD  - Department of Mathematics,Michigan State University, MI 48824, USA.
AD  - Department of Electrical and Computer Engineering, Michigan State University, MI 
      48824, USA.
AD  - Department of Biochemistry and Molecular Biology, Michigan State University, MI 
      48824, USA.
LA  - eng
GR  - R01 AI164266/AI/NIAID NIH HHS/United States
GR  - R01 GM126189/GM/NIGMS NIH HHS/United States
GR  - R35 GM148196/GM/NIGMS NIH HHS/United States
PT  - Preprint
DEP - 20231019
PL  - United States
TA  - ArXiv
JT  - ArXiv
JID - 101759493
UIN - J Chem Inf Model. 2023 Nov 13;:. PMID: 37956228
PMC - PMC10462169
OTO - NOTNLM
OT  - AutoEncoder
OT  - ChatGPT
OT  - Cocaine Addition
OT  - Drug Discovery
OT  - Langevin Equation
COIS- Competing interests The authors declare no competing interests.
EDAT- 2023/08/30 06:47
MHDA- 2023/08/30 06:48
PMCR- 2023/10/19
CRDT- 2023/08/30 03:46
PHST- 2023/08/30 06:47 [pubmed]
PHST- 2023/08/30 06:48 [medline]
PHST- 2023/08/30 03:46 [entrez]
PHST- 2023/10/19 00:00 [pmc-release]
AID - 2308.06920 [pii]
PST - epublish
SO  - ArXiv [Preprint]. 2023 Oct 19:arXiv:2308.06920v2.

PMID- 38354748
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240217
IS  - 2633-1462 (Electronic)
IS  - 2633-1462 (Linking)
VI  - 5
IP  - 2
DP  - 2024 Feb 15
TI  - Is ChatGPT a trusted source of information for total hip and knee arthroplasty 
      patients?
PG  - 139-146
LID - 10.1302/2633-1462.52.BJO-2023-0113.R1 [doi]
AB  - AIMS: While internet search engines have been the primary information source for 
      patients' questions, artificial intelligence large language models like ChatGPT 
      are trending towards becoming the new primary source. The purpose of this study 
      was to determine if ChatGPT can answer patient questions about total hip (THA) 
      and knee arthroplasty (TKA) with consistent accuracy, comprehensiveness, and easy 
      readability. METHODS: We posed the 20 most Google-searched questions about THA 
      and TKA, plus ten additional postoperative questions, to ChatGPT. Each question 
      was asked twice to evaluate for consistency in quality. Following each response, 
      we responded with, "Please explain so it is easier to understand," to evaluate 
      ChatGPT's ability to reduce response reading grade level, measured as 
      Flesch-Kincaid Grade Level (FKGL). Five resident physicians rated the 120 
      responses on 1 to 5 accuracy and comprehensiveness scales. Additionally, they 
      answered a "yes" or "no" question regarding acceptability. Mean scores were 
      calculated for each question, and responses were deemed acceptable if ≥ four 
      raters answered "yes." RESULTS: The mean accuracy and comprehensiveness scores 
      were 4.26 (95% confidence interval (CI) 4.19 to 4.33) and 3.79 (95% CI 3.69 to 
      3.89), respectively. Out of all the responses, 59.2% (71/120; 95% CI 50.0% to 
      67.7%) were acceptable. ChatGPT was consistent when asked the same question 
      twice, giving no significant difference in accuracy (t = 0.821; p = 0.415), 
      comprehensiveness (t = 1.387; p = 0.171), acceptability (χ(2) = 1.832; p = 
      0.176), and FKGL (t = 0.264; p = 0.793). There was a significantly lower FKGL (t 
      = 2.204; p = 0.029) for easier responses (11.14; 95% CI 10.57 to 11.71) than 
      original responses (12.15; 95% CI 11.45 to 12.85). CONCLUSION: ChatGPT answered 
      THA and TKA patient questions with accuracy comparable to previous reports of 
      websites, with adequate comprehensiveness, but with limited acceptability as the 
      sole information source. ChatGPT has potential for answering patient questions 
      about THA and TKA, but needs improvement.
CI  - © 2024 Wright et al.
FAU - Wright, Benjamin M
AU  - Wright BM
AUID- ORCID: 0009-0000-8354-6540
AD  - Morsani College of Medicine, University of South Florida, Tampa, Florida, USA.
FAU - Bodnar, Michael S
AU  - Bodnar MS
AD  - Morsani College of Medicine, University of South Florida, Tampa, Florida, USA.
FAU - Moore, Andrew D
AU  - Moore AD
AD  - Department of Orthopaedic Surgery, University of South Florida, Tampa, Florida, 
      USA.
FAU - Maseda, Meghan C
AU  - Maseda MC
AD  - Department of Orthopaedic Surgery, University of South Florida, Tampa, Florida, 
      USA.
FAU - Kucharik, Michael P
AU  - Kucharik MP
AD  - Department of Orthopaedic Surgery, University of South Florida, Tampa, Florida, 
      USA.
FAU - Diaz, Connor C
AU  - Diaz CC
AD  - Department of Orthopaedic Surgery, University of South Florida, Tampa, Florida, 
      USA.
FAU - Schmidt, Christian M
AU  - Schmidt CM
AD  - Department of Orthopaedic Surgery, University of South Florida, Tampa, Florida, 
      USA.
FAU - Mir, Hassan R
AU  - Mir HR
AD  - Orthopaedic Trauma Service, Florida Orthopedic Institute, Tampa, Florida, USA.
LA  - eng
PT  - Journal Article
DEP - 20240215
PL  - England
TA  - Bone Jt Open
JT  - Bone &amp; joint open
JID - 101770336
PMC - PMC10867788
COIS- No authors have any relevant disclosures or conflicts of interest. H. Mir reports 
      consulting fees from Smith &amp; Nephew and Synthes DePuy, unrelated to this article. 
      H. Mir also reports a role on the board of directors for the Orthopaedic Trauma 
      Association and the Center for Orthopaedics.
EDAT- 2024/02/15 00:42
MHDA- 2024/02/15 00:43
PMCR- 2024/02/15
CRDT- 2024/02/14 19:03
PHST- 2024/02/15 00:43 [medline]
PHST- 2024/02/15 00:42 [pubmed]
PHST- 2024/02/14 19:03 [entrez]
PHST- 2024/02/15 00:00 [pmc-release]
AID - BJO-2023-0113.R1 [pii]
AID - 10.1302/2633-1462.52.BJO-2023-0113.R1 [doi]
PST - epublish
SO  - Bone Jt Open. 2024 Feb 15;5(2):139-146. doi: 
      10.1302/2633-1462.52.BJO-2023-0113.R1.

PMID- 38382637
OWN - NLM
STAT- Publisher
LR  - 20240326
IS  - 1098-8947 (Electronic)
IS  - 0743-684X (Linking)
DP  - 2024 Mar 26
TI  - Both Patients and Plastic Surgeons Prefer Artificial Intelligence-Generated 
      Microsurgical Information.
LID - 10.1055/a-2273-4163 [doi]
AB  - BACKGROUND:  With the growing relevance of artificial intelligence (AI)-based 
      patient-facing information, microsurgical-specific online information provided by 
      professional organizations was compared with that of ChatGPT (Chat Generative 
      Pre-Trained Transformer) and assessed for accuracy, comprehensiveness, clarity, 
      and readability. METHODS:  Six plastic and reconstructive surgeons blindly 
      assessed responses to 10 microsurgery-related medical questions written either by 
      the American Society of Reconstructive Microsurgery (ASRM) or ChatGPT based on 
      accuracy, comprehensiveness, and clarity. Surgeons were asked to choose which 
      source provided the overall highest-quality microsurgical patient-facing 
      information. Additionally, 30 individuals with no medical background (ages: 
      18-81, μ = 49.8) were asked to determine a preference when blindly comparing 
      materials. Readability scores were calculated, and all numerical scores were 
      analyzed using the following six reliability formulas: Flesch-Kincaid Grade 
      Level, Flesch-Kincaid Readability Ease, Gunning Fog Index, Simple Measure of 
      Gobbledygook Index, Coleman-Liau Index, Linsear Write Formula, and Automated 
      Readability Index. Statistical analysis of microsurgical-specific online sources 
      was conducted utilizing paired t-tests. RESULTS:  Statistically significant 
      differences in comprehensiveness and clarity were seen in favor of ChatGPT. 
      Surgeons, 70.7% of the time, blindly choose ChatGPT as the source that overall 
      provided the highest-quality microsurgical patient-facing information. Nonmedical 
      individuals 55.9% of the time selected AI-generated microsurgical materials as 
      well. Neither ChatGPT nor ASRM-generated materials were found to contain 
      inaccuracies. Readability scores for both ChatGPT and ASRM materials were found 
      to exceed recommended levels for patient proficiency across six readability 
      formulas, with AI-based material scored as more complex. CONCLUSION: 
       AI-generated patient-facing materials were preferred by surgeons in terms of 
      comprehensiveness and clarity when blindly compared with online material provided 
      by ASRM. Studied AI-generated material was not found to contain inaccuracies. 
      Additionally, surgeons and nonmedical individuals consistently indicated an 
      overall preference for AI-generated material. A readability analysis suggested 
      that both materials sourced from ChatGPT and ASRM surpassed recommended reading 
      levels across six readability scores.
CI  - Thieme. All rights reserved.
FAU - Berry, Charlotte E
AU  - Berry CE
AUID- ORCID: 0000-0002-6310-5921
AD  - Department of Surgery, Division of Plastic and Reconstructive Surgery, Hagey 
      Laboratory for Pediatric Regenerative Medicine, Stanford University School of 
      Medicine, Stanford, California.
FAU - Fazilat, Alexander Z
AU  - Fazilat AZ
AD  - Department of Surgery, Division of Plastic and Reconstructive Surgery, Hagey 
      Laboratory for Pediatric Regenerative Medicine, Stanford University School of 
      Medicine, Stanford, California.
FAU - Lavin, Christopher
AU  - Lavin C
AD  - Department of Surgery, Division of Plastic and Reconstructive Surgery, Hagey 
      Laboratory for Pediatric Regenerative Medicine, Stanford University School of 
      Medicine, Stanford, California.
FAU - Lintel, Hendrik
AU  - Lintel H
AD  - Department of Surgery, Division of Plastic and Reconstructive Surgery, Hagey 
      Laboratory for Pediatric Regenerative Medicine, Stanford University School of 
      Medicine, Stanford, California.
FAU - Cole, Naomi
AU  - Cole N
AD  - Department of Surgery, Division of Plastic and Reconstructive Surgery, Hagey 
      Laboratory for Pediatric Regenerative Medicine, Stanford University School of 
      Medicine, Stanford, California.
FAU - Stingl, Cybil S
AU  - Stingl CS
AD  - Department of Surgery, Division of Plastic and Reconstructive Surgery, Hagey 
      Laboratory for Pediatric Regenerative Medicine, Stanford University School of 
      Medicine, Stanford, California.
FAU - Valencia, Caleb
AU  - Valencia C
AD  - Department of Surgery, Division of Plastic and Reconstructive Surgery, Hagey 
      Laboratory for Pediatric Regenerative Medicine, Stanford University School of 
      Medicine, Stanford, California.
FAU - Morgan, Annah G
AU  - Morgan AG
AD  - Department of Surgery, Division of Plastic and Reconstructive Surgery, Hagey 
      Laboratory for Pediatric Regenerative Medicine, Stanford University School of 
      Medicine, Stanford, California.
FAU - Momeni, Arash
AU  - Momeni A
AUID- ORCID: 0000-0002-6452-751X
AD  - Department of Surgery, Division of Plastic and Reconstructive Surgery, Hagey 
      Laboratory for Pediatric Regenerative Medicine, Stanford University School of 
      Medicine, Stanford, California.
FAU - Wan, Derrick C
AU  - Wan DC
AD  - Department of Surgery, Division of Plastic and Reconstructive Surgery, Hagey 
      Laboratory for Pediatric Regenerative Medicine, Stanford University School of 
      Medicine, Stanford, California.
LA  - eng
PT  - Journal Article
DEP - 20240326
PL  - United States
TA  - J Reconstr Microsurg
JT  - Journal of reconstructive microsurgery
JID - 8502670
SB  - IM
COIS- None declared.
EDAT- 2024/02/22 00:43
MHDA- 2024/02/22 00:43
CRDT- 2024/02/21 19:14
PHST- 2024/02/22 00:43 [pubmed]
PHST- 2024/02/22 00:43 [medline]
PHST- 2024/02/21 19:14 [entrez]
AID - 10.1055/a-2273-4163 [doi]
PST - aheadofprint
SO  - J Reconstr Microsurg. 2024 Mar 26. doi: 10.1055/a-2273-4163.

PMID- 38526538
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240325
IS  - 2561-326X (Electronic)
IS  - 2561-326X (Linking)
VI  - 8
DP  - 2024 Mar 25
TI  - Performance of ChatGPT on the India Undergraduate Community Medicine Examination: 
      Cross-Sectional Study.
PG  - e49964
LID - 10.2196/49964 [doi]
AB  - BACKGROUND: Medical students may increasingly use large language models (LLMs) in 
      their learning. ChatGPT is an LLM at the forefront of this new development in 
      medical education with the capacity to respond to multidisciplinary questions. 
      OBJECTIVE: The aim of this study was to evaluate the ability of ChatGPT 3.5 to 
      complete the Indian undergraduate medical examination in the subject of community 
      medicine. We further compared ChatGPT scores with the scores obtained by the 
      students. METHODS: The study was conducted at a publicly funded medical college 
      in Hyderabad, India. The study was based on the internal assessment examination 
      conducted in January 2023 for students in the Bachelor of Medicine and Bachelor 
      of Surgery Final Year-Part I program; the examination of focus included 40 
      questions (divided between two papers) from the community medicine subject 
      syllabus. Each paper had three sections with different weightage of marks for 
      each section: section one had two long essay-type questions worth 15 marks each, 
      section two had 8 short essay-type questions worth 5 marks each, and section 
      three had 10 short-answer questions worth 3 marks each. The same questions were 
      administered as prompts to ChatGPT 3.5 and the responses were recorded. Apart 
      from scoring ChatGPT responses, two independent evaluators explored the responses 
      to each question to further analyze their quality with regard to three 
      subdomains: relevancy, coherence, and completeness. Each question was scored in 
      these subdomains on a Likert scale of 1-5. The average of the two evaluators was 
      taken as the subdomain score of the question. The proportion of questions with a 
      score 50% of the maximum score (5) in each subdomain was calculated. RESULTS: 
      ChatGPT 3.5 scored 72.3% on paper 1 and 61% on paper 2. The mean score of the 94 
      students was 43% on paper 1 and 45% on paper 2. The responses of ChatGPT 3.5 were 
      also rated to be satisfactorily relevant, coherent, and complete for most of the 
      questions (&gt;80%). CONCLUSIONS: ChatGPT 3.5 appears to have substantial and 
      sufficient knowledge to understand and answer the Indian medical undergraduate 
      examination in the subject of community medicine. ChatGPT may be introduced to 
      students to enable the self-directed learning of community medicine in pilot 
      mode. However, faculty oversight will be required as ChatGPT is still in the 
      initial stages of development, and thus its potential and reliability of medical 
      content from the Indian context need to be further explored comprehensively.
CI  - ©Aravind P Gandhi, Felista Karen Joesph, Vineeth Rajagopal, P Aparnavi, Sushma 
      Katkuri, Sonal Dayama, Prakasini Satapathy, Mahalaqua Nazli Khatib, Shilpa 
      Gaidhane, Quazi Syed Zahiruddin, Ashish Behera. Originally published in JMIR 
      Formative Research (https://formative.jmir.org), 25.03.2024.
FAU - Gandhi, Aravind P
AU  - Gandhi AP
AUID- ORCID: 0000-0003-3898-5450
AD  - Department of Community Medicine, All India Institute of Medical Sciences, 
      Nagpur, Maharashtra, India.
FAU - Joesph, Felista Karen
AU  - Joesph FK
AUID- ORCID: 0009-0004-6872-4539
AD  - Melmaruvathur Adhiparasakthi Institute of Medical Sciences and Research, 
      Melmaruvathur, India.
FAU - Rajagopal, Vineeth
AU  - Rajagopal V
AUID- ORCID: 0000-0001-7204-8888
AD  - Department of Community Medicine and School of Public Health, Postgraduate 
      Institute of Medical Education and Research, Chandigarh, India.
FAU - Aparnavi, P
AU  - Aparnavi P
AUID- ORCID: 0000-0001-8671-7261
AD  - Department of Community Medicine, KMCH Institute of Health Sciences and Research, 
      Coimbatore, India.
FAU - Katkuri, Sushma
AU  - Katkuri S
AUID- ORCID: 0000-0002-0762-3425
AD  - Department of Community Medicine, ESIC Medical College &amp; Hospital, Sanathnagar, 
      Hyderabad, India.
FAU - Dayama, Sonal
AU  - Dayama S
AUID- ORCID: 0000-0003-4938-5020
AD  - Department of Community Medicine, ESIC Medical College &amp; Hospital, Sanathnagar, 
      Hyderabad, India.
FAU - Satapathy, Prakasini
AU  - Satapathy P
AUID- ORCID: 0000-0001-7614-8587
AD  - Center for Global Health Research, Saveetha Medical College and Hospital, 
      Saveetha Institute of Medical and Technical Sciences, Saveetha University, 
      Chennai, India.
AD  - Medical Laboratories Techniques Department, AL-Mustaqbal University, Hillah, 
      Babil, Iraq.
FAU - Khatib, Mahalaqua Nazli
AU  - Khatib MN
AUID- ORCID: 0000-0001-5875-8277
AD  - Division of Evidence Synthesis, Global Consortium of Public Health and Research, 
      Datta Meghe Institute of Higher Education, Wardha, India.
FAU - Gaidhane, Shilpa
AU  - Gaidhane S
AUID- ORCID: 0000-0002-3012-7438
AD  - Centre for One Health Education, Research &amp; Development, Jawaharlal Nehru Medical 
      College, Datta Meghe Institute of Higher Education, Wardha, India.
FAU - Zahiruddin, Quazi Syed
AU  - Zahiruddin QS
AUID- ORCID: 0000-0002-1435-899X
AD  - Global Health Academy Division of Evidence Synthesis, School of Epidemiology and 
      Public Health and Research, Jawaharlal Nehru Medical College, Datta Meghe 
      Institute of Higher Education and Research, Wardha, India.
FAU - Behera, Ashish
AU  - Behera A
AUID- ORCID: 0000-0002-1750-2352
AD  - Department of Internal Medicine, Postgraduate Institute of Medical Education and 
      Research, Chandigarh, India.
LA  - eng
PT  - Journal Article
DEP - 20240325
PL  - Canada
TA  - JMIR Form Res
JT  - JMIR formative research
JID - 101726394
OTO - NOTNLM
OT  - ChatGPT
OT  - India
OT  - artificial intelligence
OT  - community medicine
OT  - digitalization
OT  - large language model
OT  - medical education
EDAT- 2024/03/25 12:45
MHDA- 2024/03/25 12:46
CRDT- 2024/03/25 11:53
PHST- 2023/06/14 00:00 [received]
PHST- 2023/11/22 00:00 [accepted]
PHST- 2023/09/01 00:00 [revised]
PHST- 2024/03/25 12:46 [medline]
PHST- 2024/03/25 12:45 [pubmed]
PHST- 2024/03/25 11:53 [entrez]
AID - v8i1e49964 [pii]
AID - 10.2196/49964 [doi]
PST - epublish
SO  - JMIR Form Res. 2024 Mar 25;8:e49964. doi: 10.2196/49964.

PMID- 38103973
OWN - NLM
STAT- Publisher
LR  - 20231216
IS  - 1878-4046 (Electronic)
IS  - 1076-6332 (Linking)
DP  - 2023 Dec 15
TI  - Evaluation of ChatGPT and Google Bard Using Prompt Engineering in Cancer 
      Screening Algorithms.
LID - S1076-6332(23)00618-9 [pii]
LID - 10.1016/j.acra.2023.11.002 [doi]
AB  - Large language models (LLMs) such as ChatGPT and Bard have emerged as powerful 
      tools in medicine, showcasing strong results in tasks such as radiology report 
      translations and research paper drafting. While their implementation in clinical 
      practice holds promise, their response accuracy remains variable. This study 
      aimed to evaluate the accuracy of ChatGPT and Bard in clinical decision-making 
      based on the American College of Radiology Appropriateness Criteria for various 
      cancers. Both LLMs were evaluated in terms of their responses to open-ended (OE) 
      and select-all-that-apply (SATA) prompts. Furthermore, the study incorporated 
      prompt engineering (PE) techniques to enhance the accuracy of LLM outputs. The 
      results revealed similar performances between ChatGPT and Bard on OE prompts, 
      with ChatGPT exhibiting marginally higher accuracy in SATA scenarios. The 
      introduction of PE also marginally improved LLM outputs in OE prompts but did not 
      enhance SATA responses. The results highlight the potential of LLMs in aiding 
      clinical decision-making processes, especially when guided by optimally 
      engineered prompts. Future studies in diverse clinical situations are imperative 
      to better understand the impact of LLMs in radiology.
CI  - Copyright © 2023 The Association of University Radiologists. Published by 
      Elsevier Inc. All rights reserved.
FAU - Nguyen, Daniel
AU  - Nguyen D
AD  - University of Massachusetts Chan Medical School, Worcester, Massachusetts (D.N., 
      D.S.). Electronic address: dan.nguyen@umassmed.edu.
FAU - Swanson, Daniel
AU  - Swanson D
AD  - University of Massachusetts Chan Medical School, Worcester, Massachusetts (D.N., 
      D.S.).
FAU - Newbury, Alex
AU  - Newbury A
AD  - Department of Radiology, University of Massachusetts Chan Medical School, 
      Worcester, Massachusetts (A.N., Y.H.K.).
FAU - Kim, Young H
AU  - Kim YH
AD  - Department of Radiology, University of Massachusetts Chan Medical School, 
      Worcester, Massachusetts (A.N., Y.H.K.).
LA  - eng
PT  - Journal Article
DEP - 20231215
PL  - United States
TA  - Acad Radiol
JT  - Academic radiology
JID - 9440159
SB  - IM
COIS- DECLARATION OF COMPETING INTEREST The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2023/12/17 09:44
MHDA- 2023/12/17 09:44
CRDT- 2023/12/16 21:58
PHST- 2023/08/29 00:00 [received]
PHST- 2023/10/31 00:00 [revised]
PHST- 2023/11/01 00:00 [accepted]
PHST- 2023/12/17 09:44 [medline]
PHST- 2023/12/17 09:44 [pubmed]
PHST- 2023/12/16 21:58 [entrez]
AID - S1076-6332(23)00618-9 [pii]
AID - 10.1016/j.acra.2023.11.002 [doi]
PST - aheadofprint
SO  - Acad Radiol. 2023 Dec 15:S1076-6332(23)00618-9. doi: 10.1016/j.acra.2023.11.002.

PMID- 37973369
OWN - NLM
STAT- MEDLINE
DCOM- 20240125
LR  - 20240202
IS  - 1473-4257 (Electronic)
IS  - 0306-6800 (Linking)
VI  - 50
IP  - 2
DP  - 2024 Jan 23
TI  - Assessing the performance of ChatGPT in bioethics: a large language model's moral 
      compass in medicine.
PG  - 97-101
LID - 10.1136/jme-2023-109366 [doi]
AB  - Chat Generative Pre-Trained Transformer (ChatGPT) has been a growing point of 
      interest in medical education yet has not been assessed in the field of 
      bioethics. This study evaluated the accuracy of ChatGPT-3.5 (April 2023 version) 
      in answering text-based, multiple choice bioethics questions at the level of US 
      third-year and fourth-year medical students. A total of 114 bioethical questions 
      were identified from the widely utilised question banks UWorld and AMBOSS. 
      Accuracy, bioethical categories, difficulty levels, specialty data, error 
      analysis and character count were analysed. We found that ChatGPT had an accuracy 
      of 59.6%, with greater accuracy in topics surrounding death and patient-physician 
      relationships and performed poorly on questions pertaining to informed consent. 
      Of all the specialties, it performed best in paediatrics. Yet, certain 
      specialties and bioethical categories were under-represented. Among the errors 
      made, it tended towards content errors and application errors. There were no 
      significant associations between character count and accuracy. Nevertheless, this 
      investigation contributes to the ongoing dialogue on artificial intelligence's 
      (AI) role in healthcare and medical education, advocating for further research to 
      fully understand AI systems' capabilities and constraints in the nuanced field of 
      medical bioethics.
CI  - © Author(s) (or their employer(s)) 2024. No commercial re-use. See rights and 
      permissions. Published by BMJ.
FAU - Chen, Jamie
AU  - Chen J
AUID- ORCID: 0000-0003-0572-2291
AD  - Hackensack Meridian School of Medicine, Nutley, New Jersey, USA 
      jamie.chen@hmhn.org.
FAU - Cadiente, Angelo
AU  - Cadiente A
AD  - Hackensack Meridian School of Medicine, Nutley, New Jersey, USA.
FAU - Kasselman, Lora J
AU  - Kasselman LJ
AD  - Research Institute, Hackensack Meridian Health, Edison, New Jersey, USA.
FAU - Pilkington, Bryan
AU  - Pilkington B
AD  - Hackensack Meridian School of Medicine, Nutley, New Jersey, USA.
LA  - eng
PT  - Journal Article
DEP - 20240123
PL  - England
TA  - J Med Ethics
JT  - Journal of medical ethics
JID - 7513619
SB  - IM
MH  - Humans
MH  - Child
MH  - Artificial Intelligence
MH  - *Medicine
MH  - *Education, Medical
MH  - Morals
MH  - Language
OTO - NOTNLM
OT  - Decision Making
OT  - Education
OT  - Ethics- Medical
COIS- Competing interests: None declared.
EDAT- 2023/11/17 15:27
MHDA- 2024/01/25 06:43
CRDT- 2023/11/16 21:03
PHST- 2023/06/26 00:00 [received]
PHST- 2023/11/02 00:00 [accepted]
PHST- 2024/01/25 06:43 [medline]
PHST- 2023/11/17 15:27 [pubmed]
PHST- 2023/11/16 21:03 [entrez]
AID - jme-2023-109366 [pii]
AID - 10.1136/jme-2023-109366 [doi]
PST - epublish
SO  - J Med Ethics. 2024 Jan 23;50(2):97-101. doi: 10.1136/jme-2023-109366.

PMID- 37476960
OWN - NLM
STAT- Publisher
LR  - 20231014
IS  - 1475-1313 (Electronic)
IS  - 0275-5408 (Linking)
VI  - 43
IP  - 6
DP  - 2023 Nov
TI  - Assessing the utility of ChatGPT as an artificial intelligence-based large 
      language model for information to answer questions on myopia.
PG  - 1562-1570
LID - 10.1111/opo.13207 [doi]
AB  - PURPOSE: ChatGPT is an artificial intelligence language model, which uses natural 
      language processing to simulate human conversation. It has seen a wide range of 
      applications including healthcare education, research and clinical practice. This 
      study evaluated the accuracy of ChatGPT in providing accurate and quality 
      information to answer questions on myopia. METHODS: A series of 11 questions 
      (nine categories of general summary, cause, symptom, onset, prevention, 
      complication, natural history, treatment and prognosis) were generated for this 
      cross-sectional study. Each question was entered five times into fresh ChatGPT 
      sessions (free from influence of prior questions). The responses were evaluated 
      by a five-member team of optometry teaching and research staff. The evaluators 
      individually rated the accuracy and quality of responses on a Likert scale, where 
      a higher score indicated greater quality of information (1: very poor; 2: poor; 
      3: acceptable; 4: good; 5: very good). Median scores for each question were 
      estimated and compared between evaluators. Agreement between the five evaluators 
      and the reliability statistics of the questions were estimated. RESULTS: Of the 
      11 questions on myopia, ChatGPT provided good quality information (median scores: 
      4.0) for 10 questions and acceptable responses (median scores: 3.0) for one 
      question. Out of 275 responses in total, 66 (24%) were rated very good, 134 (49%) 
      were rated good, whereas 60 (22%) were rated acceptable, 10 (3.6%) were rated 
      poor and 5 (1.8%) were rated very poor. Cronbach's α of 0.807 indicated good 
      level of agreement between test items. Evaluators' ratings demonstrated 'slight 
      agreement' (Fleiss's κ, 0.005) with a significant difference in scoring among the 
      evaluators (Kruskal-Wallis test, p &lt; 0.001). CONCLUSION: Overall, ChatGPT 
      generated good quality information to answer questions on myopia. Although 
      ChatGPT shows great potential in rapidly providing information on myopia, the 
      presence of inaccurate responses demonstrates that further evaluation and 
      awareness concerning its limitations are crucial to avoid potential 
      misinterpretation.
CI  - © 2023 The Authors. Ophthalmic and Physiological Optics published by John Wiley &amp; 
      Sons Ltd on behalf of College of Optometrists.
FAU - Biswas, Sayantan
AU  - Biswas S
AUID- ORCID: 0000-0001-6011-0365
AD  - School of Optometry, College of Health and Life Sciences, Aston University, 
      Birmingham, UK.
FAU - Logan, Nicola S
AU  - Logan NS
AUID- ORCID: 0000-0002-0538-9516
AD  - School of Optometry, College of Health and Life Sciences, Aston University, 
      Birmingham, UK.
FAU - Davies, Leon N
AU  - Davies LN
AUID- ORCID: 0000-0002-1554-0566
AD  - School of Optometry, College of Health and Life Sciences, Aston University, 
      Birmingham, UK.
FAU - Sheppard, Amy L
AU  - Sheppard AL
AUID- ORCID: 0000-0003-0035-8267
AD  - School of Optometry, College of Health and Life Sciences, Aston University, 
      Birmingham, UK.
FAU - Wolffsohn, James S
AU  - Wolffsohn JS
AUID- ORCID: 0000-0003-4673-8927
AD  - School of Optometry, College of Health and Life Sciences, Aston University, 
      Birmingham, UK.
LA  - eng
PT  - Journal Article
DEP - 20230721
PL  - England
TA  - Ophthalmic Physiol Opt
JT  - Ophthalmic &amp; physiological optics : the journal of the British College of 
      Ophthalmic Opticians (Optometrists)
JID - 8208839
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - chatbot response
OT  - myopia
OT  - patient information
EDAT- 2023/07/21 06:43
MHDA- 2023/07/21 06:43
CRDT- 2023/07/21 04:33
PHST- 2023/06/04 00:00 [revised]
PHST- 2023/03/21 00:00 [received]
PHST- 2023/07/11 00:00 [accepted]
PHST- 2023/07/21 06:43 [pubmed]
PHST- 2023/07/21 06:43 [medline]
PHST- 2023/07/21 04:33 [entrez]
AID - 10.1111/opo.13207 [doi]
PST - ppublish
SO  - Ophthalmic Physiol Opt. 2023 Nov;43(6):1562-1570. doi: 10.1111/opo.13207. Epub 
      2023 Jul 21.

PMID- 38352437
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240325
DP  - 2024 Jan 31
TI  - Evaluating ChatGPT's Accuracy in Providing Screening Mammography Recommendations 
      among Older Women: Artificial Intelligence and Cancer Communication.
LID - rs.3.rs-3911155 [pii]
LID - 10.21203/rs.3.rs-3911155/v1 [doi]
AB  - Abstract Objective: The U.S. Preventive Services Task Force (USPSTF) recommends 
      biennial screening mammography through age 74. Guidelines vary as to whether or 
      not they recommended mammography screening to women aged 75 and older. This study 
      aims to determine the ability of ChatGPT to provide appropriate recommendations 
      for breast cancer screening in patients aged 75 years and older. Methods: 12 
      questions and 4 clinical vignettes addressing fundamental concepts about breast 
      cancer screening and prevention in patients aged 75 years and older were created 
      and asked to ChatGPT three consecutive times to generate 3 sets of responses. The 
      responses were graded by a multi-disciplinary panel of experts in the 
      intersection of breast cancer screening and aging . The responses were graded as 
      'appropriate', 'inappropriate', or 'unreliable' based on the reviewer's clinical 
      judgment, content of the response, and whether the content was consistent across 
      the three responses . Appropriateness was determined through a majority 
      consensus. Results: The responses generated by ChatGPT were appropriate for 11/17 
      questions (64%). Three questions were graded as inappropriate (18%) and 2 
      questions were graded as unreliable (12%). A consensus was not reached on one 
      question (6%) and was graded as no consensus. Conclusions: While recognizing the 
      limitations of ChatGPT, it has potential to provide accurate health care 
      information and could be utilized by healthcare professionals to assist in 
      providing recommendations for breast cancer screening in patients age 75 years 
      and older. Physician oversight will be necessary, due to the possibility of 
      ChatGPT to provide inappropriate and unreliable responses, and the importance of 
      accuracy in medicine.
FAU - Braithwaite, Dejana
AU  - Braithwaite D
AUID- ORCID: 0000-0001-8376-5903
FAU - Karanth, Shama D
AU  - Karanth SD
FAU - Divaker, Joel
AU  - Divaker J
FAU - Schoenborn, Nancy
AU  - Schoenborn N
FAU - Lin, Kenneth
AU  - Lin K
FAU - Richman, Ilana
AU  - Richman I
FAU - Hochhegger, Bruno
AU  - Hochhegger B
FAU - O'Neill, Suzanne
AU  - O'Neill S
FAU - Schonberg, Mara
AU  - Schonberg M
LA  - eng
PT  - Preprint
DEP - 20240131
PL  - United States
TA  - Res Sq
JT  - Research square
JID - 101768035
UIN - J Am Geriatr Soc. 2024 Mar 14;:. PMID: 38485652
PMC - PMC10862946
EDAT- 2024/02/14 06:43
MHDA- 2024/02/14 06:44
PMCR- 2024/02/13
CRDT- 2024/02/14 03:53
PHST- 2024/02/14 06:43 [pubmed]
PHST- 2024/02/14 06:44 [medline]
PHST- 2024/02/14 03:53 [entrez]
PHST- 2024/02/13 00:00 [pmc-release]
AID - rs.3.rs-3911155 [pii]
AID - 10.21203/rs.3.rs-3911155/v1 [doi]
PST - epublish
SO  - Res Sq [Preprint]. 2024 Jan 31:rs.3.rs-3911155. doi: 10.21203/rs.3.rs-3911155/v1.

PMID- 37263804
OWN - NLM
STAT- MEDLINE
DCOM- 20231211
LR  - 20231211
IS  - 1535-6302 (Electronic)
IS  - 0363-0188 (Linking)
VI  - 53
IP  - 1
DP  - 2024 Jan-Feb
TI  - Ability of ChatGPT to generate competent radiology reports for distal radius 
      fracture by use of RSNA template items and integrated AO classifier.
PG  - 102-110
LID - S0363-0188(23)00052-X [pii]
LID - 10.1067/j.cpradiol.2023.04.001 [doi]
AB  - The amount of acquired radiology imaging studies grows worldwide at a rapid pace. 
      Novel information technology tools for radiologists promise an increase of 
      reporting quality and as well quantity at the same time. Automated text report 
      drafting is one branch of this development. We defined for the present study in 
      total 9 cases of distal radius fracture. Command files structured according to a 
      template of the Radiological Society of North America (RSNA) and to 
      Arbeitsgemeinschaft Osteosynthese (AO) classifiers were given as input to the 
      natural language processing tool ChatGPT. ChatGPT was tasked with drafting an 
      appropriate radiology report. A parameter study (n&nbsp;=&nbsp;5 iterations) was performed. 
      An overall high appraisal of ChatGPT radiology report quality was obtained in a 
      score card based assessment. ChatGPT demonstrates the capability to adjust output 
      files in response to minor changes in input command files. Existing shortcomings 
      were found in technical terminology and medical interpretation of findings. Text 
      drafting tools might well support work of radiologists in the future. They would 
      allow a radiologist to focus time on the observation of image details and patient 
      pathology. ChatGPT can be considered a substantial step forward towards that aim.
CI  - Copyright © 2023 The Author(s). Published by Elsevier Inc. All rights reserved.
FAU - Bosbach, Wolfram A
AU  - Bosbach WA
AD  - Department of Diagnostic, Interventional and Pediatric Radiology (DIPR), 
      Inselspital, Bern University Hospital, University of Bern, Switzerland. 
      Electronic address: WolframAndreas.Bosbach@Insel.CH.
FAU - Senge, Jan F
AU  - Senge JF
AD  - Department of Mathematics and Computer Science, University of Bremen, Bremen, 
      Germany; Max-Planck Dioscuri Centre for Topological Data Analysis, Warsaw, 
      Poland.
FAU - Nemeth, Bence
AU  - Nemeth B
AD  - Department of Diagnostic, Interventional and Pediatric Radiology (DIPR), 
      Inselspital, Bern University Hospital, University of Bern, Switzerland; Faculty 
      of Information Technology and Bionics, Pázmány Péter Catholic University, 
      Budapest, Hungary.
FAU - Omar, Siti H
AU  - Omar SH
AD  - Department of Diagnostic, Interventional and Pediatric Radiology (DIPR), 
      Inselspital, Bern University Hospital, University of Bern, Switzerland; 
      Department for Radiology, Kuala Lumpur Hospital, Ministry of Health of Malaysia, 
      Kuala Lumpur, Malaysia.
FAU - Mitrakovic, Milena
AU  - Mitrakovic M
AD  - Department of Diagnostic, Interventional and Pediatric Radiology (DIPR), 
      Inselspital, Bern University Hospital, University of Bern, Switzerland.
FAU - Beisbart, Claus
AU  - Beisbart C
AD  - Institute of Philosophy, University of Bern, Bern, Switzerland; Center for 
      Artificial Intelligence in Medicine, University of Bern, Bern, Switzerland.
FAU - Horváth, András
AU  - Horváth A
AD  - Faculty of Information Technology and Bionics, Pázmány Péter Catholic University, 
      Budapest, Hungary.
FAU - Heverhagen, Johannes
AU  - Heverhagen J
AD  - Department of Diagnostic, Interventional and Pediatric Radiology (DIPR), 
      Inselspital, Bern University Hospital, University of Bern, Switzerland.
FAU - Daneshvar, Keivan
AU  - Daneshvar K
AD  - Department of Diagnostic, Interventional and Pediatric Radiology (DIPR), 
      Inselspital, Bern University Hospital, University of Bern, Switzerland.
LA  - eng
PT  - Journal Article
DEP - 20230417
PL  - United States
TA  - Curr Probl Diagn Radiol
JT  - Current problems in diagnostic radiology
JID - 7607123
SB  - IM
MH  - Humans
MH  - *Wrist Fractures
MH  - Radiography
MH  - *Radiology
MH  - Diagnostic Imaging
MH  - North America
EDAT- 2023/06/02 01:07
MHDA- 2023/12/11 12:43
CRDT- 2023/06/01 21:57
PHST- 2023/03/21 00:00 [received]
PHST- 2023/04/13 00:00 [accepted]
PHST- 2023/12/11 12:43 [medline]
PHST- 2023/06/02 01:07 [pubmed]
PHST- 2023/06/01 21:57 [entrez]
AID - S0363-0188(23)00052-X [pii]
AID - 10.1067/j.cpradiol.2023.04.001 [doi]
PST - ppublish
SO  - Curr Probl Diagn Radiol. 2024 Jan-Feb;53(1):102-110. doi: 
      10.1067/j.cpradiol.2023.04.001. Epub 2023 Apr 17.

PMID- 37802673
OWN - NLM
STAT- Publisher
LR  - 20231006
IS  - 1878-4046 (Electronic)
IS  - 1076-6332 (Linking)
DP  - 2023 Oct 5
TI  - Potential Applications and Impact of ChatGPT in Radiology.
LID - S1076-6332(23)00460-9 [pii]
LID - 10.1016/j.acra.2023.08.039 [doi]
AB  - Radiology has always gone hand-in-hand with technology and artificial 
      intelligence (AI) is not new to the field. While various AI devices and 
      algorithms have already been integrated in the daily clinical practice of 
      radiology, with applications ranging from scheduling patient appointments to 
      detecting and diagnosing certain clinical conditions on imaging, the use of 
      natural language processing&nbsp;and large language model&nbsp;based software have been in 
      discussion for a long time.&nbsp;Algorithms like ChatGPT can help in improving patient 
      outcomes, increasing the efficiency of radiology interpretation, and aiding in 
      the overall workflow of radiologists and here we discuss some of its potential 
      applications.
CI  - Copyright © 2023 The Association of University Radiologists. Published by 
      Elsevier Inc. All rights reserved.
FAU - Bajaj, Suryansh
AU  - Bajaj S
AD  - Department of Radiology, University of Arkansas for Medical Sciences, Little 
      Rock, Arkansas&nbsp;72205 (S.B.).
FAU - Gandhi, Darshan
AU  - Gandhi D
AD  - Department of Diagnostic Radiology, University of Tennessee Health Science 
      Center, Memphis, Tennessee&nbsp;38103 (D.G.). Electronic address: 
      darshangandhi7@gmail.com.
FAU - Nayar, Divya
AU  - Nayar D
AD  - Department of Neurology, University of Arkansas for Medical Sciences, Little 
      Rock, Arkansas&nbsp;72205 (D.N.).
LA  - eng
PT  - Journal Article
DEP - 20231005
PL  - United States
TA  - Acad Radiol
JT  - Academic radiology
JID - 9440159
SB  - IM
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - ChatGPT
OT  - GPT-4
OT  - Radiology
COIS- Declaration of Competing Interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2023/10/07 00:42
MHDA- 2023/10/07 00:42
CRDT- 2023/10/06 21:58
PHST- 2023/07/13 00:00 [received]
PHST- 2023/08/15 00:00 [revised]
PHST- 2023/08/28 00:00 [accepted]
PHST- 2023/10/07 00:42 [medline]
PHST- 2023/10/07 00:42 [pubmed]
PHST- 2023/10/06 21:58 [entrez]
AID - S1076-6332(23)00460-9 [pii]
AID - 10.1016/j.acra.2023.08.039 [doi]
PST - aheadofprint
SO  - Acad Radiol. 2023 Oct 5:S1076-6332(23)00460-9. doi: 10.1016/j.acra.2023.08.039.

PMID- 37855875
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20231121
LR  - 20231121
IS  - 1432-119X (Electronic)
IS  - 0948-6143 (Linking)
VI  - 160
IP  - 5
DP  - 2023 Nov
TI  - A note to our authors: Histochemistry and Cell Biology implements&nbsp;guidelines for 
      the use of large language models&nbsp;(including ChatGPT).
PG  - 375
LID - 10.1007/s00418-023-02245-x [doi]
FAU - Roth, Jürgen
AU  - Roth J
AD  - University of Zurich, CH-8091, Zurich, Switzerland. hcb.editor@bluewin.ch.
LA  - eng
PT  - Editorial
PL  - Germany
TA  - Histochem Cell Biol
JT  - Histochemistry and cell biology
JID - 9506663
SB  - IM
EDAT- 2023/10/19 12:48
MHDA- 2023/11/06 06:42
CRDT- 2023/10/19 11:12
PHST- 2023/11/06 06:42 [medline]
PHST- 2023/10/19 12:48 [pubmed]
PHST- 2023/10/19 11:12 [entrez]
AID - 10.1007/s00418-023-02245-x [pii]
AID - 10.1007/s00418-023-02245-x [doi]
PST - ppublish
SO  - Histochem Cell Biol. 2023 Nov;160(5):375. doi: 10.1007/s00418-023-02245-x.

PMID- 38371667
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240220
IS  - 2666-3287 (Electronic)
IS  - 2666-3287 (Linking)
VI  - 15
DP  - 2024 Jun
TI  - Patients and dermatologists are largely satisfied with ChatGPT-generated 
      after-visit summaries: A pilot study.
PG  - 33-35
LID - 10.1016/j.jdin.2023.12.004 [doi]
FAU - Young, Albert T
AU  - Young AT
AD  - Department of Dermatology, Henry Ford Hospital, Detroit, Michigan.
FAU - Lane, Brittany N
AU  - Lane BN
AD  - Department of Medicine, Michigan State University College of Human Medicine, East 
      Lansing, Michigan.
FAU - Ozog, David
AU  - Ozog D
AD  - Department of Dermatology, Henry Ford Hospital, Detroit, Michigan.
AD  - Department of Medicine, Michigan State University College of Human Medicine, East 
      Lansing, Michigan.
FAU - Matthews, Natalie H
AU  - Matthews NH
AD  - Department of Dermatology, Henry Ford Hospital, Detroit, Michigan.
AD  - Department of Medicine, Michigan State University College of Human Medicine, East 
      Lansing, Michigan.
LA  - eng
PT  - Journal Article
DEP - 20231229
PL  - United States
TA  - JAAD Int
JT  - JAAD international
JID - 101774762
PMC - PMC10869927
OTO - NOTNLM
OT  - ChatGPT
OT  - after-visit summary
OT  - artificial intelligence
OT  - large language model
OT  - machine learning
OT  - patient education
COIS- None disclosed.
EDAT- 2024/02/19 06:42
MHDA- 2024/02/19 06:43
PMCR- 2023/12/29
CRDT- 2024/02/19 04:30
PHST- 2024/02/19 06:43 [medline]
PHST- 2024/02/19 06:42 [pubmed]
PHST- 2024/02/19 04:30 [entrez]
PHST- 2023/12/29 00:00 [pmc-release]
AID - S2666-3287(23)00185-2 [pii]
AID - 10.1016/j.jdin.2023.12.004 [doi]
PST - epublish
SO  - JAAD Int. 2023 Dec 29;15:33-35. doi: 10.1016/j.jdin.2023.12.004. eCollection 2024 
      Jun.

PMID- 37567084
OWN - NLM
STAT- MEDLINE
DCOM- 20231010
LR  - 20231018
IS  - 1876-2026 (Electronic)
IS  - 1876-2018 (Linking)
VI  - 88
DP  - 2023 Oct
TI  - A digital ally: The potential roles of ChatGPT in mental health services.
PG  - 103726
LID - S1876-2018(23)00282-4 [pii]
LID - 10.1016/j.ajp.2023.103726 [doi]
FAU - He, Yunhan
AU  - He Y
AD  - School of Psychology, Shenzhen University, Shenzhen, China. Electronic address: 
      psylink_hyh@163.com.
FAU - Liang, Kaixin
AU  - Liang K
AD  - Department of Psychology, Faculty of Social Sciences, University of Macau, Macau, 
      China.
FAU - Han, Binyao
AU  - Han B
AD  - Warner School of Education, University of Rochester, New York, United States.
FAU - Chi, Xinli
AU  - Chi X
AD  - School of Psychology, Shenzhen University, Shenzhen, China.
LA  - eng
PT  - Letter
DEP - 20230807
PL  - Netherlands
TA  - Asian J Psychiatr
JT  - Asian journal of psychiatry
JID - 101517820
SB  - IM
MH  - Humans
MH  - *Mental Health Services
MH  - *Artificial Intelligence
OTO - NOTNLM
OT  - ChatGPT
OT  - Mental health services
COIS- Declaration of Competing Interest The author declares no conflict of interest.
EDAT- 2023/08/12 10:42
MHDA- 2023/10/09 06:42
CRDT- 2023/08/11 18:07
PHST- 2023/06/19 00:00 [received]
PHST- 2023/08/02 00:00 [revised]
PHST- 2023/08/04 00:00 [accepted]
PHST- 2023/10/09 06:42 [medline]
PHST- 2023/08/12 10:42 [pubmed]
PHST- 2023/08/11 18:07 [entrez]
AID - S1876-2018(23)00282-4 [pii]
AID - 10.1016/j.ajp.2023.103726 [doi]
PST - ppublish
SO  - Asian J Psychiatr. 2023 Oct;88:103726. doi: 10.1016/j.ajp.2023.103726. Epub 2023 
      Aug 7.

PMID- 38565486
OWN - NLM
STAT- Publisher
LR  - 20240402
IS  - 1532-8171 (Electronic)
IS  - 0735-6757 (Linking)
DP  - 2024 Mar 19
TI  - Methodological issues on precision and prediction value of ChatGPT in emergency 
      department triage decisions.
LID - S0735-6757(24)00129-3 [pii]
LID - 10.1016/j.ajem.2024.03.019 [doi]
FAU - Sabour, Amirhossein
AU  - Sabour A
AD  - Department of Computing and Software, McMaster University, Hamilton, ON, Canada. 
      Electronic address: saboua4@mcmaster.ca.
FAU - Ghassemi, Fariba
AU  - Ghassemi F
AD  - Eye Research Center, Farabi Eye Hospital, Tehran University of Medical Sciences, 
      Tehran, Iran; Retina and Vitreous Service, Farabi Eye Hospital, Tehran University 
      of Medical Sciences, Tehran, Iran. Electronic address: fariba.ghassemi@gmail.com.
LA  - eng
PT  - Letter
DEP - 20240319
PL  - United States
TA  - Am J Emerg Med
JT  - The American journal of emergency medicine
JID - 8309942
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Chatbot
OT  - Emergency department
OT  - Triage
COIS- Declaration of competing interest The author declare that there are no competing 
      interests.
EDAT- 2024/04/03 00:44
MHDA- 2024/04/03 00:44
CRDT- 2024/04/02 22:35
PHST- 2024/02/06 00:00 [received]
PHST- 2024/03/15 00:00 [accepted]
PHST- 2024/04/03 00:44 [medline]
PHST- 2024/04/03 00:44 [pubmed]
PHST- 2024/04/02 22:35 [entrez]
AID - S0735-6757(24)00129-3 [pii]
AID - 10.1016/j.ajem.2024.03.019 [doi]
PST - aheadofprint
SO  - Am J Emerg Med. 2024 Mar 19:S0735-6757(24)00129-3. doi: 
      10.1016/j.ajem.2024.03.019.

PMID- 38506966
OWN - NLM
STAT- Publisher
LR  - 20240320
IS  - 1432-2161 (Electronic)
IS  - 0364-2348 (Linking)
DP  - 2024 Mar 20
TI  - ChatGPT's limited accuracy in generating anatomical images for medical education.
LID - 10.1007/s00256-024-04655-x [doi]
FAU - Koga, Shunsuke
AU  - Koga S
AUID- ORCID: 0000-0001-8868-9700
AD  - Department of Pathology and Laboratory Medicine, Hospital of the University of 
      Pennsylvania, 3400 Spruce Street, Philadelphia, PA, 19104, USA. 
      shunsuke.koga@pennmedicine.upenn.edu.
FAU - Du, Wei
AU  - Du W
AD  - Department of Pathology and Laboratory Medicine, Hospital of the University of 
      Pennsylvania, 3400 Spruce Street, Philadelphia, PA, 19104, USA.
LA  - eng
PT  - Comment
PT  - Letter
DEP - 20240320
PL  - Germany
TA  - Skeletal Radiol
JT  - Skeletal radiology
JID - 7701953
SB  - IM
CON - Skeletal Radiol. 2024 Mar 4;:. PMID: 38438538
OTO - NOTNLM
OT  - ChatGPT
OT  - Images
OT  - Large language model
OT  - Multimodal
EDAT- 2024/03/20 18:45
MHDA- 2024/03/20 18:45
CRDT- 2024/03/20 12:05
PHST- 2024/03/06 00:00 [received]
PHST- 2024/03/11 00:00 [accepted]
PHST- 2024/03/06 00:00 [revised]
PHST- 2024/03/20 18:45 [medline]
PHST- 2024/03/20 18:45 [pubmed]
PHST- 2024/03/20 12:05 [entrez]
AID - 10.1007/s00256-024-04655-x [pii]
AID - 10.1007/s00256-024-04655-x [doi]
PST - aheadofprint
SO  - Skeletal Radiol. 2024 Mar 20. doi: 10.1007/s00256-024-04655-x.

PMID- 38283041
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240131
IS  - 2296-858X (Print)
IS  - 2296-858X (Electronic)
IS  - 2296-858X (Linking)
VI  - 10
DP  - 2023
TI  - ChatGPT in dermatology: exploring the limited utility amidst the tech hype.
PG  - 1308229
LID - 10.3389/fmed.2023.1308229 [doi]
LID - 1308229
FAU - Zhang, Zhuoya
AU  - Zhang Z
AD  - Affiliated Hospital of Nanjing University of Chinese Medicine, Nanjing, China.
FAU - Zhang, Jiale
AU  - Zhang J
AD  - Institute of Basic Theory for Chinese Medicine, China Academy of Chinese Medical 
      Sciences, Beijing, China.
FAU - Duan, Lianyuan
AU  - Duan L
AD  - Nanjing Hongtu Artificial Intelligence Technology Research Institute, Nanjing, 
      China.
FAU - Tan, Cheng
AU  - Tan C
AD  - Affiliated Hospital of Nanjing University of Chinese Medicine, Nanjing, China.
LA  - eng
PT  - Journal Article
DEP - 20240111
PL  - Switzerland
TA  - Front Med (Lausanne)
JT  - Frontiers in medicine
JID - 101648047
PMC - PMC10811787
OTO - NOTNLM
OT  - ChatGPT
OT  - GPT-4
OT  - artificial intelligence (AI)
OT  - dermatology
OT  - dermatology education
OT  - medical education
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2024/01/29 06:44
MHDA- 2024/01/29 06:45
PMCR- 2024/01/11
CRDT- 2024/01/29 04:14
PHST- 2023/10/06 00:00 [received]
PHST- 2023/12/21 00:00 [accepted]
PHST- 2024/01/29 06:45 [medline]
PHST- 2024/01/29 06:44 [pubmed]
PHST- 2024/01/29 04:14 [entrez]
PHST- 2024/01/11 00:00 [pmc-release]
AID - 10.3389/fmed.2023.1308229 [doi]
PST - epublish
SO  - Front Med (Lausanne). 2024 Jan 11;10:1308229. doi: 10.3389/fmed.2023.1308229. 
      eCollection 2023.

PMID- 38111835
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231220
IS  - 1985-207X (Print)
IS  - 1985-2274 (Electronic)
IS  - 1985-2274 (Linking)
VI  - 18
DP  - 2023
TI  - Should ChatGPT be considered a medical writer?
PG  - 69
LID - 10.51866/lte.483 [doi]
FAU - Wattanapisit, Apichai
AU  - Wattanapisit A
AD  - MD, Dip., Thai Board of Family Medicine, Academic Fellowship Department of 
      Clinical Medicine, School of Medicine, Walailak University, Thasala, Nakhon Si 
      Thammarat, Thailand.
AD  - Family Medicine Clinic, Walailak University Hospital, Nakhon Si Thammarat, 
      Thailand. Email: apichai.wa@wu.ac.th.
FAU - Photia, Apichat
AU  - Photia A
AD  - MD, Dip., Thai Board of Pediatrics, Pediatric Hematology and Oncology, Research 
      Fellowship Phramongkutklao Hospital and College of Medicine, Bangkok, Thailand.
FAU - Wattanapisit, Sanhapan
AU  - Wattanapisit S
AD  - MD, Dip., Thai Board of Family Medicine, MSc Family Medicine Unit, Thasala 
      Hospital, Nakhon Si Thammarat, Thailand.
LA  - eng
PT  - Journal Article
DEP - 20231128
PL  - Malaysia
TA  - Malays Fam Physician
JT  - Malaysian family physician : the official journal of the Academy of Family 
      Physicians of Malaysia
JID - 101466855
PMC - PMC10726750
OTO - NOTNLM
OT  - Authorship
OT  - ChatGPT
OT  - Writing
COIS- AW is an editorial board member of the journal. The other authors declare no 
      conflicts of interest.
EDAT- 2023/12/19 06:42
MHDA- 2023/12/19 06:43
PMCR- 2023/11/28
CRDT- 2023/12/19 04:00
PHST- 2023/12/19 06:43 [medline]
PHST- 2023/12/19 06:42 [pubmed]
PHST- 2023/12/19 04:00 [entrez]
PHST- 2023/11/28 00:00 [pmc-release]
AID - 10.51866/lte.483 [doi]
PST - epublish
SO  - Malays Fam Physician. 2023 Nov 28;18:69. doi: 10.51866/lte.483. eCollection 2023.

PMID- 37727444
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230921
IS  - 2162-2531 (Print)
IS  - 2162-2531 (Electronic)
IS  - 2162-2531 (Linking)
VI  - 33
DP  - 2023 Sep 12
TI  - Can artificial intelligence-strengthened ChatGPT or other large language models 
      transform nucleic acid research?
PG  - 205-207
LID - 10.1016/j.omtn.2023.06.019 [doi]
FAU - Chatterjee, Srijan
AU  - Chatterjee S
AD  - Institute for Skeletal Aging &amp; Orthopaedic Surgery, Hallym University-Chuncheon 
      Sacred Heart Hospital, Chuncheon-si, Gangwon-do 24252, Republic of Korea.
FAU - Bhattacharya, Manojit
AU  - Bhattacharya M
AD  - Department of Zoology, Fakir Mohan University, Vyasa Vihar, Balasore, Odisha 
      756020, India.
FAU - Lee, Sang-Soo
AU  - Lee SS
AD  - Institute for Skeletal Aging &amp; Orthopaedic Surgery, Hallym University-Chuncheon 
      Sacred Heart Hospital, Chuncheon-si, Gangwon-do 24252, Republic of Korea.
FAU - Chakraborty, Chiranjib
AU  - Chakraborty C
AD  - Department of Biotechnology, School of Life Science and Biotechnology, Adamas 
      University, Kolkata, West Bengal 700126, India.
LA  - eng
PT  - News
DEP - 20230712
PL  - United States
TA  - Mol Ther Nucleic Acids
JT  - Molecular therapy. Nucleic acids
JID - 101581621
PMC - PMC10505907
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - large language models
OT  - nucleic acid
COIS- The authors declare no competing interests.
EDAT- 2023/09/20 06:42
MHDA- 2023/09/20 06:43
PMCR- 2023/07/12
CRDT- 2023/09/20 03:50
PHST- 2023/09/20 06:43 [medline]
PHST- 2023/09/20 06:42 [pubmed]
PHST- 2023/09/20 03:50 [entrez]
PHST- 2023/07/12 00:00 [pmc-release]
AID - S2162-2531(23)00169-5 [pii]
AID - 10.1016/j.omtn.2023.06.019 [doi]
PST - epublish
SO  - Mol Ther Nucleic Acids. 2023 Jul 12;33:205-207. doi: 10.1016/j.omtn.2023.06.019. 
      eCollection 2023 Sep 12.

PMID- 37207953
OWN - NLM
STAT- MEDLINE
DCOM- 20230822
LR  - 20230830
IS  - 1097-6787 (Electronic)
IS  - 0190-9622 (Linking)
VI  - 89
IP  - 3
DP  - 2023 Sep
TI  - The utility of ChatGPT in generating patient-facing and clinical responses for 
      melanoma.
PG  - 602-604
LID - S0190-9622(23)00908-8 [pii]
LID - 10.1016/j.jaad.2023.05.024 [doi]
FAU - Young, Jade N
AU  - Young JN
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York.
FAU - Ross O'Hagan
AU  - Ross O'Hagan
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York.
FAU - Poplausky, Dina
AU  - Poplausky D
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York.
FAU - Levoska, Melissa A
AU  - Levoska MA
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York.
FAU - Gulati, Nicholas
AU  - Gulati N
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York.
FAU - Ungar, Benjamin
AU  - Ungar B
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York.
FAU - Ungar, Jonathan
AU  - Ungar J
AD  - Department of Dermatology, Icahn School of Medicine at Mount Sinai, New York, New 
      York. Electronic address: jonathan.ungar@mountsinai.org.
LA  - eng
PT  - Journal Article
DEP - 20230518
PL  - United States
TA  - J Am Acad Dermatol
JT  - Journal of the American Academy of Dermatology
JID - 7907132
SB  - IM
MH  - Humans
MH  - *Melanoma/diagnosis
MH  - Syndrome
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - language learning model
OT  - melanoma
COIS- Conflicts of interest None disclosed.
EDAT- 2023/05/20 09:42
MHDA- 2023/08/22 06:42
CRDT- 2023/05/19 19:27
PHST- 2023/04/06 00:00 [received]
PHST- 2023/04/26 00:00 [revised]
PHST- 2023/05/03 00:00 [accepted]
PHST- 2023/08/22 06:42 [medline]
PHST- 2023/05/20 09:42 [pubmed]
PHST- 2023/05/19 19:27 [entrez]
AID - S0190-9622(23)00908-8 [pii]
AID - 10.1016/j.jaad.2023.05.024 [doi]
PST - ppublish
SO  - J Am Acad Dermatol. 2023 Sep;89(3):602-604. doi: 10.1016/j.jaad.2023.05.024. Epub 
      2023 May 18.

PMID- 37879994
OWN - NLM
STAT- MEDLINE
DCOM- 20240105
LR  - 20240214
IS  - 0219-3108 (Electronic)
IS  - 1015-9584 (Linking)
VI  - 47
IP  - 1
DP  - 2024 Jan
TI  - Surgery in the era of ChatGPT: A bibliometric analysis based on web of science.
PG  - 784-785
LID - S1015-9584(23)01646-9 [pii]
LID - 10.1016/j.asjsur.2023.10.034 [doi]
FAU - He, Si-Ke
AU  - He SK
AD  - Department of Urology, West China Hospital, Sichuan University, Chengdu, China.
FAU - Tu, Teng
AU  - Tu T
AD  - West China School of Medicine, Sichuan University, Chengdu, China.
FAU - Deng, Bo-Wen
AU  - Deng BW
AD  - Southwest Medical University, Luzhou, China.
FAU - Bai, Yun-Jin
AU  - Bai YJ
AD  - Department of Urology, West China Hospital, Sichuan University, Chengdu, China. 
      Electronic address: baiyunjin@scu.edu.cn.
LA  - eng
PT  - Letter
DEP - 20231023
PL  - Netherlands
TA  - Asian J Surg
JT  - Asian journal of surgery
JID - 8900600
SB  - IM
MH  - *Bibliometrics
MH  - *Surgical Procedures, Operative
MH  - *Artificial Intelligence
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Bibliometric analysis
OT  - ChatGPT
OT  - Surgery
EDAT- 2023/10/26 00:42
MHDA- 2024/01/05 06:43
CRDT- 2023/10/25 21:57
PHST- 2023/09/17 00:00 [received]
PHST- 2023/10/06 00:00 [accepted]
PHST- 2024/01/05 06:43 [medline]
PHST- 2023/10/26 00:42 [pubmed]
PHST- 2023/10/25 21:57 [entrez]
AID - S1015-9584(23)01646-9 [pii]
AID - 10.1016/j.asjsur.2023.10.034 [doi]
PST - ppublish
SO  - Asian J Surg. 2024 Jan;47(1):784-785. doi: 10.1016/j.asjsur.2023.10.034. Epub 
      2023 Oct 23.

PMID- 37394880
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230704
LR  - 20230727
IS  - 2001-1326 (Electronic)
IS  - 2001-1326 (Linking)
VI  - 13
IP  - 7
DP  - 2023 Jul
TI  - The application of ChatGPT in healthcare progress notes: A commentary from a 
      clinical and research perspective.
PG  - e1324
LID - 10.1002/ctm2.1324 [doi]
LID - e1324
FAU - Nguyen, Josh
AU  - Nguyen J
AUID- ORCID: 0000-0002-9106-0388
AD  - Centre for Youth Mental Health, University of Melbourne, Melbourne, Victoria, 
      Australia.
AD  - School of Psychology and Public Health, La Trobe University, Melbourne, Victoria, 
      Australia.
FAU - Pepping, Christopher A
AU  - Pepping CA
AD  - School of Psychology and Public Health, La Trobe University, Melbourne, Victoria, 
      Australia.
AD  - School of Applied Psychology, Griffith University, Brisbane, Queensland, 
      Australia.
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Clin Transl Med
JT  - Clinical and translational medicine
JID - 101597971
SB  - IM
PMC - PMC10315641
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - progress notes
OT  - prompt engineering
EDAT- 2023/07/03 06:42
MHDA- 2023/07/03 06:43
PMCR- 2023/07/02
CRDT- 2023/07/03 02:23
PHST- 2023/06/20 00:00 [received]
PHST- 2023/06/25 00:00 [accepted]
PHST- 2023/07/03 06:43 [medline]
PHST- 2023/07/03 06:42 [pubmed]
PHST- 2023/07/03 02:23 [entrez]
PHST- 2023/07/02 00:00 [pmc-release]
AID - CTM21324 [pii]
AID - 10.1002/ctm2.1324 [doi]
PST - ppublish
SO  - Clin Transl Med. 2023 Jul;13(7):e1324. doi: 10.1002/ctm2.1324.

PMID- 37410015
OWN - NLM
STAT- MEDLINE
DCOM- 20230821
LR  - 20230908
IS  - 2352-0787 (Electronic)
IS  - 2352-0779 (Linking)
VI  - 10
IP  - 5
DP  - 2023 Sep
TI  - Bridging the Gap Between Urological Research and Patient Understanding: The Role 
      of Large Language Models in Automated Generation of Layperson's Summaries.
PG  - 436-443
LID - 10.1097/UPJ.0000000000000428 [doi]
AB  - INTRODUCTION: This study assessed ChatGPT's ability to generate readable, 
      accurate, and clear layperson summaries of urological studies, and compared the 
      performance of ChatGPT-generated summaries with original abstracts and 
      author-written patient summaries to determine its effectiveness as a potential 
      solution for creating accessible medical literature for the public. METHODS: 
      Articles from the top 5 ranked urology journals were selected. A ChatGPT prompt 
      was developed following guidelines to maximize readability, accuracy, and 
      clarity, minimizing variability. Readability scores and grade-level indicators 
      were calculated for the ChatGPT summaries, original abstracts, and patient 
      summaries. Two MD physicians independently rated the accuracy and clarity of the 
      ChatGPT-generated layperson summaries. Statistical analyses were conducted to 
      compare readability scores. Cohen's κ coefficient was used to assess interrater 
      reliability for correctness and clarity evaluations. RESULTS: A total of 256 
      journal articles were included. The ChatGPT-generated summaries were created with 
      an average time of 17.5 (SD 15.0) seconds. The readability scores of the 
      ChatGPT-generated summaries were significantly better than the original 
      abstracts, with Global Readability Score 54.8 (12.3) vs 29.8 (18.5), Flesch 
      Kincade Reading Ease 54.8 (12.3) vs 29.8 (18.5), Flesch Kincaid Grade Level 10.4 
      (2.2) vs 13.5 (4.0), Gunning Fog Score 12.9 (2.6) vs 16.6 (4.1), Smog Index 9.1 
      (2.0) vs 12.0 (3.0), Coleman Liau Index 12.9 (2.1) vs 14.9 (3.7), and Automated 
      Readability Index 11.1 (2.5) vs 12.0 (5.7; P &lt; .0001 for all except Automated 
      Readability Index, which was P = .037). The correctness rate of ChatGPT outputs 
      was &gt;85% across all categories assessed, with interrater agreement (Cohen's κ) 
      between 2 independent physician reviewers ranging from 0.76-0.95. CONCLUSIONS: 
      ChatGPT can create accurate summaries of scientific abstracts for patients, with 
      well-crafted prompts enhancing user-friendliness. Although the summaries are 
      satisfactory, expert verification is necessary for improved accuracy.
FAU - Eppler, Michael B
AU  - Eppler MB
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - Ganjavi, Conner
AU  - Ganjavi C
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - Knudsen, J Everett
AU  - Knudsen JE
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - Davis, Ryan J
AU  - Davis RJ
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - Ayo-Ajibola, Oluwatobiloba
AU  - Ayo-Ajibola O
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - Desai, Aditya
AU  - Desai A
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - Storino Ramacciotti, Lorenzo
AU  - Storino Ramacciotti L
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - Chen, Andrew
AU  - Chen A
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - De Castro Abreu, Andre
AU  - De Castro Abreu A
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - Desai, Mihir M
AU  - Desai MM
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - Gill, Inderbir S
AU  - Gill IS
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
FAU - Cacciamani, Giovanni E
AU  - Cacciamani GE
AD  - USC Institute of Urology and Catherine and Joseph Aresty Department of Urology, 
      Keck School of Medicine, University of Southern California, Los Angeles, 
      California.
AD  - Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
      University of Southern California, Los Angeles, California.
LA  - eng
PT  - Journal Article
DEP - 20230705
PL  - United States
TA  - Urol Pract
JT  - Urology practice
JID - 101635343
SB  - IM
CIN - Urol Pract. 2023 Sep;10(5):444. PMID: 37410018
CIN - Urol Pract. 2023 Sep;10(5):443-444. PMID: 37410020
MH  - Humans
MH  - *Health Literacy
MH  - Reproducibility of Results
MH  - *Urology
MH  - Comprehension
MH  - Language
OTO - NOTNLM
OT  - artificial intelligence
OT  - communication
OT  - comprehension
OT  - patient education as topic
OT  - urology
EDAT- 2023/07/06 13:14
MHDA- 2023/08/21 06:42
CRDT- 2023/07/06 10:33
PHST- 2023/08/21 06:42 [medline]
PHST- 2023/07/06 13:14 [pubmed]
PHST- 2023/07/06 10:33 [entrez]
AID - 10.1097/UPJ.0000000000000428 [doi]
PST - ppublish
SO  - Urol Pract. 2023 Sep;10(5):436-443. doi: 10.1097/UPJ.0000000000000428. Epub 2023 
      Jul 5.

PMID- 37073200
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230421
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - Mitigating the Burden of Severe Pediatric Respiratory Viruses in the 
      Post-COVID-19 Era: ChatGPT Insights and Recommendations.
PG  - e36263
LID - 10.7759/cureus.36263 [doi]
LID - e36263
AB  - In the current post-pandemic era, the rapid spread of respiratory viruses among 
      children and infants resulted in hospitals and pediatric intensive care units 
      (PICUs) becoming overwhelmed. Healthcare providers around the world faced a 
      significant challenge from the outbreak of respiratory viruses like respiratory 
      syncytial virus (RSV), metapneumovirus, and influenza viruses.&nbsp;The chatbot 
      generative pre-trained transformer, ChatGPT, which was launched by OpenAI in 
      November 2022,&nbsp;had both positive and negative aspects in medical writing. Still, 
      it has the potential to&nbsp;generate mitigation suggestions that could be rapidly 
      implemented. We describe the generated suggestion from ChatGPT on 27 Feb 2023 in 
      response to the question "What's your advice for the pediatric intensivists?"&nbsp;We 
      as human authors and healthcare providers, do agree with and supplement with 
      references these suggestions of ChatGPT. We also advocate that artificial 
      intelligence (AI)-enabled chatbots could be utilized in seeking a vigilant and 
      robust healthcare system to rapidly adapt to changing respiratory viruses 
      circulating around the seasons, but AI-generated suggestions need&nbsp;experts to 
      validate them, and further research is warranted.
CI  - Copyright © 2023, Alhasan et al.
FAU - Alhasan, Khalid
AU  - Alhasan K
AD  - Department of Pediatrics, King Saud University, Riyadh, SAU.
AD  - Department of Kidney and Pancreas Transplant, King Faisal Specialist Hospital and 
      Research Center, Riyadh, SAU.
FAU - Al-Tawfiq, Jaffar
AU  - Al-Tawfiq J
AD  - Department of Specialty Internal Medicine and Quality, Johns Hopkins Aramco 
      Healthcare, Dhahran, SAU.
AD  - Department of Medicine, Indiana University School of Medicine, Indianapolis, USA.
AD  - Department of Medicine, Johns Hopkins University School of Medicine, Baltimore, 
      USA.
FAU - Aljamaan, Fadi
AU  - Aljamaan F
AD  - Department of Critical Care, King Saud University, Riyadh, SAU.
FAU - Jamal, Amr
AU  - Jamal A
AD  - Department of Family and Community Medicine, King Saud University, Riyadh, SAU.
FAU - Al-Eyadhy, Ayman
AU  - Al-Eyadhy A
AD  - Department of Pediatrics, King Saud University, Riyadh, SAU.
AD  - Department of Pediatrics, King Saud University Medical City, Riyadh, SAU.
FAU - Temsah, Mohamad-Hani
AU  - Temsah MH
AD  - Department of Pediatrics, King Saud University, Riyadh, SAU.
LA  - eng
PT  - Editorial
DEP - 20230316
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10105647
OTO - NOTNLM
OT  - ai chatbot
OT  - ai language model
OT  - chatgpt
OT  - human metapneumovirus (hmpv)
OT  - influenza vaccine
OT  - pediatric intensive care unit(picu)
OT  - post-covid sequelae
OT  - respiratory syncytial virus (rsv)
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/04/19 06:41
MHDA- 2023/04/19 06:42
PMCR- 2023/03/16
CRDT- 2023/04/19 01:44
PHST- 2023/03/16 00:00 [accepted]
PHST- 2023/04/19 06:42 [medline]
PHST- 2023/04/19 06:41 [pubmed]
PHST- 2023/04/19 01:44 [entrez]
PHST- 2023/03/16 00:00 [pmc-release]
AID - 10.7759/cureus.36263 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 16;15(3):e36263. doi: 10.7759/cureus.36263. eCollection 2023 
      Mar.

PMID- 38049136
OWN - NLM
STAT- MEDLINE
DCOM- 20231206
LR  - 20231216
IS  - 2208-7958 (Electronic)
VI  - 52
IP  - 12
DP  - 2023 Dec
TI  - Universal precautions required: Artificial intelligence takes on the Australian 
      Medical Council's trial examination.
PG  - 863-865
LID - 10.31128/AJGP-02-23-6708 [doi]
AB  - BACKGROUND AND OBJECTIVES: The potential of artificial intelligence in medical 
      practice is increasingly being investigated. This study aimed to examine OpenAI's 
      ChatGPT in answering medical multiple choice questions (MCQ) in an Australian 
      context. METHOD: We provided MCQs from the Australian Medical Council's (AMC) 
      medical licencing practice examination to ChatGPT. The chatbot's responses 
      were&nbsp;graded using AMC's online portal. This experiment was repeated twice. 
      RESULTS: ChatGPT was moderately accurate in answering the questions, achieving a 
      score of 29/50. It was able to generate answer explanations to most questions 
      (45/50). The chatbot was moderately consistent, providing the same overall answer 
      to 40 of the 50 questions between trial runs. DISCUSSION: The moderate accuracy 
      of ChatGPT demonstrates potential risks for both patients and physicians using 
      this tool. Further research is required to create more accurate models and to 
      critically appraise such models.
FAU - Kleinig, Oliver
AU  - Kleinig O
AD  - MBBS III, Faculty of Health and Medical Sciences, The University of Adelaide, 
      Adelaide, SA; Royal Adelaide Hospital, Adelaide, SA.
FAU - Kovoor, Joshua G
AU  - Kovoor JG
AD  - MS, Surgical Resident Medical Officer, Department of General Surgery, Queen 
      Elizabeth Hospital, Woodville South, SA.
FAU - Gupta, Aashray K
AU  - Gupta AK
AD  - MBBS, MS, Cardiothoracic Registrar, Department of Cardiothoracic Surgery, Gold 
      Coast University Hospital, Gold Coast, QLD.
FAU - Bacchi, Stephen
AU  - Bacchi S
AD  - MBBS, PhD, Neurology Registrar, Department of Neurology, Royal Adelaide Hospital, 
      Adelaide, SA; Neurology Registrar, Department of Neurology, Flinders University, 
      Bedford Park, SA.
LA  - eng
PT  - Journal Article
PL  - Australia
TA  - Aust J Gen Pract
JT  - Australian journal of general practice
JID - 101718099
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Universal Precautions
MH  - Australia
MH  - *Physicians
EDAT- 2023/12/05 00:42
MHDA- 2023/12/06 06:42
CRDT- 2023/12/04 19:53
PHST- 2023/12/06 06:42 [medline]
PHST- 2023/12/05 00:42 [pubmed]
PHST- 2023/12/04 19:53 [entrez]
AID - 10.31128/AJGP-02-23-6708 [doi]
PST - ppublish
SO  - Aust J Gen Pract. 2023 Dec;52(12):863-865. doi: 10.31128/AJGP-02-23-6708.

PMID- 37833131
OWN - NLM
STAT- MEDLINE
DCOM- 20231113
LR  - 20240312
IS  - 0968-0004 (Print)
IS  - 0968-0004 (Linking)
VI  - 48
IP  - 12
DP  - 2023 Dec
TI  - AI interprets the Central Dogma and Genetic Code.
PG  - 1014-1018
LID - S0968-0004(23)00230-X [pii]
LID - 10.1016/j.tibs.2023.09.004 [doi]
AB  - Generative artificial intelligence (AI) is a burgeoning field with widespread 
      applications, including in science. Here, we explore two paradigms that provide 
      insight into the capabilities and limitations of Chat Generative Pre-trained 
      Transformer (ChatGPT): its ability to (i) define a core biological concept (the 
      Central Dogma of molecular biology); and (ii) interpret the genetic code.
CI  - Copyright © 2023 Elsevier Ltd. All rights reserved.
FAU - Ille, Alexander M
AU  - Ille AM
AD  - School of Graduate Studies, Rutgers University, Newark, NJ, USA. Electronic 
      address: mai86@gsbs.rutgers.edu.
FAU - Mathews, Michael B
AU  - Mathews MB
AD  - School of Graduate Studies, Rutgers University, Newark, NJ, USA; Department of 
      Medicine, Rutgers New Jersey Medical School, Newark, NJ, USA. Electronic address: 
      mathews@njms.rutgers.edu.
LA  - eng
PT  - Journal Article
DEP - 20231011
PL  - England
TA  - Trends Biochem Sci
JT  - Trends in biochemical sciences
JID - 7610674
SB  - IM
MH  - *Artificial Intelligence
MH  - *Genetic Code
MH  - Molecular Biology
OTO - NOTNLM
OT  - Central Dogma
OT  - ChatGPT
OT  - Sequence Hypothesis
OT  - artificial intelligence
OT  - large language models
OT  - natural language processing
COIS- Declaration of interests No interests are declared.
EDAT- 2023/10/14 10:43
MHDA- 2023/11/13 06:42
CRDT- 2023/10/13 21:59
PHST- 2023/06/19 00:00 [received]
PHST- 2023/09/01 00:00 [revised]
PHST- 2023/09/13 00:00 [accepted]
PHST- 2023/11/13 06:42 [medline]
PHST- 2023/10/14 10:43 [pubmed]
PHST- 2023/10/13 21:59 [entrez]
AID - S0968-0004(23)00230-X [pii]
AID - 10.1016/j.tibs.2023.09.004 [doi]
PST - ppublish
SO  - Trends Biochem Sci. 2023 Dec;48(12):1014-1018. doi: 10.1016/j.tibs.2023.09.004. 
      Epub 2023 Oct 11.

PMID- 37332004
OWN - NLM
STAT- MEDLINE
DCOM- 20231109
LR  - 20231111
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Print)
IS  - 0090-6964 (Linking)
VI  - 51
IP  - 12
DP  - 2023 Dec
TI  - Success Through Simplicity: What Other Artificial Intelligence Applications in 
      Medicine Should Learn from History and ChatGPT.
PG  - 2657-2658
LID - 10.1007/s10439-023-03287-x [doi]
AB  - Many artificial intelligence (AI) algorithms have been developed for medical 
      practice, but few have led to clinically used products. The recent hype of 
      ChatGPT shows us that simple, user-friendly interfaces are one major factor in 
      the applications' popularity. The majority of AI-based applications in clinical 
      practice are still far from simple-to-use applications with user-friendly 
      interfaces. Therefore, simplifying operations is one key to AI-based medical 
      applications' success.
CI  - © 2023. The Author(s).
FAU - Sedaghat, Sam
AU  - Sedaghat S
AUID- ORCID: 0000-0003-2804-3718
AD  - Department of Diagnostic and Interventional Radiology, University Hospital of 
      Heidelberg, Heidelberg, Germany. samsedaghat1@gmail.com.
LA  - eng
PT  - Letter
DEP - 20230618
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - *Artificial Intelligence
MH  - *Algorithms
PMC - PMC10632240
OTO - NOTNLM
OT  - AI
OT  - Application
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Future
OT  - Medicine
COIS- Author declares no conflict of interest.
EDAT- 2023/06/19 00:42
MHDA- 2023/11/09 06:42
PMCR- 2023/06/18
CRDT- 2023/06/18 23:35
PHST- 2023/06/07 00:00 [received]
PHST- 2023/06/13 00:00 [accepted]
PHST- 2023/11/09 06:42 [medline]
PHST- 2023/06/19 00:42 [pubmed]
PHST- 2023/06/18 23:35 [entrez]
PHST- 2023/06/18 00:00 [pmc-release]
AID - 10.1007/s10439-023-03287-x [pii]
AID - 3287 [pii]
AID - 10.1007/s10439-023-03287-x [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2023 Dec;51(12):2657-2658. doi: 10.1007/s10439-023-03287-x. Epub 
      2023 Jun 18.

PMID- 38145370
OWN - NLM
STAT- Publisher
LR  - 20231229
IS  - 1305-3612 (Electronic)
IS  - 1305-3825 (Linking)
DP  - 2023 Dec 25
TI  - Educating the next generation of radiologists: a comparative report of ChatGPT 
      and e-learning resources.
LID - 10.4274/dir.2023.232496 [doi]
AB  - Rapid technological advances have transformed medical education, particularly in 
      radiology, which depends on advanced imaging and visual data. Traditional 
      electronic learning (e-learning) platforms have long served as a cornerstone in 
      radiology education, offering rich visual content, interactive sessions, and 
      peer-reviewed materials. They excel in teaching intricate concepts and techniques 
      that necessitate visual aids, such as image interpretation and procedural 
      demonstrations. However, Chat Generative Pre-Trained Transformer (ChatGPT), an 
      artificial intelligence (AI)-powered language model, has made its mark in 
      radiology education. It can generate learning assessments, create lesson plans, 
      act as a round-the-clock virtual tutor, enhance critical thinking, translate 
      materials for broader accessibility, summarize vast amounts of information, and 
      provide real-time feedback for any subject, including radiology. Concerns have 
      arisen regarding ChatGPT's data accuracy, currency, and potential biases, 
      especially in specialized fields such as radiology. However, the quality, 
      accessibility, and currency of e-learning content can also be imperfect. To 
      enhance the educational journey for radiology residents, the integration of 
      ChatGPT with expert-curated e-learning resources is imperative for ensuring 
      accuracy and reliability and addressing ethical concerns. While AI is unlikely to 
      entirely supplant traditional radiology study methods, the synergistic 
      combination of AI with traditional e-learning can create a holistic educational 
      experience.
FAU - Meşe, İsmail
AU  - Meşe İ
AUID- ORCID: 0000-0002-4429-6996
AD  - Clinic of Radiology, University of Health Sciences Turkey, Erenköy Mental Health 
      and Neurology Training and Research Hospital, İstanbul, Turkey.
FAU - Altıntaş Taşlıçay, Ceylan
AU  - Altıntaş Taşlıçay C
AUID- ORCID: 0000-0003-4459-4114
AD  - Department of Radiology, MD Anderson Cancer Center, Houston, Texas, United 
      States.
FAU - Kuzan, Beyza Nur
AU  - Kuzan BN
AUID- ORCID: 0000-0002-5001-3649
AD  - Clinic of Radiology, Kartal Dr. Lütfi Kırdar City Hospital, İstanbul, Turkey.
FAU - Kuzan, Taha Yusuf
AU  - Kuzan TY
AUID- ORCID: 0000-0002-5420-8507
AD  - Clinic of Radiology, Sancaktepe Şehit Prof. Dr. İlhan Varank Training and 
      Research Hospital, İstanbul, Turkey.
FAU - Sivrioğlu, Ali Kemal
AU  - Sivrioğlu AK
AUID- ORCID: 0000-0002-0500-6335
AD  - Clinic of Radiology, Liv Hospital Vadistanbul, İstanbul, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20231225
PL  - Turkey
TA  - Diagn Interv Radiol
JT  - Diagnostic and interventional radiology (Ankara, Turkey)
JID - 101241152
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - digital case studies
OT  - educational videos
OT  - radiology education
EDAT- 2023/12/25 12:42
MHDA- 2023/12/25 12:42
CRDT- 2023/12/25 07:54
PHST- 2023/12/25 12:42 [medline]
PHST- 2023/12/25 12:42 [pubmed]
PHST- 2023/12/25 07:54 [entrez]
AID - 10.4274/dir.2023.232496 [doi]
PST - aheadofprint
SO  - Diagn Interv Radiol. 2023 Dec 25. doi: 10.4274/dir.2023.232496.

PMID- 38457428
OWN - NLM
STAT- Publisher
LR  - 20240308
IS  - 1539-2031 (Electronic)
IS  - 0192-0790 (Linking)
DP  - 2024 Mar 7
TI  - Colorectal Cancer Prevention and Chat Generative Pretrained Transformer 
      (ChatGPT).
LID - 10.1097/MCG.0000000000001989 [doi]
FAU - Daungsupawong, Hinpetch
AU  - Daungsupawong H
AD  - Private Academic Consultant, Phonhong, Lao People's Democratic Republic.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AD  - Saveetha Medical College, Saveetha Institute of Medical and Technical Sciences 
      Chennai, India.
LA  - eng
PT  - Journal Article
DEP - 20240307
PL  - United States
TA  - J Clin Gastroenterol
JT  - Journal of clinical gastroenterology
JID - 7910017
SB  - IM
EDAT- 2024/03/08 18:42
MHDA- 2024/03/08 18:42
CRDT- 2024/03/08 13:35
PHST- 2024/02/08 00:00 [received]
PHST- 2024/02/08 00:00 [accepted]
PHST- 2024/03/08 18:42 [medline]
PHST- 2024/03/08 18:42 [pubmed]
PHST- 2024/03/08 13:35 [entrez]
AID - 00004836-990000000-00272 [pii]
AID - 10.1097/MCG.0000000000001989 [doi]
PST - aheadofprint
SO  - J Clin Gastroenterol. 2024 Mar 7. doi: 10.1097/MCG.0000000000001989.

PMID- 37428337
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 52
IP  - 3
DP  - 2024 Mar
TI  - A Domain-Specific Next-Generation Large Language Model (LLM) or ChatGPT is 
      Required for Biomedical Engineering and Research.
PG  - 451-454
LID - 10.1007/s10439-023-03306-x [doi]
AB  - Large language models or ChatGPT have recently gained extensive media coverage. 
      At the same time, the use of ChatGPT has increased deistically. Biomedical 
      researchers, engineers, and clinicians have shown significant interest and 
      started using it due to its diverse applications, especially in the biomedical 
      field. However, it has been found that ChatGPT sometimes provided incorrect or 
      partly correct information. It&nbsp;is unable to give the most recent information. 
      Therefore, we urgently advocate a domain-specific next-generation, ChatBot for 
      biomedical engineering and research, providing error-free, more accurate, and 
      updated information. The domain-specific ChatBot can perform diversified 
      functions in biomedical engineering, such as performing innovation in biomedical 
      engineering, designing a medical device, etc. The domain-specific artificial 
      intelligence enabled device will revolutionize biomedical engineering and 
      research if a biomedical domain-specific ChatBot is produced.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Pal, Soumen
AU  - Pal S
AUID- ORCID: 0000-0001-9508-2712
AD  - School of Mechanical Engineering, Vellore Institute of Technology, Vellore, Tamil 
      Nadu, 632014, India.
FAU - Bhattacharya, Manojit
AU  - Bhattacharya M
AUID- ORCID: 0000-0001-9669-1835
AD  - Department of Zoology, Fakir Mohan University, Vyasa Vihar, Balasore, Odisha, 
      756020, India.
FAU - Lee, Sang-Soo
AU  - Lee SS
AUID- ORCID: 0000-0001-5074-7581
AD  - Institute for Skeletal Aging &amp; Orthopaedic Surgery, Hallym University-Chuncheon 
      Sacred Heart Hospital, Chuncheon, Gangwon-Do, 24252, Republic of Korea.
FAU - Chakraborty, Chiranjib
AU  - Chakraborty C
AUID- ORCID: 0000-0002-3958-239X
AD  - Department of Biotechnology, School of Life Science and Biotechnology, Adamas 
      University, Kolkata, West Bengal, 700126, India. drchiranjib@yahoo.com.
LA  - eng
PT  - Letter
DEP - 20230710
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - *Biomedical Engineering
MH  - *Artificial Intelligence
MH  - Bioengineering
MH  - Language
MH  - Software
OTO - NOTNLM
OT  - Biomedical engineering
OT  - ChatBot
OT  - ChatGPT
OT  - Large language model
EDAT- 2023/07/10 13:05
MHDA- 2024/02/12 15:42
CRDT- 2023/07/10 11:12
PHST- 2023/06/29 00:00 [received]
PHST- 2023/07/03 00:00 [accepted]
PHST- 2024/02/12 15:42 [medline]
PHST- 2023/07/10 13:05 [pubmed]
PHST- 2023/07/10 11:12 [entrez]
AID - 10.1007/s10439-023-03306-x [pii]
AID - 10.1007/s10439-023-03306-x [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2024 Mar;52(3):451-454. doi: 10.1007/s10439-023-03306-x. Epub 
      2023 Jul 10.

PMID- 38192545
OWN - NLM
STAT- MEDLINE
DCOM- 20240110
LR  - 20240116
IS  - 2296-2565 (Electronic)
IS  - 2296-2565 (Linking)
VI  - 11
DP  - 2023
TI  - Revolutionizing ocular cancer management: a narrative review on exploring the 
      potential role of ChatGPT.
PG  - 1338215
LID - 10.3389/fpubh.2023.1338215 [doi]
LID - 1338215
AB  - This paper pioneers the exploration of ocular cancer, and its management with the 
      help of Artificial Intelligence (AI) technology. Existing literature presents a 
      significant increase in new eye cancer cases in 2023, experiencing a higher 
      incidence rate. Extensive research was conducted using online databases such as 
      PubMed, ACM Digital Library, ScienceDirect, and Springer. To conduct this review, 
      Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) 
      guidelines are used. Of the collected 62 studies, only 20 documents met the 
      inclusion criteria. The review study identifies seven ocular cancer types. 
      Important challenges associated with ocular cancer are highlighted, including 
      limited awareness about eye cancer, restricted healthcare access, financial 
      barriers, and insufficient infrastructure support. Financial barriers is one of 
      the widely examined ocular cancer challenges in the literature. The potential 
      role and limitations of ChatGPT are discussed, emphasizing its usefulness in 
      providing general information to physicians, noting its inability to deliver 
      up-to-date information. The paper concludes by presenting the potential future 
      applications of ChatGPT to advance research on ocular cancer globally.
CI  - Copyright © 2023 Alotaibi, Rehman and Hasnain.
FAU - Alotaibi, Saud S
AU  - Alotaibi SS
AD  - Information Systems Department, Umm Al-Qura University, Makkah, Saudi Arabia.
FAU - Rehman, Amna
AU  - Rehman A
AD  - Department of Computer Science, Lahore Leads University, Lahore, Pakistan.
FAU - Hasnain, Muhammad
AU  - Hasnain M
AD  - Department of Computer Science, Lahore Leads University, Lahore, Pakistan.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20231215
PL  - Switzerland
TA  - Front Public Health
JT  - Frontiers in public health
JID - 101616579
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - Databases, Factual
MH  - *Eye Neoplasms/epidemiology/therapy
MH  - *Physicians
MH  - Technology
PMC - PMC10773849
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - challenges
OT  - eye cancer
OT  - oncology
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2024/01/09 06:42
MHDA- 2024/01/10 06:42
PMCR- 2023/12/15
CRDT- 2024/01/09 03:51
PHST- 2023/11/14 00:00 [received]
PHST- 2023/12/04 00:00 [accepted]
PHST- 2024/01/10 06:42 [medline]
PHST- 2024/01/09 06:42 [pubmed]
PHST- 2024/01/09 03:51 [entrez]
PHST- 2023/12/15 00:00 [pmc-release]
AID - 10.3389/fpubh.2023.1338215 [doi]
PST - epublish
SO  - Front Public Health. 2023 Dec 15;11:1338215. doi: 10.3389/fpubh.2023.1338215. 
      eCollection 2023.

PMID- 37356806
OWN - NLM
STAT- MEDLINE
DCOM- 20231103
LR  - 20240218
IS  - 1558-349X (Electronic)
IS  - 1546-1440 (Print)
IS  - 1546-1440 (Linking)
VI  - 20
IP  - 10
DP  - 2023 Oct
TI  - Evaluating GPT as an Adjunct for Radiologic Decision Making: GPT-4 Versus GPT-3.5 
      in a Breast Imaging Pilot.
PG  - 990-997
LID - S1546-1440(23)00394-0 [pii]
LID - 10.1016/j.jacr.2023.05.003 [doi]
AB  - OBJECTIVE: Despite rising popularity and performance, studies evaluating the use 
      of large language models for clinical decision support are lacking. Here, we 
      evaluate ChatGPT (Generative Pre-trained Transformer)-3.5 and GPT-4's (OpenAI, 
      San Francisco, California) capacity for clinical decision support in radiology 
      via the identification of appropriate imaging services for two important clinical 
      presentations: breast cancer screening and breast pain. METHODS: We compared 
      ChatGPT's responses to the ACR Appropriateness Criteria for breast pain and 
      breast cancer screening. Our prompt formats included an open-ended (OE) and a 
      select all that apply (SATA) format. Scoring criteria evaluated whether proposed 
      imaging modalities were in accordance with ACR guidelines. Three replicate 
      entries were conducted for each prompt, and the average of these was used to 
      determine final scores. RESULTS: Both ChatGPT-3.5 and ChatGPT-4 achieved an 
      average OE score of 1.830 (out of 2) for breast cancer screening prompts. 
      ChatGPT-3.5 achieved a SATA average percentage correct of 88.9%, compared with 
      ChatGPT-4's average percentage correct of 98.4% for breast cancer screening 
      prompts. For breast pain, ChatGPT-3.5 achieved an average OE score of 1.125 (out 
      of 2) and a SATA average percentage correct of 58.3%, as compared with an average 
      OE score of 1.666 (out of 2) and a SATA average percentage correct of 77.7%. 
      DISCUSSION: Our results demonstrate the eventual feasibility of using large 
      language models like ChatGPT for radiologic decision making, with the potential 
      to improve clinical workflow and responsible use of radiology services. More use 
      cases and greater accuracy are necessary to evaluate and implement such tools.
CI  - Copyright © 2023 American College of Radiology. Published by Elsevier Inc. All 
      rights reserved.
FAU - Rao, Arya
AU  - Rao A
AD  - Harvard Medical School, Boston, Massachusetts; Medically Engineered Solutions in 
      Healthcare, Innovation in Operations Research Center, Massachusetts General 
      Hospital, Boston, Massachusetts.
FAU - Kim, John
AU  - Kim J
AD  - Harvard Medical School, Boston, Massachusetts; Medically Engineered Solutions in 
      Healthcare, Innovation in Operations Research Center, Massachusetts General 
      Hospital, Boston, Massachusetts.
FAU - Kamineni, Meghana
AU  - Kamineni M
AD  - Harvard Medical School, Boston, Massachusetts; Medically Engineered Solutions in 
      Healthcare, Innovation in Operations Research Center, Massachusetts General 
      Hospital, Boston, Massachusetts.
FAU - Pang, Michael
AU  - Pang M
AD  - Harvard Medical School, Boston, Massachusetts; Medically Engineered Solutions in 
      Healthcare, Innovation in Operations Research Center, Massachusetts General 
      Hospital, Boston, Massachusetts.
FAU - Lie, Winston
AU  - Lie W
AD  - Harvard Medical School, Boston, Massachusetts; Medically Engineered Solutions in 
      Healthcare, Innovation in Operations Research Center, Massachusetts General 
      Hospital, Boston, Massachusetts.
FAU - Dreyer, Keith J
AU  - Dreyer KJ
AD  - Harvard Medical School, Boston, Massachusetts; Medically Engineered Solutions in 
      Healthcare, Innovation in Operations Research Center, Massachusetts General 
      Hospital, Boston, Massachusetts; Department of Radiology, Massachusetts General 
      Hospital, Boston, Massachusetts; and Chief Data Science Officer and Chief Imaging 
      Information Officer for Mass General Brigham, Boston, Massachusetts.
FAU - Succi, Marc D
AU  - Succi MD
AD  - Harvard Medical School, Boston, Massachusetts; Medically Engineered Solutions in 
      Healthcare, Innovation in Operations Research Center and Associate Chair of 
      Innovation &amp; Commercialization, Mass General Brigham Enterprise Radiology; 
      Executive Director, MESH Incubator. Massachusetts General Hospital, Boston, 
      Massachusetts; and Department of Radiology, Massachusetts General Hospital, 
      Boston, Massachusetts. Electronic address: msucci@mgh.harvard.edu.
LA  - eng
GR  - T32 GM007753/GM/NIGMS NIH HHS/United States
GR  - T32 GM144273/GM/NIGMS NIH HHS/United States
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
DEP - 20230621
PL  - United States
TA  - J Am Coll Radiol
JT  - Journal of the American College of Radiology : JACR
JID - 101190326
RN  - 76931-93-6 (N-hydroxysuccinimide S-acetylthioacetate)
SB  - IM
UOF - medRxiv. 2023 Feb 07;:. PMID: 36798292
MH  - Humans
MH  - Female
MH  - *Mastodynia
MH  - *Radiology
MH  - *Breast Neoplasms/diagnostic imaging
MH  - Decision Making
PMC - PMC10733745
MID - NIHMS1938414
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - breast imaging
OT  - clinical decision making
OT  - clinical decision support
COIS- The authors state that they have no conflict of interest related to the material 
      discussed in this article. The authors are non-partner/non-partnership 
      track/employees.
EDAT- 2023/06/26 00:41
MHDA- 2023/11/03 06:44
PMCR- 2024/02/16
CRDT- 2023/06/25 19:20
PHST- 2023/04/26 00:00 [received]
PHST- 2023/05/16 00:00 [revised]
PHST- 2023/05/23 00:00 [accepted]
PHST- 2023/11/03 06:44 [medline]
PHST- 2023/06/26 00:41 [pubmed]
PHST- 2023/06/25 19:20 [entrez]
PHST- 2024/02/16 00:00 [pmc-release]
AID - S1546-1440(23)00394-0 [pii]
AID - 10.1016/j.jacr.2023.05.003 [doi]
PST - ppublish
SO  - J Am Coll Radiol. 2023 Oct;20(10):990-997. doi: 10.1016/j.jacr.2023.05.003. Epub 
      2023 Jun 21.

PMID- 38277743
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 1618-0631 (Electronic)
IS  - 0344-0338 (Linking)
VI  - 254
DP  - 2024 Feb
TI  - Navigating the path to precision: ChatGPT as a tool in pathology.
PG  - 155141
LID - S0344-0338(24)00052-9 [pii]
LID - 10.1016/j.prp.2024.155141 [doi]
AB  - In recent years, the integration of Artificial Intelligence (AI) into medicine 
      has marked a transformative shift in healthcare practices. This study explores 
      the application of ChatGPT 3.5, an AI-based natural language processing model, in 
      the field of pathology, with a focus on Clinical Pathology, Histopathology, and 
      Hematology. Leveraging a dataset of 30 clinical cases from an online source, the 
      model's performance was evaluated, revealing moderate proficiency in data 
      analysis and decision support. While ChatGPT demonstrated strengths in swift 
      narrative comprehension and foundational insights, limitations were observed in 
      generating detailed and comprehensive information. The study emphasizes the 
      evolving nature of AI in pathology, highlighting the need for ongoing refinement 
      and collaborative efforts between AI researchers and healthcare professionals.
CI  - Copyright © 2024 Elsevier GmbH. All rights reserved.
FAU - Vaidyanathaiyer, Rajalakshmi
AU  - Vaidyanathaiyer R
AD  - Department of Pathology, Panimalar Medical College Hospital &amp; Research Institute, 
      Varadharajapuram, Poonamallee, Chennai&nbsp;600 123, Tamil Nadu, India.
FAU - Thanigaimani, Gayathri Devi
AU  - Thanigaimani GD
AD  - Department of Pathology, Panimalar Medical College Hospital &amp; Research Institute, 
      Varadharajapuram, Poonamallee, Chennai&nbsp;600 123, Tamil Nadu, India.
FAU - Arumugam, Prathiba
AU  - Arumugam P
AD  - Department of Pathology, Panimalar Medical College Hospital &amp; Research Institute, 
      Varadharajapuram, Poonamallee, Chennai&nbsp;600 123, Tamil Nadu, India.
FAU - Einstien, Dinisha
AU  - Einstien D
AD  - Department of Pathology, Panimalar Medical College Hospital &amp; Research Institute, 
      Varadharajapuram, Poonamallee, Chennai&nbsp;600 123, Tamil Nadu, India.
FAU - Ganesan, Sarumathy
AU  - Ganesan S
AD  - Department of Pathology, Panimalar Medical College Hospital &amp; Research Institute, 
      Varadharajapuram, Poonamallee, Chennai&nbsp;600 123, Tamil Nadu, India.
FAU - Surapaneni, Krishna Mohan
AU  - Surapaneni KM
AD  - Department of Biochemistry, Panimalar Medical College Hospital &amp; Research 
      Institute, Varadharajapuram, Poonamallee, Chennai 600 123, Tamil Nadu, India; 
      Department of Medical Education, Panimalar Medical College Hospital &amp; Research 
      Institute, Varadharajapuram, Poonamallee, Chennai 600 123, Tamil Nadu, India. 
      Electronic address: krishnamohan.surapaneni@gmail.com.
LA  - eng
PT  - Journal Article
DEP - 20240117
PL  - Germany
TA  - Pathol Res Pract
JT  - Pathology, research and practice
JID - 7806109
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Pathology, Clinical
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Clinical pathology
OT  - Diagnostic decision support
OT  - Hematology
OT  - Histopathology
OT  - Natural language processing
COIS- Declaration of Competing Interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2024/01/27 12:42
MHDA- 2024/02/11 07:43
CRDT- 2024/01/26 18:03
PHST- 2023/12/26 00:00 [received]
PHST- 2024/01/13 00:00 [revised]
PHST- 2024/01/13 00:00 [accepted]
PHST- 2024/02/11 07:43 [medline]
PHST- 2024/01/27 12:42 [pubmed]
PHST- 2024/01/26 18:03 [entrez]
AID - S0344-0338(24)00052-9 [pii]
AID - 10.1016/j.prp.2024.155141 [doi]
PST - ppublish
SO  - Pathol Res Pract. 2024 Feb;254:155141. doi: 10.1016/j.prp.2024.155141. Epub 2024 
      Jan 17.

PMID- 38242810
OWN - NLM
STAT- Publisher
LR  - 20240119
IS  - 1875-595X (Electronic)
IS  - 0020-6539 (Linking)
DP  - 2024 Jan 18
TI  - Performance of Generative Artificial Intelligence in Dental Licensing 
      Examinations.
LID - S0020-6539(23)00989-9 [pii]
LID - 10.1016/j.identj.2023.12.007 [doi]
AB  - OBJECTIVES: Generative artificial intelligence (GenAI), including large language 
      models (LLMs), has vast potential applications in health care and education. 
      However, it is unclear how proficient LLMs are in interpreting written input and 
      providing accurate answers in dentistry. This study aims to investigate the 
      accuracy of GenAI in answering questions from dental licensing examinations. 
      METHODS: A total of 1461 multiple-choice questions from question books for the US 
      and the UK dental licensing examinations were input into 2 versions of ChatGPT 
      3.5 and 4.0. The passing rates of the US and UK dental examinations were 75.0% 
      and 50.0%, respectively. The performance of the 2 versions of GenAI in individual 
      examinations and dental subjects was analysed and compared. RESULTS: ChatGPT 3.5 
      correctly answered 68.3% (n&nbsp;=&nbsp;509) and 43.3% (n&nbsp;=&nbsp;296) of questions from the US 
      and UK dental licensing examinations, respectively. The scores for ChatGPT 4.0 
      were 80.7% (n&nbsp;=&nbsp;601) and 62.7% (n&nbsp;=&nbsp;429), respectively. ChatGPT 4.0 passed both 
      written dental licensing examinations, whilst ChatGPT 3.5 failed. ChatGPT 4.0 
      answered 327 more questions correctly and 102 incorrectly compared to ChatGPT 3.5 
      when comparing the 2 versions. CONCLUSIONS: The newer version of GenAI has shown 
      good proficiency in answering multiple-choice questions from dental licensing 
      examinations. Whilst the more recent version of GenAI generally performed better, 
      this observation may not hold true in all scenarios, and further improvements are 
      necessary. The use of GenAI in dentistry will have significant implications for 
      dentist-patient communication and the training of dental professionals.
CI  - Copyright © 2023 The Authors. Published by Elsevier Inc. All rights reserved.
FAU - Chau, Reinhard Chun Wang
AU  - Chau RCW
AD  - Faculty of Dentistry, The University of Hong Kong, Hong Kong Special 
      Administrative Region, China.
FAU - Thu, Khaing Myat
AU  - Thu KM
AD  - Faculty of Dentistry, The University of Hong Kong, Hong Kong Special 
      Administrative Region, China.
FAU - Yu, Ollie Yiru
AU  - Yu OY
AD  - Faculty of Dentistry, The University of Hong Kong, Hong Kong Special 
      Administrative Region, China.
FAU - Hsung, Richard Tai-Chiu
AU  - Hsung RT
AD  - Faculty of Dentistry, The University of Hong Kong, Hong Kong Special 
      Administrative Region, China; Department of Computer Science, Hong Kong Chu Hai 
      College, Hong Kong Special Administrative Region, China.
FAU - Lo, Edward Chin Man
AU  - Lo ECM
AD  - Faculty of Dentistry, The University of Hong Kong, Hong Kong Special 
      Administrative Region, China.
FAU - Lam, Walter Yu Hang
AU  - Lam WYH
AD  - Faculty of Dentistry, The University of Hong Kong, Hong Kong Special 
      Administrative Region, China; Musketeers Foundation Institute of Data Science, 
      The University of Hong Kong, Hong Kong Special Administrative Region, China. 
      Electronic address: retlaw@hku.hk.
LA  - eng
PT  - Journal Article
DEP - 20240118
PL  - England
TA  - Int Dent J
JT  - International dental journal
JID - 0374714
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Communication
OT  - Dental education
OT  - Digital technology
OT  - Examination questions
OT  - Specialties, Dental
COIS- Conflict of interest None disclosed.
EDAT- 2024/01/20 05:42
MHDA- 2024/01/20 05:42
CRDT- 2024/01/19 21:57
PHST- 2023/08/23 00:00 [received]
PHST- 2023/12/15 00:00 [revised]
PHST- 2023/12/22 00:00 [accepted]
PHST- 2024/01/20 05:42 [medline]
PHST- 2024/01/20 05:42 [pubmed]
PHST- 2024/01/19 21:57 [entrez]
AID - S0020-6539(23)00989-9 [pii]
AID - 10.1016/j.identj.2023.12.007 [doi]
PST - aheadofprint
SO  - Int Dent J. 2024 Jan 18:S0020-6539(23)00989-9. doi: 10.1016/j.identj.2023.12.007.

PMID- 37945282
OWN - NLM
STAT- MEDLINE
DCOM- 20231118
LR  - 20231118
IS  - 2053-3624 (Print)
IS  - 2053-3624 (Electronic)
IS  - 2053-3624 (Linking)
VI  - 10
IP  - 2
DP  - 2023 Nov
TI  - Heart-to-heart with ChatGPT: the impact of patients consulting AI for 
      cardiovascular health advice.
LID - 10.1136/openhrt-2023-002455 [doi]
LID - e002455
AB  - OBJECTIVES: The advent of conversational artificial intelligence (AI) systems 
      employing large language models such as ChatGPT has sparked public, professional 
      and academic debates on the capabilities of such technologies. This mixed-methods 
      study sets out to review and systematically explore the capabilities of ChatGPT 
      to adequately provide health advice to patients when prompted regarding four 
      topics from the field of cardiovascular diseases. METHODS: As of 30 May 2023, 528 
      items on PubMed contained the term ChatGPT in their title and/or abstract, with 
      258 being classified as journal articles and included in our thematic 
      state-of-the-art review. For the experimental part, we systematically developed 
      and assessed 123 prompts across the four topics based on three classes of users 
      and two languages. Medical and communications experts scored ChatGPT's responses 
      according to the 4Cs of language model evaluation proposed in this article: 
      correct, concise, comprehensive and comprehensible. RESULTS: The articles 
      reviewed were fairly evenly distributed across discussing how ChatGPT could be 
      used for medical publishing, in clinical practice and for education of medical 
      personnel and/or patients. Quantitatively and qualitatively assessing the 
      capability of ChatGPT on the 123 prompts demonstrated that, while the responses 
      generally received above-average scores, they occupy a spectrum from the concise 
      and correct via the absurd to what only can be described as hazardously incorrect 
      and incomplete. Prompts formulated at higher levels of health literacy generally 
      yielded higher-quality answers. Counterintuitively, responses in a lower-resource 
      language were often of higher quality. CONCLUSIONS: The results emphasise the 
      relationship between prompt and response quality and hint at potentially 
      concerning futures in personalised medicine. The widespread use of large language 
      models for health advice might amplify existing health inequalities and will 
      increase the pressure on healthcare systems by providing easy access to many 
      seemingly likely differential diagnoses and recommendations for seeing a doctor 
      for even harmless ailments.
CI  - © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No 
      commercial re-use. See rights and permissions. Published by BMJ.
FAU - Lautrup, Anton Danholt
AU  - Lautrup AD
AD  - Department of Mathematics and Computer Science, University of Southern Denmark, 
      Odense, Denmark.
FAU - Hyrup, Tobias
AU  - Hyrup T
AD  - Department of Mathematics and Computer Science, University of Southern Denmark, 
      Odense, Denmark.
FAU - Schneider-Kamp, Anna
AU  - Schneider-Kamp A
AD  - Department of Business and Management, University of Southern Denmark Faculty of 
      Business and Social Sciences, Odense, Denmark.
FAU - Dahl, Marie
AU  - Dahl M
AD  - Department of Clinical Medicine, Aarhus University, Aarhus, Denmark.
FAU - Lindholt, Jes Sanddal
AU  - Lindholt JS
AD  - Department of Clinical Research, University of Southern Denmark, Odense, Denmark.
FAU - Schneider-Kamp, Peter
AU  - Schneider-Kamp P
AUID- ORCID: 0000-0003-4000-5570
AD  - Department of Mathematics and Computer Science, University of Southern Denmark, 
      Odense, Denmark petersk@sdu.dk.
LA  - eng
PT  - Journal Article
PT  - Review
PL  - England
TA  - Open Heart
JT  - Open heart
JID - 101631219
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Cardiovascular Diseases/diagnosis/therapy
MH  - Heart
MH  - Patients
MH  - Referral and Consultation
PMC - PMC10649823
OTO - NOTNLM
OT  - computer simulation
OT  - myocardial infarction
OT  - systematic reviews as topic
COIS- Competing interests: None declared.
EDAT- 2023/11/10 00:44
MHDA- 2023/11/13 06:42
PMCR- 2023/11/09
CRDT- 2023/11/09 21:02
PHST- 2023/08/15 00:00 [received]
PHST- 2023/10/26 00:00 [accepted]
PHST- 2023/11/13 06:42 [medline]
PHST- 2023/11/10 00:44 [pubmed]
PHST- 2023/11/09 21:02 [entrez]
PHST- 2023/11/09 00:00 [pmc-release]
AID - openhrt-2023-002455 [pii]
AID - 10.1136/openhrt-2023-002455 [doi]
PST - ppublish
SO  - Open Heart. 2023 Nov;10(2):e002455. doi: 10.1136/openhrt-2023-002455.

PMID- 37535043
OWN - NLM
STAT- MEDLINE
DCOM- 20231127
LR  - 20231127
IS  - 1827-1898 (Electronic)
IS  - 0031-0808 (Linking)
VI  - 65
IP  - 4
DP  - 2023 Dec
TI  - ChatGPT: unlocking the potential of Artifical Intelligence in COVID-19 monitoring 
      and prediction.
PG  - 461-466
LID - 10.23736/S0031-0808.23.04853-X [doi]
AB  - BACKGROUND: The COVID-19 pandemic has had an unprecedent impact of everyday life 
      with deleterious consequences on global health, economics, and society. Thus, 
      accurate and timely information is critical for monitoring its spread and 
      mitigating its impact. ChatGPT is a large language model chatbot with artificial 
      intelligence, developed by OpenAI, that can provide both textual content and R 
      code for predictive models. It may prove to be useful in analyzing and 
      interpreting COVID-19-related data. METHODS: This paper explores the application 
      of ChatGPT to the monitoring of the COVID-19 pandemic, presenting R code for 
      predictive models and demonstrating the model's capabilities in sentiment 
      analysis, information extraction, and predictive modelling. We used the 
      prediction models suggested by ChatGPT to predict the daily number of COVID-19 
      deaths in Italy. The prediction accuracy of the models was compared using the 
      following metrics: mean squared error (MSE), mean absolute deviation (MAD) and 
      root mean squared error (RMSE). RESULTS: ChatGPT suggested three different 
      predictive models, including ARIMA, Random Forest and Prophet. The ARIMA model 
      outperformed the other two models in predicting the daily number of COVID-19 
      deaths in Italy, with lower MSE, MAD, and RMSE values as compared to the Random 
      Forest and Prophet. CONCLUSIONS: This paper demonstrates the potential of ChatGPT 
      as a valuable tool in the monitoring of the pandemic. By processing large amounts 
      of data and providing relevant information, ChatGPT has the potential to provide 
      accurate and timely insights, and support decision-making processes to mitigate 
      the spread and impact of pandemics. The paper highlights the importance of 
      exploring the capabilities of artificial intelligence in the management of public 
      emergencies and provides a starting point for future research in this area.
FAU - Gerli, Alberto G
AU  - Gerli AG
AD  - Department of Clinical Sciences and Community Health, University of Milan, Milan, 
      Italy - alberto.gerli@unimi.it.
FAU - Soriano, Joan B
AU  - Soriano JB
AD  - Unit of Pulmonology, Hospital Universitario de la Princesa, Madrid, Spain.
AD  - Faculty of Medicine, Autonomous University of Madrid, Madrid, Spain.
AD  - Centro de Investigación Biomédica en Red de Enfermedades Respiratorias (CIBERES), 
      Carlos III Health Institute, Madrid, Spain.
FAU - Alicandro, Gianfranco
AU  - Alicandro G
AD  - Department of Pathophysiology and Transplantation, University of Milan, Milan, 
      Italy.
AD  - Cystic Fibrosis Center, Fondazione IRCCS Ca' Granda Ospedale Maggiore 
      Policlinico, Milan, Italy.
FAU - Salvagno, Michele
AU  - Salvagno M
AD  - Department of Intensive Care, Erasme Hospital, Université Libre de Bruxelles, 
      Brussels, Belgium.
FAU - Taccone, Fabio
AU  - Taccone F
AD  - Department of Intensive Care, Erasme Hospital, Université Libre de Bruxelles, 
      Brussels, Belgium.
FAU - Centanni, Stefano
AU  - Centanni S
AD  - Respiratory Unit, Department of Health Sciences, ASST Santi Paolo e Carlo, 
      University of Milan, Milan, Italy.
FAU - LA Vecchia, Carlo
AU  - LA Vecchia C
AD  - Department of Clinical Sciences and Community Health, University of Milan, Milan, 
      Italy.
LA  - eng
PT  - Journal Article
DEP - 20230803
PL  - Italy
TA  - Panminerva Med
JT  - Panminerva medica
JID - 0421110
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Pandemics
MH  - *COVID-19/epidemiology
MH  - Intelligence
MH  - Italy/epidemiology
EDAT- 2023/08/03 13:09
MHDA- 2023/11/27 12:45
CRDT- 2023/08/03 10:44
PHST- 2023/11/27 12:45 [medline]
PHST- 2023/08/03 13:09 [pubmed]
PHST- 2023/08/03 10:44 [entrez]
AID - S0031-0808.23.04853-X [pii]
AID - 10.23736/S0031-0808.23.04853-X [doi]
PST - ppublish
SO  - Panminerva Med. 2023 Dec;65(4):461-466. doi: 10.23736/S0031-0808.23.04853-X. Epub 
      2023 Aug 3.

PMID- 38237025
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 1549-960X (Electronic)
IS  - 1549-9596 (Linking)
VI  - 64
IP  - 3
DP  - 2024 Feb 12
TI  - ChatGPT in the Material Design: Selected Case Studies to Assess the Potential of 
      ChatGPT.
PG  - 799-811
LID - 10.1021/acs.jcim.3c01702 [doi]
AB  - The pursuit of designing smart and functional materials is of paramount 
      importance across various domains, such as material science, engineering, 
      chemical technology, electronics, biomedicine, energy, and numerous others. 
      Consequently, researchers are actively involved in the development of innovative 
      models and strategies for material design. Recent advancements in analytical 
      tools, experimentation, and computer technology additionally enhance the material 
      design possibilities. Notably, data-driven techniques like artificial 
      intelligence and machine learning have achieved substantial progress in exploring 
      various applications within material science. One such approach, ChatGPT, a large 
      language model, holds transformative potential for addressing complex queries. In 
      this article, we explore ChatGPT's understanding of material science by assigning 
      some simple tasks across various subareas of computational material science. The 
      findings indicate that while ChatGPT may make some minor errors in accomplishing 
      general tasks, it demonstrates the capability to learn and adapt through human 
      interactions. However, issues like output consistency, probable hidden errors, 
      and ethical consequences should be addressed.
FAU - Deb, Jyotirmoy
AU  - Deb J
AUID- ORCID: 0000-0002-1428-5137
AD  - Advanced Computation and Data Sciences Division, CSIR-North East Institute of 
      Science and Technology, Jorhat 785006, Assam, India.
FAU - Saikia, Lakshi
AU  - Saikia L
AUID- ORCID: 0000-0003-0892-7233
AD  - Advanced Materials Group, Materials Sciences &amp; Technology Division, CSIR-North 
      East Institute of Science and Technology, Jorhat 785006, Assam, India.
AD  - Academy of Scientific and Innovative Research (AcSIR), Ghaziabad 201002, Uttar 
      Pradesh, India.
FAU - Dihingia, Kripa Dristi
AU  - Dihingia KD
AUID- ORCID: 0000-0002-9906-325X
AD  - Advanced Computation and Data Sciences Division, CSIR-North East Institute of 
      Science and Technology, Jorhat 785006, Assam, India.
AD  - Academy of Scientific and Innovative Research (AcSIR), Ghaziabad 201002, Uttar 
      Pradesh, India.
FAU - Sastry, G Narahari
AU  - Sastry GN
AUID- ORCID: 0000-0003-3181-7673
AD  - Advanced Computation and Data Sciences Division, CSIR-North East Institute of 
      Science and Technology, Jorhat 785006, Assam, India.
AD  - Academy of Scientific and Innovative Research (AcSIR), Ghaziabad 201002, Uttar 
      Pradesh, India.
LA  - eng
PT  - Journal Article
DEP - 20240118
PL  - United States
TA  - J Chem Inf Model
JT  - Journal of chemical information and modeling
JID - 101230060
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Electronics
MH  - Language
MH  - Machine Learning
MH  - Materials Science
EDAT- 2024/01/18 18:42
MHDA- 2024/02/12 15:42
CRDT- 2024/01/18 14:23
PHST- 2024/02/12 15:42 [medline]
PHST- 2024/01/18 18:42 [pubmed]
PHST- 2024/01/18 14:23 [entrez]
AID - 10.1021/acs.jcim.3c01702 [doi]
PST - ppublish
SO  - J Chem Inf Model. 2024 Feb 12;64(3):799-811. doi: 10.1021/acs.jcim.3c01702. Epub 
      2024 Jan 18.

PMID- 37538021
OWN - NLM
STAT- MEDLINE
DCOM- 20240109
LR  - 20240109
IS  - 1365-2850 (Electronic)
IS  - 1351-0126 (Linking)
VI  - 31
IP  - 1
DP  - 2024 Feb
TI  - Could artificial intelligence write mental health nursing care plans?
PG  - 79-86
LID - 10.1111/jpm.12965 [doi]
AB  - WHAT IS KNOWN ON THE SUBJECT?: Artificial intelligence (AI) is freely available, 
      responds to very basic text input (such as a question) and can now create a wide 
      range of outputs, communicating in many languages or art forms. AI platforms like 
      OpenAI's ChatGPT can now create passages of text that could be used to create 
      plans of care for people with mental health needs. As such, AI output can be 
      difficult to distinguish from human-output, and there is a risk that its use 
      could go unnoticed. WHAT THIS PAPER ADDS TO EXISTING KNOWLEDGE?: Whilst it is 
      known that AI can produce text or pass pre-registration health-profession exams, 
      it is not known if AI can produce meaningful results for care delivery. We asked 
      ChatGPT basic questions about a fictitious person who presents with self-harm and 
      then evaluated the quality of the output. We found that the output could look 
      reasonable to laypersons but there were significant errors and ethical issues. 
      There are potential harms to people in care if AI is used without an expert 
      correcting or removing these errors. WHAT ARE THE IMPLICATIONS FOR PRACTICE?: We 
      suggest that there is a risk that AI use could cause harm if it was used in 
      direct care delivery. There is a lack of policy and research to safeguard people 
      receiving care - and this needs to be in place before AI should be used in this 
      way. Key aspects of the role of a mental health nurse are relational and AI use 
      may diminish mental health nurses' ability to provide safe care in its current 
      form. Many aspects of mental health recovery are linked to relationships and 
      social engagement, however AI is not able to provide this and may push the people 
      who are in most need of help further away from services that assist recovery. 
      ABSTRACT: Background Artificial intelligence (AI) is being increasingly used and 
      discussed in care contexts. ChatGPT has gained significant attention in popular 
      and scientific literature although how ChatGPT can be used in care-delivery is 
      not yet known. Aims To use artificial intelligence (ChatGPT) to create a mental 
      health nursing care plan and evaluate the quality of the output against the 
      authors' clinical experience and existing guidance. Materials &amp; Methods Basic 
      text commands were input into ChatGPT about a fictitious person called 'Emily' 
      who presents with self-injurious behaviour. The output from ChatGPT was then 
      evaluated against the authors' clinical experience and current (national) care 
      guidance. Results ChatGPT was able to provide a care plan that incorporated some 
      principles of dialectical behaviour therapy, but the output had significant 
      errors and limitations and thus there is a reasonable likelihood of harm if used 
      in this way. Discussion AI use is increasing in direct-care contexts through the 
      use of chatbots or other means. However, AI can inhibit clinician to 
      care-recipient engagement, 'recycle' existing stigma, and introduce error, which 
      may thus diminish the ability for care to uphold personhood and therefore lead to 
      significant avoidable harms. Conclusion Use of AI in this context should be 
      avoided until a point where policy and guidance can safeguard the wellbeing of 
      care recipients and the sophistication of AI output has increased. Given 
      ChatGPT's ability to provide superficially reasonable outputs there is a risk 
      that errors may go unnoticed and thus increase the likelihood of patient harms. 
      Further research evaluating AI output is needed to consider how AI may be used 
      safely in care delivery.
CI  - © 2023 The Authors. Journal of Psychiatric and Mental Health Nursing published by 
      John Wiley &amp; Sons Ltd.
FAU - Woodnutt, Samuel
AU  - Woodnutt S
AUID- ORCID: 0000-0001-6821-3158
AD  - School of Health Sciences, University of Southampton, Southampton, Hampshire, UK.
FAU - Allen, Chris
AU  - Allen C
AD  - School of Health Sciences, University of Southampton, Southampton, Hampshire, UK.
FAU - Snowden, Jasmine
AU  - Snowden J
AUID- ORCID: 0000-0001-5290-4587
AD  - School of Health Sciences, University of Southampton, Southampton, Hampshire, UK.
FAU - Flynn, Matt
AU  - Flynn M
AUID- ORCID: 0009-0005-7354-7490
AD  - School of Health Sciences, University of Southampton, Southampton, Hampshire, UK.
FAU - Hall, Simon
AU  - Hall S
AD  - School of Health Sciences, University of Southampton, Southampton, Hampshire, UK.
FAU - Libberton, Paula
AU  - Libberton P
AD  - School of Health Sciences, University of Southampton, Southampton, Hampshire, UK.
FAU - Purvis, Francesca
AU  - Purvis F
AD  - School of Health Sciences, University of Southampton, Southampton, Hampshire, UK.
LA  - eng
PT  - Journal Article
DEP - 20230804
PL  - England
TA  - J Psychiatr Ment Health Nurs
JT  - Journal of psychiatric and mental health nursing
JID - 9439514
MH  - Humans
MH  - Artificial Intelligence
MH  - *Psychiatric Nursing
MH  - Writing
MH  - Mental Health
MH  - *Self-Injurious Behavior
OTO - NOTNLM
OT  - art of nursing
OT  - nursing role
OT  - quality of care
OT  - self-harm
OT  - therapeutic relationships
EDAT- 2023/08/04 06:43
MHDA- 2024/01/09 06:41
CRDT- 2023/08/04 03:30
PHST- 2023/07/14 00:00 [revised]
PHST- 2023/04/05 00:00 [received]
PHST- 2023/07/23 00:00 [accepted]
PHST- 2024/01/09 06:41 [medline]
PHST- 2023/08/04 06:43 [pubmed]
PHST- 2023/08/04 03:30 [entrez]
AID - 10.1111/jpm.12965 [doi]
PST - ppublish
SO  - J Psychiatr Ment Health Nurs. 2024 Feb;31(1):79-86. doi: 10.1111/jpm.12965. Epub 
      2023 Aug 4.

PMID- 37575206
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230815
IS  - 2624-8212 (Electronic)
IS  - 2624-8212 (Linking)
VI  - 6
DP  - 2023
TI  - Using ChatGPT to navigate ambivalent and contradictory research findings on 
      artificial intelligence.
PG  - 1195797
LID - 10.3389/frai.2023.1195797 [doi]
LID - 1195797
AB  - With the rapid development and integration of AI in various domains, 
      understanding the nuances of AI research has become critical for policymakers, 
      researchers, and practitioners. However, the results are vast and diverse and 
      even can be contradictory or ambivalent, presenting a significant challenge for 
      individuals seeking to grasp and synthesize the findings. This perspective paper 
      discusses the ambivalent and contradictory research findings in the literature on 
      artificial intelligence (AI) and explores whether ChatGPT can be used to navigate 
      and make sense of the AI literature.
CI  - Copyright © 2023 Sohail, Madsen, Himeur and Ashraf.
FAU - Sohail, Shahab Saquib
AU  - Sohail SS
AD  - Department of Computer Science and Engineering, School of Engineering Sciences 
      and Technology, Jamia Hamdard, New Delhi, India.
FAU - Madsen, Dag Øivind
AU  - Madsen DØ
AD  - USN School of Business, University of South-Eastern Norway, Hønefoss, Norway.
FAU - Himeur, Yassine
AU  - Himeur Y
AD  - College of Engineering and Information Technology, University of Dubai, Dubai, 
      United Arab Emirates.
FAU - Ashraf, Maheen
AU  - Ashraf M
AD  - Department of Computer Science and Engineering, School of Engineering Sciences 
      and Technology, Jamia Hamdard, New Delhi, India.
LA  - eng
PT  - Journal Article
DEP - 20230727
PL  - Switzerland
TA  - Front Artif Intell
JT  - Frontiers in artificial intelligence
JID - 101770551
PMC - PMC10413582
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - perspective
OT  - reaching consensus
OT  - research findings
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2023/08/14 06:41
MHDA- 2023/08/14 06:42
PMCR- 2023/07/27
CRDT- 2023/08/14 04:25
PHST- 2023/05/25 00:00 [received]
PHST- 2023/07/11 00:00 [accepted]
PHST- 2023/08/14 06:42 [medline]
PHST- 2023/08/14 06:41 [pubmed]
PHST- 2023/08/14 04:25 [entrez]
PHST- 2023/07/27 00:00 [pmc-release]
AID - 10.3389/frai.2023.1195797 [doi]
PST - epublish
SO  - Front Artif Intell. 2023 Jul 27;6:1195797. doi: 10.3389/frai.2023.1195797. 
      eCollection 2023.

PMID- 38499197
OWN - NLM
STAT- Publisher
LR  - 20240318
IS  - 1438-8812 (Electronic)
IS  - 0013-726X (Linking)
DP  - 2024 Mar 18
TI  - Comparative Evaluation of a Language Model and Human Specialists in the 
      Application of ECCO Guidelines for the Management of Inflammatory Bowel Diseases 
      and Malignancies.
LID - 10.1055/a-2289-5732 [doi]
AB  - BACKGROUND AND AIMS: Societal guidelines on colorectal dysplasia screening, 
      surveillance and endoscopic management in inflammatory bowel diseases are rather 
      complex, and physician adherence to them is suboptimal. We aimed to evaluate the 
      use of ChatGPT, a large language model, in generating accurate guideline-based 
      recommendations for colorectal dysplasia screening, surveillance and endoscopic 
      management in inflammatory bowel diseases in line with European Crohn's and 
      Colitis Organization (ECCO) guidelines. METHODS: Thirty clinical scenarios in the 
      form of free text were prepared and presented to three separate sessions of 
      ChatGPT and to eight gastroenterologists, four of them specializing in IBD and 
      four with non-IBD specialties. Two additional IBD specialists subsequently 
      assessed all responses provided by ChatGPT and the four gastroenterologists, 
      judging their accuracy according to ECCO guidelines. RESULTS: ChatGPT had a mean 
      correct response rate of 87.8%. Among the eight gastroenterologists the mean 
      correct response rates were 85.8% for the IBD experts and 89.2% for the non-IBD 
      experts. No statistically significant differences were observed between the 
      accuracy of ChatGPT versus all gastroenterologists (p=0.95), or between the 
      accuracy of ChatGPT versus the IBD experts and non-IBD expert gastroenterologists 
      (p=0.82). CONCLUSIONS: This study highlights the potential of language models in 
      enhancing guideline adherence regarding colorectal dysplasia in IBD. Further 
      investigation of additional resources and prospective evaluation in real-world 
      settings are warranted.
CI  - Thieme. All rights reserved.
FAU - Ghersin, Itai
AU  - Ghersin I
AD  - Gastroenterology, Rambam Health Care Campus, Haifa, Israel.
FAU - Weisshof, Roni
AU  - Weisshof R
AD  - Gastroenterology, Rambam Health Care Campus, Haifa, Israel.
FAU - Koifman, Eduard
AU  - Koifman E
AD  - Gastroenterology, Rambam Health Care Campus, Haifa, Israel.
FAU - Bar-Yoseph, Haggai
AU  - Bar-Yoseph H
AD  - gastroenterology, Rambam Health Care Campus, Haifa, Israel.
FAU - Ben Hur, Dana
AU  - Ben Hur D
AD  - Department of Gastroenterology, Rambam Health Care Campus, Haifa, Israel.
FAU - Maza, Itay
AU  - Maza I
AD  - Gastroenterology, Rambam Health Care Campus, Haifa, Israel.
FAU - Hasnis, Erez
AU  - Hasnis E
AD  - Gastroenterology, Rambam Health Care Campus, Haifa, Israel.
FAU - Nasser, Roni
AU  - Nasser R
AD  - Gastroenterology, Rambam Health Care Campus, Haifa, Israel.
FAU - Ovadia, Baruch
AU  - Ovadia B
AD  - Gastroenterology, Hillel Yaffe Medical Center, Hadera, Israel.
FAU - Dror Zur, Dikla
AU  - Dror Zur D
AD  - Gastroenterology, Nahariya Western Galilee Hospital, Nahariya, Israel.
FAU - Waterman, Matti
AU  - Waterman M
AD  - Gastroenterology, Rambam Health Care Campus, Haifa, Israel.
FAU - Gorelik, Yuri
AU  - Gorelik Y
AUID- ORCID: 0000-0003-2070-5182
AD  - Gastroenterology, Rambam Health Care Campus, Haifa, Israel.
LA  - eng
PT  - Journal Article
DEP - 20240318
PL  - Germany
TA  - Endoscopy
JT  - Endoscopy
JID - 0215166
SB  - IM
COIS- The authors declare that they have no conflict of interest.
EDAT- 2024/03/19 00:42
MHDA- 2024/03/19 00:42
CRDT- 2024/03/18 20:26
PHST- 2024/03/19 00:42 [medline]
PHST- 2024/03/19 00:42 [pubmed]
PHST- 2024/03/18 20:26 [entrez]
AID - 10.1055/a-2289-5732 [doi]
PST - aheadofprint
SO  - Endoscopy. 2024 Mar 18. doi: 10.1055/a-2289-5732.

PMID- 38197146
OWN - NLM
STAT- Publisher
LR  - 20240110
IS  - 1943-3670 (Electronic)
IS  - 0022-3492 (Linking)
DP  - 2024 Jan 10
TI  - Artificial intelligence in dental education: ChatGPT's performance on the 
      periodontic in-service examination.
LID - 10.1002/JPER.23-0514 [doi]
AB  - BACKGROUND: ChatGPT's (Chat Generative Pre-Trained Transformer) remarkable 
      capacity to generate human-like output makes it an appealing learning tool for 
      healthcare students worldwide. Nevertheless, the chatbot's responses may be 
      subject to inaccuracies, putting forth an intense risk of misinformation. 
      ChatGPT's capabilities should be examined in every corner of healthcare 
      education, including dentistry and its specialties, to understand the potential 
      of misinformation associated with the chatbot's use as a learning tool. Our 
      investigation aims to explore ChatGPT's foundation of knowledge in the field of 
      periodontology by evaluating the chatbot's performance on questions obtained from 
      an in-service examination administered by the American Academy of Periodontology 
      (AAP). METHODS: ChatGPT3.5 and ChatGPT4 were evaluated on 311 multiple-choice 
      questions obtained from the 2023 in-service examination administered by the AAP. 
      The dataset of in-service examination questions was accessed through Nova 
      Southeastern University's Department of Periodontology. Our study excluded 
      questions containing an image as ChatGPT does not accept image inputs. RESULTS: 
      ChatGPT3.5 and ChatGPT4 answered 57.9% and 73.6% of in-service questions 
      correctly on the 2023 Periodontics In-Service Written Examination, respectively. 
      A two-tailed t test was incorporated to compare independent sample means, and 
      sample proportions were compared using a two-tailed χ(2) test. A p value below 
      the threshold of 0.05 was deemed statistically significant. CONCLUSION: While 
      ChatGPT4 showed a higher proficiency compared to ChatGPT3.5, both chatbot models 
      leave considerable room for misinformation with their responses relating to 
      periodontology. The findings of the study encourage residents to scrutinize the 
      periodontic information generated by ChatGPT to account for the chatbot's current 
      limitations.
CI  - © 2024 American Academy of Periodontology.
FAU - Danesh, Arman
AU  - Danesh A
AD  - Schulich School of Medicine and Dentistry, Western University, London, Ontario, 
      Canada.
FAU - Pazouki, Hirad
AU  - Pazouki H
AD  - Faculty of Science, Western University, London, Ontario, Canada.
FAU - Danesh, Farzad
AU  - Danesh F
AD  - Elgin Mills Endodontic Specialists, Richmond Hill, Ontario, Canada.
FAU - Danesh, Arsalan
AU  - Danesh A
AD  - Department of Periodontology, College of Dental Medicine, Nova Southeastern 
      University, Davie, Florida, USA.
FAU - Vardar-Sengul, Saynur
AU  - Vardar-Sengul S
AD  - Department of Periodontology, College of Dental Medicine, Nova Southeastern 
      University, Davie, Florida, USA.
LA  - eng
PT  - Journal Article
DEP - 20240110
PL  - United States
TA  - J Periodontol
JT  - Journal of periodontology
JID - 8000345
SB  - IM
OTO - NOTNLM
OT  - artificial intelligence
OT  - continuing dental education
OT  - dentistry
OT  - periodontics
EDAT- 2024/01/10 06:41
MHDA- 2024/01/10 06:41
CRDT- 2024/01/10 03:54
PHST- 2023/11/15 00:00 [revised]
PHST- 2023/08/31 00:00 [received]
PHST- 2023/11/16 00:00 [accepted]
PHST- 2024/01/10 06:41 [medline]
PHST- 2024/01/10 06:41 [pubmed]
PHST- 2024/01/10 03:54 [entrez]
AID - 10.1002/JPER.23-0514 [doi]
PST - aheadofprint
SO  - J Periodontol. 2024 Jan 10. doi: 10.1002/JPER.23-0514.

PMID- 37061037
OWN - NLM
STAT- MEDLINE
DCOM- 20230925
LR  - 20230926
IS  - 2468-7855 (Electronic)
IS  - 2468-7855 (Linking)
VI  - 124
IP  - 5
DP  - 2023 Oct
TI  - Can ChatGPT be used in oral and maxillofacial surgery?
PG  - 101471
LID - S2468-7855(23)00093-9 [pii]
LID - 10.1016/j.jormas.2023.101471 [doi]
AB  - OBJECTIVE: The aim of this study is to assess the usability of the information 
      generated by ChatGPT in oral and maxillofacial surgery. This assessment will have 
      two components: Firstly, to measure the quality of patient information provided, 
      and secondly, to measure the quality of educational information in this field. 
      MATERIALS AND METHODS: Commonly asked questions by patients about oral and 
      maxillofacial surgical procedures and technical questions for training purposes 
      were selected to be posed to ChatGPT. The questions were divided into two 
      categories, consisting of 60 Patient Questions and 60 Technical Questions. The 
      specific topics covered were Impacted Teeth, Dental Implants, Temporomandibular 
      Joint Diseases, and Orthognathic Surgery. ChatGPT provided responses to these 
      questions on February 23, 2023. The answers were evaluated by oral and 
      maxillofacial surgeons using a modified global quality scale. RESULTS: The study 
      involved 33 participating surgeons. The mean score for the answers to the Patient 
      Questions was 4.62±0.78, while the mean score for answers to the Technical 
      Questions was 3.1&nbsp;±&nbsp;1.49. The difference in scores between the two question 
      categories was found to be statistically significant (P&lt;0.001). There was no 
      significant difference between the quality of the answers given to the questions 
      across the different topics (P&gt;0.05). CONCLUSIONS: ChatGPT has significant 
      potential as a tool for patient information in oral and maxillofacial surgery. 
      However, its use in training may not be completely safe at present. Surgeons 
      should exercise caution when using ChatGPT and consider it as a supplement to 
      their clinical knowledge and experience.
CI  - Copyright © 2023 Elsevier Masson SAS. All rights reserved.
FAU - Balel, Yunus
AU  - Balel Y
AD  - Department of Oral and Maxillofacial Surgery, Faculty of Dentistry, Tokat 
      Gaziosmanpasa University, Tokat, Turkey. Electronic address: 
      yunusbalel@hotmail.com.
LA  - eng
PT  - Journal Article
DEP - 20230413
PL  - France
TA  - J Stomatol Oral Maxillofac Surg
JT  - Journal of stomatology, oral and maxillofacial surgery
JID - 101701089
SB  - IM
CIN - J Stomatol Oral Maxillofac Surg. 2023 Oct;124(5):101492. PMID: 37149261
MH  - Humans
MH  - *Surgery, Oral
MH  - *Temporomandibular Joint Disorders
MH  - *Tooth, Impacted
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Chatbot
OT  - Patient information
COIS- Declaration of Competing Interest The author declares that there is no conflict 
      of interest.
EDAT- 2023/04/16 06:00
MHDA- 2023/09/25 06:42
CRDT- 2023/04/15 19:26
PHST- 2023/04/03 00:00 [received]
PHST- 2023/04/05 00:00 [revised]
PHST- 2023/04/12 00:00 [accepted]
PHST- 2023/09/25 06:42 [medline]
PHST- 2023/04/16 06:00 [pubmed]
PHST- 2023/04/15 19:26 [entrez]
AID - S2468-7855(23)00093-9 [pii]
AID - 10.1016/j.jormas.2023.101471 [doi]
PST - ppublish
SO  - J Stomatol Oral Maxillofac Surg. 2023 Oct;124(5):101471. doi: 
      10.1016/j.jormas.2023.101471. Epub 2023 Apr 13.

PMID- 38483582
OWN - NLM
STAT- MEDLINE
DCOM- 20240318
LR  - 20240318
IS  - 1433-8726 (Electronic)
IS  - 0724-4983 (Print)
IS  - 0724-4983 (Linking)
VI  - 42
IP  - 1
DP  - 2024 Mar 14
TI  - The efficacy of artificial intelligence in urology: a detailed analysis of kidney 
      stone-related queries.
PG  - 158
LID - 10.1007/s00345-024-04847-z [doi]
LID - 158
AB  - PURPOSE: The study aimed to assess the efficacy of OpenAI's advanced AI model, 
      ChatGPT, in diagnosing urological conditions, focusing on kidney stones. 
      MATERIALS AND METHODS: A set of 90 structured questions, compliant with EAU 
      Guidelines 2023, was curated by seasoned urologists for this investigation. We 
      evaluated ChatGPT's performance based on the accuracy and completeness of its 
      responses to two types of questions [binary (true/false) and descriptive 
      (multiple-choice)], stratified into difficulty levels: easy, moderate, and 
      complex. Furthermore, we analyzed the model's learning and adaptability capacity 
      by reassessing the initially incorrect responses after a 2 week interval. 
      RESULTS: The model demonstrated commendable accuracy, correctly answering 80% of 
      binary questions (n:45) and 93.3% of descriptive questions (n:45). The model's 
      performance showed no significant variation across different question difficulty 
      levels, with p-values of 0.548 for accuracy and 0.417 for completeness, 
      respectively. Upon reassessment of initially 12 incorrect responses (9 binary to 
      3 descriptive) after two weeks, ChatGPT's accuracy showed substantial 
      improvement. The mean accuracy score significantly increased from 1.58 ± 0.51 to 
      2.83 ± 0.93 (p = 0.004), underlining the model's ability to learn and adapt over 
      time. CONCLUSION: These findings highlight the potential of ChatGPT in urological 
      diagnostics, but also underscore areas requiring enhancement, especially in the 
      completeness of responses to complex queries. The study endorses AI's 
      incorporation into healthcare, while advocating for prudence and professional 
      supervision in its application.
CI  - © 2024. The Author(s).
FAU - Cil, Gökhan
AU  - Cil G
AUID- ORCID: 0000-0001-8997-3164
AD  - Department of Urology, Bagcilar Training and Research Hospital, University of 
      Health Sciences, Istanbul, Turkey. cilgok@gmail.com.
FAU - Dogan, Kazim
AU  - Dogan K
AUID- ORCID: 0000-0002-1773-7119
AD  - Department of Urology, Faculty of Medicine, Istinye University, Istanbul, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20240314
PL  - Germany
TA  - World J Urol
JT  - World journal of urology
JID - 8307716
SB  - IM
MH  - Humans
MH  - *Urology
MH  - Artificial Intelligence
MH  - *Kidney Calculi/diagnosis
MH  - Urologists
MH  - Learning
PMC - PMC10940482
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Kidney stones
OT  - Urology
COIS- The authors have no conflict of interest to declare.
EDAT- 2024/03/14 18:42
MHDA- 2024/03/18 06:42
PMCR- 2024/03/14
CRDT- 2024/03/14 12:15
PHST- 2023/10/26 00:00 [received]
PHST- 2024/01/24 00:00 [accepted]
PHST- 2024/03/18 06:42 [medline]
PHST- 2024/03/14 18:42 [pubmed]
PHST- 2024/03/14 12:15 [entrez]
PHST- 2024/03/14 00:00 [pmc-release]
AID - 10.1007/s00345-024-04847-z [pii]
AID - 4847 [pii]
AID - 10.1007/s00345-024-04847-z [doi]
PST - epublish
SO  - World J Urol. 2024 Mar 14;42(1):158. doi: 10.1007/s00345-024-04847-z.

PMID- 37537126
OWN - NLM
STAT- MEDLINE
DCOM- 20230829
LR  - 20230829
IS  - 1773-0597 (Electronic)
IS  - 0181-5512 (Linking)
VI  - 46
IP  - 7
DP  - 2023 Sep
TI  - Success of ChatGPT, an AI language model, in taking the French language version 
      of the European Board of Ophthalmology examination: A novel approach to medical 
      knowledge assessment.
PG  - 706-711
LID - S0181-5512(23)00305-4 [pii]
LID - 10.1016/j.jfo.2023.05.006 [doi]
AB  - PURPOSE: The purpose of this study was to evaluate the performance of ChatGPT, a 
      cutting-edge artificial intelligence (AI) language model developed by OpenAI, in 
      successfully completing the French language version of the European Board of 
      Ophthalmology (EBO) examination and to assess its potential role in medical 
      education and knowledge assessment. METHODS: ChatGPT, based on the GPT-4 
      architecture, was exposed to a series of EBO examination questions in French, 
      covering various aspects of ophthalmology. The AI's performance was evaluated by 
      comparing its responses with the correct answers provided by ophthalmology 
      experts. Additionally, the study assessed the time taken by ChatGPT to answer 
      each question as a measure of efficiency. RESULTS: ChatGPT achieved a 91% success 
      rate on the EBO examination, demonstrating a high level of competency in 
      ophthalmology knowledge and application. The AI provided correct answers across 
      all question categories, indicating a strong understanding of basic sciences, 
      clinical knowledge, and clinical management. The AI model also answered the 
      questions rapidly, taking only a fraction of the time needed by human 
      test-takers. CONCLUSION: ChatGPT's performance on the French language version of 
      the EBO examination demonstrates its potential to be a valuable tool in medical 
      education and knowledge assessment. Further research is needed to explore optimal 
      ways to implement AI language models in medical education and to address the 
      associated ethical and practical concerns.
CI  - Copyright © 2023 The Author(s). Published by Elsevier Masson SAS.. All rights 
      reserved.
FAU - Panthier, C
AU  - Panthier C
AD  - Department of Ophthalmology, Rothschild Foundation Hospital, 25, rue Manin, 75019 
      Paris, France; Center of Expertise and Research in Optics for Vision (CEROV), 
      Paris, France.
FAU - Gatinel, D
AU  - Gatinel D
AD  - Department of Ophthalmology, Rothschild Foundation Hospital, 25, rue Manin, 75019 
      Paris, France; Center of Expertise and Research in Optics for Vision (CEROV), 
      Paris, France. Electronic address: gatinel@gmail.com.
LA  - eng
PT  - Journal Article
DEP - 20230801
PL  - France
TA  - J Fr Ophtalmol
JT  - Journal francais d'ophtalmologie
JID - 7804128
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Ophthalmology
MH  - Language
OTO - NOTNLM
OT  - AI applications
OT  - Apprentissage par la machine
OT  - Apprentissage profond
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Deep learning
OT  - Entraînement sur base de données
OT  - Ethics in AI
OT  - European Board of Ophthalmology
OT  - Examen médical
OT  - Generative AI
OT  - Générateur de texte
OT  - Génération d’IA
OT  - Human-like interaction
OT  - Intelligence artificielle
OT  - Language model
OT  - Machine learning
OT  - Medical examination
OT  - Modèle conversationnel
OT  - Natural language processing
OT  - OpenAI
OT  - Ophtalmologie
OT  - Ophthalmology
OT  - Simulation d’interaction humaine
OT  - Text generation
OT  - Training dataset
OT  - Transformateur d’architecture
OT  - Transformer architecture
OT  - Éthique en IA
EDAT- 2023/08/04 01:08
MHDA- 2023/08/29 12:43
CRDT- 2023/08/03 21:57
PHST- 2023/03/27 00:00 [received]
PHST- 2023/05/27 00:00 [revised]
PHST- 2023/05/31 00:00 [accepted]
PHST- 2023/08/29 12:43 [medline]
PHST- 2023/08/04 01:08 [pubmed]
PHST- 2023/08/03 21:57 [entrez]
AID - S0181-5512(23)00305-4 [pii]
AID - 10.1016/j.jfo.2023.05.006 [doi]
PST - ppublish
SO  - J Fr Ophtalmol. 2023 Sep;46(7):706-711. doi: 10.1016/j.jfo.2023.05.006. Epub 2023 
      Aug 1.

PMID- 37315798
OWN - NLM
STAT- MEDLINE
DCOM- 20230920
LR  - 20230920
IS  - 1097-6787 (Electronic)
IS  - 0190-9622 (Linking)
VI  - 89
IP  - 4
DP  - 2023 Oct
TI  - ChatGPT for healthcare providers and patients: Practical implications within 
      dermatology.
PG  - 870-871
LID - S0190-9622(23)01106-4 [pii]
LID - 10.1016/j.jaad.2023.05.081 [doi]
FAU - Jin, Joy Q
AU  - Jin JQ
AD  - School of Medicine, University of California San Francisco, San Francisco, 
      California; Department of Dermatology, University of California San Francisco, 
      San Francisco, California. Electronic address: joy.jin@ucsf.edu.
FAU - Dobry, Allison S
AU  - Dobry AS
AD  - Department of Dermatology, University of California San Francisco, San Francisco, 
      California.
LA  - eng
PT  - Journal Article
DEP - 20230612
PL  - United States
TA  - J Am Acad Dermatol
JT  - Journal of the American Academy of Dermatology
JID - 7907132
SB  - IM
MH  - Humans
MH  - *Dermatology
MH  - Health Personnel
MH  - Patients
OTO - NOTNLM
OT  - ChatGPT
OT  - administrative support
OT  - artificial intelligence
OT  - clinical decision-making
OT  - clinical research
OT  - dermatology
OT  - health literacy
OT  - healthcare providers
OT  - innovation
OT  - large language models
OT  - medical education
OT  - patients
OT  - technology
COIS- Conflicts of interest None disclosed.
EDAT- 2023/06/15 01:08
MHDA- 2023/09/20 06:42
CRDT- 2023/06/14 19:16
PHST- 2023/03/02 00:00 [received]
PHST- 2023/05/09 00:00 [revised]
PHST- 2023/05/10 00:00 [accepted]
PHST- 2023/09/20 06:42 [medline]
PHST- 2023/06/15 01:08 [pubmed]
PHST- 2023/06/14 19:16 [entrez]
AID - S0190-9622(23)01106-4 [pii]
AID - 10.1016/j.jaad.2023.05.081 [doi]
PST - ppublish
SO  - J Am Acad Dermatol. 2023 Oct;89(4):870-871. doi: 10.1016/j.jaad.2023.05.081. Epub 
      2023 Jun 12.

PMID- 37267643
OWN - NLM
STAT- MEDLINE
DCOM- 20230612
LR  - 20240229
IS  - 1532-2793 (Electronic)
IS  - 0260-6917 (Linking)
VI  - 127
DP  - 2023 Aug
TI  - What is ChatGPT and what do we do with it? Implications of the age of AI for 
      nursing and midwifery practice and education: An editorial.
PG  - 105835
LID - S0260-6917(23)00129-6 [pii]
LID - 10.1016/j.nedt.2023.105835 [doi]
FAU - Irwin, Pauletta
AU  - Irwin P
AD  - Charles Sturt University, School of Nursing Paramedicine and Healthcare Sciences, 
      Faculty of Science and Health, Australia. Electronic address: pirwin@csu.edu.au.
FAU - Jones, Donovan
AU  - Jones D
AD  - Charles Sturt University, School of Nursing Paramedicine and Healthcare Sciences, 
      Faculty of Science and Health, Australia; University of Newcastle, School of 
      Medicine and Public Health, College of Health Medicine and Wellbeing, Australia.
FAU - Fealy, Shanna
AU  - Fealy S
AD  - Charles Sturt University, School of Nursing Paramedicine and Healthcare Sciences, 
      Faculty of Science and Health, Australia; University of Newcastle, School of 
      Medicine and Public Health, College of Health Medicine and Wellbeing, Australia.
LA  - eng
PT  - Editorial
DEP - 20230530
PL  - Scotland
TA  - Nurse Educ Today
JT  - Nurse education today
JID - 8511379
MH  - Female
MH  - Humans
MH  - Pregnancy
MH  - Artificial Intelligence
MH  - Educational Status
MH  - *Midwifery
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Midwifery
OT  - Nursing
COIS- Declaration of competing interest All listed authors declare that they have no 
      conflicts of interest.
EDAT- 2023/06/02 19:13
MHDA- 2023/06/12 06:43
CRDT- 2023/06/02 18:00
PHST- 2023/02/08 00:00 [received]
PHST- 2023/04/10 00:00 [revised]
PHST- 2023/04/23 00:00 [accepted]
PHST- 2023/06/12 06:43 [medline]
PHST- 2023/06/02 19:13 [pubmed]
PHST- 2023/06/02 18:00 [entrez]
AID - S0260-6917(23)00129-6 [pii]
AID - 10.1016/j.nedt.2023.105835 [doi]
PST - ppublish
SO  - Nurse Educ Today. 2023 Aug;127:105835. doi: 10.1016/j.nedt.2023.105835. Epub 2023 
      May 30.

PMID- 37151080
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230908
LR  - 20230921
IS  - 0036-9330 (Print)
IS  - 0036-9330 (Linking)
VI  - 68
IP  - 3
DP  - 2023 Aug
TI  - ChatGPT in academic publishing: An ally or an adversary?
PG  - 129-130
LID - 10.1177/00369330231174231 [doi]
FAU - Ariyaratne, Sisith
AU  - Ariyaratne S
AD  - Department of Musculoskeletal Radiology, Royal Orthopaedic Hospital, Birmingham, 
      UK.
FAU - Botchu, Rajesh
AU  - Botchu R
AUID- ORCID: 0000-0001-7998-2980
AD  - Department of Musculoskeletal Radiology, Royal Orthopaedic Hospital, Birmingham, 
      UK.
FAU - Iyengar, Karthikeyan P
AU  - Iyengar KP
AUID- ORCID: 0000-0002-4379-1266
AD  - Department of Orthopaedics, Southport and Ormskirk NHS Trust, Southport, UK.
LA  - eng
PT  - Letter
DEP - 20230507
PL  - Scotland
TA  - Scott Med J
JT  - Scottish medical journal
JID - 2983335R
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Chatbots
OT  - algorithms
OT  - artificial intelligence and writing
OT  - machine learning
OT  - scientific writing
EDAT- 2023/05/08 06:42
MHDA- 2023/05/08 06:43
CRDT- 2023/05/08 03:41
PHST- 2023/05/08 06:43 [medline]
PHST- 2023/05/08 06:42 [pubmed]
PHST- 2023/05/08 03:41 [entrez]
AID - 10.1177/00369330231174231 [doi]
PST - ppublish
SO  - Scott Med J. 2023 Aug;68(3):129-130. doi: 10.1177/00369330231174231. Epub 2023 
      May 7.

PMID- 38029791
OWN - NLM
STAT- Publisher
LR  - 20240110
IS  - 1098-8793 (Electronic)
IS  - 0736-6825 (Linking)
DP  - 2024 Jan 10
TI  - ChatGPT and Rhinoplasty Recovery: An Exploration of AI's Role in Postoperative 
      Guidance.
LID - 10.1055/a-2219-4901 [doi]
AB  - The potential applications of artificial intelligence (AI) in health care have 
      garnered significant interest in recent years. This study presents the first 
      published exploration of ChatGPT, an AI language model, as a tool for providing 
      postoperative guidance during rhinoplasty recovery. The primary objective was to 
      shed light on the role of ChatGPT in augmenting patient care during the critical 
      postoperative phase. Using the Rhinobase database, standardized questions were 
      formulated to evaluate AI-generated responses addressing pain management, 
      swelling, bruising, and potential asymmetries. Results demonstrated that ChatGPT 
      has the potential to enhance patient education and alleviate emotional distress 
      by providing general information and reassurance during the recovery process. 
      However, the study emphasized that AI should not replace personalized advice from 
      qualified health care professionals. This pioneering investigation offers 
      valuable insights into the integration of AI and human expertise, paving the way 
      for optimized postrhinoplasty recovery care.
CI  - Thieme. All rights reserved.
FAU - Capelleras, Marta
AU  - Capelleras M
AD  - Department of Otolaryngology Head &amp; Neck Surgery, Ege University, Izmir, Türkiye.
FAU - Soto-Galindo, Germán A
AU  - Soto-Galindo GA
AD  - Department of Otolaryngology Head &amp; Neck Surgery, Ege University, Izmir, Türkiye.
FAU - Cruellas, Marc
AU  - Cruellas M
AD  - Department of Otorhinolaryngology, Hospital Universitari de Bellvitge, Barcelona, 
      Spain.
FAU - Apaydin, Fazil
AU  - Apaydin F
AD  - Department of Otorhinolaryngology, Ege University Medical Faculty, Izmir, 
      Türkiye.
LA  - eng
PT  - Journal Article
DEP - 20240110
PL  - United States
TA  - Facial Plast Surg
JT  - Facial plastic surgery : FPS
JID - 8405303
COIS- None declared.
EDAT- 2023/11/30 00:42
MHDA- 2023/11/30 00:42
CRDT- 2023/11/29 19:13
PHST- 2023/11/30 00:42 [pubmed]
PHST- 2023/11/30 00:42 [medline]
PHST- 2023/11/29 19:13 [entrez]
AID - 10.1055/a-2219-4901 [doi]
PST - aheadofprint
SO  - Facial Plast Surg. 2024 Jan 10. doi: 10.1055/a-2219-4901.

PMID- 37809772
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231018
IS  - 2405-8440 (Print)
IS  - 2405-8440 (Electronic)
IS  - 2405-8440 (Linking)
VI  - 9
IP  - 9
DP  - 2023 Sep
TI  - The impact of Artificial Intelligence in academia: Views of Turkish academics on 
      ChatGPT.
PG  - e19688
LID - 10.1016/j.heliyon.2023.e19688 [doi]
LID - e19688
AB  - In the past decade, Artificial Intelligence (AI) and machine learning 
      technologies have become increasingly prevalent in the academic world. This 
      growing trend has led to debates about the impact of these technologies on 
      academia. The purpose of this article is to examine the impact of ChatGPT, an AI 
      and machine learning technology, in the academic field and to determine 
      academics' perceptions of it. To achieve this goal, in-depth interviews were 
      conducted with 10 academics, and their views on the subject were analyzed. It is 
      seen that academics believe that ChatGPT will play a helpful role as a tool in 
      scientific research and educational processes and can serve as an inspiration for 
      new topics and research areas. Despite these advantages, academics also have 
      ethical concerns, such as plagiarism and misinformation. The study found that 
      ChatGPT is viewed positively as a useful tool in scientific research and 
      education, but ethical concerns such as plagiarism and misinformation need to be 
      addressed.
CI  - © 2023 The Authors.
FAU - Livberber, Tuba
AU  - Livberber T
AD  - Department of Journalism, Faculty of Communication, University of Akdeniz, 
      Antalya, Turkey.
FAU - Ayvaz, Süheyla
AU  - Ayvaz S
AD  - Department of Advertising, Faculty of Communication, University of Selcuk, Konya, 
      Turkey.
LA  - eng
PT  - Journal Article
DEP - 20230901
PL  - England
TA  - Heliyon
JT  - Heliyon
JID - 101672560
PMC - PMC10558923
OTO - NOTNLM
OT  - Academia
OT  - Artificial intelligence
OT  - Human-AI collaboration
OT  - Learning &amp; teaching
OT  - Machine learning
COIS- The authors declare that they have no known competing financial interests or 
      personal relationships that could have appeared to influence the work reported in 
      this paper.
EDAT- 2023/10/09 06:41
MHDA- 2023/10/09 06:42
PMCR- 2023/09/01
CRDT- 2023/10/09 05:57
PHST- 2023/06/06 00:00 [received]
PHST- 2023/08/15 00:00 [revised]
PHST- 2023/08/30 00:00 [accepted]
PHST- 2023/10/09 06:42 [medline]
PHST- 2023/10/09 06:41 [pubmed]
PHST- 2023/10/09 05:57 [entrez]
PHST- 2023/09/01 00:00 [pmc-release]
AID - S2405-8440(23)06896-2 [pii]
AID - e19688 [pii]
AID - 10.1016/j.heliyon.2023.e19688 [doi]
PST - epublish
SO  - Heliyon. 2023 Sep 1;9(9):e19688. doi: 10.1016/j.heliyon.2023.e19688. eCollection 
      2023 Sep.

PMID- 37790062
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231005
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 9
DP  - 2023 Sep
TI  - A Radiation Oncology Board Exam of ChatGPT.
PG  - e44541
LID - 10.7759/cureus.44541 [doi]
LID - e44541
AB  - As artificial intelligence (AI) models improve and become widely integrated into 
      healthcare systems, healthcare providers must understand the strengths and 
      limitations of AI tools to realize the full spectrum of potential patient-care 
      benefits. However, most providers have a poor understanding of AI, leading to 
      distrust and poor adoption of this emerging technology. To bridge this divide, 
      this editorial presents a novel view of ChatGPT's current capabilities in the 
      medical field of radiation oncology. By replicating the format of the oral 
      qualification exam required for radiation oncology board certification, we 
      demonstrate ChatGPT's ability to analyze a commonly encountered patient case, 
      make diagnostic decisions, and integrate information to generate treatment 
      recommendations. Through this simulation, we highlight ChatGPT's strengths and 
      limitations in replicating human decision-making in clinical radiation oncology, 
      while providing an accessible resource to educate radiation oncologists on the 
      capabilities of AI chatbots.
CI  - Copyright © 2023, Barbour et al.
FAU - Barbour, Andrew B
AU  - Barbour AB
AD  - Radiation Oncology, University of Washington - Fred Hutchinson Cancer Center, 
      Seattle, USA.
FAU - Barbour, T Aleksandr
AU  - Barbour TA
AD  - Starlink, SpaceX, Redmond, USA.
LA  - eng
PT  - Editorial
DEP - 20230901
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10544698
OTO - NOTNLM
OT  - ai &amp; robotics in healthcare
OT  - ai in medicine
OT  - ai tools
OT  - radiation oncology
OT  - radiotherapy
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/10/04 06:44
MHDA- 2023/10/04 06:45
PMCR- 2023/09/01
CRDT- 2023/10/04 03:58
PHST- 2023/09/01 00:00 [accepted]
PHST- 2023/10/04 06:45 [medline]
PHST- 2023/10/04 06:44 [pubmed]
PHST- 2023/10/04 03:58 [entrez]
PHST- 2023/09/01 00:00 [pmc-release]
AID - 10.7759/cureus.44541 [doi]
PST - epublish
SO  - Cureus. 2023 Sep 1;15(9):e44541. doi: 10.7759/cureus.44541. eCollection 2023 Sep.

PMID- 36812645
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230224
IS  - 2767-3170 (Electronic)
IS  - 2767-3170 (Linking)
VI  - 2
IP  - 2
DP  - 2023 Feb
TI  - Performance of ChatGPT on USMLE: Potential for AI-assisted medical education 
      using large language models.
PG  - e0000198
LID - 10.1371/journal.pdig.0000198 [doi]
LID - e0000198
AB  - We evaluated the performance of a large language model called ChatGPT on the 
      United States Medical Licensing Exam (USMLE), which consists of three exams: Step 
      1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for 
      all three exams without any specialized training or reinforcement. Additionally, 
      ChatGPT demonstrated a high level of concordance and insight in its explanations. 
      These results suggest that large language models may have the potential to assist 
      with medical education, and potentially, clinical decision-making.
CI  - Copyright: © 2023 Kung et al. This is an open access article distributed under 
      the terms of the Creative Commons Attribution License, which permits unrestricted 
      use, distribution, and reproduction in any medium, provided the original author 
      and source are credited.
FAU - Kung, Tiffany H
AU  - Kung TH
AD  - AnsibleHealth, Inc Mountain View, California, United States of America.
AD  - Department of Anesthesiology, Massachusetts General Hospital, Harvard School of 
      Medicine Boston, Massachusetts, United States of America.
FAU - Cheatham, Morgan
AU  - Cheatham M
AD  - Warren Alpert Medical School; Brown University Providence, Rhode Island, United 
      States of America.
FAU - Medenilla, Arielle
AU  - Medenilla A
AD  - AnsibleHealth, Inc Mountain View, California, United States of America.
FAU - Sillos, Czarina
AU  - Sillos C
AD  - AnsibleHealth, Inc Mountain View, California, United States of America.
FAU - De Leon, Lorie
AU  - De Leon L
AD  - AnsibleHealth, Inc Mountain View, California, United States of America.
FAU - Elepaño, Camille
AU  - Elepaño C
AD  - AnsibleHealth, Inc Mountain View, California, United States of America.
FAU - Madriaga, Maria
AU  - Madriaga M
AD  - AnsibleHealth, Inc Mountain View, California, United States of America.
FAU - Aggabao, Rimel
AU  - Aggabao R
AD  - AnsibleHealth, Inc Mountain View, California, United States of America.
FAU - Diaz-Candido, Giezel
AU  - Diaz-Candido G
AD  - AnsibleHealth, Inc Mountain View, California, United States of America.
FAU - Maningo, James
AU  - Maningo J
AD  - AnsibleHealth, Inc Mountain View, California, United States of America.
FAU - Tseng, Victor
AU  - Tseng V
AUID- ORCID: 0000-0003-0211-512X
AD  - AnsibleHealth, Inc Mountain View, California, United States of America.
AD  - Department of Medical Education, UWorld, LLC Dallas, Texas, United States of 
      America.
LA  - eng
PT  - Journal Article
DEP - 20230209
PL  - United States
TA  - PLOS Digit Health
JT  - PLOS digital health
JID - 9918335064206676
CIN - ChatGPT passing USMLE shines a spotlight on the flaws of medical education.
PMC - PMC9931230
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/02/23 06:00
MHDA- 2023/02/23 06:01
PMCR- 2023/02/09
CRDT- 2023/02/22 17:35
PHST- 2022/12/19 00:00 [received]
PHST- 2023/01/23 00:00 [accepted]
PHST- 2023/02/22 17:35 [entrez]
PHST- 2023/02/23 06:00 [pubmed]
PHST- 2023/02/23 06:01 [medline]
PHST- 2023/02/09 00:00 [pmc-release]
AID - PDIG-D-22-00371 [pii]
AID - 10.1371/journal.pdig.0000198 [doi]
PST - epublish
SO  - PLOS Digit Health. 2023 Feb 9;2(2):e0000198. doi: 10.1371/journal.pdig.0000198. 
      eCollection 2023 Feb.

PMID- 37650428
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 1437-4331 (Electronic)
IS  - 1434-6621 (Linking)
VI  - 62
IP  - 3
DP  - 2024 Feb 26
TI  - Predicting hemoglobinopathies using ChatGPT.
PG  - e59-e61
LID - 10.1515/cclm-2023-0885 [doi]
FAU - Kurstjens, Steef
AU  - Kurstjens S
AD  - Laboratory of Clinical Chemistry and Hematology, Jeroen Bosch Hospital, 's 
      Hertogenbosch, The Netherlands.
FAU - Schipper, Anoeska
AU  - Schipper A
AD  - Laboratory of Clinical Chemistry and Hematology, Jeroen Bosch Hospital, 's 
      Hertogenbosch, The Netherlands.
AD  - Diagnostic Image Analysis Group, Radboudumc, Nijmegen, The Netherlands.
FAU - Krabbe, Johannes
AU  - Krabbe J
AD  - Laboratory of Clinical Chemistry and Laboratory Medicine, Medisch Spectrum 
      Twente, Enschede, The Netherlands.
AD  - Laboratory of Clinical Chemistry and Laboratory Medicine, Medlon BV, Enschede, 
      The Netherlands.
FAU - Kusters, Ron
AU  - Kusters R
AD  - Laboratory of Clinical Chemistry and Hematology, Jeroen Bosch Hospital, 's 
      Hertogenbosch, The Netherlands.
AD  - Department of Health Technology and Services Research, Technical Medical Centre, 
      University of Twente, Enschede, Netherlands.
LA  - eng
PT  - Letter
DEP - 20230829
PL  - Germany
TA  - Clin Chem Lab Med
JT  - Clinical chemistry and laboratory medicine
JID - 9806306
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - diagnostics
OT  - laboratory results
OT  - large language models
EDAT- 2023/08/31 12:42
MHDA- 2023/08/31 12:43
CRDT- 2023/08/31 06:45
PHST- 2023/08/11 00:00 [received]
PHST- 2023/08/22 00:00 [accepted]
PHST- 2023/08/31 12:43 [medline]
PHST- 2023/08/31 12:42 [pubmed]
PHST- 2023/08/31 06:45 [entrez]
AID - cclm-2023-0885 [pii]
AID - 10.1515/cclm-2023-0885 [doi]
PST - epublish
SO  - Clin Chem Lab Med. 2023 Aug 29;62(3):e59-e61. doi: 10.1515/cclm-2023-0885. Print 
      2024 Feb 26.

PMID- 38499716
OWN - NLM
STAT- MEDLINE
DCOM- 20240320
LR  - 20240320
IS  - 1862-3514 (Electronic)
VI  - 19
IP  - 1
DP  - 2024 Mar 19
TI  - Artificial intelligence insights into osteoporosis: assessing ChatGPT's 
      information quality and readability.
PG  - 17
LID - 10.1007/s11657-024-01376-5 [doi]
AB  - Accessible, accurate information, and readability play crucial role in empowering 
      individuals managing osteoporosis. This study showed that the responses generated 
      by ChatGPT regarding osteoporosis had serious problems with quality and were at a 
      level of complexity that that necessitates an educational background of 
      approximately 17&nbsp;years. PURPOSE: The use of artificial intelligence&nbsp;(AI) 
      applications as a source of information in the field of health is increasing. 
      Readable and accurate information plays a critical role in empowering patients to 
      make decisions about their disease. The aim was to examine the quality and 
      readability of responses provided by ChatGPT, an AI chatbot, to commonly asked 
      questions regarding osteoporosis, representing a major public health problem. 
      METHODS: "Osteoporosis," "female osteoporosis," and "male osteoporosis" were 
      identified by using Google trends for the 25 most frequently searched keywords on 
      Google. A selected set of 38 keywords was sequentially inputted into the chat 
      interface of the ChatGPT. The responses were evaluated with tools of the Ensuring 
      Quality Information for Patients (EQIP), the Flesch-Kincaid Grade Level (FKGL), 
      and the Flesch-Kincaid Reading Ease (FKRE). RESULTS: The EQIP score of the texts 
      ranged from a minimum of 36.36 to a maximum of 61.76 with a mean value of 48.71 
      as having "serious problems with quality." The FKRE scores spanned from 13.71 to 
      56.06 with a mean value of 28.71 and the FKGL varied between 8.48 and 17.63, with 
      a mean value of 13.25. There were no statistically significant correlations 
      between the EQIP score and the FKGL or FKRE scores. CONCLUSIONS: Although ChatGPT 
      is easily accessible for patients to obtain information about osteoporosis, its 
      current quality and readability fall short of meeting comprehensive healthcare 
      standards.
CI  - © 2024. International Osteoporosis Foundation and Bone Health and Osteoporosis 
      Foundation.
FAU - Erden, Yakup
AU  - Erden Y
AUID- ORCID: 0000-0003-3742-9903
AD  - Clinic of Physical Medicine and Rehabilitation, İzzet Baysal Physical Treatment 
      and Rehabilitation Training and Research Hospital, Orüs Street, No. 59, 14020, 
      Bolu, Turkey. yakuperden@hotmail.com.
FAU - Temel, Mustafa Hüseyin
AU  - Temel MH
AUID- ORCID: 0000-0003-0256-5833
AD  - Department of Physical Medicine and Rehabilitation, Üsküdar State Hospital, 
      Barbaros, Veysi Paşa Street, No. 14, 34662, Istanbul, Turkey.
FAU - Bağcıer, Fatih
AU  - Bağcıer F
AUID- ORCID: 0000-0002-6103-7873
AD  - Clinic of Physical Medicine and Rehabilitation, Başakşehir Çam and Sakura City 
      Hospital, Olympic Boulevard Road, 34480, Istanbul, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20240319
PL  - England
TA  - Arch Osteoporos
JT  - Archives of osteoporosis
JID - 101318988
SB  - IM
MH  - Humans
MH  - Female
MH  - Male
MH  - *Artificial Intelligence
MH  - Comprehension
MH  - *Osteoporosis
MH  - Public Health
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Chatbot
OT  - Health information
OT  - Osteoporosis
EDAT- 2024/03/19 06:43
MHDA- 2024/03/20 06:45
CRDT- 2024/03/19 00:28
PHST- 2023/11/24 00:00 [received]
PHST- 2024/03/07 00:00 [accepted]
PHST- 2024/03/20 06:45 [medline]
PHST- 2024/03/19 06:43 [pubmed]
PHST- 2024/03/19 00:28 [entrez]
AID - 10.1007/s11657-024-01376-5 [pii]
AID - 10.1007/s11657-024-01376-5 [doi]
PST - epublish
SO  - Arch Osteoporos. 2024 Mar 19;19(1):17. doi: 10.1007/s11657-024-01376-5.

PMID- 37888068
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231029
IS  - 2075-4426 (Print)
IS  - 2075-4426 (Electronic)
IS  - 2075-4426 (Linking)
VI  - 13
IP  - 10
DP  - 2023 Sep 30
TI  - Navigating the Landscape of Personalized Medicine: The Relevance of ChatGPT, 
      BingChat, and Bard AI in Nephrology Literature Searches.
LID - 10.3390/jpm13101457 [doi]
LID - 1457
AB  - BACKGROUND AND OBJECTIVES: Literature reviews are foundational to understanding 
      medical evidence. With AI tools like ChatGPT, Bing Chat and Bard AI emerging as 
      potential aids in this domain, this study aimed to individually assess their 
      citation accuracy within Nephrology, comparing their performance in providing 
      precise. MATERIALS AND METHODS: We generated the prompt to solicit 20 references 
      in Vancouver style in each 12 Nephrology topics, using ChatGPT, Bing Chat and 
      Bard. We verified the existence and accuracy of the provided references using 
      PubMed, Google Scholar, and Web of Science. We categorized the validity of the 
      references from the AI chatbot into (1) incomplete, (2) fabricated, (3) 
      inaccurate, and (4) accurate. RESULTS: A total of 199 (83%), 158 (66%) and 112 
      (47%) unique references were provided from ChatGPT, Bing Chat and Bard, 
      respectively. ChatGPT provided 76 (38%) accurate, 82 (41%) inaccurate, 32 (16%) 
      fabricated and 9 (5%) incomplete references. Bing Chat provided 47 (30%) 
      accurate, 77 (49%) inaccurate, 21 (13%) fabricated and 13 (8%) incomplete 
      references. In contrast, Bard provided 3 (3%) accurate, 26 (23%) inaccurate, 71 
      (63%) fabricated and 12 (11%) incomplete references. The most common error type 
      across platforms was incorrect DOIs. CONCLUSIONS: In the field of medicine, the 
      necessity for faultless adherence to research integrity is highlighted, asserting 
      that even small errors cannot be tolerated. The outcomes of this investigation 
      draw attention to inconsistent citation accuracy across the different AI tools 
      evaluated. Despite some promising results, the discrepancies identified call for 
      a cautious and rigorous vetting of AI-sourced references in medicine. Such 
      chatbots, before becoming standard tools, need substantial refinements to assure 
      unwavering precision in their outputs.
FAU - Aiumtrakul, Noppawit
AU  - Aiumtrakul N
AUID- ORCID: 0000-0003-0479-7785
AD  - Department of Medicine, John A. Burns School of Medicine, University of Hawaii, 
      Honolulu, HI 96813, USA.
FAU - Thongprayoon, Charat
AU  - Thongprayoon C
AD  - Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
      Rochester, MN 55905, USA.
FAU - Suppadungsuk, Supawadee
AU  - Suppadungsuk S
AUID- ORCID: 0000-0003-1597-2411
AD  - Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
      Rochester, MN 55905, USA.
AD  - Chakri Naruebodindra Medical Institute, Faculty of Medicine Ramathibodi Hospital, 
      Mahidol University, Samut Prakan 10540, Thailand.
FAU - Krisanapan, Pajaree
AU  - Krisanapan P
AUID- ORCID: 0000-0002-2888-881X
AD  - Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
      Rochester, MN 55905, USA.
AD  - Department of Internal Medicine, Faculty of Medicine, Thammasat University, 
      Pathum Thani 12120, Thailand.
FAU - Miao, Jing
AU  - Miao J
AUID- ORCID: 0000-0003-0642-9740
AD  - Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
      Rochester, MN 55905, USA.
FAU - Qureshi, Fawad
AU  - Qureshi F
AD  - Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
      Rochester, MN 55905, USA.
FAU - Cheungpasitporn, Wisit
AU  - Cheungpasitporn W
AUID- ORCID: 0000-0001-9954-9711
AD  - Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
      Rochester, MN 55905, USA.
LA  - eng
PT  - Journal Article
DEP - 20230930
PL  - Switzerland
TA  - J Pers Med
JT  - Journal of personalized medicine
JID - 101602269
PMC - PMC10608326
OTO - NOTNLM
OT  - Bard AI
OT  - Bing Chat
OT  - ChatGPT
OT  - accuracy
OT  - literature review
OT  - nephrology references
OT  - personalized medicine
OT  - precision medicine
COIS- The authors declare no conflict of interest. All authors had access to the data 
      and played essential roles in writing of the manuscript.
EDAT- 2023/10/27 12:43
MHDA- 2023/10/27 12:44
PMCR- 2023/09/30
CRDT- 2023/10/27 07:09
PHST- 2023/09/10 00:00 [received]
PHST- 2023/09/29 00:00 [revised]
PHST- 2023/09/29 00:00 [accepted]
PHST- 2023/10/27 12:44 [medline]
PHST- 2023/10/27 12:43 [pubmed]
PHST- 2023/10/27 07:09 [entrez]
PHST- 2023/09/30 00:00 [pmc-release]
AID - jpm13101457 [pii]
AID - jpm-13-01457 [pii]
AID - 10.3390/jpm13101457 [doi]
PST - epublish
SO  - J Pers Med. 2023 Sep 30;13(10):1457. doi: 10.3390/jpm13101457.

PMID- 37073184
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230421
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - The Capability of ChatGPT in Predicting and Explaining Common Drug-Drug 
      Interactions.
PG  - e36272
LID - 10.7759/cureus.36272 [doi]
LID - e36272
AB  - Background Drug-drug interactions (DDIs) can have serious consequences for 
      patient health and well-being. Patients who are taking multiple medications may 
      be at an increased risk of experiencing adverse events or drug toxicity if they 
      are not aware of potential interactions between their medications. Many times, 
      patients self-prescribe medications without knowing DDI. Objective The objective 
      is to investigate the effectiveness of ChatGPT, a large language model, in 
      predicting and explaining common DDIs. Methods A total of 40 DDIs lists were 
      prepared from previously published literature. This list was used to converse 
      with ChatGPT with a two-stage question. The first question was asked as "can I 
      take X and Y together?" with two drug names. After storing the output, the next 
      question was asked. The second question was asked as "why should I not take X and 
      Y together?" The output was stored for further analysis. The responses were 
      checked by two pharmacologists and the consensus output was categorized as 
      "correct" and "incorrect." The "correct" ones were further classified as 
      "conclusive" and "inconclusive." The text was checked for reading ease scores and 
      grades of education required to understand the text. Data were tested by 
      descriptive and inferential statistics. Results Among the 40 DDI pairs, one 
      answer was incorrect in the first question. Among correct answers, 19 were 
      conclusive and 20 were inconclusive. For the second question, one answer was 
      wrong. Among correct answers, 17 were conclusive and 22 were inconclusive. The 
      mean Flesch reading ease score was 27.64±10.85 in answers to the first question 
      and 29.35±10.16 in answers to the second question, p = 0.47. The mean 
      Flesh-Kincaid grade level was 15.06±2.79 in answers to the first question and 
      14.85±1.97 in answers to the second question, p = 0.69. When we compared the 
      reading levels with hypothetical 6th grade, the grades were significantly higher 
      than expected (t = 20.57, p &lt; 0.0001 for first answers and t = 28.43, p &lt; 0.0001 
      for second answers). Conclusion ChatGPT is a partially effective tool for 
      predicting and explaining DDIs. Patients, who may not have immediate access to 
      the healthcare facility for getting information about DDIs, may take help from 
      ChatGPT. However, on several occasions, it may provide incomplete guidance. 
      Further improvement is required for potential usage by patients for getting ideas 
      about DDI.
CI  - Copyright © 2023, Juhi et al.
FAU - Juhi, Ayesha
AU  - Juhi A
AD  - Physiology, All India Institute of Medical Sciences, Deoghar, Deoghar, IND.
FAU - Pipil, Neha
AU  - Pipil N
AD  - Pharmacology, All India Institute of Medical Sciences, Bilaspur, Bilaspur, IND.
FAU - Santra, Soumya
AU  - Santra S
AD  - Pharmacology, College of Medicine and JNM Hospital, Kalyani, IND.
FAU - Mondal, Shaikat
AU  - Mondal S
AD  - Physiology, Raiganj Government Medical College and Hospital, Raiganj, IND.
FAU - Behera, Joshil Kumar
AU  - Behera JK
AD  - Physiology, Dharanidhar Medical College, Keonjhar, Keonjhar, IND.
FAU - Mondal, Himel
AU  - Mondal H
AD  - Physiology, All India Institute of Medical Sciences, Deoghar, Deoghar, IND.
LA  - eng
PT  - Journal Article
DEP - 20230317
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10105894
OTO - NOTNLM
OT  - adverse reactions
OT  - artificial intelligence
OT  - chatgpt
OT  - drug interactions
OT  - drug-drug interaction
OT  - explaining
OT  - language model
OT  - patient education
OT  - predicting
OT  - side effects
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/04/19 06:41
MHDA- 2023/04/19 06:42
PMCR- 2023/03/17
CRDT- 2023/04/19 01:44
PHST- 2023/03/16 00:00 [accepted]
PHST- 2023/04/19 06:42 [medline]
PHST- 2023/04/19 06:41 [pubmed]
PHST- 2023/04/19 01:44 [entrez]
PHST- 2023/03/17 00:00 [pmc-release]
AID - 10.7759/cureus.36272 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 17;15(3):e36272. doi: 10.7759/cureus.36272. eCollection 2023 
      Mar.

PMID- 37059815
OWN - NLM
STAT- MEDLINE
DCOM- 20230418
LR  - 20230418
IS  - 1437-2320 (Electronic)
IS  - 0344-5607 (Linking)
VI  - 46
IP  - 1
DP  - 2023 Apr 14
TI  - The role of an open artificial intelligence platform in modern neurosurgical 
      education: a preliminary study.
PG  - 86
LID - 10.1007/s10143-023-01998-2 [doi]
AB  - The use of artificial intelligence in neurosurgical education has been growing in 
      recent times. ChatGPT, a free and easily accessible language model, has been 
      gaining popularity as an alternative education method. It is necessary to explore 
      the potential of this program in neurosurgery education and to evaluate its 
      reliability. This study aimed to show the reliability of ChatGPT by asking 
      various questions to the chat engine, how it can contribute to neurosurgery 
      education by preparing case reports or questions, and its contributions when 
      writing academic articles. The results of the study showed that while ChatGPT 
      provided intriguing and interesting responses, it should not be considered a 
      dependable source of information. The absence of citations for scientific queries 
      raises doubts about the credibility of the answers provided. Therefore, it is not 
      advisable to solely rely on ChatGPT as an educational resource. With further 
      updates and more specific prompts, it may be possible to improve its accuracy. In 
      conclusion, while ChatGPT has potential as an educational tool, its reliability 
      needs to be further evaluated and improved before it can be widely adopted in 
      neurosurgical education.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Sevgi, Umut Tan
AU  - Sevgi UT
AUID- ORCID: 0000-0003-3875-859X
AD  - Department of Neurosurgery, University of Health Sciences, Tepecik Training and 
      Research Hospital, Izmir, Turkey.
AD  - Department of Neurosurgery, Yeditepe University Microsurgical Neuroanatomy 
      Laboratory, Istanbul, Turkey.
FAU - Erol, Gökberk
AU  - Erol G
AUID- ORCID: 0000-0001-6651-5486
AD  - Department of Neurosurgery, Yeditepe University Microsurgical Neuroanatomy 
      Laboratory, Istanbul, Turkey.
AD  - Department of Neurosurgery, Faculty of Medicine, Gazi University, Ankara, Turkey.
FAU - Doğruel, Yücel
AU  - Doğruel Y
AUID- ORCID: 0000-0003-4314-5579
AD  - Department of Neurosurgery, Yeditepe University Microsurgical Neuroanatomy 
      Laboratory, Istanbul, Turkey.
AD  - The Neurosurgical Atlas, Carmel, IN, USA.
FAU - Sönmez, Osman Fikret
AU  - Sönmez OF
AUID- ORCID: 0000-0003-1050-1645
AD  - Department of Neurosurgery, University of Health Sciences, Tepecik Training and 
      Research Hospital, Izmir, Turkey.
FAU - Tubbs, Richard Shane
AU  - Tubbs RS
AUID- ORCID: 0000-0003-1317-1047
AD  - Department of Neurosurgery, Tulane Center for Clinical Neurosciences, Tulane 
      University School of Medicine, New Orleans, LA, USA.
AD  - Department of Anatomical Sciences, St. George's University, St. George's, West 
      Indies, Grenada.
AD  - Department of Structural and Cellular Biology, Tulane University School of 
      Medicine, New Orleans, LA, USA.
AD  - Department of Neurosurgery and Ochsner Neuroscience Institute, Ochsner Health 
      System, New Orleans, LA, USA.
AD  - Department of Neurology, Tulane University School of Medicine, New Orleans, LA, 
      USA.
FAU - Güngor, Abuzer
AU  - Güngor A
AUID- ORCID: 0000-0002-2792-7610
AD  - Department of Neurosurgery, Yeditepe University Microsurgical Neuroanatomy 
      Laboratory, Istanbul, Turkey. abuzergungor@gmail.com.
AD  - Department of Neurosurgery, University of Health Sciences, Bakirkoy Research and 
      Training Hospital for Neurology, Neurosurgery and Psychiatry, Istanbul, Turkey. 
      abuzergungor@gmail.com.
LA  - eng
PT  - Journal Article
DEP - 20230414
PL  - Germany
TA  - Neurosurg Rev
JT  - Neurosurgical review
JID - 7908181
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Reproducibility of Results
MH  - Language
MH  - *Neurosurgery
MH  - Neurosurgical Procedures
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Chatbot
OT  - Neurosurgical education
OT  - Scientific article
EDAT- 2023/04/15 06:00
MHDA- 2023/04/18 06:42
CRDT- 2023/04/14 23:18
PHST- 2023/02/09 00:00 [received]
PHST- 2023/04/08 00:00 [accepted]
PHST- 2023/02/09 00:00 [revised]
PHST- 2023/04/18 06:42 [medline]
PHST- 2023/04/14 23:18 [entrez]
PHST- 2023/04/15 06:00 [pubmed]
AID - 10.1007/s10143-023-01998-2 [pii]
AID - 10.1007/s10143-023-01998-2 [doi]
PST - epublish
SO  - Neurosurg Rev. 2023 Apr 14;46(1):86. doi: 10.1007/s10143-023-01998-2.

PMID- 37263382
OWN - NLM
STAT- MEDLINE
DCOM- 20230920
LR  - 20240322
IS  - 1097-6787 (Electronic)
IS  - 0190-9622 (Linking)
VI  - 89
IP  - 4
DP  - 2023 Oct
TI  - The complex ethics of applying ChatGPT and language model artificial intelligence 
      in dermatology.
PG  - e157-e158
LID - S0190-9622(23)00993-3 [pii]
LID - 10.1016/j.jaad.2023.05.054 [doi]
FAU - Ferreira, Alana Luna
AU  - Ferreira AL
AD  - Department of Dermatology, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania.
FAU - Lipoff, Jules B
AU  - Lipoff JB
AD  - Department of Dermatology, Lewis Katz School of Medicine, Temple University, 
      Philadelphia, Pennsylvania. Electronic address: jules.lipoff@temple.edu.
LA  - eng
PT  - Comment
PT  - Letter
DEP - 20230530
PL  - United States
TA  - J Am Acad Dermatol
JT  - Journal of the American Academy of Dermatology
JID - 7907132
SB  - IM
CON - J Am Acad Dermatol. 2023 Mar 11;:null. PMID: 36907556
CIN - J Am Acad Dermatol. 2023 Oct;89(4):e159-e160. PMID: 37268021
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Dermatology
OTO - NOTNLM
OT  - ChatGPT
OT  - accessibility to dermatology
OT  - artificial intelligence
OT  - beneficence
OT  - clinical practice
OT  - ethics
OT  - justice
OT  - language models
OT  - nonmaleficence
COIS- Conflicts of interest None disclosed.
EDAT- 2023/06/02 01:07
MHDA- 2023/09/20 06:42
CRDT- 2023/06/01 19:23
PHST- 2023/03/28 00:00 [received]
PHST- 2023/05/02 00:00 [revised]
PHST- 2023/05/06 00:00 [accepted]
PHST- 2023/09/20 06:42 [medline]
PHST- 2023/06/02 01:07 [pubmed]
PHST- 2023/06/01 19:23 [entrez]
AID - S0190-9622(23)00993-3 [pii]
AID - 10.1016/j.jaad.2023.05.054 [doi]
PST - ppublish
SO  - J Am Acad Dermatol. 2023 Oct;89(4):e157-e158. doi: 10.1016/j.jaad.2023.05.054. 
      Epub 2023 May 30.

PMID- 38436756
OWN - NLM
STAT- Publisher
LR  - 20240304
IS  - 1434-4726 (Electronic)
IS  - 0937-4477 (Linking)
DP  - 2024 Mar 4
TI  - ChatGPT as an information tool in rhinology. Can we trust each other today?
LID - 10.1007/s00405-024-08581-5 [doi]
AB  - PURPOSE: ChatGPT (Chat-Generative Pre-trained Transformer) has proven to be a 
      powerful information tool on various topics, including healthcare. This system is 
      based on information obtained on the Internet, but this information is not always 
      reliable. Currently, few studies analyze the validity of these responses in 
      rhinology. Our work aims to assess the quality and reliability of the information 
      provided by AI regarding the main rhinological pathologies. METHODS: We asked to 
      the default ChatGPT version (GPT-3.5) 65 questions about the most prevalent 
      pathologies in rhinology. The focus was learning about the causes, risk factors, 
      treatments, prognosis, and outcomes. We use the Discern questionnaire and a 
      hexagonal radar schema to evaluate the quality of the information. We use 
      Fleiss's kappa statistical analysis to determine the consistency of agreement 
      between different observers. RESULTS: The overall evaluation of the Discern 
      questionnaire resulted in a score of 4.05 (± 0.6). The results in the Reliability 
      section are worse, with an average score of 3.18. (± 1.77). This score is 
      affected by the responses to questions about the source of the information 
      provided. The average score for the Quality section was 3.59 (± 1.18). Fleiss's 
      Kappa shows substantial agreement, with a K of 0.69 (p &lt; 0.001). CONCLUSION: The 
      ChatGPT answers are accurate and reliable. It generates a simple and 
      understandable description of the pathology for the patient's benefit. Our team 
      considers that ChatGPT could be a useful tool to provide information under prior 
      supervision by a health professional.
CI  - © 2024. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Riestra-Ayora, Juan
AU  - Riestra-Ayora J
AUID- ORCID: 0000-0003-0297-2711
AD  - Department of Medicine, Faculty of Biomedical and Health Sciences, Universidad 
      Europea de Madrid, Villaviciosa de Odón, 28670, Madrid, Spain. 
      juan.riestra@hotmail.com.
AD  - Department of Otolaryngology-Head and Neck Surgery, Hospital Universitario de 
      Getafe, Carretera de Toledo, Km 12.500, Getafe, 28905, Madrid, Spain. 
      juan.riestra@hotmail.com.
FAU - Vaduva, Cristina
AU  - Vaduva C
AD  - Department of Medicine, Faculty of Biomedical and Health Sciences, Universidad 
      Europea de Madrid, Villaviciosa de Odón, 28670, Madrid, Spain.
AD  - Department of Otolaryngology-Head and Neck Surgery, Hospital Universitario de 
      Getafe, Carretera de Toledo, Km 12.500, Getafe, 28905, Madrid, Spain.
FAU - Esteban-Sánchez, Jonathan
AU  - Esteban-Sánchez J
AD  - Department of Medicine, Faculty of Biomedical and Health Sciences, Universidad 
      Europea de Madrid, Villaviciosa de Odón, 28670, Madrid, Spain.
AD  - Department of Otolaryngology-Head and Neck Surgery, Hospital Universitario de 
      Getafe, Carretera de Toledo, Km 12.500, Getafe, 28905, Madrid, Spain.
FAU - Garrote-Garrote, María
AU  - Garrote-Garrote M
AD  - Department of Otolaryngology-Head and Neck Surgery, Hospital Universitario de 
      Getafe, Carretera de Toledo, Km 12.500, Getafe, 28905, Madrid, Spain.
FAU - Fernández-Navarro, Carlos
AU  - Fernández-Navarro C
AD  - Department of Otolaryngology-Head and Neck Surgery, Hospital Universitario de 
      Getafe, Carretera de Toledo, Km 12.500, Getafe, 28905, Madrid, Spain.
FAU - Sánchez-Rodríguez, Carolina
AU  - Sánchez-Rodríguez C
AD  - Department of Medicine, Faculty of Biomedical and Health Sciences, Universidad 
      Europea de Madrid, Villaviciosa de Odón, 28670, Madrid, Spain.
FAU - Martin-Sanz, Eduardo
AU  - Martin-Sanz E
AD  - Department of Medicine, Faculty of Biomedical and Health Sciences, Universidad 
      Europea de Madrid, Villaviciosa de Odón, 28670, Madrid, Spain.
AD  - Department of Otolaryngology-Head and Neck Surgery, Hospital Universitario de 
      Getafe, Carretera de Toledo, Km 12.500, Getafe, 28905, Madrid, Spain.
LA  - eng
PT  - Journal Article
DEP - 20240304
PL  - Germany
TA  - Eur Arch Otorhinolaryngol
JT  - European archives of oto-rhino-laryngology : official journal of the European 
      Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the 
      German Society for Oto-Rhino-Laryngology - Head and Neck Surgery
JID - 9002937
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Chatbot
OT  - Healthcare
OT  - Natural language processing
OT  - Rhinology
EDAT- 2024/03/04 12:47
MHDA- 2024/03/04 12:47
CRDT- 2024/03/04 11:07
PHST- 2023/12/26 00:00 [received]
PHST- 2024/02/23 00:00 [accepted]
PHST- 2024/03/04 12:47 [medline]
PHST- 2024/03/04 12:47 [pubmed]
PHST- 2024/03/04 11:07 [entrez]
AID - 10.1007/s00405-024-08581-5 [pii]
AID - 10.1007/s00405-024-08581-5 [doi]
PST - aheadofprint
SO  - Eur Arch Otorhinolaryngol. 2024 Mar 4. doi: 10.1007/s00405-024-08581-5.

PMID- 38379623
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240222
IS  - 1664-1078 (Print)
IS  - 1664-1078 (Electronic)
IS  - 1664-1078 (Linking)
VI  - 15
DP  - 2024
TI  - Artificial intelligence and social intelligence: preliminary comparison study 
      between AI models and psychologists.
PG  - 1353022
LID - 10.3389/fpsyg.2024.1353022 [doi]
LID - 1353022
AB  - BACKGROUND: Social intelligence (SI) is of great importance in the success of the 
      counseling and psychotherapy, whether for the psychologist or for the artificial 
      intelligence systems that help the psychologist, as it is the ability to 
      understand the feelings, emotions, and needs of people during the counseling 
      process. Therefore, this study aims to identify the Social Intelligence (SI) of 
      artificial intelligence represented by its large linguistic models, "ChatGPT; 
      Google Bard; and Bing" compared to psychologists. METHODS: A stratified random 
      manner sample of 180 students of counseling psychology from the bachelor's and 
      doctoral stages at King Khalid University was selected, while the large 
      linguistic models included ChatGPT-4, Google Bard, and Bing. They (the 
      psychologists and the AI models) responded to the social intelligence scale. 
      RESULTS: There were significant differences in SI between psychologists and AI's 
      ChatGPT-4 and Bing. ChatGPT-4 exceeded 100% of all the psychologists, and Bing 
      outperformed 50% of PhD holders and 90% of bachelor's holders. The differences in 
      SI between Google Bard and bachelor students were not significant, whereas the 
      differences with PhDs were significant; Where 90% of PhD holders excel on Google 
      Bird. CONCLUSION: We explored the possibility of using human measures on AI 
      entities, especially language models, and the results indicate that the 
      development of AI in understanding emotions and social behavior related to social 
      intelligence is very rapid. AI will help the psychotherapist a great deal in new 
      ways. The psychotherapist needs to be aware of possible areas of further 
      development of AI given their benefits in counseling and psychotherapy. Studies 
      using humanistic and non-humanistic criteria with large linguistic models are 
      needed.
CI  - Copyright © 2024 Sufyan, Fadhel, Alkhathami and Mukhadi.
FAU - Sufyan, Nabil Saleh
AU  - Sufyan NS
AD  - Psychology Department, College of Education, King Khalid University, Abha, Saudi 
      Arabia.
FAU - Fadhel, Fahmi H
AU  - Fadhel FH
AD  - Psychology Program, Social Science Department, College of Arts and Sciences, 
      Qatar University, Doha, Qatar.
FAU - Alkhathami, Saleh Safeer
AU  - Alkhathami SS
AD  - Psychology Department, College of Education, King Khalid University, Abha, Saudi 
      Arabia.
FAU - Mukhadi, Jubran Y A
AU  - Mukhadi JYA
AD  - Psychology Department, College of Education, King Khalid University, Abha, Saudi 
      Arabia.
LA  - eng
PT  - Journal Article
DEP - 20240202
PL  - Switzerland
TA  - Front Psychol
JT  - Frontiers in psychology
JID - 101550902
PMC - PMC10878391
OTO - NOTNLM
OT  - Bing
OT  - ChatGPT
OT  - Google Bard
OT  - artificial intelligence
OT  - psychologists
OT  - social intelligence
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2024/02/21 11:15
MHDA- 2024/02/21 11:16
PMCR- 2024/02/02
CRDT- 2024/02/21 03:55
PHST- 2023/12/09 00:00 [received]
PHST- 2024/01/22 00:00 [accepted]
PHST- 2024/02/21 11:16 [medline]
PHST- 2024/02/21 11:15 [pubmed]
PHST- 2024/02/21 03:55 [entrez]
PHST- 2024/02/02 00:00 [pmc-release]
AID - 10.3389/fpsyg.2024.1353022 [doi]
PST - epublish
SO  - Front Psychol. 2024 Feb 2;15:1353022. doi: 10.3389/fpsyg.2024.1353022. 
      eCollection 2024.

PMID- 38236632
OWN - NLM
STAT- MEDLINE
DCOM- 20240119
LR  - 20240204
IS  - 2369-3762 (Electronic)
IS  - 2369-3762 (Linking)
VI  - 10
DP  - 2024 Jan 18
TI  - Performance of ChatGPT on Ophthalmology-Related Questions Across Various 
      Examination Levels: Observational Study.
PG  - e50842
LID - 10.2196/50842 [doi]
LID - e50842
AB  - BACKGROUND: ChatGPT and language learning models have gained attention recently 
      for their ability to answer questions on various examinations across various 
      disciplines. The question of whether ChatGPT could be used to aid in medical 
      education is yet to be answered, particularly in the field of ophthalmology. 
      OBJECTIVE: The aim of this study is to assess the ability of ChatGPT-3.5 
      (GPT-3.5) and ChatGPT-4.0 (GPT-4.0) to answer ophthalmology-related questions 
      across different levels of ophthalmology training. METHODS: Questions from the 
      United States Medical Licensing Examination (USMLE) steps 1 (n=44), 2 (n=60), and 
      3 (n=28) were extracted from AMBOSS, and 248 questions (64 easy, 122 medium, and 
      62 difficult questions) were extracted from the book, Ophthalmology Board Review 
      Q&amp;A, for the Ophthalmic Knowledge Assessment Program and the Board of 
      Ophthalmology (OB) Written Qualifying Examination (WQE). Questions were prompted 
      identically and inputted to GPT-3.5 and GPT-4.0. RESULTS: GPT-3.5 achieved a 
      total of 55% (n=210) of correct answers, while GPT-4.0 achieved a total of 70% 
      (n=270) of correct answers. GPT-3.5 answered 75% (n=33) of questions correctly in 
      USMLE step 1, 73.33% (n=44) in USMLE step 2, 60.71% (n=17) in USMLE step 3, and 
      46.77% (n=116) in the OB-WQE. GPT-4.0 answered 70.45% (n=31) of questions 
      correctly in USMLE step 1, 90.32% (n=56) in USMLE step 2, 96.43% (n=27) in USMLE 
      step 3, and 62.90% (n=156) in the OB-WQE. GPT-3.5 performed poorer as examination 
      levels advanced (P&lt;.001), while GPT-4.0 performed better on USMLE steps 2 and 3 
      and worse on USMLE step 1 and the OB-WQE (P&lt;.001). The coefficient of correlation 
      (r) between ChatGPT answering correctly and human users answering correctly was 
      0.21 (P=.01) for GPT-3.5 as compared to -0.31 (P&lt;.001) for GPT-4.0. GPT-3.5 
      performed similarly across difficulty levels, while GPT-4.0 performed more poorly 
      with an increase in the difficulty level. Both GPT models performed significantly 
      better on certain topics than on others. CONCLUSIONS: ChatGPT is far from being 
      considered a part of mainstream medical education. Future models with higher 
      accuracy are needed for the platform to be effective in medical education.
CI  - ©Firas Haddad, Joanna S Saade. Originally published in JMIR Medical Education 
      (https://mededu.jmir.org), 18.01.2024.
FAU - Haddad, Firas
AU  - Haddad F
AUID- ORCID: 0000-0002-3420-3880
AD  - Faculty of Medicine, American University of Beirut, Beirut, Lebanon.
FAU - Saade, Joanna S
AU  - Saade JS
AUID- ORCID: 0000-0003-1098-4923
AD  - Department of Ophthalmology, American University of Beirut Medical Center, 
      Beirut, Lebanon.
LA  - eng
PT  - Journal Article
PT  - Observational Study
DEP - 20240118
PL  - Canada
TA  - JMIR Med Educ
JT  - JMIR medical education
JID - 101684518
SB  - IM
MH  - Humans
MH  - *Ophthalmology
MH  - Books
MH  - *Education, Medical
MH  - Eye
MH  - Face
PMC - PMC10835593
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - artificial intelligence
OT  - board examinations
OT  - ophthalmology
OT  - testing
COIS- Conflicts of Interest: None declared.
EDAT- 2024/01/18 12:43
MHDA- 2024/01/19 06:42
PMCR- 2024/01/18
CRDT- 2024/01/18 11:53
PHST- 2023/07/14 00:00 [received]
PHST- 2023/12/27 00:00 [accepted]
PHST- 2023/12/09 00:00 [revised]
PHST- 2024/01/19 06:42 [medline]
PHST- 2024/01/18 12:43 [pubmed]
PHST- 2024/01/18 11:53 [entrez]
PHST- 2024/01/18 00:00 [pmc-release]
AID - v10i1e50842 [pii]
AID - 10.2196/50842 [doi]
PST - epublish
SO  - JMIR Med Educ. 2024 Jan 18;10:e50842. doi: 10.2196/50842.

PMID- 37143631
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230507
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 4
DP  - 2023 Apr
TI  - Evaluating ChatGPT's Ability to Solve Higher-Order Questions on the 
      Competency-Based Medical Education Curriculum in Medical Biochemistry.
PG  - e37023
LID - 10.7759/cureus.37023 [doi]
LID - e37023
AB  - Background Healthcare-related artificial intelligence (AI) is developing. The 
      capacity of the system to carry out sophisticated cognitive processes, such as 
      problem-solving, decision-making, reasoning, and perceiving, is referred to as 
      higher cognitive thinking in AI. This kind of thinking requires more than just 
      processing facts; it also entails comprehending and working with abstract ideas, 
      evaluating and applying data relevant to the context, and producing new insights 
      based on prior learning and experience. ChatGPT is an artificial 
      intelligence-based conversational software that can engage with people to answer 
      questions and uses natural language&nbsp;processing models. The platform has created a 
      worldwide buzz and keeps setting an ongoing trend in solving many complex 
      problems in various dimensions. Nevertheless, ChatGPT's capacity to correctly 
      respond to queries requiring higher-level thinking in medical biochemistry has 
      not yet been investigated. So, this research aimed to evaluate ChatGPT's aptitude 
      for responding to higher-order questions on medical biochemistry. Objective In 
      this study, our objective was to determine whether ChatGPT can address 
      higher-order problems related to medical biochemistry.​​​​​​ Methods​​​ This 
      cross-sectional study was done online by conversing with the current version of 
      ChatGPT (14 March 2023, which is presently free for registered users).&nbsp;It was 
      presented with 200 medical biochemistry reasoning questions that require 
      higher-order thinking. These questions were randomly picked from the 
      institution's question bank and classified according to the Competency-Based 
      Medical Education (CBME) curriculum's competency modules. The responses were 
      collected and archived for subsequent research. Two expert biochemistry 
      academicians examined the replies on a zero to five scale. The score's accuracy 
      was determined by a one-sample Wilcoxon signed rank test using hypothetical 
      values. Result The AI software answered 200 questions requiring higher-order 
      thinking with a median score of 4.0&nbsp;(Q1=3.50, Q3=4.50). Using a single sample 
      Wilcoxon signed rank test, the result was less than the hypothetical maximum of 
      five (p=0.001) and comparable to four (p=0.16). There was no difference in the 
      replies to questions from different CBME modules in medical biochemistry 
      (Kruskal-Wallis p=0.39). The inter-rater reliability of the scores scored by two 
      biochemistry faculty members was outstanding (ICC=0.926 (95% CI: 0.814-0.971); 
      F=19; p=0.001)​​​​​​ Conclusion The results of this research indicate that 
      ChatGPT has the potential to be a successful tool for answering questions 
      requiring higher-order thinking in medical biochemistry, with a median score of 
      four out of five. However, continuous training and development with data of 
      recent advances are essential to improve performance and make it functional for 
      the ever-growing field of academic medical usage.
CI  - Copyright © 2023, Ghosh et al.
FAU - Ghosh, Arindam
AU  - Ghosh A
AD  - Biochemistry, Indian Institute of Technology Kharagpur, Dr. B.C. Roy 
      Multi-Speciality Medical Research Centre, Kharagpur, IND.
FAU - Bir, Aritri
AU  - Bir A
AD  - Biochemistry, Indian Institute of Technology Kharagpur, Dr. B.C. Roy 
      Multi-Speciality Medical Research Centre, Kharagpur, IND.
LA  - eng
PT  - Journal Article
DEP - 20230402
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10152308
OTO - NOTNLM
OT  - artificial intelligence
OT  - biochemistry
OT  - chatgpt
OT  - competency-based medical education
OT  - higher order cognitive skills
OT  - laboratory medicine
OT  - mcqs
OT  - medical biochemistry
OT  - medical education
OT  - solving multiple choice questions
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/05/05 06:42
MHDA- 2023/05/05 06:43
PMCR- 2023/04/02
CRDT- 2023/05/05 03:48
PHST- 2023/04/02 00:00 [accepted]
PHST- 2023/05/05 06:43 [medline]
PHST- 2023/05/05 06:42 [pubmed]
PHST- 2023/05/05 03:48 [entrez]
PHST- 2023/04/02 00:00 [pmc-release]
AID - 10.7759/cureus.37023 [doi]
PST - epublish
SO  - Cureus. 2023 Apr 2;15(4):e37023. doi: 10.7759/cureus.37023. eCollection 2023 Apr.

PMID- 38199604
OWN - NLM
STAT- MEDLINE
DCOM- 20240112
LR  - 20240130
IS  - 2009-8774 (Electronic)
IS  - 2305-6983 (Print)
IS  - 2305-6983 (Linking)
VI  - 12
IP  - Suppl 1
DP  - 2024 Jan 9
TI  - Assessing prognosis in depression: comparing perspectives of AI models, mental 
      health professionals and the general public.
LID - 10.1136/fmch-2023-002583 [doi]
LID - e002583
AB  - BACKGROUND: Artificial intelligence (AI) has rapidly permeated various sectors, 
      including healthcare, highlighting its potential to facilitate mental health 
      assessments. This study explores the underexplored domain of AI's role in 
      evaluating prognosis and long-term outcomes in depressive disorders, offering 
      insights into how AI large language models (LLMs) compare with human 
      perspectives. METHODS: Using case vignettes, we conducted a comparative analysis 
      involving different LLMs (ChatGPT-3.5, ChatGPT-4, Claude and Bard), mental health 
      professionals (general practitioners, psychiatrists, clinical psychologists and 
      mental health nurses), and the general public that reported previously. We 
      evaluate the LLMs ability to generate prognosis, anticipated outcomes with and 
      without professional intervention, and envisioned long-term positive and negative 
      consequences for individuals with depression. RESULTS: In most of the examined 
      cases, the four LLMs consistently identified depression as the primary diagnosis 
      and recommended a combined treatment of psychotherapy and antidepressant 
      medication. ChatGPT-3.5 exhibited a significantly pessimistic prognosis distinct 
      from other LLMs, professionals and the public. ChatGPT-4, Claude and Bard aligned 
      closely with mental health professionals and the general public perspectives, all 
      of whom anticipated no improvement or worsening without professional help. 
      Regarding long-term outcomes, ChatGPT 3.5, Claude and Bard consistently projected 
      significantly fewer negative long-term consequences of treatment than ChatGPT-4. 
      CONCLUSIONS: This study underscores the potential of AI to complement the 
      expertise of mental health professionals and promote a collaborative paradigm in 
      mental healthcare. The observation that three of the four LLMs closely mirrored 
      the anticipations of mental health experts in scenarios involving treatment 
      underscores the technology's prospective value in offering professional clinical 
      forecasts. The pessimistic outlook presented by ChatGPT 3.5 is concerning, as it 
      could potentially diminish patients' drive to initiate or continue depression 
      therapy. In summary, although LLMs show potential in enhancing healthcare 
      services, their utilisation requires thorough verification and a seamless 
      integration with human judgement and skills.
CI  - © Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No 
      commercial re-use. See rights and permissions. Published by BMJ.
FAU - Elyoseph, Zohar
AU  - Elyoseph Z
AD  - Department of Psychology and Educational Counseling, The Center for 
      Psychobiological Research, Max Stern Yezreel Valley College, Yezreel Valley, 
      Israel zohare@yvc.ac.il.
AD  - Department of Brain Sciences, Imperial College London, London, UK.
FAU - Levkovich, Inbar
AU  - Levkovich I
AUID- ORCID: 0000-0002-5717-4074
AD  - Faculty of Graduate Studies, Oranim Academic College, Tivon, Israel.
FAU - Shinan-Altman, Shiri
AU  - Shinan-Altman S
AD  - The Louis and Gabi Weisfeld School of Social Work, Bar-Ilan University, Ramat 
      Gan, Tel Aviv, Israel.
LA  - eng
PT  - Journal Article
DEP - 20240109
PL  - England
TA  - Fam Med Community Health
JT  - Family medicine and community health
JID - 101700650
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Depression/diagnosis/therapy
MH  - Prospective Studies
MH  - Prognosis
MH  - *General Practitioners
MH  - Models, Psychological
PMC - PMC10806564
OTO - NOTNLM
OT  - depression
OT  - general practice
OT  - mental health
OT  - nurses
OT  - psychiatry
COIS- Competing interests: None declared.
EDAT- 2024/01/11 00:42
MHDA- 2024/01/12 06:43
PMCR- 2024/01/10
CRDT- 2024/01/10 20:42
PHST- 2024/01/12 06:43 [medline]
PHST- 2024/01/11 00:42 [pubmed]
PHST- 2024/01/10 20:42 [entrez]
PHST- 2024/01/10 00:00 [pmc-release]
AID - fmch-2023-002583 [pii]
AID - 10.1136/fmch-2023-002583 [doi]
PST - epublish
SO  - Fam Med Community Health. 2024 Jan 9;12(Suppl 1):e002583. doi: 
      10.1136/fmch-2023-002583.

PMID- 38115345
OWN - NLM
STAT- MEDLINE
DCOM- 20231221
LR  - 20231223
IS  - 1536-5964 (Electronic)
IS  - 0025-7974 (Print)
IS  - 0025-7974 (Linking)
VI  - 102
IP  - 50
DP  - 2023 Dec 15
TI  - Circle packing charts generated by ChatGPT to identify the characteristics of 
      articles by anesthesiology authors in 2022: Bibliometric analysis.
PG  - e34511
LID - 10.1097/MD.0000000000034511 [doi]
LID - e34511
AB  - BACKGROUND: The ChatGPT (Open AI, San Francisco, CA), denoted by the Chat 
      Generative Pretrained Transformer, has been a hot topic for discussion over the 
      past few months. A verification of whether the code for drawing circle packing 
      charts (CPCs) with R can be generated by ChatGPT and used to identify 
      characteristics of articles by anesthesiology authors is needed. This study aimed 
      to provide insights into article characteristics in the field of anesthesiology 
      and to highlight the potential of ChatGPT for data visualization techniques 
      (e.g., CPCs) in bibliometric analysis. METHODS: A total of 23,012 articles were 
      indexed in PubMed in 2022 by authors in the field of anesthesiology. The code for 
      drawing CPCs with R was generated by ChatGPT and then modified by the authors to 
      identify the characteristics of articles in 2 forms: 23,012 and 100 top-impact 
      factors in journals (T100IF). Using CPCs and 3 other visualizations-network 
      charts, impact beam plots, and Sankey diagrams-we were able to display article 
      features commonly used in bibliometric analysis. The author-weighted scheme and 
      absolute advantage coefficient were used to assess dominant entities, such as 
      countries, institutes, authors, and themes (defined by PubMed and MeSH terms). 
      RESULTS: Our findings indicate that: further modifications should be made to the 
      code generated by ChatGPT for drawing CPCs in R; publications in the field of 
      anesthesiology are dominated by China, followed by the United States and Japan; 
      Capital Medical University (China) and Showa University Hospital (Japan) dominate 
      research institutes in terms of publications and IF, respectively; and COVID-19 
      is the most frequently reported theme in T100IF, accounting for 29%. CONCLUSIONS: 
      No such articles with CPCs regarding bibliometrics have ever been found in 
      PubMed. The code for drawing CPCs with R can be generated by ChatGPT, but further 
      modification is required for implementation in bibliometrics. CPCs should be used 
      in future studies to identify the characteristics of articles in other areas of 
      research rather than limiting them to anesthesiology, as we did in this study.
CI  - Copyright © 2023 the Author(s). Published by Wolters Kluwer Health, Inc.
FAU - Ho, Sam Yu-Chieh
AU  - Ho SY
AD  - Department of Emergency Medicine, Chi-Mei Medical Center, Tainan, Taiwan.
AD  - Department of Geriatrics and Gerontology, ChiMei Medical Center, Tainan, Taiwan.
FAU - Chien, Tsair-Wei
AU  - Chien TW
AD  - Department of Medical Research, Chi-Mei Medical Center, Tainan, Taiwan.
FAU - Chou, Willy
AU  - Chou W
AUID- ORCID: 0000-0002-1132-9341
AD  - Department of Physical Medicine and Rehabilitation, Chiali Chi-Mei Hospital, 
      Tainan 710, Taiwan.
AD  - Department of Physical Medicine and Rehabilitation, Chung San Medical University 
      Hospital, Taichung, Taiwan.
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Medicine (Baltimore)
JT  - Medicine
JID - 2985248R
SB  - IM
MH  - Humans
MH  - United States
MH  - *Anesthesiology
MH  - Bibliometrics
MH  - PubMed
MH  - Medical Subject Headings
MH  - China
PMC - PMC10727539
COIS- The authors have no funding and conflicts of interest to disclose.
EDAT- 2023/12/20 06:42
MHDA- 2023/12/21 06:42
PMCR- 2023/12/15
CRDT- 2023/12/20 01:02
PHST- 2023/12/21 06:42 [medline]
PHST- 2023/12/20 06:42 [pubmed]
PHST- 2023/12/20 01:02 [entrez]
PHST- 2023/12/15 00:00 [pmc-release]
AID - 00005792-202312150-00106 [pii]
AID - 10.1097/MD.0000000000034511 [doi]
PST - ppublish
SO  - Medicine (Baltimore). 2023 Dec 15;102(50):e34511. doi: 
      10.1097/MD.0000000000034511.

PMID- 37295794
OWN - NLM
STAT- Publisher
LR  - 20230609
IS  - 1532-8651 (Electronic)
IS  - 1098-7339 (Linking)
DP  - 2023 Jun 9
TI  - Addition of dexamethasone to prolong peripheral nerve blocks: a ChatGPT-created 
      narrative review.
LID - rapm-2023-104646 [pii]
LID - 10.1136/rapm-2023-104646 [doi]
AB  - Chat Generative Pre-trained Transformer (ChatGPT), an artificial intelligence 
      chatbot, produces detailed responses and human-like coherent answers, and has 
      been used in the clinical and academic medicine. To evaluate its accuracy in 
      regional anesthesia topics, we produced a ChatGPT review on the addition of 
      dexamethasone to prolong peripheral nerve blocks. A group of experts in regional 
      anesthesia and pain medicine were invited to help shape the topic to be studied, 
      refine the questions entered in to the ChatGPT program, vet the manuscript for 
      accuracy, and create a commentary on the article. Although ChatGPT produced an 
      adequate summary of the topic for a general medical or lay audience, the review 
      that were created appeared to be inadequate for a subspecialty audience as the 
      expert authors. Major concerns raised by the authors included the poor search 
      methodology, poor organization/lack of flow, inaccuracies/omissions of text or 
      references, and lack of novelty. At this time, we do not believe ChatGPT is able 
      to replace human experts and is extremely limited in providing original, creative 
      solutions/ideas and interpreting data for a subspecialty medical review article.
CI  - © American Society of Regional Anesthesia &amp; Pain Medicine 2023. No commercial 
      re-use. See rights and permissions. Published by BMJ.
FAU - Wu, Christopher L
AU  - Wu CL
AUID- ORCID: 0000-0002-4484-0787
AD  - Department of Anesthesiology, Critical Care Medicine and Pain Management, 
      Hospital for Special Surgery, New York, New York, USA wuch@hss.edu.
AD  - Anesthesiology, Weill Cornell Medicine, New York, New York, USA.
AD  - 3Pain Prevention Research Center at Hospital for Special Surgery, Hospital for 
      Special Surgery, New York City, New Yrok, USA.
FAU - Cho, Brian
AU  - Cho B
AD  - Anesthesiology and Critical Care Medicine, Johns Hopkins School of Medicine, 
      Baltimore, Maryland, USA.
FAU - Gabriel, Rodney
AU  - Gabriel R
AUID- ORCID: 0000-0003-4443-0021
AD  - Anesthesiology, University of California, La Jolla, California, USA.
FAU - Hurley, Robert
AU  - Hurley R
AD  - Department of Anesthesiology, Department of Neurobiology and Anatomy, Wake Forest 
      University, Wake Forest, North Carolina, USA.
FAU - Liu, Jiabin
AU  - Liu J
AUID- ORCID: 0000-0002-1029-2786
AD  - Anesthesiology, Critical Care and Pain Management, Hospital for Special Surgery, 
      New York, New York, USA.
FAU - Mariano, Edward R
AU  - Mariano ER
AUID- ORCID: 0000-0003-2735-248X
AD  - Anesthesiology and Perioperative Care Service, VA Palo Alto Health Care System, 
      Palo Alto, California, USA.
AD  - Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
      University School of Medicine, Stanford, California, USA.
FAU - Mathur, Vineesh
AU  - Mathur V
AD  - Anesthesiology and Critical Care Medicine, Johns Hopkins School of Medicine, 
      Baltimore, Maryland, USA.
FAU - Memtsoudis, Stavros G
AU  - Memtsoudis SG
AD  - Department of Anesthesiology, Critical Care Medicine and Pain Management, 
      Hospital for Special Surgery, New York, New York, USA.
AD  - Anesthesiology, Weill Cornell Medicine, New York, New York, USA.
FAU - Grant, Michael Conrad
AU  - Grant MC
AD  - Anesthesiology and Critical Care Medicine, Johns Hopkins School of Medicine, 
      Baltimore, Maryland, USA.
LA  - eng
PT  - Journal Article
DEP - 20230609
PL  - England
TA  - Reg Anesth Pain Med
JT  - Regional anesthesia and pain medicine
JID - 9804508
SB  - IM
OTO - NOTNLM
OT  - OUTCOMES
OT  - Pharmacology
OT  - REGIONAL ANESTHESIA
COIS- Competing interests: BC is a speaker for Haemonetics. RG’s institution has 
      received funding and/or product for research purposes from Epimed, Infutronix, 
      SPR Therapeutics, Merck, and Precision Genetics. RG’s institution serves as a 
      consultant for Avanos. RH’s institution has received funding for his research 
      from Avanos, NIH and AHRQ. SGM is the owner SGM Consulting. Partner Parvizi 
      Surgical Innovations, Patent holder Multicatheter infusion system.
EDAT- 2023/06/10 15:14
MHDA- 2023/06/10 15:14
CRDT- 2023/06/09 20:33
PHST- 2023/05/01 00:00 [received]
PHST- 2023/05/30 00:00 [accepted]
PHST- 2023/06/10 15:14 [medline]
PHST- 2023/06/10 15:14 [pubmed]
PHST- 2023/06/09 20:33 [entrez]
AID - rapm-2023-104646 [pii]
AID - 10.1136/rapm-2023-104646 [doi]
PST - aheadofprint
SO  - Reg Anesth Pain Med. 2023 Jun 9:rapm-2023-104646. doi: 10.1136/rapm-2023-104646.

PMID- 37444647
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231106
IS  - 2227-9032 (Print)
IS  - 2227-9032 (Electronic)
IS  - 2227-9032 (Linking)
VI  - 11
IP  - 13
DP  - 2023 Jun 21
TI  - ChatGPT and the Future of Digital Health: A Study on Healthcare Workers' 
      Perceptions and Expectations.
LID - 10.3390/healthcare11131812 [doi]
LID - 1812
AB  - This study aimed to assess the knowledge, attitudes, and intended practices of 
      healthcare workers (HCWs) in Saudi Arabia towards ChatGPT, an artificial 
      intelligence (AI) Chatbot, within the first three months after its launch. We 
      also aimed to identify potential barriers to AI Chatbot adoption among healthcare 
      professionals. A cross-sectional survey was conducted among 1057 HCWs in Saudi 
      Arabia, distributed electronically via social media channels from 21 February to 
      6 March 2023. The survey evaluated HCWs' familiarity with ChatGPT-3.5, their 
      satisfaction, intended future use, and perceived usefulness in healthcare 
      practice. Of the respondents, 18.4% had used ChatGPT for healthcare purposes, 
      while 84.1% of non-users expressed interest in utilizing AI Chatbots in the 
      future. Most participants (75.1%) were comfortable with incorporating ChatGPT 
      into their healthcare practice. HCWs perceived the Chatbot to be useful in 
      various aspects of healthcare, such as medical decision-making (39.5%), patient 
      and family support (44.7%), medical literature appraisal (48.5%), and medical 
      research assistance (65.9%). A majority (76.7%) believed ChatGPT could positively 
      impact the future of healthcare systems. Nevertheless, concerns about credibility 
      and the source of information provided by AI Chatbots (46.9%) were identified as 
      the main barriers. Although HCWs recognize ChatGPT as a valuable addition to 
      digital health in the early stages of adoption, addressing concerns regarding 
      accuracy, reliability, and medicolegal implications is crucial. Therefore, due to 
      their unreliability, the current forms of ChatGPT and other Chatbots should not 
      be used for diagnostic or treatment purposes without human expert oversight. 
      Ensuring the trustworthiness and dependability of AI Chatbots is essential for 
      successful implementation in healthcare settings. Future research should focus on 
      evaluating the clinical outcomes of ChatGPT and benchmarking its performance 
      against other AI Chatbots.
FAU - Temsah, Mohamad-Hani
AU  - Temsah MH
AUID- ORCID: 0000-0002-4389-9322
AD  - College of Medicine, King Saud University, Riyadh 11587, Saudi Arabia.
AD  - Pediatric Department, King Saud University Medical City, King Saud University, 
      Riyadh 11411, Saudi Arabia.
AD  - Evidence-Based Health Care &amp; Knowledge Translation Research Chair, King Saud 
      University, Riyadh 11587, Saudi Arabia.
FAU - Aljamaan, Fadi
AU  - Aljamaan F
AUID- ORCID: 0000-0001-8404-6652
AD  - College of Medicine, King Saud University, Riyadh 11587, Saudi Arabia.
AD  - Critical Care Department, King Saud University Medical City, Riyadh 11411, Saudi 
      Arabia.
FAU - Malki, Khalid H
AU  - Malki KH
AUID- ORCID: 0000-0002-5986-7843
AD  - Research Chair of Voice, Swallowing, and Communication Disorders, ENT Department, 
      College of Medicine, King Saud University, Riyadh 11587, Saudi Arabia.
FAU - Alhasan, Khalid
AU  - Alhasan K
AUID- ORCID: 0000-0002-4291-8536
AD  - College of Medicine, King Saud University, Riyadh 11587, Saudi Arabia.
AD  - Pediatric Department, King Saud University Medical City, King Saud University, 
      Riyadh 11411, Saudi Arabia.
AD  - Solid Organ Transplant Center of Excellence, King Faisal Specialist Hospital and 
      Research Center, Riyadh 11564, Saudi Arabia.
FAU - Altamimi, Ibraheem
AU  - Altamimi I
AUID- ORCID: 0000-0002-5149-0877
AD  - College of Medicine, King Saud University, Riyadh 11587, Saudi Arabia.
FAU - Aljarbou, Razan
AU  - Aljarbou R
AD  - Pediatric Department, King Saud University Medical City, King Saud University, 
      Riyadh 11411, Saudi Arabia.
FAU - Bazuhair, Faisal
AU  - Bazuhair F
AD  - Pediatric Department, King Saud University Medical City, King Saud University, 
      Riyadh 11411, Saudi Arabia.
FAU - Alsubaihin, Abdulmajeed
AU  - Alsubaihin A
AD  - College of Medicine, King Saud University, Riyadh 11587, Saudi Arabia.
AD  - Pediatric Department, King Saud University Medical City, King Saud University, 
      Riyadh 11411, Saudi Arabia.
FAU - Abdulmajeed, Naif
AU  - Abdulmajeed N
AD  - Pediatric Department, King Saud University Medical City, King Saud University, 
      Riyadh 11411, Saudi Arabia.
AD  - Pediatric Nephrology Department, Prince Sultan Military Medical City, Riyadh 
      12233, Saudi Arabia.
FAU - Alshahrani, Fatimah S
AU  - Alshahrani FS
AD  - College of Medicine, King Saud University, Riyadh 11587, Saudi Arabia.
AD  - Division of Infectious Diseases, Department of Internal Medicine, College of 
      Medicine, King Saud University, Riyadh 11451, Saudi Arabia.
FAU - Temsah, Reem
AU  - Temsah R
AD  - College of Pharmacy, Alfaisal University, Riyadh 11533, Saudi Arabia.
FAU - Alshahrani, Turki
AU  - Alshahrani T
AD  - Pediatric Department, King Saud University Medical City, King Saud University, 
      Riyadh 11411, Saudi Arabia.
FAU - Al-Eyadhy, Lama
AU  - Al-Eyadhy L
AD  - College of Medicine, King Saud University, Riyadh 11587, Saudi Arabia.
FAU - Alkhateeb, Serin Mohammed
AU  - Alkhateeb SM
AD  - College of Medicine, Jordan University of Science and Technology, Irbid 22110, 
      Jordan.
FAU - Saddik, Basema
AU  - Saddik B
AUID- ORCID: 0000-0002-4682-5927
AD  - Sharjah Institute of Medical Research, University of Sharjah, Sharjah 27272, 
      United Arab Emirates.
AD  - Department of Community and Family Medicine, College of Medicine, University of 
      Sharjah, Sharjah 27272, United Arab Emirates.
AD  - School of Population Health, Faculty of Medicine &amp; Health, UNSW Sydney, Sydney, 
      NSW 2052, Australia.
FAU - Halwani, Rabih
AU  - Halwani R
AD  - Sharjah Institute of Medical Research, University of Sharjah, Sharjah 27272, 
      United Arab Emirates.
AD  - Department of Clinical Sciences, College of Medicine, University of Sharjah, 
      Sharjah 27272, United Arab Emirates.
FAU - Jamal, Amr
AU  - Jamal A
AUID- ORCID: 0000-0002-4051-6592
AD  - College of Medicine, King Saud University, Riyadh 11587, Saudi Arabia.
AD  - Evidence-Based Health Care &amp; Knowledge Translation Research Chair, King Saud 
      University, Riyadh 11587, Saudi Arabia.
AD  - Department of Family and Community Medicine, King Saud University Medical City, 
      Riyadh 11411, Saudi Arabia.
FAU - Al-Tawfiq, Jaffar A
AU  - Al-Tawfiq JA
AUID- ORCID: 0000-0002-5752-2235
AD  - Specialty Internal Medicine and Quality Department, Johns Hopkins Aramco 
      Healthcare, Dhahran 34465, Saudi Arabia.
AD  - Infectious Disease Division, Department of Medicine, Indiana University School of 
      Medicine, Indianapolis, IN 46202, USA.
AD  - Infectious Disease Division, Department of Medicine, Johns Hopkins University 
      School of Medicine, Baltimore, MD 21218, USA.
FAU - Al-Eyadhy, Ayman
AU  - Al-Eyadhy A
AUID- ORCID: 0000-0002-6051-9125
AD  - College of Medicine, King Saud University, Riyadh 11587, Saudi Arabia.
AD  - Pediatric Department, King Saud University Medical City, King Saud University, 
      Riyadh 11411, Saudi Arabia.
LA  - eng
PT  - Journal Article
DEP - 20230621
PL  - Switzerland
TA  - Healthcare (Basel)
JT  - Healthcare (Basel, Switzerland)
JID - 101666525
PMC - PMC10340744
OTO - NOTNLM
OT  - AI chatbots
OT  - ChatGPT
OT  - artificial intelligence
OT  - credibility
OT  - healthcare workers
OT  - medicolegal implications
OT  - perception
COIS- All authors declare no conflict of interest.
EDAT- 2023/07/14 13:07
MHDA- 2023/07/14 13:08
PMCR- 2023/06/21
CRDT- 2023/07/14 01:07
PHST- 2023/04/25 00:00 [received]
PHST- 2023/06/14 00:00 [revised]
PHST- 2023/06/19 00:00 [accepted]
PHST- 2023/07/14 13:08 [medline]
PHST- 2023/07/14 13:07 [pubmed]
PHST- 2023/07/14 01:07 [entrez]
PHST- 2023/06/21 00:00 [pmc-release]
AID - healthcare11131812 [pii]
AID - healthcare-11-01812 [pii]
AID - 10.3390/healthcare11131812 [doi]
PST - epublish
SO  - Healthcare (Basel). 2023 Jun 21;11(13):1812. doi: 10.3390/healthcare11131812.

PMID- 38198321
OWN - NLM
STAT- MEDLINE
DCOM- 20240112
LR  - 20240112
IS  - 1678-4561 (Electronic)
IS  - 1413-8123 (Linking)
VI  - 29
IP  - 1
DP  - 2024 Jan
TI  - Health literacy in ChatGPT: exploring the potential of the use of artificial 
      intelligence to produce academic text.
PG  - e02412023
LID - S1413-81232024000100203 [pii]
LID - 10.1590/1413-81232024291.02412023 [doi]
AB  - The aim of this study was to identify and analyze the main constituent elements 
      of text generated by ChatGPT in response to questions on an emerging topic in the 
      academic literature in Portuguese - health literacy - and discuss how the 
      evidence produced can contribute to improving our understanding of the limits and 
      challenges of using artificial intelligence (AI) in academic writing. We 
      conducted an exploratory descriptive study based on responses to five consecutive 
      questions in Portuguese and English with increasing levels of complexity put to 
      ChatGPT. Our findings reveal the potential of the use of widely available, 
      unrestricted access AI-based technologies like ChatGPT for academic writing. 
      Featuring a simple and intuitive interface, the tool generated structured and 
      coherent text using natural-like language. Considering that academic productivism 
      is associated with a growing trend in professional misconduct, especially 
      plagiarism, there is a need too take a careful look at academic writing and 
      scientific knowledge dissemination processes mediated by AI technologies.
FAU - Peres, Frederico
AU  - Peres F
AUID- ORCID: 0000-0003-2715-6622
AD  - Escola Nacional de Saúde Pública Sergio Arouca, Fundação Oswaldo Cruz. R. 
      Leopoldo Bulhões 1480, Manguinhos. 21041-210 Rio de Janeiro RJ Brasil. 
      frederico.peres@fiocruz.br.
LA  - por
LA  - eng
PT  - Journal Article
TT  - A literacia em saúde no ChatGPT: explorando o potencial de uso de inteligência 
      artificial para a elaboração de textos acadêmicos.
DEP - 20230317
PL  - Brazil
TA  - Cien Saude Colet
JT  - Ciencia &amp; saude coletiva
JID - 9713483
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Health Literacy
MH  - Ethnicity
MH  - Knowledge
MH  - Language
EDAT- 2024/01/10 18:42
MHDA- 2024/01/12 06:42
CRDT- 2024/01/10 13:03
PHST- 2023/03/02 00:00 [received]
PHST- 2023/03/15 00:00 [accepted]
PHST- 2024/01/12 06:42 [medline]
PHST- 2024/01/10 18:42 [pubmed]
PHST- 2024/01/10 13:03 [entrez]
AID - S1413-81232024000100203 [pii]
AID - 10.1590/1413-81232024291.02412023 [doi]
PST - ppublish
SO  - Cien Saude Colet. 2024 Jan;29(1):e02412023. doi: 
      10.1590/1413-81232024291.02412023. Epub 2023 Mar 17.

PMID- 37071282
OWN - NLM
STAT- MEDLINE
DCOM- 20230615
LR  - 20230615
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Print)
IS  - 0090-6964 (Linking)
VI  - 51
IP  - 7
DP  - 2023 Jul
TI  - Curious Questions About Covid-19 Pandemic with ChatGPT: Answers and 
      Recommendations.
PG  - 1371-1373
LID - 10.1007/s10439-023-03209-x [doi]
AB  - This article aims to answer frequently asked questions about the Covid-19 
      pandemic using ChatGPT and contribute to the spread of accurate information about 
      the pandemic. The article provides general information about the ways Covid-19 is 
      spread, symptoms, diagnosis, treatment, vaccines and pandemic management. It also 
      provides advice on infection control, vaccination campaigns and emergency 
      preparedness.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Tekinay, Osman Nuri
AU  - Tekinay ON
AUID- ORCID: 0000-0002-3668-2336
AD  - Department of Business Administration, Institute of Graduate Studies, Yalova 
      University, Yalova, Turkey. www.osmannuritekinay@gmail.com.
LA  - eng
PT  - Letter
DEP - 20230418
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - Humans
MH  - *COVID-19
MH  - Pandemics/prevention &amp; control
MH  - Infection Control
PMC - PMC10112300
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - Chatbots
OT  - Covid-19
COIS- No benefits in any form have been or will be received from a commercial party 
      related directly or indirectly to the subject of this manuscript. The author 
      declare no conflict of interest.
EDAT- 2023/04/19 06:00
MHDA- 2023/06/15 06:42
PMCR- 2023/04/18
CRDT- 2023/04/18 11:23
PHST- 2023/04/11 00:00 [received]
PHST- 2023/04/12 00:00 [accepted]
PHST- 2023/06/15 06:42 [medline]
PHST- 2023/04/19 06:00 [pubmed]
PHST- 2023/04/18 11:23 [entrez]
PHST- 2023/04/18 00:00 [pmc-release]
AID - 10.1007/s10439-023-03209-x [pii]
AID - 3209 [pii]
AID - 10.1007/s10439-023-03209-x [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2023 Jul;51(7):1371-1373. doi: 10.1007/s10439-023-03209-x. Epub 
      2023 Apr 18.

PMID- 36950398
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231031
IS  - 1682-024X (Print)
IS  - 1681-715X (Electronic)
IS  - 1681-715X (Linking)
VI  - 39
IP  - 2
DP  - 2023 Mar-Apr
TI  - ChatGPT - Reshaping medical education and clinical management.
PG  - 605-607
LID - 10.12669/pjms.39.2.7653 [doi]
AB  - Artificial Intelligence is no more the talk of the fiction read in novels or seen 
      in movies. It has been making inroads slowly and gradually in medical education 
      and clinical management of patients apart from all other walks of life. Recently, 
      chatbots particularly ChatGPT, were developed and trained, using a huge amount of 
      textual data from the internet. This has made a significant impact on our 
      approach in medical science. Though there are benefits of this new technology, a 
      lot of caution is required for its use.
CI  - Copyright: © Pakistan Journal of Medical Sciences.
FAU - Khan, Rehan Ahmed
AU  - Khan RA
AD  - Rehan Ahmed Khan, MBBS, FCPS, FRCS, MHPE, PhD (Medical Education), Dean Riphah 
      Institute of Assessment, HOD &amp; Prof. of Surgery, Islamic International Medical 
      College, Riphah International University.
FAU - Jawaid, Masood
AU  - Jawaid M
AD  - Masood Jawaid, MBBS, MCPS, MRCS, FCPS, MHPE, Director Medical Affairs, PharmEvo 
      (Pvt) Ltd.
FAU - Khan, Aymen Rehan
AU  - Khan AR
AD  - Aymen Rehan Khan, Final year student, Department of English, Foundation 
      University, Rawalpindi.
FAU - Sajjad, Madiha
AU  - Sajjad M
AD  - Madiha Sajjad, MBBS, FCPS, MHPE, Prof. of Pathology, Islamic International 
      Medical College, Riphah International University.
LA  - eng
PT  - Journal Article
PL  - Pakistan
TA  - Pak J Med Sci
JT  - Pakistan journal of medical sciences
JID - 100913117
PMC - PMC10025693
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - ChatGPT
OT  - Education
OT  - NLP
OT  - Open AI
OT  - clinical management
OT  - medical education
EDAT- 2023/03/24 06:00
MHDA- 2023/03/24 06:01
PMCR- 2023/03/01
CRDT- 2023/03/23 02:26
PHST- 2023/02/04 00:00 [received]
PHST- 2023/02/06 00:00 [revised]
PHST- 2023/02/07 00:00 [accepted]
PHST- 2023/03/23 02:26 [entrez]
PHST- 2023/03/24 06:00 [pubmed]
PHST- 2023/03/24 06:01 [medline]
PHST- 2023/03/01 00:00 [pmc-release]
AID - PJMS-39-605 [pii]
AID - 10.12669/pjms.39.2.7653 [doi]
PST - ppublish
SO  - Pak J Med Sci. 2023 Mar-Apr;39(2):605-607. doi: 10.12669/pjms.39.2.7653.

PMID- 38531015
OWN - NLM
STAT- Publisher
LR  - 20240326
IS  - 2325-8179 (Electronic)
IS  - 2325-8160 (Linking)
DP  - 2024 Mar 1
TI  - Assessing ChatGPT-3.5 Versus ChatGPT-4 Performance in Surgical Treatment of 
      Retinal Diseases: A Comparative Study.
PG  - 1-2
LID - 10.3928/23258160-20240227-02 [doi]
FAU - Momenaei, Bita
AU  - Momenaei B
FAU - Wakabayashi, Taku
AU  - Wakabayashi T
FAU - Shahlaee, Abtin
AU  - Shahlaee A
FAU - Durrani, Asad F
AU  - Durrani AF
FAU - Pandit, Saagar A
AU  - Pandit SA
FAU - Wang, Kristine
AU  - Wang K
FAU - Mansour, Hana A
AU  - Mansour HA
FAU - Abishek, Robert M
AU  - Abishek RM
FAU - Xu, David
AU  - Xu D
FAU - Sridhar, Jayanth
AU  - Sridhar J
FAU - Yonekawa, Yoshihiro
AU  - Yonekawa Y
FAU - Kuriyan, Ajay E
AU  - Kuriyan AE
LA  - eng
PT  - Letter
DEP - 20240301
PL  - United States
TA  - Ophthalmic Surg Lasers Imaging Retina
JT  - Ophthalmic surgery, lasers &amp; imaging retina
JID - 101599215
SB  - IM
EDAT- 2024/03/26 18:43
MHDA- 2024/03/26 18:43
CRDT- 2024/03/26 16:12
PHST- 2024/03/26 18:43 [medline]
PHST- 2024/03/26 18:43 [pubmed]
PHST- 2024/03/26 16:12 [entrez]
AID - 10.3928/23258160-20240227-02 [doi]
PST - aheadofprint
SO  - Ophthalmic Surg Lasers Imaging Retina. 2024 Mar 1:1-2. doi: 
      10.3928/23258160-20240227-02.

PMID- 37944140
OWN - NLM
STAT- MEDLINE
DCOM- 20231216
LR  - 20231216
IS  - 1873-233X (Electronic)
IS  - 0029-7844 (Linking)
VI  - 143
IP  - 1
DP  - 2024 Jan 1
TI  - The Genie Is Out of the Bottle: What ChatGPT Can and Cannot Do for Medical 
      Professionals.
PG  - e1-e6
LID - 10.1097/AOG.0000000000005446 [doi]
AB  - ChatGPT is a cutting-edge artificial intelligence technology that was released 
      for public use in November 2022. Its rapid adoption has raised questions about 
      capabilities, limitations, and risks. This article presents an overview of 
      ChatGPT, and it highlights the current state of this technology for the medical 
      field. The article seeks to provide a balanced perspective on what the model can 
      and cannot do in three specific domains: clinical practice, research, and medical 
      education. It also provides suggestions on how to optimize the use of this tool.
CI  - Copyright © 2023 by the American College of Obstetricians and Gynecologists. 
      Published by Wolters Kluwer Health, Inc. All rights reserved.
FAU - Morales-Ramirez, Pedro
AU  - Morales-Ramirez P
AD  - University of Missouri-Kansas City School of Medicine, Kansas City, Missouri.
FAU - Mishek, Henry
AU  - Mishek H
FAU - Dasgupta, Arhita
AU  - Dasgupta A
LA  - eng
PT  - Journal Article
DEP - 20231109
PL  - United States
TA  - Obstet Gynecol
JT  - Obstetrics and gynecology
JID - 0401101
RN  - 80106-42-9 (Genie)
RN  - 0 (Methylmethacrylates)
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Methylmethacrylates
MH  - *Education, Medical
COIS- Financial Disclosure The authors did not report any potential conflicts of 
      interest.
EDAT- 2023/11/09 18:42
MHDA- 2023/12/17 09:43
CRDT- 2023/11/09 17:03
PHST- 2023/08/24 00:00 [received]
PHST- 2023/10/12 00:00 [accepted]
PHST- 2023/12/17 09:43 [medline]
PHST- 2023/11/09 18:42 [pubmed]
PHST- 2023/11/09 17:03 [entrez]
AID - 00006250-990000000-00955 [pii]
AID - 10.1097/AOG.0000000000005446 [doi]
PST - ppublish
SO  - Obstet Gynecol. 2024 Jan 1;143(1):e1-e6. doi: 10.1097/AOG.0000000000005446. Epub 
      2023 Nov 9.

PMID- 37458761
OWN - NLM
STAT- MEDLINE
DCOM- 20231023
LR  - 20231115
IS  - 1432-0711 (Electronic)
IS  - 0932-0067 (Print)
IS  - 0932-0067 (Linking)
VI  - 308
IP  - 6
DP  - 2023 Dec
TI  - Evaluating ChatGPT as an adjunct for the multidisciplinary tumor board 
      decision-making in primary breast cancer cases.
PG  - 1831-1844
LID - 10.1007/s00404-023-07130-5 [doi]
AB  - BACKGROUND: As the available information about breast cancer is growing every 
      day, the decision-making process for the therapy is getting more complex. ChatGPT 
      as a transformer-based language model possesses the ability to write scientific 
      articles and pass medical exams. But is it able to support the multidisciplinary 
      tumor board (MDT) in the planning of the therapy of patients with breast cancer? 
      MATERIAL AND METHODS: We performed a pilot study on 10 consecutive cases of 
      breast cancer patients discussed in MDT at our department in January 2023. 
      Included were patients with a primary diagnosis of early breast cancer. The 
      recommendation of MDT was compared with the recommendation of the ChatGPT for 
      particular patients and the clinical score of the agreement was calculated. 
      RESULTS: Results showed that ChatGPT provided mostly general answers regarding 
      chemotherapy, breast surgery, radiation therapy, chemotherapy, and antibody 
      therapy. It was able to identify risk factors for hereditary breast cancer and 
      point out the elderly patient indicated for chemotherapy to evaluate the 
      cost/benefit effect. ChatGPT wrongly identified the patient with Her2 1 + and 
      2 + (FISH negative) as in need of therapy with an antibody and called endocrine 
      therapy "hormonal treatment". CONCLUSIONS: Support of artificial intelligence by 
      finding individualized and personalized therapy for our patients in the time of 
      rapidly expanding amount of information is looking for the ways in the clinical 
      routine. ChatGPT has the potential to find its spot in clinical medicine, but the 
      current version is not able to provide specific recommendations for the therapy 
      of patients with primary breast cancer.
CI  - © 2023. The Author(s).
FAU - Lukac, Stefan
AU  - Lukac S
AUID- ORCID: 0000-0002-9336-2267
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany. stefanlukacjr@gmail.com.
FAU - Dayan, Davut
AU  - Dayan D
AUID- ORCID: 0000-0003-1589-2016
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany.
FAU - Fink, Visnja
AU  - Fink V
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany.
FAU - Leinert, Elena
AU  - Leinert E
AUID- ORCID: 0000-0001-5795-3417
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany.
FAU - Hartkopf, Andreas
AU  - Hartkopf A
AUID- ORCID: 0000-0003-1227-1118
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany.
FAU - Veselinovic, Kristina
AU  - Veselinovic K
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany.
FAU - Janni, Wolfgang
AU  - Janni W
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany.
FAU - Rack, Brigitte
AU  - Rack B
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany.
FAU - Pfister, Kerstin
AU  - Pfister K
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany.
FAU - Heitmeir, Benedikt
AU  - Heitmeir B
AUID- ORCID: 0000-0003-2989-5234
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany.
FAU - Ebner, Florian
AU  - Ebner F
AUID- ORCID: 0000-0003-2066-5344
AD  - Department of Gynecology and Obstetrics, University Hospital Ulm, Prittwitzstr. 
      43, 89075, Ulm, Germany.
AD  - Gynäkologische Gemeinschaftspraxis Freising &amp; Moosburg, Munich, Germany.
LA  - eng
PT  - Journal Article
DEP - 20230717
PL  - Germany
TA  - Arch Gynecol Obstet
JT  - Archives of gynecology and obstetrics
JID - 8710213
RN  - 0 (Antibodies)
SB  - IM
MH  - Aged
MH  - Humans
MH  - Female
MH  - *Breast Neoplasms/therapy
MH  - Artificial Intelligence
MH  - Pilot Projects
MH  - Oncogenes
MH  - Antibodies
PMC - PMC10579162
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Breast cancer
OT  - ChatGPT
OT  - Multidisciplinary tumor board
COIS- All authors confirm that they have no conflict of interest regarding this 
      paper.The authors have no relevant financial or non-financial interests to 
      disclose.
EDAT- 2023/07/17 15:09
MHDA- 2023/10/23 00:42
PMCR- 2023/07/17
CRDT- 2023/07/17 11:04
PHST- 2023/04/03 00:00 [received]
PHST- 2023/06/27 00:00 [accepted]
PHST- 2023/10/23 00:42 [medline]
PHST- 2023/07/17 15:09 [pubmed]
PHST- 2023/07/17 11:04 [entrez]
PHST- 2023/07/17 00:00 [pmc-release]
AID - 10.1007/s00404-023-07130-5 [pii]
AID - 7130 [pii]
AID - 10.1007/s00404-023-07130-5 [doi]
PST - ppublish
SO  - Arch Gynecol Obstet. 2023 Dec;308(6):1831-1844. doi: 10.1007/s00404-023-07130-5. 
      Epub 2023 Jul 17.

PMID- 37122982
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230502
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 4
DP  - 2023 Apr
TI  - Enhancing Expert Panel Discussions in Pediatric Palliative Care: Innovative 
      Scenario Development and Summarization With ChatGPT-4.
PG  - e38249
LID - 10.7759/cureus.38249 [doi]
LID - e38249
AB  - This study presents a novel approach to enhance expert panel discussions in a 
      medical conference through the use of ChatGPT-4 (Generative Pre-trained 
      Transformer version 4), a recently launched powerful artificial intelligence (AI) 
      language model. We report on ChatGPT-4's ability to optimize and summarize the 
      medical conference panel recommendations of the&nbsp;first Pan-Arab Pediatric 
      Palliative Critical Care Hybrid Conference, held in Riyadh, Saudi 
      Arabia.&nbsp;ChatGPT-4 was incorporated into the discussions in two sequential phases: 
      first, scenarios were optimized by the AI model to stimulate in-depth 
      conversations; second, the model identified, summarized, and contrasted key 
      themes from the panel and audience discussions. The results suggest that 
      ChatGPT-4 effectively facilitated complex do-not-resuscitate (DNR) conflict 
      resolution by summarizing key themes such as effective communication, 
      collaboration, patient and family-centered care, trust, and ethical 
      considerations. The inclusion of ChatGPT-4 in pediatric palliative care panel 
      discussions demonstrated potential benefits for enhancing critical thinking among 
      medical professionals. Further research is warranted to validate and broaden 
      these insights across various settings and cultures.
CI  - Copyright © 2023, Almazyad et al.
FAU - Almazyad, Mohammed
AU  - Almazyad M
AD  - Pediatric Intensive Care Unit, Pediatric Department, College of Medicine, King 
      Saud University, Riyadh, SAU.
FAU - Aljofan, Fahad
AU  - Aljofan F
AD  - Pediatric Intensive Care Unit, Pediatric Department, King Faisal Specialist 
      Hospital &amp; Research Centre, Riyadh, SAU.
FAU - Abouammoh, Noura A
AU  - Abouammoh NA
AD  - Department of Family and Community Medicine, College of Medicine, King Saud 
      University, Riyadh, SAU.
FAU - Muaygil, Ruaim
AU  - Muaygil R
AD  - Medical Education Department, College of Medicine, King Saud University, Riyadh, 
      SAU.
FAU - Malki, Khalid H
AU  - Malki KH
AD  - Department of Otolaryngology, College of Medicine, King Saud University, Riyadh, 
      SAU.
FAU - Aljamaan, Fadi
AU  - Aljamaan F
AD  - Critical Care Department, College of Medicine, King Saud University, Riyadh, SAU.
FAU - Alturki, Abdullah
AU  - Alturki A
AD  - Pediatric Intensive Care Unit, Pediatric Department, King Faisal Specialist 
      Hospital &amp; Research Centre, Riyadh, SAU.
FAU - Alayed, Tareq
AU  - Alayed T
AD  - Pediatric Critical Care Medicine, King Faisal Specialist Hospital &amp; Research 
      Centre, Riyadh, SAU.
FAU - Alshehri, Saleh S
AU  - Alshehri SS
AD  - Pediatric Intensive Care Unit, King Saud Medical City, Riyadh, SAU.
FAU - Alrbiaan, Abdullah
AU  - Alrbiaan A
AD  - Critical Care Department, King Faisal Specialist Hospital &amp; Research Centre, 
      Riyadh, SAU.
FAU - Alsatrawi, Mohammed
AU  - Alsatrawi M
AD  - Pediatric Critical Care Medicine, King Saud University Medical City, Riyadh, SAU.
FAU - Temsah, Hazar A
AU  - Temsah HA
AD  - Biomedical Engineering Department, Faculty of Electrical and Computer 
      Engineering, Beirut Arab University, Beirut, LBN.
FAU - Alsohime, Fahad
AU  - Alsohime F
AD  - Pediatric Critical Care Department, King Saud University, Riyadh, SAU.
FAU - Alhaboob, Ali A
AU  - Alhaboob AA
AD  - Department of Pediatrics, King Saud University, Riyadh, SAU.
FAU - Alabdulhafid, Majed
AU  - Alabdulhafid M
AD  - Pediatric Intensive Care Unit, Pediatric Department, College of Medicine, King 
      Saud University, Riyadh, SAU.
FAU - Jamal, Amr
AU  - Jamal A
AD  - Department of Family and Community Medicine, King Saud University, Riyadh, SAU.
FAU - Alhasan, Khalid
AU  - Alhasan K
AD  - Department of Pediatric Nephrology, King Saud University, Riyadh, SAU.
FAU - Al-Eyadhy, Ayman
AU  - Al-Eyadhy A
AD  - Pediatric Intensive Care Unit, Pediatric Department, College of Medicine, King 
      Saud University, Riyadh, SAU.
AD  - Pediatric Intensive Care Unit, King Saud University Medical City, Riyadh, SAU.
FAU - Temsah, Mohamad-Hani
AU  - Temsah MH
AD  - Pediatric Intensive Care Unit, Pediatric Department, King Saud University Medical 
      City, Riyadh, SAU.
AD  - Pediatric Intensive Care Unit, Pediatric Department, College of Medicine, King 
      Saud University, Riyadh, SAU.
LA  - eng
PT  - Journal Article
DEP - 20230428
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10143975
OTO - NOTNLM
OT  - artificial intelligence chatgpt-4
OT  - chat generative pre-trained transformer
OT  - do-not-resuscitate (dnr) conflicts discussion
OT  - ethical considerations
OT  - expert panel discussions
OT  - human-ai scenario development
OT  - medical conferences
OT  - patient and family-centered care
OT  - pediatric palliative critical care
OT  - picu multidisciplinary approach
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/05/01 06:42
MHDA- 2023/05/01 06:43
PMCR- 2023/04/28
CRDT- 2023/05/01 03:25
PHST- 2023/04/28 00:00 [accepted]
PHST- 2023/05/01 06:43 [medline]
PHST- 2023/05/01 06:42 [pubmed]
PHST- 2023/05/01 03:25 [entrez]
PHST- 2023/04/28 00:00 [pmc-release]
AID - 10.7759/cureus.38249 [doi]
PST - epublish
SO  - Cureus. 2023 Apr 28;15(4):e38249. doi: 10.7759/cureus.38249. eCollection 2023 
      Apr.

PMID- 37932006
OWN - NLM
STAT- Publisher
LR  - 20231106
IS  - 1468-2079 (Electronic)
IS  - 0007-1161 (Linking)
DP  - 2023 Nov 6
TI  - Performance of ChatGPT and Bard on the official part 1 FRCOphth practice 
      questions.
LID - bjo-2023-324091 [pii]
LID - 10.1136/bjo-2023-324091 [doi]
AB  - BACKGROUND: Chat Generative Pre-trained Transformer (ChatGPT), a large language 
      model by OpenAI, and Bard, Google's artificial intelligence (AI) chatbot, have 
      been evaluated in various contexts. This study aims to assess these models' 
      proficiency in the part 1 Fellowship of the Royal College of Ophthalmologists 
      (FRCOphth) Multiple Choice Question (MCQ) examination, highlighting their 
      potential in medical education. METHODS: Both models were tested on a sample 
      question bank for the part 1 FRCOphth MCQ exam. Their performances were compared 
      with historical human performance on the exam, focusing on the ability to 
      comprehend, retain and apply information related to ophthalmology. We also tested 
      it on the book 'MCQs for FRCOpth part 1', and assessed its performance across 
      subjects. RESULTS: ChatGPT demonstrated a strong performance, surpassing 
      historical human pass marks and examination performance, while Bard 
      underperformed. The comparison indicates the potential of certain AI models to 
      match, and even exceed, human standards in such tasks. CONCLUSION: The results 
      demonstrate the potential of AI models, such as ChatGPT, in processing and 
      applying medical knowledge at a postgraduate level. However, performance varied 
      among different models, highlighting the importance of appropriate AI selection. 
      The study underlines the potential for AI applications in medical education and 
      the necessity for further investigation into their strengths and limitations.
CI  - © Author(s) (or their employer(s)) 2023. No commercial re-use. See rights and 
      permissions. Published by BMJ.
FAU - Fowler, Thomas
AU  - Fowler T
AUID- ORCID: 0009-0003-9166-1954
AD  - Department of Medicine, Barking Havering and Redbridge University Hospitals NHS 
      Trust, London, UK thomas.fowler6@nhs.net.
FAU - Pullen, Simon
AU  - Pullen S
AD  - Department of Anaesthetics, Princess Alexandra Hospital, Harlow, UK.
FAU - Birkett, Liam
AU  - Birkett L
AD  - Emergency Medicine, Royal Free Hospital, London, UK.
LA  - eng
PT  - Journal Article
DEP - 20231106
PL  - England
TA  - Br J Ophthalmol
JT  - The British journal of ophthalmology
JID - 0421041
SB  - IM
OTO - NOTNLM
OT  - Medical Education
COIS- Competing interests: None declared.
EDAT- 2023/11/07 00:42
MHDA- 2023/11/07 00:42
CRDT- 2023/11/06 21:03
PHST- 2023/06/22 00:00 [received]
PHST- 2023/10/08 00:00 [accepted]
PHST- 2023/11/07 00:42 [medline]
PHST- 2023/11/07 00:42 [pubmed]
PHST- 2023/11/06 21:03 [entrez]
AID - bjo-2023-324091 [pii]
AID - 10.1136/bjo-2023-324091 [doi]
PST - aheadofprint
SO  - Br J Ophthalmol. 2023 Nov 6:bjo-2023-324091. doi: 10.1136/bjo-2023-324091.

PMID- 37434733
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230918
IS  - 2055-2076 (Print)
IS  - 2055-2076 (Electronic)
IS  - 2055-2076 (Linking)
VI  - 9
DP  - 2023 Jan-Dec
TI  - ChatGPT's potential role in non-English-speaking outpatient clinic settings.
PG  - 20552076231184091
LID - 10.1177/20552076231184091 [doi]
LID - 20552076231184091
AB  - Researchers recently utilized ChatGPT as a tool for composing clinic letters, 
      highlighting its ability to generate accurate and empathetic communications. Here 
      we demonstrated the potential application of ChatGPT as a medical assistant in 
      Mandarin Chinese-speaking outpatient clinics, aiming to improve patient 
      satisfaction in high-patient volume settings. ChatGPT achieved an average score 
      of 72.4% in the Chinese Medical Licensing Examination's Clinical Knowledge 
      section, ranking within the top 20th percentile. It also demonstrated its 
      potential for clinical communication in non-English speaking environments. Our 
      study suggests that ChatGPT could serve as an interface between physicians and 
      patients in Chinese-speaking outpatient settings, possibly extending to other 
      languages. However, further optimization is required, including training on 
      medical-specific datasets, rigorous testing, privacy compliance, integration with 
      existing systems, user-friendly interfaces, and the development of guidelines for 
      medical professionals. Controlled clinical trials and regulatory approval are 
      necessary before widespread implementation. As chatbots' integration into medical 
      practice becomes more feasible, rigorous early investigations and pilot studies 
      can help mitigate potential risks.
CI  - © The Author(s) 2023.
FAU - Zhu, Zhoule
AU  - Zhu Z
AD  - Department of Neurosurgery, Zhejiang University School of Medicine Second 
      Affiliated Hospital, Hangzhou, Zhejiang, China. RINGGOLD: 89681
AD  - Clinical Research Center for Neurological Diseases of Zhejiang Province, 
      Hangzhou, Zhejiang, China.
FAU - Ying, Yuqi
AU  - Ying Y
AD  - Department of Neurosurgery, Zhejiang University School of Medicine Second 
      Affiliated Hospital, Hangzhou, Zhejiang, China. RINGGOLD: 89681
AD  - Clinical Research Center for Neurological Diseases of Zhejiang Province, 
      Hangzhou, Zhejiang, China.
FAU - Zhu, Junming
AU  - Zhu J
AD  - Department of Neurosurgery, Zhejiang University School of Medicine Second 
      Affiliated Hospital, Hangzhou, Zhejiang, China. RINGGOLD: 89681
AD  - Clinical Research Center for Neurological Diseases of Zhejiang Province, 
      Hangzhou, Zhejiang, China.
FAU - Wu, Hemmings
AU  - Wu H
AUID- ORCID: 0000-0001-9665-2617
AD  - Department of Neurosurgery, Zhejiang University School of Medicine Second 
      Affiliated Hospital, Hangzhou, Zhejiang, China. RINGGOLD: 89681
AD  - Clinical Research Center for Neurological Diseases of Zhejiang Province, 
      Hangzhou, Zhejiang, China.
LA  - eng
PT  - Journal Article
DEP - 20230626
PL  - United States
TA  - Digit Health
JT  - Digital health
JID - 101690863
PMC - PMC10331772
OTO - NOTNLM
OT  - Artificial intelligence
OT  - digital health
OT  - general medicine
OT  - health communications
COIS- The author(s) declared no potential conflicts of interest with respect to the 
      research, authorship, and/or publication of this article.
EDAT- 2023/07/12 06:42
MHDA- 2023/07/12 06:43
PMCR- 2023/06/26
CRDT- 2023/07/12 03:46
PHST- 2023/04/26 00:00 [received]
PHST- 2023/06/07 00:00 [accepted]
PHST- 2023/07/12 06:42 [pubmed]
PHST- 2023/07/12 06:43 [medline]
PHST- 2023/07/12 03:46 [entrez]
PHST- 2023/06/26 00:00 [pmc-release]
AID - 10.1177_20552076231184091 [pii]
AID - 10.1177/20552076231184091 [doi]
PST - epublish
SO  - Digit Health. 2023 Jun 26;9:20552076231184091. doi: 10.1177/20552076231184091. 
      eCollection 2023 Jan-Dec.

PMID- 37746411
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230926
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 8
DP  - 2023 Aug
TI  - Artificial Intelligence (AI) in Radiology: A Deep Dive Into ChatGPT 4.0's 
      Accuracy with the American Journal of Neuroradiology's (AJNR) "Case of the 
      Month".
PG  - e43958
LID - 10.7759/cureus.43958 [doi]
LID - e43958
AB  - The advent of artificial intelligence (AI), particularly large language models 
      (LLMs) such as ChatGPT 4.0, holds significant potential in healthcare, 
      specifically in radiology. This study examined the accuracy of ChatGPT 4.0 (July 
      20, 2023, version) in solving diagnostic quizzes from the American Journal of 
      Neuroradiology's (AJNR) "Case of the Month." We evaluated the diagnostic accuracy 
      of ChatGPT 4.0 when provided with a patient's history and imaging findings weekly 
      over four weeks, using 140 cases from the AJNR "Case of the Month" portal (from 
      November 2011 to July 2023). The overall diagnostic accuracy was found to be 
      57.86% (81 out of 140 cases). The diagnostic performance varied across brain, 
      head and neck, and spine subgroups, with accuracy rates of 54.65%, 67.65%, and 
      55.0%, respectively. These findings suggest that AI models such as ChatGPT 4.0 
      could serve as useful adjuncts in radiological diagnostics, thus potentially 
      enhancing patient care and revolutionizing medical education.
CI  - Copyright © 2023, Suthar et al.
FAU - Suthar, Pokhraj P
AU  - Suthar PP
AD  - Department of Diagnostic Radiology and Nuclear Medicine, Rush University Medical 
      Center, Chicago, USA.
FAU - Kounsal, Avin
AU  - Kounsal A
AD  - Department of Diagnostic Radiology and Nuclear Medicine, Rush University Medical 
      Center, Chicago, USA.
FAU - Chhetri, Lavanya
AU  - Chhetri L
AD  - Department of Clinical Nutrition, Rush University Medical Center, Chicago, USA.
FAU - Saini, Divya
AU  - Saini D
AD  - Department of Public Health, Johns Hopkins Bloomberg School of Public Health, 
      Baltimore, USA.
FAU - Dua, Sumeet G
AU  - Dua SG
AD  - Department of Diagnostic Radiology and Nuclear Medicine, Rush University Medical 
      Center, Chicago, USA.
LA  - eng
PT  - Journal Article
DEP - 20230823
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10516448
OTO - NOTNLM
OT  - accuracy
OT  - artificial intelligence in radiology
OT  - chat gpt
OT  - large language models (llms)
OT  - neuroradiology
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/09/25 06:42
MHDA- 2023/09/25 06:43
PMCR- 2023/08/23
CRDT- 2023/09/25 05:15
PHST- 2023/08/23 00:00 [accepted]
PHST- 2023/09/25 06:43 [medline]
PHST- 2023/09/25 06:42 [pubmed]
PHST- 2023/09/25 05:15 [entrez]
PHST- 2023/08/23 00:00 [pmc-release]
AID - 10.7759/cureus.43958 [doi]
PST - epublish
SO  - Cureus. 2023 Aug 23;15(8):e43958. doi: 10.7759/cureus.43958. eCollection 2023 
      Aug.

PMID- 37720897
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230921
IS  - 1664-0640 (Print)
IS  - 1664-0640 (Electronic)
IS  - 1664-0640 (Linking)
VI  - 14
DP  - 2023
TI  - The plasticity of ChatGPT's mentalizing abilities: personalization for 
      personality structures.
PG  - 1234397
LID - 10.3389/fpsyt.2023.1234397 [doi]
LID - 1234397
AB  - This study evaluated the potential of ChatGPT, a large language model, to 
      generate mentalizing-like abilities that are tailored to a specific personality 
      structure and/or psychopathology. Mentalization is the ability to understand and 
      interpret one's own and others' mental states, including thoughts, feelings, and 
      intentions. Borderline Personality Disorder (BPD) and Schizoid Personality 
      Disorder (SPD) are characterized by distinct patterns of emotional regulation. 
      Individuals with BPD tend to experience intense and unstable emotions, while 
      individuals with SPD tend to experience flattened or detached emotions. We used 
      ChatGPT's free version 23.3 and assessed the extent to which its responses akin 
      to emotional awareness (EA) were customized to the distinctive personality 
      structure-character characterized by Borderline Personality Disorder (BPD) and 
      Schizoid Personality Disorder (SPD), employing the Levels of Emotional Awareness 
      Scale (LEAS). ChatGPT was able to accurately describe the emotional reactions of 
      individuals with BPD as more intense, complex, and rich than those with SPD. This 
      finding suggests that ChatGPT can generate mentalizing-like responses consistent 
      with a range of psychopathologies in line with clinical and theoretical 
      knowledge. However, the study also raises concerns regarding the potential for 
      stigmas or biases related to mental diagnoses to impact the validity and 
      usefulness of chatbot-based clinical interventions. We emphasize the need for the 
      responsible development and deployment of chatbot-based interventions in mental 
      health, which considers diverse theoretical frameworks.
CI  - Copyright © 2023 Hadar-Shoval, Elyoseph and Lvovsky.
FAU - Hadar-Shoval, Dorit
AU  - Hadar-Shoval D
AD  - Department of Psychology and Educational Counseling, The Center for 
      Psychobiological Research, Max Stern Yezreel Valley College, Emek Yezreel, 
      Israel.
FAU - Elyoseph, Zohar
AU  - Elyoseph Z
AD  - Department of Psychology and Educational Counseling, The Center for 
      Psychobiological Research, Max Stern Yezreel Valley College, Emek Yezreel, 
      Israel.
AD  - Department of Brain Sciences, Faculty of Medicine, Imperial College London, 
      London, United Kingdom.
AD  - Educational Psychology Department, Center for Psychobiological Research, Max 
      Stern Yezreel Valley College, Emek Yezreel, Israel.
FAU - Lvovsky, Maya
AU  - Lvovsky M
AD  - Educational Psychology Department, Center for Psychobiological Research, Max 
      Stern Yezreel Valley College, Emek Yezreel, Israel.
LA  - eng
PT  - Journal Article
DEP - 20230901
PL  - Switzerland
TA  - Front Psychiatry
JT  - Frontiers in psychiatry
JID - 101545006
PMC - PMC10503434
OTO - NOTNLM
OT  - Schizoid Personality Disorder
OT  - artificial intelligence
OT  - borderline personality disorder
OT  - emotional awareness
OT  - emotional intelligence
OT  - empathy
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2023/09/18 06:42
MHDA- 2023/09/18 06:43
PMCR- 2023/09/01
CRDT- 2023/09/18 04:47
PHST- 2023/06/04 00:00 [received]
PHST- 2023/08/22 00:00 [accepted]
PHST- 2023/09/18 06:43 [medline]
PHST- 2023/09/18 06:42 [pubmed]
PHST- 2023/09/18 04:47 [entrez]
PHST- 2023/09/01 00:00 [pmc-release]
AID - 10.3389/fpsyt.2023.1234397 [doi]
PST - epublish
SO  - Front Psychiatry. 2023 Sep 1;14:1234397. doi: 10.3389/fpsyt.2023.1234397. 
      eCollection 2023.

PMID- 37609022
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230928
IS  - 0019-5413 (Print)
IS  - 1998-3727 (Electronic)
IS  - 0019-5413 (Linking)
VI  - 57
IP  - 9
DP  - 2023 Sep
TI  - ChatGPT-4: Transforming Medical Education and Addressing Clinical Exposure 
      Challenges in the Post-pandemic Era.
PG  - 1527-1544
LID - 10.1007/s43465-023-00967-7 [doi]
AB  - BACKGROUND: The COVID-19 pandemic has affected medical education, constraining 
      clinical exposure and posing unprecedented challenges for students and junior 
      doctors. This research explores the potential of artificial intelligence (AI), 
      specifically the ChatGPT-4 language model, to transform medical education and 
      address the deficiencies in clinical exposure during the post-pandemic era. 
      RESEARCH QUESTIONS/PURPOSE: What is the potential of AI large language models in 
      delivering safe and coherent medical advice to junior doctors for clinical 
      orthopaedic scenarios? PATIENTS AND METHODS: A series of diverse orthopaedic 
      questions was presented to ChatGPT-4, from general medicine to highly specialised 
      fields. The questions were based on a variety of common orthopaedic presentations 
      including neck of femur fracture, compartment syndrome, pulmonary embolism, and a 
      motor vehicle accident. A validated questionnaire (Likert Scale) was implemented 
      to evaluate the answers produced by ChatGPT-4. RESULTS: Our results indicate that 
      ChatGPT-4 exhibits exceptional proficiency in delivering accurate and coherent 
      medical advice. Its intuitive interface, accessibility, and sophisticated 
      algorithm render it an ideal supplementary tool for medical students and junior 
      doctors. Despite certain limitations, such as its inability to fully address 
      highly specialised areas, this study highlights the potential of AI and ChatGPT-4 
      to revolutionise medical education and fill the clinical exposure void generated 
      by the pandemic. Future research should concentrate on the practical application 
      of ChatGPT-4 in real-world medical environments and its integration with other 
      emerging technologies to optimise its influence on the education and training of 
      healthcare professionals. CONCLUSIONS: ChatGPT-4's integration into orthopaedic 
      education and practice can mitigate pandemic-related experience gaps, promoting 
      self-directed, personalised learning and decision-making support for interns and 
      residents. Future advancements may address limitations to enhance healthcare 
      professionals' learning and expertise. LEVEL OF EVIDENCE: Level III 
      evidence-observational study.
CI  - © Crown 2023.
FAU - Lower, Kirk
AU  - Lower K
AUID- ORCID: 0000-0001-6722-3426
AD  - Department of Orthopaedics, St George Hospital, Gray Street, Sydney, NSW 2000 
      Australia. GRID: grid.416398.1. ISNI: 0000 0004 0417 5393
FAU - Seth, Ishith
AU  - Seth I
AUID- ORCID: 0000-0001-5444-8925
AD  - Faculty of Science, Medicine, and Health, Central Clinical School at Monash 
      University, The Alfred Centre, 99 Commercial Rd, Melbourne, VIC 3004 Australia. 
      GRID: grid.1002.3. ISNI: 0000 0004 1936 7857
AD  - Department of Orthopaedic Surgery, Peninsula Health, Melbourne, VIC 3004 
      Australia. GRID: grid.466993.7. ISNI: 0000 0004 0436 2893
FAU - Lim, Bryan
AU  - Lim B
AD  - Faculty of Science, Medicine, and Health, Central Clinical School at Monash 
      University, The Alfred Centre, 99 Commercial Rd, Melbourne, VIC 3004 Australia. 
      GRID: grid.1002.3. ISNI: 0000 0004 1936 7857
AD  - Department of Orthopaedic Surgery, Peninsula Health, Melbourne, VIC 3004 
      Australia. GRID: grid.466993.7. ISNI: 0000 0004 0436 2893
FAU - Seth, Nimish
AU  - Seth N
AD  - Department of Orthopaedic Surgery, Peninsula Health, Melbourne, VIC 3004 
      Australia. GRID: grid.466993.7. ISNI: 0000 0004 0436 2893
LA  - eng
PT  - Journal Article
DEP - 20230810
PL  - Switzerland
TA  - Indian J Orthop
JT  - Indian journal of orthopaedics
JID - 0137736
PMC - PMC10442004
COIS- Conflict of InterestThe authors declare no conflict of interest.
EDAT- 2023/08/23 06:42
MHDA- 2023/08/23 06:43
PMCR- 2024/08/10
CRDT- 2023/08/23 03:59
PHST- 2023/04/29 00:00 [received]
PHST- 2023/07/27 00:00 [accepted]
PHST- 2024/08/10 00:00 [pmc-release]
PHST- 2023/08/23 06:43 [medline]
PHST- 2023/08/23 06:42 [pubmed]
PHST- 2023/08/23 03:59 [entrez]
AID - 967 [pii]
AID - 10.1007/s43465-023-00967-7 [doi]
PST - epublish
SO  - Indian J Orthop. 2023 Aug 10;57(9):1527-1544. doi: 10.1007/s43465-023-00967-7. 
      eCollection 2023 Sep.

PMID- 38419470
OWN - NLM
STAT- Publisher
LR  - 20240229
IS  - 1552-6941 (Electronic)
IS  - 1534-7346 (Linking)
DP  - 2024 Feb 28
TI  - Appropriateness of Artificial Intelligence Chatbots in Diabetic Foot Ulcer 
      Management.
PG  - 15347346241236811
LID - 10.1177/15347346241236811 [doi]
AB  - Type 2 diabetes is a significant global health concern. It often causes diabetic 
      foot ulcers (DFUs), which affect millions of people and increase amputation and 
      mortality rates. Despite existing guidelines, the complexity of DFU treatment 
      makes clinical decisions challenging. Large language models such as chat 
      generative pretrained transformer (ChatGPT), which are adept at natural language 
      processing, have emerged as valuable resources in the medical field. However, 
      concerns about the accuracy and reliability of the information they provide 
      remain. We aimed to assess the accuracy of various artificial intelligence (AI) 
      chatbots, including ChatGPT, in providing information on DFUs based on 
      established guidelines. Seven AI chatbots were asked clinical questions (CQs) 
      based on the DFU guidelines. Their responses were analyzed for accuracy in terms 
      of answers to CQs, grade of recommendation, level of evidence, and agreement with 
      the reference, including verification of the authenticity of the references 
      provided by the chatbots. The AI chatbots showed a mean accuracy of 91.2% in 
      answers to CQs, with discrepancies noted in grade of recommendation and level of 
      evidence. Claude-2 outperformed other chatbots in the number of verified 
      references (99.6%), whereas ChatGPT had the lowest rate of reference authenticity 
      (66.3%). This study highlights the potential of AI chatbots as tools for 
      disseminating medical information and demonstrates their high degree of accuracy 
      in answering CQs related to DFUs. However, the variability in the accuracy of 
      these chatbots and problems like AI hallucinations necessitate cautious use and 
      further optimization for medical applications. This study underscores the 
      evolving role of AI in healthcare and the importance of refining these 
      technologies for effective use in clinical decision-making and patient education.
FAU - Shiraishi, Makoto
AU  - Shiraishi M
AUID- ORCID: 0000-0002-3734-2085
AD  - Department of Plastic and Reconstructive Surgery, The University of Tokyo 
      Hospital, Tokyo, Japan. RINGGOLD: 26782
FAU - Lee, Haesu
AU  - Lee H
AD  - Department of Plastic and Reconstructive Surgery, The University of Tokyo 
      Hospital, Tokyo, Japan. RINGGOLD: 26782
FAU - Kanayama, Koji
AU  - Kanayama K
AD  - Department of Plastic and Reconstructive Surgery, The University of Tokyo 
      Hospital, Tokyo, Japan. RINGGOLD: 26782
FAU - Moriwaki, Yuta
AU  - Moriwaki Y
AD  - Department of Plastic and Reconstructive Surgery, The University of Tokyo 
      Hospital, Tokyo, Japan. RINGGOLD: 26782
FAU - Okazaki, Mutsumi
AU  - Okazaki M
AD  - Department of Plastic and Reconstructive Surgery, The University of Tokyo 
      Hospital, Tokyo, Japan. RINGGOLD: 26782
LA  - eng
PT  - Journal Article
DEP - 20240228
PL  - United States
TA  - Int J Low Extrem Wounds
JT  - The international journal of lower extremity wounds
JID - 101128359
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - Claude-2
OT  - GPT-4
OT  - artificial intelligence
OT  - diabetic foot ulcers
COIS- Declaration of Conflicting InterestsThe authors declared no potential conflicts 
      of interest with respect to the research, authorship, and/or publication of this 
      article.
EDAT- 2024/02/29 06:43
MHDA- 2024/02/29 06:43
CRDT- 2024/02/29 02:52
PHST- 2024/02/29 06:43 [medline]
PHST- 2024/02/29 06:43 [pubmed]
PHST- 2024/02/29 02:52 [entrez]
AID - 10.1177/15347346241236811 [doi]
PST - aheadofprint
SO  - Int J Low Extrem Wounds. 2024 Feb 28:15347346241236811. doi: 
      10.1177/15347346241236811.

PMID- 38028967
OWN - NLM
STAT- MEDLINE
DCOM- 20231201
LR  - 20231201
IS  - 1947-6108 (Electronic)
IS  - 1947-6094 (Print)
IS  - 1947-6108 (Linking)
VI  - 19
IP  - 5
DP  - 2023
TI  - ChatGPT and Artificial Intelligence in Hospital Level Research: Potential, 
      Precautions, and Prospects.
PG  - 77-84
LID - 10.14797/mdcvj.1290 [doi]
AB  - Rapid advancements in artificial intelligence (AI) have revolutionized numerous 
      sectors, including medical research. Among the various AI tools, OpenAI's 
      ChatGPT, a state-of-the-art language model, has demonstrated immense potential in 
      aiding and enhancing research processes. This review explores the application of 
      ChatGPT in medical hospital level research, focusing on its capabilities for 
      academic writing assistance, data analytics, statistics handling, and code 
      generation. Notably, it delves into the model's ability to streamline tasks, 
      support decision making, and improve patient interaction. However, the article 
      also underscores the importance of exercising caution while dealing with 
      sensitive healthcare data and highlights the limitations of ChatGPT, such as its 
      potential for erroneous outputs and biases. Furthermore, the review discusses the 
      ethical considerations that arise with AI use in health care, including data 
      privacy, AI interpretability, and the risk of AI-induced disparities. The article 
      culminates by envisioning the future of AI in medical research, emphasizing the 
      need for robust regulatory frameworks and guidelines that balance the potential 
      of AI with ethical considerations. As AI continues to evolve, it holds promising 
      potential to augment medical research in a manner that is ethical, equitable, and 
      patient-centric.
CI  - Copyright: © 2023 The Author(s).
FAU - Arshad, Hassaan B
AU  - Arshad HB
AUID- ORCID: 0000-0002-2690-1646
AD  - Houston Methodist DeBakey Heart &amp; Vascular Center, Houston, Texas, US.
FAU - Butt, Sara A
AU  - Butt SA
AD  - Houston Methodist Research Institute, Houston, Texas, US.
FAU - Khan, Safi U
AU  - Khan SU
AUID- ORCID: 0000-0003-1559-6911
AD  - Houston Methodist DeBakey Heart &amp; Vascular Center, Houston, Texas, US.
FAU - Javed, Zulqarnain
AU  - Javed Z
AUID- ORCID: 0000-0003-4321-4394
AD  - Houston Methodist Research Institute, Houston, Texas, US.
FAU - Nasir, Khurram
AU  - Nasir K
AUID- ORCID: 0000-0001-5376-2269
AD  - Houston Methodist DeBakey Heart &amp; Vascular Center, Houston, Texas, US.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20231116
PL  - United States
TA  - Methodist Debakey Cardiovasc J
JT  - Methodist DeBakey cardiovascular journal
JID - 101508600
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Hospitals
MH  - *Biomedical Research
MH  - Exercise
MH  - Writing
PMC - PMC10655767
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - data analytics
OT  - medical research
OT  - outcomes research
COIS- The authors have no competing interests to declare.
EDAT- 2023/11/29 18:43
MHDA- 2023/12/01 06:44
PMCR- 2023/11/16
CRDT- 2023/11/29 17:19
PHST- 2023/09/12 00:00 [received]
PHST- 2023/10/24 00:00 [accepted]
PHST- 2023/12/01 06:44 [medline]
PHST- 2023/11/29 18:43 [pubmed]
PHST- 2023/11/29 17:19 [entrez]
PHST- 2023/11/16 00:00 [pmc-release]
AID - 10.14797/mdcvj.1290 [doi]
PST - epublish
SO  - Methodist Debakey Cardiovasc J. 2023 Nov 16;19(5):77-84. doi: 
      10.14797/mdcvj.1290. eCollection 2023.

PMID- 37634646
OWN - NLM
STAT- MEDLINE
DCOM- 20230829
LR  - 20230907
IS  - 2005-8330 (Electronic)
IS  - 1229-6929 (Print)
IS  - 1229-6929 (Linking)
VI  - 24
IP  - 9
DP  - 2023 Sep
TI  - The Integration of Large Language Models Such as ChatGPT in Scientific Writing: 
      Harnessing Potential and Addressing Pitfalls.
PG  - 924-925
LID - 10.3348/kjr.2023.0738 [doi]
FAU - Koga, Shunsuke
AU  - Koga S
AUID- ORCID: 0000-0001-8868-9700
AD  - Department of Pathology and Laboratory Medicine, Hospital of the University of 
      Pennsylvania, Philadelphia, PA, USA. shunsuke.koga@pennmedicine.upenn.edu.
LA  - eng
PT  - Comment
PT  - Letter
PL  - Korea (South)
TA  - Korean J Radiol
JT  - Korean journal of radiology
JID - 100956096
SB  - IM
CON - Korean J Radiol. 2023 Aug;24(8):715-718. PMID: 37500572
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Policy
MH  - Publications
PMC - PMC10462902
OTO - NOTNLM
OT  - Authorship
OT  - ChatGPT
OT  - Ethics
OT  - Large language model
OT  - Scientific writing
COIS- The author has no potential conflicts of interest to disclose.
EDAT- 2023/08/28 00:41
MHDA- 2023/08/29 12:44
PMCR- 2023/09/01
CRDT- 2023/08/27 19:24
PHST- 2023/08/06 00:00 [received]
PHST- 2023/08/08 00:00 [accepted]
PHST- 2023/08/29 12:44 [medline]
PHST- 2023/08/28 00:41 [pubmed]
PHST- 2023/08/27 19:24 [entrez]
PHST- 2023/09/01 00:00 [pmc-release]
AID - 24.924 [pii]
AID - 10.3348/kjr.2023.0738 [doi]
PST - ppublish
SO  - Korean J Radiol. 2023 Sep;24(9):924-925. doi: 10.3348/kjr.2023.0738.

PMID- 37183102
OWN - NLM
STAT- MEDLINE
DCOM- 20230620
LR  - 20231027
IS  - 1471-6771 (Electronic)
IS  - 0007-0912 (Linking)
VI  - 131
IP  - 1
DP  - 2023 Jul
TI  - ChatGPT in anaesthesia research: risk of fabrication in literature searches.
PG  - e29-e30
LID - S0007-0912(23)00184-8 [pii]
LID - 10.1016/j.bja.2023.04.009 [doi]
FAU - Grigio, Thiago R
AU  - Grigio TR
AD  - Department of Anaesthesiology, Pain Center, University of Groningen, University 
      Medical Center Groningen, Groningen, the Netherlands. Electronic address: 
      t.r.grigio@umcg.nl.
FAU - Timmerman, Hans
AU  - Timmerman H
AD  - Department of Anaesthesiology, Pain Center, University of Groningen, University 
      Medical Center Groningen, Groningen, the Netherlands.
FAU - Wolff, André P
AU  - Wolff AP
AD  - Department of Anaesthesiology, Pain Center, University of Groningen, University 
      Medical Center Groningen, Groningen, the Netherlands.
LA  - eng
PT  - Letter
DEP - 20230512
PL  - England
TA  - Br J Anaesth
JT  - British journal of anaesthesia
JID - 0372541
SB  - IM
CIN - Br J Anaesth. 2023 Nov;131(5):e172-e173. PMID: 37625909
MH  - Humans
MH  - *Anesthesia/adverse effects
MH  - *Anesthesiology
MH  - Epidemiologic Studies
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - data fabrication
OT  - grey literature
OT  - large language model
OT  - research integrity
EDAT- 2023/05/15 00:42
MHDA- 2023/06/20 06:42
CRDT- 2023/05/14 21:59
PHST- 2023/03/10 00:00 [received]
PHST- 2023/03/30 00:00 [revised]
PHST- 2023/04/12 00:00 [accepted]
PHST- 2023/06/20 06:42 [medline]
PHST- 2023/05/15 00:42 [pubmed]
PHST- 2023/05/14 21:59 [entrez]
AID - S0007-0912(23)00184-8 [pii]
AID - 10.1016/j.bja.2023.04.009 [doi]
PST - ppublish
SO  - Br J Anaesth. 2023 Jul;131(1):e29-e30. doi: 10.1016/j.bja.2023.04.009. Epub 2023 
      May 12.

PMID- 37155932
OWN - NLM
STAT- MEDLINE
DCOM- 20230929
LR  - 20231003
IS  - 1437-4331 (Electronic)
IS  - 1434-6621 (Linking)
VI  - 61
IP  - 11
DP  - 2023 Oct 26
TI  - AI, diabetes and getting lost in translation: a multilingual evaluation of Bing 
      with ChatGPT focused in HbA(1c).
PG  - e222-e224
LID - 10.1515/cclm-2023-0295 [doi]
FAU - Barallat, Jaume
AU  - Barallat J
AUID- ORCID: 0000-0003-3493-5958
AD  - Biochemistry Department, LCMN, Germans Trias i Pujol University Hospital 
      Badalona, Spain.
FAU - Gómez, Carolina
AU  - Gómez C
AD  - Biochemistry Department, LCMN, Germans Trias i Pujol University Hospital 
      Badalona, Spain.
FAU - Sancho-Cerro, Ana
AU  - Sancho-Cerro A
AD  - Biochemistry Department, LCMN, Germans Trias i Pujol University Hospital 
      Badalona, Spain.
LA  - eng
PT  - Letter
DEP - 20230509
PL  - Germany
TA  - Clin Chem Lab Med
JT  - Clinical chemistry and laboratory medicine
JID - 9806306
SB  - IM
MH  - Humans
MH  - *Diabetes Mellitus/diagnosis
MH  - Artificial Intelligence
OTO - NOTNLM
OT  - Bing
OT  - ChatGPT
OT  - HbA1c
OT  - artificial intelligence
OT  - diabetes
OT  - evaluation of new products
EDAT- 2023/05/08 18:42
MHDA- 2023/09/29 06:44
CRDT- 2023/05/08 15:52
PHST- 2023/03/21 00:00 [received]
PHST- 2023/04/28 00:00 [accepted]
PHST- 2023/09/29 06:44 [medline]
PHST- 2023/05/08 18:42 [pubmed]
PHST- 2023/05/08 15:52 [entrez]
AID - cclm-2023-0295 [pii]
AID - 10.1515/cclm-2023-0295 [doi]
PST - epublish
SO  - Clin Chem Lab Med. 2023 May 9;61(11):e222-e224. doi: 10.1515/cclm-2023-0295. 
      Print 2023 Oct 26.

PMID- 37124601
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230502
IS  - 0970-9134 (Print)
IS  - 0973-7723 (Electronic)
IS  - 0970-9134 (Linking)
VI  - 39
IP  - 3
DP  - 2023 May
TI  - ChatGPT-a foe or an ally?
PG  - 217-221
LID - 10.1007/s12055-023-01507-6 [doi]
FAU - Yadava, Om Prakash
AU  - Yadava OP
AUID- ORCID: 0000-0001-5088-7226
AD  - National Heart Institute, New Delhi, India. GRID: grid.512696.f. ISNI: 0000 0004 
      1800 3031
LA  - eng
PT  - Editorial
DEP - 20230328
PL  - India
TA  - Indian J Thorac Cardiovasc Surg
JT  - Indian journal of thoracic and cardiovascular surgery
JID - 8700105
PMC - PMC10140247
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Non-human authors
OT  - OpenAI
OT  - Plagiarism
OT  - Scientific publications
COIS- Conflict of interestThe authors declare no competing interests.
EDAT- 2023/05/01 06:42
MHDA- 2023/05/01 06:43
PMCR- 2024/05/01
CRDT- 2023/05/01 03:46
PHST- 2024/05/01 00:00 [pmc-release]
PHST- 2023/05/01 06:43 [medline]
PHST- 2023/05/01 06:42 [pubmed]
PHST- 2023/05/01 03:46 [entrez]
AID - 1507 [pii]
AID - 10.1007/s12055-023-01507-6 [doi]
PST - ppublish
SO  - Indian J Thorac Cardiovasc Surg. 2023 May;39(3):217-221. doi: 
      10.1007/s12055-023-01507-6. Epub 2023 Mar 28.

PMID- 37091303
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230425
IS  - 2624-8212 (Electronic)
IS  - 2624-8212 (Linking)
VI  - 6
DP  - 2023
TI  - Impact of ChatGPT on medical chatbots as a disruptive technology.
PG  - 1166014
LID - 10.3389/frai.2023.1166014 [doi]
LID - 1166014
FAU - Chow, James C L
AU  - Chow JCL
AD  - Radiation Medicine Program, Princess Margaret Cancer Centre, University Health 
      Network, Toronto, ON, Canada.
AD  - Department of Radiation Oncology, University of Toronto, Toronto, ON, Canada.
FAU - Sanders, Leslie
AU  - Sanders L
AD  - Department of Humanities, York University, Toronto, ON, Canada.
FAU - Li, Kay
AU  - Li K
AD  - Department of English, University of Toronto, Toronto, ON, Canada.
LA  - eng
PT  - Journal Article
DEP - 20230405
PL  - Switzerland
TA  - Front Artif Intell
JT  - Frontiers in artificial intelligence
JID - 101770551
PMC - PMC10113434
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - big data
OT  - disruptive technology
OT  - healthcare
OT  - knowledge base
OT  - medical chatbot
OT  - nature language processing
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2023/04/24 06:41
MHDA- 2023/04/24 06:42
PMCR- 2023/04/05
CRDT- 2023/04/24 03:54
PHST- 2023/02/14 00:00 [received]
PHST- 2023/03/23 00:00 [accepted]
PHST- 2023/04/24 06:42 [medline]
PHST- 2023/04/24 06:41 [pubmed]
PHST- 2023/04/24 03:54 [entrez]
PHST- 2023/04/05 00:00 [pmc-release]
AID - 10.3389/frai.2023.1166014 [doi]
PST - epublish
SO  - Front Artif Intell. 2023 Apr 5;6:1166014. doi: 10.3389/frai.2023.1166014. 
      eCollection 2023.

PMID- 37377631
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230701
IS  - 2296-9144 (Electronic)
IS  - 2296-9144 (Linking)
VI  - 10
DP  - 2023
TI  - Developing ChatGPT's Theory of Mind.
PG  - 1189525
LID - 10.3389/frobt.2023.1189525 [doi]
LID - 1189525
FAU - Marchetti, Antonella
AU  - Marchetti A
AD  - Research Unit on Theory of Mind, Department of Psychology, Università Cattolica 
      del Sacro Cuore, Milan, Italy.
AD  - Research Unit on Psychology and Robotics in the Lifespan, Department of 
      Psychology, Università Cattolica del Sacro Cuore, Milan, Italy.
FAU - Di Dio, Cinzia
AU  - Di Dio C
AD  - Research Unit on Theory of Mind, Department of Psychology, Università Cattolica 
      del Sacro Cuore, Milan, Italy.
AD  - Research Unit on Psychology and Robotics in the Lifespan, Department of 
      Psychology, Università Cattolica del Sacro Cuore, Milan, Italy.
FAU - Cangelosi, Angelo
AU  - Cangelosi A
AD  - Manchester Centre for Robotics and AI, University of Manchester, Manchester, 
      United Kingdom.
FAU - Manzi, Federico
AU  - Manzi F
AD  - Research Unit on Theory of Mind, Department of Psychology, Università Cattolica 
      del Sacro Cuore, Milan, Italy.
AD  - Research Unit on Psychology and Robotics in the Lifespan, Department of 
      Psychology, Università Cattolica del Sacro Cuore, Milan, Italy.
FAU - Massaro, Davide
AU  - Massaro D
AD  - Research Unit on Theory of Mind, Department of Psychology, Università Cattolica 
      del Sacro Cuore, Milan, Italy.
AD  - Research Unit on Psychology and Robotics in the Lifespan, Department of 
      Psychology, Università Cattolica del Sacro Cuore, Milan, Italy.
LA  - eng
PT  - Journal Article
DEP - 20230530
PL  - Switzerland
TA  - Front Robot AI
JT  - Frontiers in robotics and AI
JID - 101749350
PMC - PMC10292745
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - social cognition
OT  - social perception
OT  - theory of mind
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2023/06/28 13:09
MHDA- 2023/06/28 13:10
PMCR- 2023/05/30
CRDT- 2023/06/28 09:17
PHST- 2023/03/19 00:00 [received]
PHST- 2023/05/16 00:00 [accepted]
PHST- 2023/06/28 13:10 [medline]
PHST- 2023/06/28 13:09 [pubmed]
PHST- 2023/06/28 09:17 [entrez]
PHST- 2023/05/30 00:00 [pmc-release]
AID - 1189525 [pii]
AID - 10.3389/frobt.2023.1189525 [doi]
PST - epublish
SO  - Front Robot AI. 2023 May 30;10:1189525. doi: 10.3389/frobt.2023.1189525. 
      eCollection 2023.

PMID- 37283095
OWN - NLM
STAT- MEDLINE
DCOM- 20240208
LR  - 20240208
IS  - 1488-2361 (Electronic)
IS  - 0846-5371 (Linking)
VI  - 75
IP  - 1
DP  - 2024 Feb
TI  - Response to ChatGPT in Radiology: A Deeper Look Into its Limitations and 
      Potential Pathways for Improvement.
PG  - 203
LID - 10.1177/08465371231181910 [doi]
FAU - Wagner, Matthias W
AU  - Wagner MW
AUID- ORCID: 0000-0001-6501-839X
AD  - Division of Neuroradiology, Department of Diagnostic Imaging, The Hospital for 
      Sick Children, Toronto, ON, Canada. RINGGOLD: 7979
AD  - Department of Medical Imaging, University of Toronto, Canada. RINGGOLD: 12366
AD  - Department of Diagnostic and Interventional Neuroradiology, University Hospital 
      Augsburg, Germany.
FAU - Ertl-Wagner, Birgit B
AU  - Ertl-Wagner BB
AD  - Division of Neuroradiology, Department of Diagnostic Imaging, The Hospital for 
      Sick Children, Toronto, ON, Canada. RINGGOLD: 7979
AD  - Department of Medical Imaging, University of Toronto, Canada. RINGGOLD: 12366
LA  - eng
PT  - Letter
DEP - 20230607
PL  - United States
TA  - Can Assoc Radiol J
JT  - Canadian Association of Radiologists journal = Journal l'Association canadienne 
      des radiologistes
JID - 8812910
SB  - IM
MH  - Humans
MH  - Radiography
MH  - *Radiology
OTO - NOTNLM
OT  - ChatGPT
OT  - Radiology
COIS- Declaration of Conflicting InterestsThe author(s) declared no potential conflicts 
      of interest with respect to the research, authorship, and/or publication of this 
      article.
EDAT- 2023/06/07 06:42
MHDA- 2024/02/08 06:42
CRDT- 2023/06/07 05:32
PHST- 2024/02/08 06:42 [medline]
PHST- 2023/06/07 06:42 [pubmed]
PHST- 2023/06/07 05:32 [entrez]
AID - 10.1177/08465371231181910 [doi]
PST - ppublish
SO  - Can Assoc Radiol J. 2024 Feb;75(1):203. doi: 10.1177/08465371231181910. Epub 2023 
      Jun 7.

PMID- 37254022
OWN - NLM
STAT- MEDLINE
DCOM- 20230811
LR  - 20240213
IS  - 1873-4626 (Electronic)
IS  - 1091-255X (Linking)
VI  - 27
IP  - 8
DP  - 2023 Aug
TI  - Quality of ChatGPT Responses to Questions Related To Liver Transplantation.
PG  - 1716-1719
LID - 10.1007/s11605-023-05714-9 [doi]
FAU - Endo, Yutaka
AU  - Endo Y
AD  - Department of Surgery, The Ohio State University Wexner Medical Center, Columbus, 
      OH, USA.
FAU - Sasaki, Kazunari
AU  - Sasaki K
AD  - Department of Surgery, Stanford University, Stanford, CA, USA.
FAU - Moazzam, Zorays
AU  - Moazzam Z
AD  - Department of Surgery, The Ohio State University Wexner Medical Center, Columbus, 
      OH, USA.
FAU - Lima, Henrique A
AU  - Lima HA
AD  - Department of Surgery, The Ohio State University Wexner Medical Center, Columbus, 
      OH, USA.
FAU - Schenk, Austin
AU  - Schenk A
AD  - Department of Surgery, The Ohio State University Wexner Medical Center, Columbus, 
      OH, USA.
FAU - Limkemann, Ashley
AU  - Limkemann A
AD  - Department of Surgery, The Ohio State University Wexner Medical Center, Columbus, 
      OH, USA.
FAU - Washburn, Kenneth
AU  - Washburn K
AD  - Department of Surgery, The Ohio State University Wexner Medical Center, Columbus, 
      OH, USA.
FAU - Pawlik, Timothy M
AU  - Pawlik TM
AUID- ORCID: 0000-0002-4828-8096
AD  - Department of Surgery, The Ohio State University, Wexner Medical Center, 395 W. 
      12th Ave., Suite 670 614 293 8701, Columbus, USA. Tim.Pawlik@osumc.edu.
LA  - eng
PT  - Journal Article
DEP - 20230530
PL  - Netherlands
TA  - J Gastrointest Surg
JT  - Journal of gastrointestinal surgery : official journal of the Society for Surgery 
      of the Alimentary Tract
JID - 9706084
SB  - IM
MH  - Humans
MH  - *Liver Transplantation
MH  - *Artificial Intelligence
OTO - NOTNLM
OT  - ChatGPT
OT  - Patient information
OT  - Transplantation
EDAT- 2023/05/31 01:09
MHDA- 2023/08/11 06:43
CRDT- 2023/05/30 23:31
PHST- 2023/04/27 00:00 [received]
PHST- 2023/05/14 00:00 [accepted]
PHST- 2023/08/11 06:43 [medline]
PHST- 2023/05/31 01:09 [pubmed]
PHST- 2023/05/30 23:31 [entrez]
AID - S1091-255X(23)00463-8 [pii]
AID - 10.1007/s11605-023-05714-9 [doi]
PST - ppublish
SO  - J Gastrointest Surg. 2023 Aug;27(8):1716-1719. doi: 10.1007/s11605-023-05714-9. 
      Epub 2023 May 30.

PMID- 37567487
OWN - NLM
STAT- MEDLINE
DCOM- 20240226
LR  - 20240226
IS  - 1526-3231 (Electronic)
IS  - 0749-8063 (Linking)
VI  - 40
IP  - 3
DP  - 2024 Mar
TI  - Evaluation High-Quality of Information from ChatGPT (Artificial 
      Intelligence-Large Language Model) Artificial Intelligence on Shoulder 
      Stabilization Surgery.
PG  - 726-731.e6
LID - S0749-8063(23)00642-4 [pii]
LID - 10.1016/j.arthro.2023.07.048 [doi]
AB  - PURPOSE: To analyze the quality and readability of information regarding shoulder 
      stabilization surgery available using an online AI software (ChatGPT), using 
      standardized scoring systems, as well as to report on the given answers by the 
      AI. METHODS: An open AI model (ChatGPT) was used to answer 23 commonly asked 
      questions from patients on shoulder stabilization surgery. These answers were 
      evaluated for medical accuracy, quality, and readability using The JAMA Benchmark 
      criteria, DISCERN score, Flesch-Kincaid Reading Ease Score (FRES) &amp; Grade Level 
      (FKGL). RESULTS: The JAMA Benchmark criteria score was 0, which is the lowest 
      score, indicating no reliable resources cited. The DISCERN score was 60, which is 
      considered a good score. The areas that open AI model did not achieve full marks 
      were also related to the lack of available source material used to compile the 
      answers, and finally some shortcomings with information not fully supported by 
      the literature. The FRES was 26.2, and the FKGL was considered to be that of a 
      college graduate. CONCLUSIONS: There was generally high quality in the answers 
      given on questions relating to shoulder stabilization surgery, but there was a 
      high reading level required to comprehend the information presented. However, it 
      is unclear where the answers came from with no source material cited. It is 
      important to note that the ChatGPT software repeatedly references the need to 
      discuss these questions with an orthopaedic surgeon and the importance of shared 
      discussion making, as well as compliance with surgeon treatment recommendations. 
      CLINICAL RELEVANCE: As shoulder instability is an injury that predominantly 
      affects younger individuals who may use the Internet for information, this study 
      shows what information patients may be getting online.
CI  - Copyright © 2023 Arthroscopy Association of North America. Published by Elsevier 
      Inc. All rights reserved.
FAU - Hurley, Eoghan T
AU  - Hurley ET
AD  - Duke University, Durham, North Carolina, U.S.A.. Electronic address: 
      eoghan.hurley@duke.edu.
FAU - Crook, Bryan S
AU  - Crook BS
AD  - Duke University, Durham, North Carolina, U.S.A.
FAU - Lorentz, Samuel G
AU  - Lorentz SG
AD  - Duke University, Durham, North Carolina, U.S.A.
FAU - Danilkowicz, Richard M
AU  - Danilkowicz RM
AD  - Duke University, Durham, North Carolina, U.S.A.
FAU - Lau, Brian C
AU  - Lau BC
AD  - Duke University, Durham, North Carolina, U.S.A.
FAU - Taylor, Dean C
AU  - Taylor DC
AD  - Duke University, Durham, North Carolina, U.S.A.
FAU - Dickens, Jonathan F
AU  - Dickens JF
AD  - Duke University, Durham, North Carolina, U.S.A.
FAU - Anakwenze, Oke
AU  - Anakwenze O
AD  - Duke University, Durham, North Carolina, U.S.A.
FAU - Klifto, Christopher S
AU  - Klifto CS
AD  - Duke University, Durham, North Carolina, U.S.A.
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20230809
PL  - United States
TA  - Arthroscopy
JT  - Arthroscopy : the journal of arthroscopic &amp; related surgery : official 
      publication of the Arthroscopy Association of North America and the International 
      Arthroscopy Association
JID - 8506498
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Joint Instability
MH  - Shoulder/surgery
MH  - *Shoulder Joint
MH  - Comprehension
MH  - Language
EDAT- 2023/08/12 10:42
MHDA- 2024/02/26 06:44
CRDT- 2023/08/11 19:27
PHST- 2023/03/29 00:00 [received]
PHST- 2023/06/27 00:00 [revised]
PHST- 2023/07/28 00:00 [accepted]
PHST- 2024/02/26 06:44 [medline]
PHST- 2023/08/12 10:42 [pubmed]
PHST- 2023/08/11 19:27 [entrez]
AID - S0749-8063(23)00642-4 [pii]
AID - 10.1016/j.arthro.2023.07.048 [doi]
PST - ppublish
SO  - Arthroscopy. 2024 Mar;40(3):726-731.e6. doi: 10.1016/j.arthro.2023.07.048. Epub 
      2023 Aug 9.

PMID- 38373482
OWN - NLM
STAT- Publisher
LR  - 20240229
IS  - 2173-5786 (Electronic)
IS  - 2173-5786 (Linking)
DP  - 2024 Feb 17
TI  - Quality of information about urologic pathology in English and Spanish from 
      ChatGPT, BARD, and Copilot.
LID - S2173-5786(24)00016-7 [pii]
LID - 10.1016/j.acuroe.2024.02.009 [doi]
AB  - INTRODUCTION AND OBJECTIVE: Generative artificial intelligence makes it possible 
      to ask about medical pathologies in dialog boxes. Our objective was to analyze 
      the quality of information about the most common urological pathologies provided 
      by ChatGPT (OpenIA), BARD (Google), and Copilot (Microsoft). METHODS: We analyzed 
      information on the following pathologies and their treatments as provided by AI: 
      prostate cancer, kidney cancer, bladder cancer, urinary lithiasis, and benign 
      prostatic hypertrophy (BPH). Questions in English and Spanish were posed in 
      dialog boxes; the answers were collected and analyzed with DISCERN questionnaires 
      and the overall appropriateness of the response. Surgical procedures were 
      performed with an informed consent questionnaire. RESULTS: The responses from the 
      three chatbots explained the pathology, detailed risk factors, and described 
      treatments. The difference is that BARD and Copilot provide external information 
      citations, which ChatGPT does not. The highest DISCERN scores, in absolute 
      numbers, were obtained in Copilot; however, on the appropriacy scale it was noted 
      that their responses were not the most appropriate. The best surgical treatment 
      scores were obtained by BARD, followed by ChatGPT, and finally Copilot. 
      CONCLUSIONS: The answers obtained from generative AI on urological diseases 
      depended on the formulation of the question. The information provided had 
      significant biases, depending on pathology, language, and above all, the dialog 
      box consulted.
CI  - Copyright © 2024 AEU. Published by Elsevier España, S.L.U. All rights reserved.
FAU - Szczesniewski, J J
AU  - Szczesniewski JJ
AD  - Servicio de Urología, Hospital Universitario de Getafe, Getafe, Madrid, Spain; 
      Departamento de Cirugía, Facultad de Medicina, Universidad de Salamanca, 
      Salamanca, Spain. Electronic address: Juliusz.szcz@gmail.com.
FAU - Ramos Alba, A
AU  - Ramos Alba A
AD  - DXC Technology, Las Rozas, Madrid, Spain; Departamento de Economía Aplicada I e 
      Historia e Instituciones Económicas, Universidad Rey Juan Carlos, Madrid, Spain.
FAU - Rodríguez Castro, P M
AU  - Rodríguez Castro PM
AD  - Servicio de Urología, Hospital Universitario de Getafe, Getafe, Madrid, Spain.
FAU - Lorenzo Gómez, M F
AU  - Lorenzo Gómez MF
AD  - Departamento de Cirugía, Facultad de Medicina, Universidad de Salamanca, 
      Salamanca, Spain; Servicio de Urología, Hospital Universitario de Salamanca, 
      Salamanca, Spain.
FAU - Sainz González, J
AU  - Sainz González J
AD  - Departamento de Economía Aplicada I e Historia e Instituciones Económicas, 
      Universidad Rey Juan Carlos, Madrid, Spain.
FAU - Llanes González, L
AU  - Llanes González L
AD  - Servicio de Urología, Hospital Universitario de Getafe, Getafe, Madrid, Spain; 
      Universidad Francisco de Vitoria, Madrid, Spain.
LA  - eng
LA  - spa
PT  - Journal Article
DEP - 20240217
PL  - Spain
TA  - Actas Urol Esp (Engl Ed)
JT  - Actas urologicas espanolas
JID - 101771154
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - BARD
OT  - Calidad de información
OT  - ChatGPT
OT  - Copilot
OT  - Information quality
OT  - Inteligencia artificial
OT  - Urology
OT  - Urología
EDAT- 2024/02/20 11:50
MHDA- 2024/02/20 11:50
CRDT- 2024/02/19 19:12
PHST- 2023/10/30 00:00 [received]
PHST- 2023/12/26 00:00 [revised]
PHST- 2023/12/27 00:00 [accepted]
PHST- 2024/02/20 11:50 [pubmed]
PHST- 2024/02/20 11:50 [medline]
PHST- 2024/02/19 19:12 [entrez]
AID - S2173-5786(24)00016-7 [pii]
AID - 10.1016/j.acuroe.2024.02.009 [doi]
PST - aheadofprint
SO  - Actas Urol Esp (Engl Ed). 2024 Feb 17:S2173-5786(24)00016-7. doi: 
      10.1016/j.acuroe.2024.02.009.

PMID- 38231545
OWN - NLM
STAT- MEDLINE
DCOM- 20240118
LR  - 20240203
IS  - 2369-3762 (Electronic)
IS  - 2369-3762 (Linking)
VI  - 10
DP  - 2024 Jan 17
TI  - ChatGPT in Medical Education: A Precursor for Automation Bias?
PG  - e50174
LID - 10.2196/50174 [doi]
LID - e50174
AB  - Artificial intelligence (AI) in health care has the promise of providing accurate 
      and efficient results. However, AI can also be a black box, where the logic 
      behind its results is nonrational. There are concerns if these questionable 
      results are used in patient care. As physicians have the duty to provide care 
      based on their clinical judgment in addition to their patients' values and 
      preferences, it is crucial that physicians validate the results from AI. Yet, 
      there are some physicians who exhibit a phenomenon known as automation bias, 
      where there is an assumption from the user that AI is always right. This is a 
      dangerous mindset, as users exhibiting automation bias will not validate the 
      results, given their trust in AI systems. Several factors impact a user's 
      susceptibility to automation bias, such as inexperience or being born in the 
      digital age. In this editorial, I argue that these factors and a lack of AI 
      education in the medical school curriculum cause automation bias. I also explore 
      the harms of automation bias and why prospective physicians need to be vigilant 
      when using AI. Furthermore, it is important to consider what attitudes are being 
      taught to students when introducing ChatGPT, which could be some students' first 
      time using AI, prior to their use of AI in the clinical setting. Therefore, in 
      attempts to avoid the problem of automation bias in the long-term, in addition to 
      incorporating AI education into the curriculum, as is necessary, the use of 
      ChatGPT in medical education should be limited to certain tasks. Otherwise, 
      having no constraints on what ChatGPT should be used for could lead to automation 
      bias.
CI  - ©Tina Nguyen. Originally published in JMIR Medical Education 
      (https://mededu.jmir.org), 17.01.2024.
FAU - Nguyen, Tina
AU  - Nguyen T
AUID- ORCID: 0000-0002-3021-8161
AD  - The University of Texas Medical Branch, Galveston, TX, United States.
LA  - eng
PT  - Editorial
DEP - 20240117
PL  - Canada
TA  - JMIR Med Educ
JT  - JMIR medical education
JID - 101684518
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Prospective Studies
MH  - Automation
MH  - *Education, Medical
MH  - Educational Status
PMC - PMC10831594
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - LLMs
OT  - artificial intelligence
OT  - automation bias
OT  - bias
OT  - large language models
OT  - medical education
OT  - medical school curriculum
OT  - medical students
OT  - residents
COIS- Conflicts of Interest: None declared.
EDAT- 2024/01/17 12:43
MHDA- 2024/01/18 06:42
PMCR- 2024/01/17
CRDT- 2024/01/17 11:53
PHST- 2023/06/21 00:00 [received]
PHST- 2023/12/11 00:00 [accepted]
PHST- 2024/01/18 06:42 [medline]
PHST- 2024/01/17 12:43 [pubmed]
PHST- 2024/01/17 11:53 [entrez]
PHST- 2024/01/17 00:00 [pmc-release]
AID - v10i1e50174 [pii]
AID - 10.2196/50174 [doi]
PST - epublish
SO  - JMIR Med Educ. 2024 Jan 17;10:e50174. doi: 10.2196/50174.

PMID- 38187653
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240322
DP  - 2023 Dec 23
TI  - Iterative Prompt Refinement for Mining Gene Relationships from ChatGPT.
LID - 2023.12.23.573201 [pii]
LID - 10.1101/2023.12.23.573201 [doi]
AB  - ChatGPT has demonstrated its potential as a surrogate knowledge graph. Trained on 
      extensive data sources, including open-access publications, peer-reviewed 
      research articles and biomedical websites, ChatGPT extracted information on gene 
      relationships and biological pathways. However, a major challenge is model 
      hallucination, i.e., high false positive rates. To assess and address this 
      challenge, we systematically evaluated ChatGPT's capacity for predicting gene 
      relationships using GPT-3.5-turbo and GPT-4. Benchmarking against the KEGG 
      Pathway Database as the ground truth, we experimented with diverse prompting 
      strategies, targeting gene relationships of activation, inhibition, and 
      phosphorylation. We introduced an innovative iterative prompt refinement 
      technique. By assessing prompt efficacy using metrics like F-1 score, precision, 
      and recall, GPT-4 was re-engaged to suggest improved prompts. A refined prompt, 
      which combines a specialized role with explanatory text, significantly enhances 
      the performance. Going beyond pairwise gene relationships, we also deciphered 
      complex gene interplays, such as gene interaction chains and pathways pertinent 
      to diseases like non-small cell lung cancer. Direct prompts showed limited 
      success, but "least-to-most" prompting exhibited significant potentials for such 
      network constructions. The methods in this study may be used for some other 
      bioinformatics prediction problems.
FAU - Chen, Yibo
AU  - Chen Y
AD  - Institute for Data Science and Informatics, University of Missouri, Columbia, 
      Missouri 65211, USA.
FAU - Gao, Jeffrey
AU  - Gao J
AD  - Marriotts Ridge High School, Marriottsville, MD, 21104, USA.
FAU - Petruc, Marius
AU  - Petruc M
AD  - Institute for Data Science and Informatics, University of Missouri, Columbia, 
      Missouri 65211, USA.
FAU - Hammer, Richard D
AU  - Hammer RD
AD  - Department of Pathology and Anatomical Sciences, University of Missouri, 
      Columbia, Missouri 65211, USA.
FAU - Popescu, Mihail
AU  - Popescu M
AD  - Department of Biomedical Informatics, Biostatistics and Medical Epidemiology, 
      University of Missouri, Columbia, MO 65211, USA.
FAU - Xu, Dong
AU  - Xu D
AUID- ORCID: 0000-0002-4809-0514
AD  - Department of Electrical Engineering and Computer Science, University of 
      Missouri, Columbia, MO, 65211, USA.
LA  - eng
GR  - P30 DK092950/DK/NIDDK NIH HHS/United States
GR  - R01 LM013392/LM/NLM NIH HHS/United States
PT  - Preprint
DEP - 20231223
PL  - United States
TA  - bioRxiv
JT  - bioRxiv : the preprint server for biology
JID - 101680187
PMC - PMC10769373
OTO - NOTNLM
OT  - Bioinformatics
OT  - ChatGPT
OT  - Gene Relation
OT  - Knowledge Graph
OT  - Prompt Refinement
EDAT- 2024/01/08 06:42
MHDA- 2024/01/08 06:43
PMCR- 2024/01/05
CRDT- 2024/01/08 04:50
PHST- 2024/01/08 06:42 [pubmed]
PHST- 2024/01/08 06:43 [medline]
PHST- 2024/01/08 04:50 [entrez]
PHST- 2024/01/05 00:00 [pmc-release]
AID - 2023.12.23.573201 [pii]
AID - 10.1101/2023.12.23.573201 [doi]
PST - epublish
SO  - bioRxiv [Preprint]. 2023 Dec 23:2023.12.23.573201. doi: 
      10.1101/2023.12.23.573201.

PMID- 37218530
OWN - NLM
STAT- MEDLINE
DCOM- 20230704
LR  - 20230704
IS  - 1756-185X (Electronic)
IS  - 1756-1841 (Linking)
VI  - 26
IP  - 7
DP  - 2023 Jul
TI  - "Dr ChatGPT": Is it a reliable and useful source for common rheumatic diseases?
PG  - 1343-1349
LID - 10.1111/1756-185X.14749 [doi]
AB  - AIM: It is inevitable that artificial intelligence applications will be used as a 
      source of information in the field of health in the near future. For this reason, 
      we aimed to evaluate whether ChatGPT, a new Large Language Model, can be used to 
      obtain information about common rheumatic diseases. MATERIALS AND METHODS: Common 
      rheumatic diseases were identified using the American College of Rheumatology and 
      European League against Rheumatism guidelines. Osteoarthritis (OA), rheumatoid 
      arthritis, ankylosing spondylitis (AS), systemic lupus erythematosus, psoriatic 
      arthritis, fibromyalgia syndrome, and gout were identified by using Google trends 
      for the four most frequently searched keywords on Google. The responses were 
      evaluated with seven-point Likert-type reliability and usefulness scales 
      developed by us. RESULTS: The highest score in terms of reliability was OA (mean 
      ± standard deviation 5.62 ± 1.17), whereas the highest score in terms of 
      usefulness was AS (mean 5.87 ± 0.17). There was no significant difference in the 
      reliability and usefulness of the answers given by the ChatGPT (p = .423 and 
      p = .387, respectively). All scores ranged between 4 and 7. CONCLUSIONS: Although 
      ChatGPT is reliable and useful for patients to obtain information about rheumatic 
      diseases, it should be kept in mind that it may give false and misleading 
      answers.
CI  - © 2023 Asia Pacific League of Associations for Rheumatology and John Wiley &amp; Sons 
      Australia, Ltd.
FAU - Uz, Cuma
AU  - Uz C
AUID- ORCID: 0000-0001-5277-0101
AD  - Department of Physical Medicine and Rehabilitation, Etlik City Hospital, Ankara, 
      Turkey.
FAU - Umay, Ebru
AU  - Umay E
AD  - Department of Physical Medicine and Rehabilitation, Etlik City Hospital, Ankara, 
      Turkey.
LA  - eng
PT  - Journal Article
DEP - 20230523
PL  - England
TA  - Int J Rheum Dis
JT  - International journal of rheumatic diseases
JID - 101474930
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - Reproducibility of Results
MH  - *Rheumatic Diseases/diagnosis
MH  - *Arthritis, Rheumatoid
MH  - *Osteoarthritis
MH  - *Spondylitis, Ankylosing
MH  - *Lupus Erythematosus, Systemic
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - chatbot
OT  - large language model
OT  - rheumatic diseases
EDAT- 2023/05/23 06:42
MHDA- 2023/07/04 06:42
CRDT- 2023/05/23 05:28
PHST- 2023/05/06 00:00 [revised]
PHST- 2023/03/11 00:00 [received]
PHST- 2023/05/09 00:00 [accepted]
PHST- 2023/07/04 06:42 [medline]
PHST- 2023/05/23 06:42 [pubmed]
PHST- 2023/05/23 05:28 [entrez]
AID - 10.1111/1756-185X.14749 [doi]
PST - ppublish
SO  - Int J Rheum Dis. 2023 Jul;26(7):1343-1349. doi: 10.1111/1756-185X.14749. Epub 
      2023 May 23.

PMID- 38359704
OWN - NLM
STAT- MEDLINE
DCOM- 20240401
LR  - 20240401
IS  - 1873-1244 (Electronic)
IS  - 0899-9007 (Linking)
VI  - 121
DP  - 2024 May
TI  - Can ChatGPT provide appropriate meal plans for NCD patients?
PG  - 112291
LID - S0899-9007(23)00319-2 [pii]
LID - 10.1016/j.nut.2023.112291 [doi]
AB  - OBJECTIVES: Dietary habits significantly affect health conditions and are closely 
      related to the onset and progression of non-communicable diseases (NCDs). 
      Consequently, a well-balanced diet plays an important role in lessening the 
      effects of various disorders, including NCDs. Several artificial intelligence 
      recommendation systems have been developed to propose healthy and nutritious 
      diets. Most of these systems use expert knowledge and guidelines to provide 
      tailored diets and encourage healthier eating habits. However, new advances in 
      large language models such as ChatGPT, with their ability to produce human-like 
      responses, have led individuals to search for advice in several tasks, including 
      diet recommendations. This study aimed to determine the ability of ChatGPT models 
      to generate appropriate personalized meal plans for patients with obesity, 
      cardiovascular diseases, and type 2 diabetes. METHODS: Using a state-of-the-art 
      knowledge-based recommendation system as a reference, we assessed the meal plans 
      generated by two large language models in terms of energy intake, nutrient 
      accuracy, and meal variability. RESULTS: Experimental results with different user 
      profiles revealed the potential of ChatGPT models to provide personalized 
      nutritional advice. CONCLUSION: Additional supervision and guidance by nutrition 
      experts or knowledge-based systems are required to ensure meal appropriateness 
      for users with NCDs.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Papastratis, Ilias
AU  - Papastratis I
AD  - The Visual Computing Lab, Information Technologies Institute, Centre for Research 
      and Technology Hellas, Thessaloniki, Central Macedonia, Greece. Electronic 
      address: papastrat@iti.gr.
FAU - Stergioulas, Andreas
AU  - Stergioulas A
AD  - The Visual Computing Lab, Information Technologies Institute, Centre for Research 
      and Technology Hellas, Thessaloniki, Central Macedonia, Greece.
FAU - Konstantinidis, Dimitrios
AU  - Konstantinidis D
AD  - The Visual Computing Lab, Information Technologies Institute, Centre for Research 
      and Technology Hellas, Thessaloniki, Central Macedonia, Greece.
FAU - Daras, Petros
AU  - Daras P
AD  - The Visual Computing Lab, Information Technologies Institute, Centre for Research 
      and Technology Hellas, Thessaloniki, Central Macedonia, Greece.
FAU - Dimitropoulos, Kosmas
AU  - Dimitropoulos K
AD  - The Visual Computing Lab, Information Technologies Institute, Centre for Research 
      and Technology Hellas, Thessaloniki, Central Macedonia, Greece.
LA  - eng
PT  - Journal Article
DEP - 20231111
PL  - United States
TA  - Nutrition
JT  - Nutrition (Burbank, Los Angeles County, Calif.)
JID - 8802712
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Diabetes Mellitus, Type 2
MH  - *Noncommunicable Diseases/prevention &amp; control
MH  - *Cardiovascular Diseases
MH  - Diet, Healthy
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Nutrition
OT  - Recommendation systems
COIS- Declaration of competing interest None.
EDAT- 2024/02/16 00:42
MHDA- 2024/04/01 06:43
CRDT- 2024/02/15 18:12
PHST- 2023/07/21 00:00 [received]
PHST- 2023/10/30 00:00 [accepted]
PHST- 2024/04/01 06:43 [medline]
PHST- 2024/02/16 00:42 [pubmed]
PHST- 2024/02/15 18:12 [entrez]
AID - S0899-9007(23)00319-2 [pii]
AID - 10.1016/j.nut.2023.112291 [doi]
PST - ppublish
SO  - Nutrition. 2024 May;121:112291. doi: 10.1016/j.nut.2023.112291. Epub 2023 Nov 11.

PMID- 38362238
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240217
IS  - 2055-2076 (Print)
IS  - 2055-2076 (Electronic)
IS  - 2055-2076 (Linking)
VI  - 10
DP  - 2024 Jan-Dec
TI  - Patients with floaters: Answers from virtual assistants and large language 
      models.
PG  - 20552076241229933
LID - 10.1177/20552076241229933 [doi]
LID - 20552076241229933
AB  - OBJECTIVES: "Floaters," a common complaint among patients of all ages, was used 
      as a query term because it affects 30% of all people searching for eye care. The 
      American Academy of Ophthalmology website's "floaters" section was used as a 
      source for questions and answers (www.aao.org). Floaters is a visual obstruction 
      that moves with the movement of the eye. They can be associated with retinal 
      detachment, which can lead to vision loss. With the advent of large language 
      model (LLM) chatbots ChatGPT, Bard versus virtual assistants (VA), Google 
      Assistant, and Alexa, we analyzed their responses to "floaters." METHODS: Using 
      AAO.org, "Public &amp; Patients," and its related subsection, "EyeHealth A-Z": 
      Floaters and Flashes link, we asked four questions: (1) What are floaters? (2) 
      What are flashes? (3) Flashes and Migraines? (4) Floaters and Flashes Treatment? 
      to ChatGPT, Bard, Google Assistant, and Alexa. The American Academy of 
      Ophthalmology (AAO) keywords were identified if they were highlighted. The 
      "Flesch-Kincaid Grade Level" formula approved by the U.S. Department of 
      Education, was used to evaluate the reading comprehension level for the 
      responses. RESULTS: Of the chatbots and virtual assistants, Google Assistant is 
      the only one that uses the term "ophthalmologist." There is no mention of the 
      urgency or emergency nature of floaters. AAO.org shows a lower reading level vs 
      the LLMs and VA (p = .11). The reading comprehension levels of ChatGPT, Bard, 
      Google Assistant, and Alexa are higher (12.3, 9.7, 13.1, 8.1 grade) vs the 
      AAO.org (7.3 grade). There is a higher word count for LLMs vs VA (p &lt; .0286). 
      CONCLUSION: Currently, ChatGPT, Bard, Google Assistant, and Alexa are similar. 
      Factual information is present but all miss the urgency of the diagnosis of a 
      retinal detachment. Translational relevance: Both the LLM and virtual assistants 
      are free and our patients will use them to obtain "floaters" information. There 
      may be errors of omission with ChatGPT and a lack of urgency to seek a 
      physician's care.
CI  - © The Author(s) 2024.
FAU - Wu, Gloria
AU  - Wu G
AUID- ORCID: 0000-0002-0050-2401
AD  - Department of Ophthalmology, University of California San Francisco School of 
      Medicine, San Francisco, California, USA. RINGGOLD: 12224
FAU - Zhao, Weichen
AU  - Zhao W
AD  - University of California, Davis, Davis, California, USA.
FAU - Wong, Adrial
AU  - Wong A
AD  - University of California, Davis, Davis, California, USA.
FAU - Lee, David A
AU  - Lee DA
AD  - University of Texas Health Science Center at Houston, McGovern Medical School, 
      Houston, Texas, USA.
LA  - eng
PT  - Journal Article
DEP - 20240214
PL  - United States
TA  - Digit Health
JT  - Digital health
JID - 101690863
PMC - PMC10868475
OTO - NOTNLM
OT  - Alexa
OT  - American Academy of Ophthalmology
OT  - Bard
OT  - ChatGPT
OT  - Google Assistant
OT  - LLM
OT  - floaters
OT  - health literacy
OT  - virtual assistants
COIS- The authors declared no potential conflicts of interest with respect to the 
      research, authorship, and/or publication of this article.
EDAT- 2024/02/16 06:42
MHDA- 2024/02/16 06:43
PMCR- 2024/02/14
CRDT- 2024/02/16 03:55
PHST- 2023/12/28 00:00 [accepted]
PHST- 2024/02/16 06:43 [medline]
PHST- 2024/02/16 06:42 [pubmed]
PHST- 2024/02/16 03:55 [entrez]
PHST- 2024/02/14 00:00 [pmc-release]
AID - 10.1177_20552076241229933 [pii]
AID - 10.1177/20552076241229933 [doi]
PST - epublish
SO  - Digit Health. 2024 Feb 14;10:20552076241229933. doi: 10.1177/20552076241229933. 
      eCollection 2024 Jan-Dec.

PMID- 37886525
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240306
DP  - 2024 Feb 26
TI  - OpenAI's Narrative Embeddings Can Be Used for Detecting Post-Traumatic Stress 
      Following Childbirth Via Birth Stories.
LID - rs.3.rs-3428787 [pii]
AB  - Free-text analysis using Machine Learning (ML)-based Natural Language Processing 
      (NLP) shows promise for diagnosing psychiatric conditions. Chat Generative 
      Pre-trained Transformer (ChatGPT) has demonstrated preliminary initial 
      feasibility for this purpose; however, whether it can accurately assess mental 
      illness remains to be determined. This study evaluates the effectiveness of 
      ChatGPT and the text-embedding-ada-002 (ADA) model in detecting post-traumatic 
      stress disorder following childbirth (CB-PTSD), a maternal postpartum mental 
      illness affecting millions of women annually, with no standard screening 
      protocol. Using a sample of 1,295 women who gave birth in the last six months and 
      were 18+ years old, recruited through hospital announcements, social media, and 
      professional organizations, we explore ChatGPT's and ADA's potential to screen 
      for CB-PTSD by analyzing maternal childbirth narratives. The PTSD Checklist for 
      DSM-5 (PCL-5; cutoff 31) was used to assess CB-PTSD. By developing an ML model 
      that utilizes numerical vector representation of the ADA model, we identify 
      CB-PTSD via narrative classification. Our model outperformed (F1 score: 0.82) 
      ChatGPT and six previously published large language models (LLMs) trained on 
      mental health or clinical domains data, suggesting that the ADA model can be 
      harnessed to identify CB-PTSD. Our modeling approach could be generalized to 
      assess other mental health disorders.
FAU - Bartal, Alon
AU  - Bartal A
AD  - The School of Business Administration, Bar-Ilan University, Max and Anna Web, 
      Ramat Gan, 5290002, Israel.
FAU - Jagodnik, Kathleen M
AU  - Jagodnik KM
AD  - The School of Business Administration, Bar-Ilan University, Max and Anna Web, 
      Ramat Gan, 5290002, Israel.
AD  - Department of Psychiatry, Massachusetts General Hospital, 55 Fruit St., Boston, 
      02114, Massachusetts, USA.
AD  - Department of Psychiatry, Harvard Medical School, 25 Shattuck St., Boston, 02115, 
      Massachusetts, USA.
FAU - Chan, Sabrina J
AU  - Chan SJ
AD  - Department of Psychiatry, Massachusetts General Hospital, 55 Fruit St., Boston, 
      02114, Massachusetts, USA.
FAU - Dekel, Sharon
AU  - Dekel S
AD  - Department of Psychiatry, Massachusetts General Hospital, 55 Fruit St., Boston, 
      02114, Massachusetts, USA.
AD  - Department of Psychiatry, Harvard Medical School, 25 Shattuck St., Boston, 02115, 
      Massachusetts, USA.
LA  - eng
GR  - R01 HD108619/HD/NICHD NIH HHS/United States
GR  - R21 HD100817/HD/NICHD NIH HHS/United States
GR  - R21 HD109546/HD/NICHD NIH HHS/United States
PT  - Preprint
DEP - 20240226
PL  - United States
TA  - Res Sq
JT  - Research square
JID - 101768035
PMC - PMC10602164
OTO - NOTNLM
OT  - Birth narratives
OT  - Birth trauma
OT  - ChatGPT
OT  - Childbirth-related post-traumatic stress disorder (CB-PTSD)
OT  - Maternal mental health
OT  - Natural Language Processing (NLP)
OT  - Postpartum psychopathology
OT  - Pre-trained large language model (PLM)
COIS- Competing Interests Statement. The authors declare no competing interests.
EDAT- 2023/10/27 06:42
MHDA- 2023/10/27 06:43
PMCR- 2024/02/28
CRDT- 2023/10/27 04:44
PHST- 2023/10/27 06:42 [pubmed]
PHST- 2023/10/27 06:43 [medline]
PHST- 2023/10/27 04:44 [entrez]
PHST- 2024/02/28 00:00 [pmc-release]
AID - rs.3.rs-3428787 [pii]
AID - 10.21203/rs.3.rs-3428787/v1 [doi]
PST - epublish
SO  - Res Sq [Preprint]. 2024 Feb 26:rs.3.rs-3428787. doi: 10.21203/rs.3.rs-3428787/v1.

PMID- 38524820
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240326
IS  - 0860-021X (Print)
IS  - 2083-1862 (Electronic)
IS  - 0860-021X (Linking)
VI  - 41
IP  - 2
DP  - 2024 Mar
TI  - Artificial intelligence in sport: Exploring the potential of using ChatGPT in 
      resistance training prescription.
PG  - 209-220
LID - 10.5114/biolsport.2024.132987 [doi]
AB  - OpenAI's Chat Generative Pre-trained Transformer (ChatGPT) technology enables 
      conversational interactions with applications across various fields, including 
      sport. Here, ChatGPT's proficiency in designing a 12-week resistance training 
      programme, following specific prompts, was investigated. GPT3.5 and GPT4.0 
      versions were requested to design 12-week resistance training programmes for male 
      and female hypothetical subjects (20-years-old, no injury, and 'intermediate' 
      resistance training experience). Subsequently, GPT4.0 was requested to design an 
      'advanced' training programme for the same profiles. The proposed training 
      programmes were compared with established guidelines and literature (e.g., 
      National Strength and Conditioning Association textbook), and discussed. ChatGPT 
      suggested 12-week training programmes comprising three, 4-week phases, each with 
      different objectives (e.g., hypertrophy/strength). GPT3.5 proposed a weekly 
      frequency of ~3 sessions, load intensity of 70-85% of one repetition-maximum, 
      repetition range of 4-8 (2-4 sets), and tempo of 2/0/2 
      (eccentric/pause/concentric/'pause'). GPT4.0 proposed intermediate- and advanced 
      programme, with a frequency of 5 or 4 sessions, 60-90% or 70-95% intensity, 3-5 
      sets or 3-6 sets, 5-12 or 3-12 repetitions, respectively. GPT3.5 proposed rest 
      intervals of 90-120 s, and exercise tempo of 2/0/2. GPT4.0 proposed 60-180 
      (intermediate) or 60-300 s (advanced), with exercise tempo of 2/1/2 for 
      intermediates, and 3/0/1/0, 2/0/1/0, and 1/0/1/0 for advanced programmes. All 
      derived programmes were objectively similar regardless of sex. ChatGPT generated 
      training programmes which likely require additional fine-tuning before 
      application. GPT4.0 synthesised more information than GPT3.5 in response to the 
      prompt, and demonstrated recognition awareness of training experience 
      (intermediate vs advanced). ChatGPT may serve as a complementary tool for writing 
      'draft' programme, but likely requires human expertise to maximise training 
      programme effectiveness.
CI  - Copyright © Biology of Sport 2024.
FAU - Washif, Jad Adrian
AU  - Washif JA
AD  - Sports Performance Division, National Sports Institute of Malaysia, Kuala Lumpur, 
      Malaysia.
FAU - Pagaduan, Jeffrey
AU  - Pagaduan J
AD  - Institute of Active Lifestyle, Palacký University Olomouc, Czech Republic.
FAU - James, Carl
AU  - James C
AD  - Department of Sport, Physical Education and Health, Hong Kong Baptist University. 
      Kowloon Tong, Hong Kong SAR.
FAU - Dergaa, Ismail
AU  - Dergaa I
AD  - Primary Health Care Corporation (PHCC), Doha, Qatar.
AD  - High Institute of Sport and Physical Education, University of Sfax, Sfax, 
      Tunisia.
FAU - Beaven, Christopher Martyn
AU  - Beaven CM
AD  - Te Huataki Waiora School of Health, University of Waikato, Tauranga, New Zealand.
LA  - eng
PT  - Journal Article
DEP - 20231120
PL  - Poland
TA  - Biol Sport
JT  - Biology of sport
JID - 8700872
PMC - PMC10955742
OTO - NOTNLM
OT  - Chatbot
OT  - Exercise prescription
OT  - Individualised training
OT  - Periodisation
OT  - Programming
OT  - Strength training
COIS- The authors declare no conflict of interest.
EDAT- 2024/03/25 06:42
MHDA- 2024/03/25 06:43
PMCR- 2024/03/01
CRDT- 2024/03/25 04:32
PHST- 2023/09/13 00:00 [received]
PHST- 2023/10/22 00:00 [revised]
PHST- 2023/10/28 00:00 [accepted]
PHST- 2024/03/25 06:43 [medline]
PHST- 2024/03/25 06:42 [pubmed]
PHST- 2024/03/25 04:32 [entrez]
PHST- 2024/03/01 00:00 [pmc-release]
AID - 51817 [pii]
AID - 10.5114/biolsport.2024.132987 [doi]
PST - ppublish
SO  - Biol Sport. 2024 Mar;41(2):209-220. doi: 10.5114/biolsport.2024.132987. Epub 2023 
      Nov 20.

PMID- 38354135
OWN - NLM
STAT- MEDLINE
DCOM- 20240216
LR  - 20240217
IS  - 1932-6203 (Electronic)
IS  - 1932-6203 (Linking)
VI  - 19
IP  - 2
DP  - 2024
TI  - Can ChatGPT assist authors with abstract writing in medical journals? Evaluating 
      the quality of scientific abstracts generated by ChatGPT and original abstracts.
PG  - e0297701
LID - 10.1371/journal.pone.0297701 [doi]
LID - e0297701
AB  - INTRODUCTION: ChatGPT, a sophisticated large language model (LLM), has garnered 
      widespread attention for its ability to mimic human-like communication. As recent 
      studies indicate a potential supportive role of ChatGPT in academic writing, we 
      assessed the LLM's capacity to generate accurate and comprehensive scientific 
      abstracts from published Randomised Controlled Trial (RCT) data, focusing on the 
      adherence to the Consolidated Standards of Reporting Trials for Abstracts 
      (CONSORT-A) statement, in comparison to the original authors' abstracts. 
      METHODOLOGY: RCTs, identified in a PubMed/MEDLINE search post-September 2021 
      across various medical disciplines, were subjected to abstract generation via 
      ChatGPT versions 3.5 and 4, following the guidelines of the respective journals. 
      The overall quality score (OQS) of each abstract was determined by the total 
      number of adequately reported components from the 18-item CONSORT-A checklist. 
      Additional outcome measures included percent adherence to each CONOSORT-A item, 
      readability, hallucination rate, and regression analysis of reporting quality 
      determinants. RESULTS: Original abstracts achieved a mean OQS of 11.89 (95% CI: 
      11.23-12.54), outperforming GPT 3.5 (7.89; 95% CI: 7.32-8.46) and GPT 4 (5.18; 
      95% CI: 4.64-5.71). Compared to GPT 3.5 and 4 outputs, original abstracts were 
      more adherent with 10 and 14 CONSORT-A items, respectively. In blind assessments, 
      GPT 3.5-generated abstracts were deemed most readable in 62.22% of cases which 
      was significantly greater than the original (31.11%; P = 0.003) and GPT 
      4-generated (6.67%; P&lt;0.001) abstracts. Moreover, ChatGPT 3.5 exhibited a 
      hallucination rate of 0.03 items per abstract compared to 1.13 by GPT 4. No 
      determinants for improved reporting quality were identified for GPT-generated 
      abstracts. CONCLUSIONS: While ChatGPT could generate more readable abstracts, 
      their overall quality was inferior to the original abstracts. Yet, its 
      proficiency to concisely relay key information with minimal error holds promise 
      for medical research and warrants further investigations to fully ascertain the 
      LLM's applicability in this domain.
CI  - Copyright: © 2024 Hwang et al. This is an open access article distributed under 
      the terms of the Creative Commons Attribution License, which permits unrestricted 
      use, distribution, and reproduction in any medium, provided the original author 
      and source are credited.
FAU - Hwang, Taesoon
AU  - Hwang T
AUID- ORCID: 0000-0002-9312-8198
AD  - Warwick Medical School, University of Warwick, Coventry, United Kingdom.
AD  - University Hospitals Coventry and Warwickshire, Coventry, United Kingdom.
FAU - Aggarwal, Nishant
AU  - Aggarwal N
AD  - Warwick Medical School, University of Warwick, Coventry, United Kingdom.
AD  - University Hospitals Coventry and Warwickshire, Coventry, United Kingdom.
FAU - Khan, Pir Zarak
AU  - Khan PZ
AD  - University Hospitals Coventry and Warwickshire, Coventry, United Kingdom.
FAU - Roberts, Thomas
AU  - Roberts T
AUID- ORCID: 0009-0006-3261-8165
AD  - Warwick Medical School, University of Warwick, Coventry, United Kingdom.
FAU - Mahmood, Amir
AU  - Mahmood A
AD  - Warwick Medical School, University of Warwick, Coventry, United Kingdom.
FAU - Griffiths, Madlen M
AU  - Griffiths MM
AD  - Royal Devon and Exeter Hospital, Exeter, United Kingdom.
FAU - Parsons, Nick
AU  - Parsons N
AD  - Warwick Medical School, University of Warwick, Coventry, United Kingdom.
FAU - Khan, Saboor
AU  - Khan S
AD  - University Hospitals Coventry and Warwickshire, Coventry, United Kingdom.
LA  - eng
PT  - Journal Article
PT  - Randomized Controlled Trial
DEP - 20240214
PL  - United States
TA  - PLoS One
JT  - PloS one
JID - 101285081
SB  - IM
MH  - Humans
MH  - *Periodicals as Topic
MH  - Writing
MH  - Reference Standards
MH  - Publications
MH  - Hallucinations
PMC - PMC10866463
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/02/14 18:42
MHDA- 2024/02/16 06:43
PMCR- 2024/02/14
CRDT- 2024/02/14 13:34
PHST- 2023/10/17 00:00 [received]
PHST- 2024/01/10 00:00 [accepted]
PHST- 2024/02/16 06:43 [medline]
PHST- 2024/02/14 18:42 [pubmed]
PHST- 2024/02/14 13:34 [entrez]
PHST- 2024/02/14 00:00 [pmc-release]
AID - PONE-D-23-33840 [pii]
AID - 10.1371/journal.pone.0297701 [doi]
PST - epublish
SO  - PLoS One. 2024 Feb 14;19(2):e0297701. doi: 10.1371/journal.pone.0297701. 
      eCollection 2024.

PMID- 37501529
OWN - NLM
STAT- MEDLINE
DCOM- 20231207
LR  - 20231217
IS  - 1533-4031 (Electronic)
IS  - 1072-4109 (Linking)
VI  - 31
IP  - 1
DP  - 2024 Jan 1
TI  - Application of ChatGPT in Routine Diagnostic Pathology: Promises, Pitfalls, and 
      Potential Future Directions.
PG  - 15-21
LID - 10.1097/PAP.0000000000000406 [doi]
AB  - Large Language Models are forms of artificial intelligence that use deep learning 
      algorithms to decipher large amounts of text and exhibit strong capabilities like 
      question answering and translation. Recently, an influx of Large Language Models 
      has emerged in the medical and academic discussion, given their potential 
      widespread application to improve patient care and provider workflow. One 
      application that has gained notable recognition in the literature is ChatGPT, 
      which is a natural language processing "chatbot" technology developed by the 
      artificial intelligence development software company OpenAI. It learns from large 
      amounts of text data to generate automated responses to inquiries in seconds. In 
      health care and academia, chatbot systems like ChatGPT have gained much 
      recognition recently, given their potential to become functional, reliable 
      virtual assistants. However, much research is required to determine the accuracy, 
      validity, and ethical concerns of the integration of ChatGPT and other chatbots 
      into everyday practice. One such field where little information and research on 
      the matter currently exists is pathology. Herein, we present a literature review 
      of pertinent articles regarding the current status and understanding of ChatGPT 
      and its potential application in routine diagnostic pathology. In this review, we 
      address the promises, possible pitfalls, and future potential of this 
      application. We provide examples of actual conversations conducted with the 
      chatbot technology that mimic hypothetical but practical diagnostic pathology 
      scenarios that may be encountered in routine clinical practice. On the basis of 
      this experience, we observe that ChatGPT and other chatbots already have a 
      remarkable ability to distill and summarize, within seconds, vast amounts of 
      publicly available data and information to assist in laying a foundation of 
      knowledge on a specific topic. We emphasize that, at this time, any use of such 
      knowledge at the patient care level in clinical medicine must be carefully vetted 
      through established sources of medical information and expertise. We suggest and 
      anticipate that with the ever-expanding knowledge base required to reliably 
      practice personalized, precision anatomic pathology, improved technologies like 
      future versions of ChatGPT (and other chatbots) enabled by expanded access to 
      reliable, diverse data, might serve as a key ally to the diagnostician. Such 
      technology has real potential to further empower the time-honored paradigm of 
      histopathologic diagnoses based on the integrative cognitive assessment of 
      clinical, gross, and microscopic findings and ancillary immunohistochemical and 
      molecular studies at a time of exploding biomedical knowledge.
CI  - Copyright © 2023 Wolters Kluwer Health, Inc. All rights reserved.
FAU - Schukow, Casey
AU  - Schukow C
AD  - Department of Pathology, Corewell Health, Royal Oak, MI.
FAU - Smith, Steven Christopher
AU  - Smith SC
AD  - Department of Pathology, Virginia Commonwealth University School of Medicine, 
      Massey Cancer Center, and Richmond Veterans Affairs Medical Center, Richmond, VA.
FAU - Landgrebe, Eric
AU  - Landgrebe E
AD  - Cornell University College of Engineering, Ithaca, NY.
FAU - Parasuraman, Surya
AU  - Parasuraman S
AD  - Department of Biological Sciences, University of Toledo, Toledo, OH.
FAU - Folaranmi, Olaleke Oluwasegun
AU  - Folaranmi OO
AD  - Department of Anatomic Pathology, University of Ilorin Teaching Hospital, Ilorin, 
      Nigeria.
FAU - Paner, Gladell P
AU  - Paner GP
AD  - Departments of Pathology and Surgery (Urology), University of Chicago, Chicago, 
      IL.
FAU - Amin, Mahul B
AU  - Amin MB
AD  - Department of Pathology and Laboratory Medicine, University of Tennessee Health 
      Science Center, Memphis, TN.
AD  - Department of Urology, University of Southern California Keck School of Medicine, 
      Los Angeles, CA.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20230727
PL  - United States
TA  - Adv Anat Pathol
JT  - Advances in anatomic pathology
JID - 9435676
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Algorithms
MH  - Communication
COIS- The authors have no conflicts of interest to disclose.
EDAT- 2023/07/28 06:43
MHDA- 2023/12/07 12:43
CRDT- 2023/07/28 02:53
PHST- 2023/12/07 12:43 [medline]
PHST- 2023/07/28 06:43 [pubmed]
PHST- 2023/07/28 02:53 [entrez]
AID - 00125480-990000000-00063 [pii]
AID - 10.1097/PAP.0000000000000406 [doi]
PST - ppublish
SO  - Adv Anat Pathol. 2024 Jan 1;31(1):15-21. doi: 10.1097/PAP.0000000000000406. Epub 
      2023 Jul 27.

PMID- 38015298
OWN - NLM
STAT- MEDLINE
DCOM- 20231129
LR  - 20231201
IS  - 1590-9999 (Electronic)
IS  - 1590-9921 (Print)
IS  - 1590-9921 (Linking)
VI  - 24
IP  - 1
DP  - 2023 Nov 28
TI  - Arthrosis diagnosis and treatment recommendations in clinical practice: an 
      exploratory investigation with the generative AI model GPT-4.
PG  - 61
LID - 10.1186/s10195-023-00740-4 [doi]
LID - 61
AB  - BACKGROUND: The spread of artificial intelligence (AI) has led to transformative 
      advancements in diverse sectors, including healthcare. Specifically, generative 
      writing systems have shown potential in various applications, but their 
      effectiveness in clinical settings has been barely investigated. In this context, 
      we evaluated the proficiency of ChatGPT-4 in diagnosing gonarthrosis and 
      coxarthrosis and recommending appropriate treatments compared with orthopaedic 
      specialists. METHODS: A retrospective review was conducted using anonymized 
      medical records of 100 patients previously diagnosed with either knee or hip 
      arthrosis. ChatGPT-4 was employed to analyse these historical records, 
      formulating both a diagnosis and potential treatment suggestions. Subsequently, a 
      comparative analysis was conducted to assess the concordance between the AI's 
      conclusions and the original clinical decisions made by the physicians. RESULTS: 
      In diagnostic evaluations, ChatGPT-4 consistently aligned with the conclusions 
      previously drawn by physicians. In terms of treatment recommendations, there was 
      an 83% agreement between the AI and orthopaedic specialists. The therapeutic 
      concordance was verified by the calculation of a Cohen's Kappa coefficient of 
      0.580 (p &lt; 0.001). This indicates a moderate-to-good level of agreement. In 
      recommendations pertaining to surgical treatment, the AI demonstrated a 
      sensitivity and specificity of 78% and 80%, respectively. Multivariable logistic 
      regression demonstrated that the variables reduced quality of life (OR 49.97, 
      p &lt; 0.001) and start-up pain (OR 12.54, p = 0.028) have an influence on 
      ChatGPT-4's recommendation for a surgery. CONCLUSION: This study emphasises 
      ChatGPT-4's notable potential in diagnosing conditions such as gonarthrosis and 
      coxarthrosis and in aligning its treatment recommendations with those of 
      orthopaedic specialists. However, it is crucial to acknowledge that AI tools such 
      as ChatGPT-4 are not meant to replace the nuanced expertise and clinical judgment 
      of seasoned orthopaedic surgeons, particularly in complex decision-making 
      scenarios regarding treatment indications. Due to the exploratory nature of the 
      study, further research with larger patient populations and more complex 
      diagnoses is necessary to validate the findings and explore the broader potential 
      of AI in healthcare. LEVEL OF EVIDENCE: Level III evidence.
CI  - © 2023. The Author(s).
FAU - Pagano, Stefano
AU  - Pagano S
AUID- ORCID: 0009-0006-6721-7564
AD  - Department of Orthopaedic Surgery, University of Regensburg, Asklepios Klinikum, 
      Bad Abbach, Germany. stefano.pagano@ukr.de.
FAU - Holzapfel, Sabrina
AU  - Holzapfel S
AD  - Department of Neonatology, University Children's Hospital Regensburg, Hospital 
      St. Hedwig of the Order of St. John, University of Regensburg, Regensburg, 
      Germany.
FAU - Kappenschneider, Tobias
AU  - Kappenschneider T
AD  - Department of Orthopaedic Surgery, University of Regensburg, Asklepios Klinikum, 
      Bad Abbach, Germany.
FAU - Meyer, Matthias
AU  - Meyer M
AD  - Department of Orthopaedic Surgery, University of Regensburg, Asklepios Klinikum, 
      Bad Abbach, Germany.
FAU - Maderbacher, Günther
AU  - Maderbacher G
AD  - Department of Orthopaedic Surgery, University of Regensburg, Asklepios Klinikum, 
      Bad Abbach, Germany.
FAU - Grifka, Joachim
AU  - Grifka J
AD  - Department of Orthopaedic Surgery, University of Regensburg, Asklepios Klinikum, 
      Bad Abbach, Germany.
FAU - Holzapfel, Dominik Emanuel
AU  - Holzapfel DE
AD  - Department of Orthopaedic Surgery, University of Regensburg, Asklepios Klinikum, 
      Bad Abbach, Germany.
LA  - eng
PT  - Journal Article
DEP - 20231128
PL  - Italy
TA  - J Orthop Traumatol
JT  - Journal of orthopaedics and traumatology : official journal of the Italian 
      Society of Orthopaedics and Traumatology
JID - 101090931
SB  - IM
MH  - Humans
MH  - *Osteoarthritis, Hip/diagnosis/therapy
MH  - Artificial Intelligence
MH  - Quality of Life
MH  - *Osteoarthritis, Knee/diagnosis/therapy
MH  - Knee Joint
PMC - PMC10684473
OTO - NOTNLM
OT  - Arthrosis
OT  - Artificial intelligence
OT  - ChatGPT-4
OT  - Large language model
OT  - Orthopaedics
OT  - Total joint replacement
COIS- The authors declare that they have no competing interests.
EDAT- 2023/11/28 12:42
MHDA- 2023/11/29 06:42
PMCR- 2023/11/28
CRDT- 2023/11/28 11:09
PHST- 2023/09/08 00:00 [received]
PHST- 2023/11/05 00:00 [accepted]
PHST- 2023/11/29 06:42 [medline]
PHST- 2023/11/28 12:42 [pubmed]
PHST- 2023/11/28 11:09 [entrez]
PHST- 2023/11/28 00:00 [pmc-release]
AID - 10.1186/s10195-023-00740-4 [pii]
AID - 740 [pii]
AID - 10.1186/s10195-023-00740-4 [doi]
PST - epublish
SO  - J Orthop Traumatol. 2023 Nov 28;24(1):61. doi: 10.1186/s10195-023-00740-4.

PMID- 37679503
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231119
IS  - 2045-2322 (Electronic)
IS  - 2045-2322 (Linking)
VI  - 13
IP  - 1
DP  - 2023 Sep 7
TI  - Fabrication and errors in the bibliographic citations generated by ChatGPT.
PG  - 14045
LID - 10.1038/s41598-023-41032-5 [doi]
LID - 14045
AB  - Although chatbots such as ChatGPT can facilitate cost-effective text generation 
      and editing, factually incorrect responses (hallucinations) limit their utility. 
      This study evaluates one particular type of hallucination: fabricated 
      bibliographic citations that do not represent actual scholarly works. We used 
      ChatGPT-3.5 and ChatGPT-4 to produce short literature reviews on 42 
      multidisciplinary topics, compiling data on the 636 bibliographic citations 
      (references) found in the 84 papers. We then searched multiple databases and 
      websites to determine the prevalence of fabricated citations, to identify errors 
      in the citations to non-fabricated papers, and to evaluate adherence to APA 
      citation format. Within this set of documents, 55% of the GPT-3.5 citations but 
      just 18% of the GPT-4 citations are fabricated. Likewise, 43% of the real 
      (non-fabricated) GPT-3.5 citations but just 24% of the real GPT-4 citations 
      include substantive citation errors. Although GPT-4 is a major improvement over 
      GPT-3.5, problems remain.
CI  - © 2023. Springer Nature Limited.
FAU - Walters, William H
AU  - Walters WH
AD  - Mary Alice &amp; Tom O'Malley Library, Manhattan College, Riverdale, NY, USA. 
      william.walters@manhattan.edu.
FAU - Wilder, Esther Isabelle
AU  - Wilder EI
AD  - Department of Sociology, Lehman College, The City University of New York, Bronx, 
      NY, USA.
AD  - Doctoral Program in Sociology, CUNY Graduate Center, The City University of New 
      York, New York, NY, USA.
LA  - eng
PT  - Journal Article
DEP - 20230907
PL  - England
TA  - Sci Rep
JT  - Scientific reports
JID - 101563288
SB  - IM
PMC - PMC10484980
COIS- The authors declare no competing interests.
EDAT- 2023/09/08 00:42
MHDA- 2023/09/08 00:43
PMCR- 2023/09/07
CRDT- 2023/09/07 23:32
PHST- 2023/07/11 00:00 [received]
PHST- 2023/08/21 00:00 [accepted]
PHST- 2023/09/08 00:43 [medline]
PHST- 2023/09/08 00:42 [pubmed]
PHST- 2023/09/07 23:32 [entrez]
PHST- 2023/09/07 00:00 [pmc-release]
AID - 10.1038/s41598-023-41032-5 [pii]
AID - 41032 [pii]
AID - 10.1038/s41598-023-41032-5 [doi]
PST - epublish
SO  - Sci Rep. 2023 Sep 7;13(1):14045. doi: 10.1038/s41598-023-41032-5.

PMID- 38493370
OWN - NLM
STAT- Publisher
LR  - 20240317
IS  - 1097-6752 (Electronic)
IS  - 0889-5406 (Linking)
DP  - 2024 Mar 14
TI  - The performance of artificial intelligence models in generating responses to 
      general orthodontic questions: ChatGPT vs Google Bard.
LID - S0889-5406(24)00059-3 [pii]
LID - 10.1016/j.ajodo.2024.01.012 [doi]
AB  - INTRODUCTION: This study aimed to evaluate and compare the performance of 2 
      artificial intelligence (AI) models, Chat Generative Pretrained Transformer-3.5 
      (ChatGPT-3.5; OpenAI, San Francisco, Calif) and Google Bidirectional Encoder 
      Representations from Transformers (Google Bard; Bard Experiment, Google, Mountain 
      View, Calif), in terms of response accuracy, completeness, generation time, and 
      response length when answering general orthodontic questions. METHODS: A team of 
      orthodontic specialists developed a set of 100 questions in 10 orthodontic 
      domains. One author submitted the questions to both ChatGPT and Google Bard. The 
      AI-generated responses from both models were randomly assigned into 2 forms and 
      sent to 5 blinded and independent assessors. The quality of AI-generated 
      responses was evaluated using a newly developed tool for accuracy of information 
      and completeness. In addition, response generation time and length were recorded. 
      RESULTS: The accuracy and completeness of responses were high in both AI models. 
      The median accuracy score was 9 (interquartile range [IQR]: 8-9) for ChatGPT and 
      8 (IQR: 8-9) for Google Bard (Median difference: 1; P&nbsp;&lt;0.001). The median 
      completeness score was similar in both models, with 8 (IQR: 8-9) for ChatGPT and 
      8 (IQR: 7-9) for Google Bard. The odds of accuracy and completeness were higher 
      by 31% and 23% in ChatGPT than in Google Bard. Google Bard's response generation 
      time was significantly shorter than that of ChatGPT by 10.4 second/question. 
      However, both models were similar in terms of response length generation. 
      CONCLUSIONS: Both ChatGPT and Google Bard generated responses were rated with a 
      high level of accuracy and completeness to the posed general orthodontic 
      questions. However, acquiring answers was generally faster using the Google Bard 
      model.
CI  - Copyright © 2024 American Association of Orthodontists. Published by Elsevier 
      Inc. All rights reserved.
FAU - Daraqel, Baraa
AU  - Daraqel B
AD  - Department of Orthodontics, Stomatological Hospital of Chongqing Medical 
      University Chongqing Key Laboratory of Oral Disease and Biomedical Sciences 
      Chongqing Municipal Key Laboratory of Oral Biomedical Engineering of Higher 
      Education, Chongqing, China; Oral Health Research and Promotion Unit, Al-Quds 
      University, Jerusalem, Palestine. Electronic address: baraa@stu.cqmu.edu.cn.
FAU - Wafaie, Khaled
AU  - Wafaie K
AD  - Department of Orthodontics, Faculty of Dentistry, First Affiliated Hospital of 
      Zhengzhou University, Zhengzhou, Henan, China.
FAU - Mohammed, Hisham
AU  - Mohammed H
AD  - Private practice, Cairo, Egypt.
FAU - Cao, Li
AU  - Cao L
AD  - Department of Orthodontics, Stomatological Hospital of Chongqing Medical 
      University Chongqing Key Laboratory of Oral Disease and Biomedical Sciences 
      Chongqing Municipal Key Laboratory of Oral Biomedical Engineering of Higher 
      Education, Chongqing, China.
FAU - Mheissen, Samer
AU  - Mheissen S
AD  - Private practice, Damascus, Syria.
FAU - Liu, Yang
AU  - Liu Y
AD  - Department of Orthodontics, Stomatological Hospital of Chongqing Medical 
      University Chongqing Key Laboratory of Oral Disease and Biomedical Sciences 
      Chongqing Municipal Key Laboratory of Oral Biomedical Engineering of Higher 
      Education, Chongqing, China.
FAU - Zheng, Leilei
AU  - Zheng L
AD  - Department of Orthodontics, Stomatological Hospital of Chongqing Medical 
      University Chongqing Key Laboratory of Oral Disease and Biomedical Sciences 
      Chongqing Municipal Key Laboratory of Oral Biomedical Engineering of Higher 
      Education, Chongqing, China. Electronic address: 
      zhengleileicqmu@hospital.cqmu.edu.cn.
LA  - eng
PT  - Journal Article
DEP - 20240314
PL  - United States
TA  - Am J Orthod Dentofacial Orthop
JT  - American journal of orthodontics and dentofacial orthopedics : official 
      publication of the American Association of Orthodontists, its constituent 
      societies, and the American Board of Orthodontics
JID - 8610224
SB  - IM
EDAT- 2024/03/17 12:44
MHDA- 2024/03/17 12:44
CRDT- 2024/03/17 11:32
PHST- 2023/10/01 00:00 [received]
PHST- 2024/01/01 00:00 [revised]
PHST- 2024/01/01 00:00 [accepted]
PHST- 2024/03/17 12:44 [medline]
PHST- 2024/03/17 12:44 [pubmed]
PHST- 2024/03/17 11:32 [entrez]
AID - S0889-5406(24)00059-3 [pii]
AID - 10.1016/j.ajodo.2024.01.012 [doi]
PST - aheadofprint
SO  - Am J Orthod Dentofacial Orthop. 2024 Mar 14:S0889-5406(24)00059-3. doi: 
      10.1016/j.ajodo.2024.01.012.

PMID- 37986970
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231129
DP  - 2023 Nov 8
TI  - ChatGPT usage in the Reactome curation process.
LID - 2023.11.08.566195 [pii]
LID - 10.1101/2023.11.08.566195 [doi]
AB  - Appreciating the rapid advancement and ubiquity of generative AI, particularly 
      ChatGPT, a chatbot using large language models like GPT, we endeavour to explore 
      the potential application of ChatGPT in the data collection and annotation stages 
      within the Reactome curation process. This exploration aimed to create an 
      automated or semi-automated framework to mitigate the extensive manual effort 
      traditionally required for gathering and annotating information pertaining to 
      biological pathways, adopting a Reactome "reaction-centric" approach. In this 
      pilot study, we used ChatGPT/GPT4 to address gaps in the pathway annotation and 
      enrichment in parallel with the conventional manual curation process. This 
      approach facilitated a comparative analysis, where we assessed the outputs 
      generated by ChatGPT against manually extracted information. The primary 
      objective of this comparison was to ascertain the efficiency of integrating 
      ChatGPT or other large language models into the Reactome curation workflow and 
      helping plan our annotation pipeline, ultimately improving our protein-to-pathway 
      association in a reliable and automated or semi-automated way. In the process, we 
      identified some promising capabilities and inherent challenges associated with 
      the utilisation of ChatGPT/GPT4 in general and also specifically in the context 
      of Reactome curation processes. We describe approaches and tools for refining the 
      output given by ChatGPT/GPT4 that aid in generating more accurate and detailed 
      output.
FAU - Tiwari, Krishna
AU  - Tiwari K
AUID- ORCID: 0000-0002-3699-0937
AD  - European Molecular Biology Laboratory, European Bioinformatics Institute 
      (EMBL-EBI), Wellcome Genome Campus, Hinxton, Cambridgeshire, CB10 1SD, UK.
AD  - Open Targets, Wellcome Genome Campus, Hinxton, Cambridgeshire, CB10 1SD, UK.
FAU - Matthews, Lisa
AU  - Matthews L
AUID- ORCID: 0000-0001-5707-3065
AD  - NYU Grossman School of Medicine, New York, NY 10016, USA.
FAU - May, Bruce
AU  - May B
AD  - Ontario Institute for Cancer Research, Toronto, Ontario, M5G 0A3, Canada.
FAU - Shamovsky, Veronica
AU  - Shamovsky V
AUID- ORCID: 0000-0002-2187-2241
AD  - NYU Grossman School of Medicine, New York, NY 10016, USA.
FAU - Orlic-Milacic, Marija
AU  - Orlic-Milacic M
AUID- ORCID: 0000-0002-3218-5631
AD  - Ontario Institute for Cancer Research, Toronto, Ontario, M5G 0A3, Canada.
FAU - Rothfels, Karen
AU  - Rothfels K
AUID- ORCID: 0000-0002-0705-7048
AD  - Ontario Institute for Cancer Research, Toronto, Ontario, M5G 0A3, Canada.
FAU - Ragueneau, Eliot
AU  - Ragueneau E
AUID- ORCID: 0000-0002-7876-6503
AD  - European Molecular Biology Laboratory, European Bioinformatics Institute 
      (EMBL-EBI), Wellcome Genome Campus, Hinxton, Cambridgeshire, CB10 1SD, UK.
AD  - Open Targets, Wellcome Genome Campus, Hinxton, Cambridgeshire, CB10 1SD, UK.
FAU - Gong, Chuqiao
AU  - Gong C
AUID- ORCID: 0000-0001-8674-1739
AD  - European Molecular Biology Laboratory, European Bioinformatics Institute 
      (EMBL-EBI), Wellcome Genome Campus, Hinxton, Cambridgeshire, CB10 1SD, UK.
AD  - Open Targets, Wellcome Genome Campus, Hinxton, Cambridgeshire, CB10 1SD, UK.
FAU - Stephan, Ralf
AU  - Stephan R
AUID- ORCID: 0000-0002-4650-631X
AD  - Ontario Institute for Cancer Research, Toronto, Ontario, M5G 0A3, Canada.
FAU - Li, Nancy
AU  - Li N
AD  - Ontario Institute for Cancer Research, Toronto, Ontario, M5G 0A3, Canada.
FAU - Wu, Guanming
AU  - Wu G
AUID- ORCID: 0000-0001-8196-1177
AD  - Oregon Health and Science University, Portland, OR 97239, USA.
FAU - Stein, Lincoln
AU  - Stein L
AUID- ORCID: 0000-0002-1983-4588
AD  - Ontario Institute for Cancer Research, Toronto, Ontario, M5G 0A3, Canada.
FAU - D'Eustachio, Peter
AU  - D'Eustachio P
AUID- ORCID: 0000-0002-5494-626X
AD  - NYU Grossman School of Medicine, New York, NY 10016, USA.
FAU - Hermjakob, Henning
AU  - Hermjakob H
AUID- ORCID: 0000-0001-8479-0262
AD  - European Molecular Biology Laboratory, European Bioinformatics Institute 
      (EMBL-EBI), Wellcome Genome Campus, Hinxton, Cambridgeshire, CB10 1SD, UK.
AD  - Open Targets, Wellcome Genome Campus, Hinxton, Cambridgeshire, CB10 1SD, UK.
LA  - eng
GR  - U41 HG003751/HG/NHGRI NIH HHS/United States
GR  - U54 GM114833/GM/NIGMS NIH HHS/United States
GR  - U24 HG012198/HG/NHGRI NIH HHS/United States
GR  - U24 HG011851/HG/NHGRI NIH HHS/United States
GR  - U01 CA239069/CA/NCI NIH HHS/United States
PT  - Preprint
DEP - 20231108
PL  - United States
TA  - bioRxiv
JT  - bioRxiv : the preprint server for biology
JID - 101680187
PMC - PMC10659344
EDAT- 2023/11/21 06:42
MHDA- 2023/11/21 06:43
PMCR- 2023/11/20
CRDT- 2023/11/21 05:09
PHST- 2023/11/21 06:42 [pubmed]
PHST- 2023/11/21 06:43 [medline]
PHST- 2023/11/21 05:09 [entrez]
PHST- 2023/11/20 00:00 [pmc-release]
AID - 2023.11.08.566195 [pii]
AID - 10.1101/2023.11.08.566195 [doi]
PST - epublish
SO  - bioRxiv [Preprint]. 2023 Nov 8:2023.11.08.566195. doi: 10.1101/2023.11.08.566195.

PMID- 37707884
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231113
IS  - 2369-3762 (Print)
IS  - 2369-3762 (Electronic)
IS  - 2369-3762 (Linking)
VI  - 9
DP  - 2023 Sep 14
TI  - The Potential and Concerns of Using AI in Scientific Research: ChatGPT 
      Performance Evaluation.
PG  - e47049
LID - 10.2196/47049 [doi]
LID - e47049
AB  - BACKGROUND: Artificial intelligence (AI) has many applications in various aspects 
      of our daily life, including health, criminal, education, civil, business, and 
      liability law. One aspect of AI that has gained significant attention is natural 
      language processing (NLP), which refers to the ability of computers to understand 
      and generate human language. OBJECTIVE: This study aims to examine the potential 
      for, and concerns of, using AI in scientific research. For this purpose, 
      high-impact research articles were generated by analyzing the quality of reports 
      generated by ChatGPT and assessing the application's impact on the research 
      framework, data analysis, and the literature review. The study also explored 
      concerns around ownership and the integrity of research when using AI-generated 
      text. METHODS: A total of 4 articles were generated using ChatGPT, and thereafter 
      evaluated by 23 reviewers. The researchers developed an evaluation form to assess 
      the quality of the articles generated. Additionally, 50 abstracts were generated 
      using ChatGPT and their quality was evaluated. The data were subjected to ANOVA 
      and thematic analysis to analyze the qualitative data provided by the reviewers. 
      RESULTS: When using detailed prompts and providing the context of the study, 
      ChatGPT would generate high-quality research that could be published in 
      high-impact journals. However, ChatGPT had a minor impact on developing the 
      research framework and data analysis. The primary area needing improvement was 
      the development of the literature review. Moreover, reviewers expressed concerns 
      around ownership and the integrity of the research when using AI-generated text. 
      Nonetheless, ChatGPT has a strong potential to increase human productivity in 
      research and can be used in academic writing. CONCLUSIONS: AI-generated text has 
      the potential to improve the quality of high-impact research articles. The 
      findings of this study suggest that decision makers and researchers should focus 
      more on the methodology part of the research, which includes research design, 
      developing research tools, and analyzing data in depth, to draw strong 
      theoretical and practical implications, thereby establishing a revolution in 
      scientific research in the era of AI. The practical implications of this study 
      can be used in different fields such as medical education to deliver materials to 
      develop the basic competencies for both medicine students and faculty members.
CI  - ©Zuheir N Khlaif, Allam Mousa, Muayad Kamal Hattab, Jamil Itmazi, Amjad A Hassan, 
      Mageswaran Sanmugam, Abedalkarim Ayyoub. Originally published in JMIR Medical 
      Education (https://mededu.jmir.org), 14.09.2023.
FAU - Khlaif, Zuheir N
AU  - Khlaif ZN
AUID- ORCID: 0000-0002-7354-7512
AD  - Faculty of Humanities and Educational Sciences, An-Najah National University, 
      Nablus, Occupied Palestinian Territory.
FAU - Mousa, Allam
AU  - Mousa A
AUID- ORCID: 0000-0001-6413-1622
AD  - Artificial Intelligence and Virtual Reality Research Center, Department of 
      Electrical and Computer Engineering, An Najah National University, Nablus, 
      Occupied Palestinian Territory.
FAU - Hattab, Muayad Kamal
AU  - Hattab MK
AUID- ORCID: 0000-0002-1096-1839
AD  - Faculty of Law and Political Sciences, An-Najah National University, Nablus, 
      Occupied Palestinian Territory.
FAU - Itmazi, Jamil
AU  - Itmazi J
AUID- ORCID: 0000-0002-4206-7989
AD  - Department of Information Technology, College of Engineering and Information 
      Technology, Palestine Ahliya University, Bethlahem, Occupied Palestinian 
      Territory.
FAU - Hassan, Amjad A
AU  - Hassan AA
AUID- ORCID: 0009-0009-6917-4782
AD  - Faculty of Law and Political Sciences, An-Najah National University, Nablus, 
      Occupied Palestinian Territory.
FAU - Sanmugam, Mageswaran
AU  - Sanmugam M
AUID- ORCID: 0000-0003-3313-4462
AD  - Centre for Instructional Technology and Multimedia, Universiti Sains Malaysia, 
      Penang, Malaysia.
FAU - Ayyoub, Abedalkarim
AU  - Ayyoub A
AUID- ORCID: 0000-0001-9111-4465
AD  - Faculty of Humanities and Educational Sciences, An-Najah National University, 
      Nablus, Occupied Palestinian Territory.
LA  - eng
PT  - Journal Article
DEP - 20230914
PL  - Canada
TA  - JMIR Med Educ
JT  - JMIR medical education
JID - 101684518
PMC - PMC10636627
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - artificial intelligence
OT  - research ethics
OT  - scientific research
COIS- Conflicts of Interest: None declared.
EDAT- 2023/09/14 12:42
MHDA- 2023/09/14 12:43
PMCR- 2023/09/14
CRDT- 2023/09/14 11:53
PHST- 2023/03/06 00:00 [received]
PHST- 2023/07/21 00:00 [accepted]
PHST- 2023/06/04 00:00 [revised]
PHST- 2023/09/14 12:43 [medline]
PHST- 2023/09/14 12:42 [pubmed]
PHST- 2023/09/14 11:53 [entrez]
PHST- 2023/09/14 00:00 [pmc-release]
AID - v9i1e47049 [pii]
AID - 10.2196/47049 [doi]
PST - epublish
SO  - JMIR Med Educ. 2023 Sep 14;9:e47049. doi: 10.2196/47049.

PMID- 37515957
OWN - NLM
STAT- MEDLINE
DCOM- 20230814
LR  - 20230814
IS  - 1532-2793 (Electronic)
IS  - 0260-6917 (Linking)
VI  - 129
DP  - 2023 Oct
TI  - Revolutionizing nursing education through Ai integration: A reflection on the 
      disruptive impact of ChatGPT.
PG  - 105916
LID - S0260-6917(23)00210-1 [pii]
LID - 10.1016/j.nedt.2023.105916 [doi]
AB  - Artificial intelligence (AI) is driving global change. An AI language model like 
      ChatGPT could revolutionize the delivery of nursing education in the future. 
      ChatGPT is an AI-enabled text generator that has garnered significant attention 
      due to its ability to engage in conversations and answer questions. Nurse 
      educators play a crucial role in preparing nursing students for a 
      technology-integrated healthcare system, and the emergence of ChatGPT presents 
      both opportunities and challenges. While the technology has limitations and 
      potential biases, it also has the potential to benefit students by facilitating 
      learning, improving digital literacy, and encouraging critical thinking about AI 
      integration in healthcare. Nurse educators can incorporate ChatGPT into their 
      curriculum through formative or summative assessments and should prioritize 
      faculty development to understand and use AI technologies effectively. 
      Collaboration between educational institutions, regulatory bodies, and educators 
      is crucial to establish provincial and national competencies and frameworks that 
      reflect the increasing importance of AI in nursing education and practice. It is 
      paramount that nurses and nurse educators be open to AI-enabled innovations as 
      well as continue to critically think about their potential value to advance the 
      profession so nurses are better prepared to lead the digital future.
CI  - Copyright © 2023 Elsevier Ltd. All rights reserved.
FAU - Castonguay, Alexandre
AU  - Castonguay A
AD  - University of Montreal, Nursing Faculty, Marguerite-d'Youville Campus, C.P. 6128 
      succ. Centre-ville, Montréal, Québec H3C 3J7, Canada. Electronic address: 
      alexandre.castonguay.2@umontreal.ca.
FAU - Farthing, Pamela
AU  - Farthing P
AD  - Saskatchewan Polytechnic, Faculty, School of Nursing, SCBScN, Canada. Electronic 
      address: pamela.farthing@saskpolytech.ca.
FAU - Davies, Shauna
AU  - Davies S
AD  - University of Regina, Canada. Electronic address: shauna.davies@uregina.ca.
FAU - Vogelsang, Laura
AU  - Vogelsang L
AD  - University of Lethbridge, Faculty of Health Sciences, Canada. Electronic address: 
      laura.vogelsang@uleth.ca.
FAU - Kleib, Manal
AU  - Kleib M
AD  - University of Alberta, Faculty of Nursing, Canada. Electronic address: 
      manal.kleib@ualberta.ca.
FAU - Risling, Tracie
AU  - Risling T
AD  - University of Calgary, Faculty of Nursing, Canada. Electronic address: 
      tracie.risling@ucalgary.ca.
FAU - Green, Nadia
AU  - Green N
AD  - University of Alberta, Faculty of Nursing, Canada. Electronic address: 
      ntgreen@ualberta.ca.
LA  - eng
PT  - Journal Article
DEP - 20230718
PL  - Scotland
TA  - Nurse Educ Today
JT  - Nurse education today
JID - 8511379
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Education, Nursing
MH  - Curriculum
MH  - Delivery of Health Care
MH  - Learning
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Authorship
OT  - Health knowledge, attitudes, practice
OT  - Nursing education research
OT  - Nursing informatics
OT  - Nursing students
OT  - Social responsibility
COIS- Declaration of competing interest The authors declare that they have no 
      conflicting interests.
EDAT- 2023/07/30 01:06
MHDA- 2023/08/14 06:42
CRDT- 2023/07/29 18:06
PHST- 2023/03/03 00:00 [received]
PHST- 2023/06/06 00:00 [revised]
PHST- 2023/07/17 00:00 [accepted]
PHST- 2023/08/14 06:42 [medline]
PHST- 2023/07/30 01:06 [pubmed]
PHST- 2023/07/29 18:06 [entrez]
AID - S0260-6917(23)00210-1 [pii]
AID - 10.1016/j.nedt.2023.105916 [doi]
PST - ppublish
SO  - Nurse Educ Today. 2023 Oct;129:105916. doi: 10.1016/j.nedt.2023.105916. Epub 2023 
      Jul 18.

PMID- 38510401
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240322
IS  - 2156-8650 (Electronic)
IS  - 2156-8650 (Linking)
VI  - 34
IP  - 1
DP  - 2024 Feb
TI  - ChatGPT-4 Performance on USMLE Step 1 Style Questions and Its Implications for 
      Medical Education: A Comparative Study Across Systems and Disciplines.
PG  - 145-152
LID - 10.1007/s40670-023-01956-z [doi]
AB  - We assessed the performance of OpenAI's ChatGPT-4 on United States Medical 
      Licensing Exam STEP 1 style questions across the systems and disciplines 
      appearing on the examination. ChatGPT-4 answered 86% of the 1300 questions 
      accurately, exceeding the estimated passing score of 60% with no significant 
      differences in performance across clinical domains. Findings demonstrated an 
      improvement over earlier models as well as consistent performance in topics 
      ranging from complex biological processes to ethical considerations in patient 
      care. Its proficiency provides support for the use of artificial intelligence 
      (AI) as an interactive learning tool and furthermore raises questions about how 
      the technology can be used to educate students in the preclinical component of 
      their medical education. The authors provide an example and discuss how students 
      can leverage AI to receive real-time analogies and explanations tailored to their 
      desired level of education. An appropriate application of this technology 
      potentially enables enhancement of learning outcomes for medical students in the 
      preclinical component of their education.
CI  - © The Author(s) under exclusive licence to International Association of Medical 
      Science Educators 2023. Springer Nature or its licensor (e.g. a society or other 
      partner) holds exclusive rights to this article under a publishing agreement with 
      the author(s) or other rightsholder(s); author self-archiving of the accepted 
      manuscript version of this article is solely governed by the terms of such 
      publishing agreement and applicable law.
FAU - Garabet, Razmig
AU  - Garabet R
AUID- ORCID: 0000-0003-2095-1084
AD  - Drexel University College of Medicine, Philadelphia, PA USA. ROR: 
      https://ror.org/04bdffz58. GRID: grid.166341.7. ISNI: 0000 0001 2181 3113
FAU - Mackey, Brendan P
AU  - Mackey BP
AD  - Drexel University College of Medicine, Philadelphia, PA USA. ROR: 
      https://ror.org/04bdffz58. GRID: grid.166341.7. ISNI: 0000 0001 2181 3113
FAU - Cross, James
AU  - Cross J
AD  - Drexel University College of Medicine, Philadelphia, PA USA. ROR: 
      https://ror.org/04bdffz58. GRID: grid.166341.7. ISNI: 0000 0001 2181 3113
FAU - Weingarten, Michael
AU  - Weingarten M
AD  - Drexel University College of Medicine, Philadelphia, PA USA. ROR: 
      https://ror.org/04bdffz58. GRID: grid.166341.7. ISNI: 0000 0001 2181 3113
LA  - eng
PT  - Journal Article
DEP - 20231227
PL  - United States
TA  - Med Sci Educ
JT  - Medical science educator
JID - 101625548
PMC - PMC10948644
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Machine learning
OT  - Medical education
OT  - Step 1
OT  - USMLE
COIS- Conflict of InterestThe authors declare no competing interests.
EDAT- 2024/03/21 06:43
MHDA- 2024/03/21 06:44
PMCR- 2025/02/01
CRDT- 2024/03/21 04:09
PHST- 2023/12/01 00:00 [accepted]
PHST- 2025/02/01 00:00 [pmc-release]
PHST- 2024/03/21 06:44 [medline]
PHST- 2024/03/21 06:43 [pubmed]
PHST- 2024/03/21 04:09 [entrez]
AID - 1956 [pii]
AID - 10.1007/s40670-023-01956-z [doi]
PST - epublish
SO  - Med Sci Educ. 2023 Dec 27;34(1):145-152. doi: 10.1007/s40670-023-01956-z. 
      eCollection 2024 Feb.

PMID- 38319707
OWN - NLM
STAT- MEDLINE
DCOM- 20240207
LR  - 20240223
IS  - 2368-7959 (Electronic)
IS  - 2368-7959 (Linking)
VI  - 11
DP  - 2024 Feb 6
TI  - Capacity of Generative AI to Interpret Human Emotions From Visual and Textual 
      Data: Pilot Evaluation Study.
PG  - e54369
LID - 10.2196/54369 [doi]
LID - e54369
AB  - BACKGROUND: Mentalization, which is integral to human cognitive processes, 
      pertains to the interpretation of one's own and others' mental states, including 
      emotions, beliefs, and intentions. With the advent of artificial intelligence 
      (AI) and the prominence of large language models in mental health applications, 
      questions persist about their aptitude in emotional comprehension. The prior 
      iteration of the large language model from OpenAI, ChatGPT-3.5, demonstrated an 
      advanced capacity to interpret emotions from textual data, surpassing human 
      benchmarks. Given the introduction of ChatGPT-4, with its enhanced visual 
      processing capabilities, and considering Google Bard's existing visual 
      functionalities, a rigorous assessment of their proficiency in visual mentalizing 
      is warranted. OBJECTIVE: The aim of the research was to critically evaluate the 
      capabilities of ChatGPT-4 and Google Bard with regard to their competence in 
      discerning visual mentalizing indicators as contrasted with their textual-based 
      mentalizing abilities. METHODS: The Reading the Mind in the Eyes Test developed 
      by Baron-Cohen and colleagues was used to assess the models' proficiency in 
      interpreting visual emotional indicators. Simultaneously, the Levels of Emotional 
      Awareness Scale was used to evaluate the large language models' aptitude in 
      textual mentalizing. Collating data from both tests provided a holistic view of 
      the mentalizing capabilities of ChatGPT-4 and Bard. RESULTS: ChatGPT-4, 
      displaying a pronounced ability in emotion recognition, secured scores of 26 and 
      27 in 2 distinct evaluations, significantly deviating from a random response 
      paradigm (P&lt;.001). These scores align with established benchmarks from the 
      broader human demographic. Notably, ChatGPT-4 exhibited consistent responses, 
      with no discernible biases pertaining to the sex of the model or the nature of 
      the emotion. In contrast, Google Bard's performance aligned with random response 
      patterns, securing scores of 10 and 12 and rendering further detailed analysis 
      redundant. In the domain of textual analysis, both ChatGPT and Bard surpassed 
      established benchmarks from the general population, with their performances being 
      remarkably congruent. CONCLUSIONS: ChatGPT-4 proved its efficacy in the domain of 
      visual mentalizing, aligning closely with human performance standards. Although 
      both models displayed commendable acumen in textual emotion interpretation, 
      Bard's capabilities in visual emotion interpretation necessitate further scrutiny 
      and potential refinement. This study stresses the criticality of ethical AI 
      development for emotional recognition, highlighting the need for inclusive data, 
      collaboration with patients and mental health experts, and stringent governmental 
      oversight to ensure transparency and protect patient privacy.
CI  - ©Zohar Elyoseph, Elad Refoua, Kfir Asraf, Maya Lvovsky, Yoav Shimoni, Dorit 
      Hadar-Shoval. Originally published in JMIR Mental Health 
      (https://mental.jmir.org), 06.02.2024.
FAU - Elyoseph, Zohar
AU  - Elyoseph Z
AUID- ORCID: 0000-0002-5717-4074
AD  - Department of Educational Psychology, The Center for Psychobiological Research, 
      The Max Stern Yezreel Valley College, Emek Yezreel, Israel.
AD  - Imperial College London, London, United Kingdom.
FAU - Refoua, Elad
AU  - Refoua E
AUID- ORCID: 0000-0003-1471-3573
AD  - Department of Psychology, Bar-Ilan University, Ramat Gan, Israel.
FAU - Asraf, Kfir
AU  - Asraf K
AUID- ORCID: 0009-0001-6699-3055
AD  - Department of Psychology, The Max Stern Yezreel Valley College, Emek Yezreel, 
      Israel.
FAU - Lvovsky, Maya
AU  - Lvovsky M
AUID- ORCID: 0009-0009-1219-1717
AD  - Department of Psychology, The Max Stern Yezreel Valley College, Emek Yezreel, 
      Israel.
FAU - Shimoni, Yoav
AU  - Shimoni Y
AUID- ORCID: 0009-0007-2834-156X
AD  - Boston Children's Hospital, Boston, MA, United States.
FAU - Hadar-Shoval, Dorit
AU  - Hadar-Shoval D
AUID- ORCID: 0000-0002-1376-3096
AD  - Department of Psychology, The Max Stern Yezreel Valley College, Emek Yezreel, 
      Israel.
LA  - eng
PT  - Journal Article
DEP - 20240206
PL  - Canada
TA  - JMIR Ment Health
JT  - JMIR mental health
JID - 101658926
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Pilot Projects
MH  - *Emotions
MH  - Benchmarking
MH  - Eye
PMC - PMC10879976
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - LLM
OT  - LLMs
OT  - RMET
OT  - Reading the Mind in the Eyes Test
OT  - algorithm
OT  - algorithms
OT  - artificial intelligence
OT  - early detection
OT  - early warning
OT  - emotional awareness
OT  - emotional comprehension
OT  - emotional cue
OT  - emotional cues
OT  - empathy
OT  - large language model
OT  - large language models
OT  - machine learning
OT  - mental disease
OT  - mental diseases
OT  - mental health
OT  - mental illness
OT  - mental illnesses
OT  - mentalization
OT  - mentalizing
OT  - practical model
OT  - practical models
OT  - predictive analytics
OT  - predictive model
OT  - predictive models
OT  - predictive system
COIS- Conflicts of Interest: None declared.
EDAT- 2024/02/06 13:43
MHDA- 2024/02/07 06:42
PMCR- 2024/02/06
CRDT- 2024/02/06 11:54
PHST- 2023/11/07 00:00 [received]
PHST- 2023/12/25 00:00 [accepted]
PHST- 2023/12/09 00:00 [revised]
PHST- 2024/02/07 06:42 [medline]
PHST- 2024/02/06 13:43 [pubmed]
PHST- 2024/02/06 11:54 [entrez]
PHST- 2024/02/06 00:00 [pmc-release]
AID - v11i1e54369 [pii]
AID - 10.2196/54369 [doi]
PST - epublish
SO  - JMIR Ment Health. 2024 Feb 6;11:e54369. doi: 10.2196/54369.

PMID- 38446541
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240323
IS  - 2562-0959 (Electronic)
IS  - 2562-0959 (Linking)
VI  - 7
DP  - 2024 Mar 6
TI  - Potential Use of ChatGPT in Responding to Patient Questions and Creating Patient 
      Resources.
PG  - e48451
LID - 10.2196/48451 [doi]
LID - e48451
AB  - ChatGPT (OpenAI) is an artificial intelligence-based free natural language 
      processing model that generates complex responses to user-generated prompts. The 
      advent of this tool comes at a time when physician burnout is at an all-time 
      high, which is attributed at least in part to time spent outside of the patient 
      encounter within the electronic medical record (documenting the encounter, 
      responding to patient messages, etc). Although ChatGPT is not specifically 
      designed to provide medical information, it can generate preliminary responses to 
      patients' questions about their medical conditions and can precipitately create 
      educational patient resources, which do inevitably require rigorous editing and 
      fact-checking on the part of the health care provider to ensure accuracy. In this 
      way, this assistive technology has the potential to not only enhance a 
      physician's efficiency and work-life balance but also enrich the 
      patient-physician relationship and ultimately improve patient outcomes.
CI  - ©Kelly Reynolds, Trilokraj Tejasvi. Originally published in JMIR Dermatology 
      (http://derma.jmir.org), 06.03.2024.
FAU - Reynolds, Kelly
AU  - Reynolds K
AUID- ORCID: 0000-0002-9719-3720
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, United States.
FAU - Tejasvi, Trilokraj
AU  - Tejasvi T
AUID- ORCID: 0000-0002-8186-8711
AD  - Department of Dermatology, University of Michigan, Ann Arbor, MI, United States.
LA  - eng
PT  - Journal Article
DEP - 20240306
PL  - Canada
TA  - JMIR Dermatol
JT  - JMIR dermatology
JID - 101770607
PMC - PMC10955382
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - artificial intelligence
OT  - chatbot
OT  - chatbots
OT  - conversational agent
OT  - conversational agents
OT  - educational
OT  - educational resource
OT  - language model
OT  - language models
OT  - natural language processing
OT  - natural language processing software
OT  - patient education
OT  - patient handouts
OT  - patient resources
COIS- Conflicts of Interest: None declared.
EDAT- 2024/03/06 12:43
MHDA- 2024/03/06 12:44
PMCR- 2024/03/06
CRDT- 2024/03/06 11:54
PHST- 2023/04/24 00:00 [received]
PHST- 2024/02/22 00:00 [accepted]
PHST- 2023/08/11 00:00 [revised]
PHST- 2024/03/06 12:44 [medline]
PHST- 2024/03/06 12:43 [pubmed]
PHST- 2024/03/06 11:54 [entrez]
PHST- 2024/03/06 00:00 [pmc-release]
AID - v7i1e48451 [pii]
AID - 10.2196/48451 [doi]
PST - epublish
SO  - JMIR Dermatol. 2024 Mar 6;7:e48451. doi: 10.2196/48451.

PMID- 37573231
OWN - NLM
STAT- MEDLINE
DCOM- 20230829
LR  - 20230829
IS  - 1773-0597 (Electronic)
IS  - 0181-5512 (Linking)
VI  - 46
IP  - 7
DP  - 2023 Sep
TI  - Evaluating the potential of ChatGPT-4 in ophthalmology: The good, the bad and the 
      ugly.
PG  - 697-705
LID - S0181-5512(23)00341-8 [pii]
LID - 10.1016/j.jfo.2023.07.001 [doi]
AB  - There is growing interest nowadays for artificial intelligence (AI) in all 
      medical fields. Beyond the direct medical application of AI to medical data, 
      generative AI such as "pre-trained transformer" (GPT) could significantly change 
      the ophthalmology landscape, opening up new avenues for enhancing precision, 
      productivity, and patient outcomes. At present, ChatGPT-4 has been investigated 
      in various ways in ophthalmology for research, medical education, and support for 
      clinical decisions purposes. This article intends to demonstrate the application 
      of ChatGPT-4 within the field of ophthalmology by employing a 'mise en abime' 
      approach. While we explore its potential to enhance the future of ophthalmology 
      care, we will also carefully outline its current limitations and potential risks.
CI  - Copyright © 2023 Elsevier Masson SAS. All rights reserved.
FAU - Khanna, R K
AU  - Khanna RK
AD  - Service d'ophtalmologie, hôpital universitaire Bretonneau, Inserm 1253 iBrain, 
      Tours, France. Electronic address: raoul.khanna@univ-tours.fr.
FAU - Ducloyer, J-B
AU  - Ducloyer JB
AD  - Service d'ophtalmologie, Nantes université, CHU de Nantes, 44000 Nantes, France.
FAU - Hage, A
AU  - Hage A
AD  - Service d'ophtalmologie, CHNO 15-20, Paris, France.
FAU - Rezkallah, A
AU  - Rezkallah A
AD  - Service d'ophtalmologie, hôpital de la Croix Rousse, Lyon, France.
FAU - Durbant, E
AU  - Durbant E
AD  - Cabinet d'ophtalmologie, Paris, France.
FAU - Bigoteau, M
AU  - Bigoteau M
AD  - Service d'ophtalmologie, hôpital Jacques-Cœur, Bourges, France.
FAU - Mouchel, R
AU  - Mouchel R
AD  - Centre ophtalmologique Kléber, Clinique du parc, Lyon, France; Centre 
      ophtalmologique du Grand Lac, Aix-les-Bains, France.
FAU - Guillon-Rolf, R
AU  - Guillon-Rolf R
AD  - Service d'ophtalmologie, Fondation Adolphe-de-Rothschild, Paris, France.
FAU - Le, L
AU  - Le L
AD  - Cabinet d'ophtalmologie, Chartres, France.
FAU - Tahiri, R
AU  - Tahiri R
AD  - Service de chirurgie ambulatoire, centre hospitalier d'Avranches-Granville, 849, 
      rue des Menneries, 50400 Granville, France.
FAU - Chammas, J
AU  - Chammas J
AD  - Service d'ophtalmologie, CHU Robert-Debré, Reims, France; Centre ophtalmologique 
      du Rhin, Strasbourg, France.
FAU - Baudouin, C
AU  - Baudouin C
AD  - Service d'ophtalmologie, CHNO 15-20, Sorbonne Université, Inserm, CNRS, Institut 
      de la Vision, IHU FOReSIGHT, Paris, France.
LA  - eng
PT  - Journal Article
DEP - 20230810
PL  - France
TA  - J Fr Ophtalmol
JT  - Journal francais d'ophtalmologie
JID - 7804128
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Ophthalmology
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - ChatGPT-4
OT  - Chatbot
OT  - Intelligence artificielle
OT  - Ophtalmologie
OT  - Ophthalmology
EDAT- 2023/08/13 00:43
MHDA- 2023/08/29 12:45
CRDT- 2023/08/12 22:06
PHST- 2023/06/12 00:00 [received]
PHST- 2023/07/10 00:00 [revised]
PHST- 2023/07/18 00:00 [accepted]
PHST- 2023/08/29 12:45 [medline]
PHST- 2023/08/13 00:43 [pubmed]
PHST- 2023/08/12 22:06 [entrez]
AID - S0181-5512(23)00341-8 [pii]
AID - 10.1016/j.jfo.2023.07.001 [doi]
PST - ppublish
SO  - J Fr Ophtalmol. 2023 Sep;46(7):697-705. doi: 10.1016/j.jfo.2023.07.001. Epub 2023 
      Aug 10.

PMID- 38044929
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231205
IS  - 0019-5049 (Print)
IS  - 0976-2817 (Electronic)
IS  - 0019-5049 (Linking)
VI  - 67
IP  - 10
DP  - 2023 Oct
TI  - Role of chat-generative pre-trained transformer (ChatGPT) in anaesthesia: Merits 
      and pitfalls.
PG  - 942-944
LID - 10.4103/ija.ija_504_23 [doi]
FAU - Reddy, Ashwini
AU  - Reddy A
AD  - Department of Anaesthesia and Intensive Care, Postgraduate Institute of Medical 
      Education and Research, Chandigarh, India.
FAU - Patel, Swati
AU  - Patel S
AD  - Department of Anaesthesia and Intensive Care, Postgraduate Institute of Medical 
      Education and Research, Chandigarh, India.
FAU - Barik, Amiya Kumar
AU  - Barik AK
AD  - Department of Anaesthesia and Intensive Care, Postgraduate Institute of Medical 
      Education and Research, Chandigarh, India.
FAU - Gowda, Punith
AU  - Gowda P
AD  - Department of Anaesthesia and Intensive Care, Postgraduate Institute of Medical 
      Education and Research, Chandigarh, India.
LA  - eng
PT  - Journal Article
DEP - 20231016
PL  - India
TA  - Indian J Anaesth
JT  - Indian journal of anaesthesia
JID - 0013243
PMC - PMC10691596
COIS- There are no conflicts of interest.
EDAT- 2023/12/04 06:42
MHDA- 2023/12/04 06:43
PMCR- 2023/10/01
CRDT- 2023/12/04 04:24
PHST- 2023/05/30 00:00 [received]
PHST- 2023/06/15 00:00 [revised]
PHST- 2023/06/18 00:00 [accepted]
PHST- 2023/12/04 06:43 [medline]
PHST- 2023/12/04 06:42 [pubmed]
PHST- 2023/12/04 04:24 [entrez]
PHST- 2023/10/01 00:00 [pmc-release]
AID - IJA-67-942 [pii]
AID - 10.4103/ija.ija_504_23 [doi]
PST - ppublish
SO  - Indian J Anaesth. 2023 Oct;67(10):942-944. doi: 10.4103/ija.ija_504_23. Epub 2023 
      Oct 16.

PMID- 38196696
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240111
IS  - 2223-4691 (Print)
IS  - 2223-4691 (Electronic)
IS  - 2223-4683 (Linking)
VI  - 12
IP  - 12
DP  - 2023 Dec 31
TI  - The computer will see you now: ChatGPT and artificial intelligence large language 
      models for health information in urology-an invited perspective.
PG  - 1772-1774
LID - 10.21037/tau-23-491 [doi]
FAU - Gabriel, Joseph
AU  - Gabriel J
AD  - Department of Urology, University Hospitals Sussex NHS Foundation Trust, Princess 
      Royal Hospital, Haywards Heath, UK.
FAU - Shafik, Lidia
AU  - Shafik L
AD  - Department of Medicine, Frimley Health NHS Foundation Trust, Frimley, UK.
FAU - Vincent, Elizabeth
AU  - Vincent E
AD  - Department of Urology, University Hospitals Sussex NHS Foundation Trust, Princess 
      Royal Hospital, Haywards Heath, UK.
FAU - Ajzajian, Jirayr
AU  - Ajzajian J
AD  - Department of Urology, University Hospitals Sussex NHS Foundation Trust, Princess 
      Royal Hospital, Haywards Heath, UK.
FAU - Alanbuki, Ammar
AU  - Alanbuki A
AD  - Department of Urology, University Hospitals Sussex NHS Foundation Trust, Princess 
      Royal Hospital, Haywards Heath, UK.
FAU - Larner, Timothy R G
AU  - Larner TRG
AD  - Department of Urology, University Hospitals Sussex NHS Foundation Trust, Princess 
      Royal Hospital, Haywards Heath, UK.
LA  - eng
PT  - Comment
PT  - Editorial
DEP - 20231207
PL  - China
TA  - Transl Androl Urol
JT  - Translational andrology and urology
JID - 101581119
CON - J Urol. 2023 Oct;210(4):688-694. PMID: 37428117
PMC - PMC10772652
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence (AI)
OT  - health information
OT  - large language model (LLM)
OT  - patient information
COIS- Conflicts of Interest: All authors have completed the ICMJE uniform disclosure 
      form (available at 
      https://tau.amegroups.com/article/view/10.21037/tau-23-491/coif). The authors 
      have no conflicts of interest to declare.
EDAT- 2024/01/10 06:41
MHDA- 2024/01/10 06:42
PMCR- 2023/12/31
CRDT- 2024/01/10 03:40
PHST- 2023/09/27 00:00 [received]
PHST- 2023/11/16 00:00 [accepted]
PHST- 2024/01/10 06:42 [medline]
PHST- 2024/01/10 06:41 [pubmed]
PHST- 2024/01/10 03:40 [entrez]
PHST- 2023/12/31 00:00 [pmc-release]
AID - tau-12-12-1772 [pii]
AID - 10.21037/tau-23-491 [doi]
PST - ppublish
SO  - Transl Androl Urol. 2023 Dec 31;12(12):1772-1774. doi: 10.21037/tau-23-491. Epub 
      2023 Dec 7.

PMID- 37604703
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20240115
LR  - 20240115
IS  - 1471-0528 (Electronic)
IS  - 1470-0328 (Linking)
VI  - 131
IP  - 3
DP  - 2024 Feb
TI  - Performance of ChatGPT in medical examinations: A systematic review and a 
      meta-analysis.
PG  - 378-380
LID - 10.1111/1471-0528.17641 [doi]
FAU - Levin, Gabriel
AU  - Levin G
AUID- ORCID: 0000-0003-1282-5379
AD  - Lady Davis Institute for Cancer Research, Jewish General Hospital, McGill 
      University, Quebec, Quebec City, Canada.
AD  - Faculty of Medicine, Department of Gynecologic Oncology, Hadassah Medical Center, 
      Hebrew University Jerusalem, Jerusalem, Israel.
FAU - Horesh, Nir
AU  - Horesh N
AD  - Ellen Leifer Shulman and Steven Shulman Digestive Disease Center, Cleveland 
      Clinic Florida, Florida, Weston, USA.
FAU - Brezinov, Yoav
AU  - Brezinov Y
AD  - Lady Davis Institute for Cancer Research, Jewish General Hospital, McGill 
      University, Quebec, Quebec City, Canada.
FAU - Meyer, Raanan
AU  - Meyer R
AD  - Division of Minimally Invasive Gynecologic Surgery, Department of Obstetrics and 
      Gynecology, Cedars Sinai Medical Center, California, Los Angeles, USA.
LA  - eng
PT  - Journal Article
DEP - 20230821
PL  - England
TA  - BJOG
JT  - BJOG : an international journal of obstetrics and gynaecology
JID - 100935741
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - education
OT  - examination
OT  - meta analysis
EDAT- 2023/08/22 00:42
MHDA- 2023/08/22 00:43
CRDT- 2023/08/21 21:53
PHST- 2023/08/04 00:00 [revised]
PHST- 2023/06/14 00:00 [received]
PHST- 2023/08/05 00:00 [accepted]
PHST- 2023/08/22 00:43 [medline]
PHST- 2023/08/22 00:42 [pubmed]
PHST- 2023/08/21 21:53 [entrez]
AID - 10.1111/1471-0528.17641 [doi]
PST - ppublish
SO  - BJOG. 2024 Feb;131(3):378-380. doi: 10.1111/1471-0528.17641. Epub 2023 Aug 21.

PMID- 37500572
OWN - NLM
STAT- MEDLINE
DCOM- 20230731
LR  - 20230827
IS  - 2005-8330 (Electronic)
IS  - 1229-6929 (Print)
IS  - 1229-6929 (Linking)
VI  - 24
IP  - 8
DP  - 2023 Aug
TI  - Use of Generative Artificial Intelligence, Including Large Language Models Such 
      as ChatGPT, in Scientific Publications: Policies of KJR and Prominent 
      Authorities.
PG  - 715-718
LID - 10.3348/kjr.2023.0643 [doi]
FAU - Park, Seong Ho
AU  - Park SH
AUID- ORCID: 0000-0002-1257-8315
AD  - Department of Radiology and Research Institute of Radiology, Asan Medical Center, 
      University of Ulsan College of Medicine, Seoul, Republic of Korea. 
      parksh.radiology@gmail.com.
LA  - eng
PT  - Editorial
PL  - Korea (South)
TA  - Korean J Radiol
JT  - Korean journal of radiology
JID - 100956096
SB  - IM
CIN - Korean J Radiol. 2023 Sep;24(9):924-925. PMID: 37634646
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Policy
MH  - Publications
PMC - PMC10400373
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Editing
OT  - Generative
OT  - Large language model
OT  - Peer review
OT  - Policy
OT  - Publication
OT  - Writing
COIS- The author has no potential conflicts of interest to disclose.
EDAT- 2023/07/28 01:08
MHDA- 2023/07/31 06:42
PMCR- 2023/08/01
CRDT- 2023/07/27 22:15
PHST- 2023/07/10 00:00 [received]
PHST- 2023/07/10 00:00 [accepted]
PHST- 2023/07/31 06:42 [medline]
PHST- 2023/07/28 01:08 [pubmed]
PHST- 2023/07/27 22:15 [entrez]
PHST- 2023/08/01 00:00 [pmc-release]
AID - 24.715 [pii]
AID - 10.3348/kjr.2023.0643 [doi]
PST - ppublish
SO  - Korean J Radiol. 2023 Aug;24(8):715-718. doi: 10.3348/kjr.2023.0643.

PMID- 37024324
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230605
LR  - 20230605
IS  - 1532-8171 (Electronic)
IS  - 0735-6757 (Linking)
VI  - 69
DP  - 2023 Jul
TI  - ChatGPT: The next-gen tool for triaging?
PG  - 215-217
LID - S0735-6757(23)00142-0 [pii]
LID - 10.1016/j.ajem.2023.03.027 [doi]
FAU - Bhattaram, Suhrith
AU  - Bhattaram S
AD  - Emergency Medicine, Senior Resident, Emergency Medicine, Dr D Y Patil Medical 
      College, Hospital &amp; Research Centre, Dr D Y Patil Vidyapeeth, Pune, India. 
      Electronic address: bsk1694@gmail.com.
FAU - Shinde, Varsha S
AU  - Shinde VS
AD  - Anaesthesia, Professor, Emergency Medicine, Dr D Y Patil Medical College, 
      Hospital &amp; Research Centre, Dr D Y Patil Vidyapeeth, Pune, India. Electronic 
      address: varsha.shinde@dpu.edu.in.
FAU - Khumujam, Princy Panthoi
AU  - Khumujam PP
AD  - Obstetrics and Gynaecology, Senior Resident, Department of Obstetrics and 
      Gynecology, North Eastern Indira Gandhi Regional Institute of Health &amp; Medical 
      Sciences, Mawdiangdiang, Meghalaya, India.
LA  - eng
PT  - Letter
DEP - 20230321
PL  - United States
TA  - Am J Emerg Med
JT  - The American journal of emergency medicine
JID - 8309942
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Emergency medicine
OT  - Triage
COIS- Declaration of Competing Interest None.
EDAT- 2023/04/07 06:00
MHDA- 2023/04/07 06:01
CRDT- 2023/04/06 22:00
PHST- 2023/02/28 00:00 [received]
PHST- 2023/03/09 00:00 [revised]
PHST- 2023/03/16 00:00 [accepted]
PHST- 2023/04/07 06:01 [medline]
PHST- 2023/04/07 06:00 [pubmed]
PHST- 2023/04/06 22:00 [entrez]
AID - S0735-6757(23)00142-0 [pii]
AID - 10.1016/j.ajem.2023.03.027 [doi]
PST - ppublish
SO  - Am J Emerg Med. 2023 Jul;69:215-217. doi: 10.1016/j.ajem.2023.03.027. Epub 2023 
      Mar 21.

PMID- 36808255
OWN - NLM
STAT- MEDLINE
DCOM- 20230316
LR  - 20230928
IS  - 2515-5091 (Electronic)
IS  - 2515-5091 (Linking)
VI  - 7
IP  - 2
DP  - 2023 Mar 1
TI  - Artificial intelligence chatbots will revolutionize how cancer patients access 
      information: ChatGPT represents a paradigm-shift.
LID - 10.1093/jncics/pkad010 [doi]
LID - pkad010
AB  - On November 30, 2022, OpenAI enabled public access to ChatGPT, a next-generation 
      artificial intelligence with a highly sophisticated ability to write, solve 
      coding issues, and answer questions. This communication draws attention to the 
      prospect that ChatGPT and its successors will become important virtual assistants 
      to patients and health-care providers. In our assessments, ranging from answering 
      basic fact-based questions to responding to complex clinical questions, ChatGPT 
      demonstrated a remarkable ability to formulate interpretable responses, which 
      appeared to minimize the likelihood of alarm compared with Google's feature 
      snippet. Arguably, the ChatGPT use case presents an urgent need for regulators 
      and health-care professionals to be involved in developing standards for minimum 
      quality and to raise patient awareness of current limitations of emerging 
      artificial intelligence assistants. This commentary aims to raise awareness at 
      the tipping point of a paradigm shift.
CI  - © The Author(s) 2023. Published by Oxford University Press.
FAU - Hopkins, Ashley M
AU  - Hopkins AM
AUID- ORCID: 0000-0001-7652-4378
AD  - College of Medicine and Public Health, Flinders University, Adelaide, South 
      Australia, Australia.
FAU - Logan, Jessica M
AU  - Logan JM
AUID- ORCID: 0000-0003-4046-6908
AD  - Clinical and Health Sciences, University of South Australia, Adelaide, South 
      Australia, Australia.
FAU - Kichenadasse, Ganessan
AU  - Kichenadasse G
AUID- ORCID: 0000-0001-9923-5149
AD  - College of Medicine and Public Health, Flinders University, Adelaide, South 
      Australia, Australia.
AD  - Flinders Centre for Innovation in Cancer, Department of Medical Oncology, 
      Flinders Medical Centre, Adelaide, South Australia, Australia.
FAU - Sorich, Michael J
AU  - Sorich MJ
AD  - College of Medicine and Public Health, Flinders University, Adelaide, South 
      Australia, Australia.
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PL  - England
TA  - JNCI Cancer Spectr
JT  - JNCI cancer spectrum
JID - 101721827
SB  - IM
CIN - JNCI Cancer Spectr. 2023 Aug 31;7(5):. PMID: 37656938
MH  - Humans
MH  - *Artificial Intelligence
MH  - Communication
MH  - Health Personnel
MH  - Probability
MH  - *Neoplasms/therapy
PMC - PMC10013638
EDAT- 2023/02/23 06:00
MHDA- 2023/03/17 06:00
PMCR- 2023/02/21
CRDT- 2023/02/22 12:43
PHST- 2023/01/10 00:00 [received]
PHST- 2023/02/06 00:00 [revised]
PHST- 2023/02/13 00:00 [accepted]
PHST- 2023/02/23 06:00 [pubmed]
PHST- 2023/03/17 06:00 [medline]
PHST- 2023/02/22 12:43 [entrez]
PHST- 2023/02/21 00:00 [pmc-release]
AID - 7049531 [pii]
AID - pkad010 [pii]
AID - 10.1093/jncics/pkad010 [doi]
PST - ppublish
SO  - JNCI Cancer Spectr. 2023 Mar 1;7(2):pkad010. doi: 10.1093/jncics/pkad010.

PMID- 38523987
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240326
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 16
IP  - 2
DP  - 2024 Feb
TI  - Ethical Concerns About ChatGPT in Healthcare: A Useful Tool or the Tombstone of 
      Original and Reflective Thinking?
PG  - e54759
LID - 10.7759/cureus.54759 [doi]
LID - e54759
AB  - Artificial intelligence (AI), the uprising technology of computer science aiming 
      to create digital systems with human behavior and intelligence, seems to have 
      invaded almost every field of modern life. Launched in November 2022, ChatGPT 
      (Chat Generative Pre-trained Transformer) is a textual AI application capable of 
      creating human-like responses characterized by original language and high 
      coherence. Although AI-based language models have demonstrated impressive 
      capabilities in healthcare, ChatGPT has received controversial annotations from 
      the scientific and academic communities. This chatbot already appears to have a 
      massive impact as an educational tool for healthcare professionals and 
      transformative potential for clinical practice&nbsp;and could lead to dramatic changes 
      in scientific research. Nevertheless, rational concerns were raised regarding 
      whether the pre-trained, AI-generated text would be a menace not only for 
      original thinking and new scientific ideas but also for academic and research 
      integrity, as it gets more and more difficult to distinguish its AI origin due to 
      the coherence and fluency of the produced text. This short review aims to 
      summarize the potential applications and the consequential implications of 
      ChatGPT in the&nbsp;three critical pillars of medicine: education, research, and 
      clinical practice. In addition, this paper discusses whether the current use of 
      this chatbot is in compliance with the ethical principles for the safe use of AI 
      in healthcare, as determined by the World Health Organization. Finally, this 
      review highlights the need for an updated ethical framework and the increased 
      vigilance of healthcare stakeholders to harvest the potential benefits and limit 
      the imminent dangers of this new innovative technology.
CI  - Copyright © 2024, Kapsali et al.
FAU - Kapsali, Marina Z
AU  - Kapsali MZ
AD  - Postgraduate Program on Bioethics, Laboratory of Bioethics, Democritus University 
      of Thrace, Alexandroupolis, GRC.
FAU - Livanis, Efstratios
AU  - Livanis E
AD  - Department of Accounting and Finance, University of Macedonia, Thessaloniki, GRC.
FAU - Tsalikidis, Christos
AU  - Tsalikidis C
AD  - Department of General Surgery, Democritus University of Thrace, Alexandroupolis, 
      GRC.
FAU - Oikonomou, Panagoula
AU  - Oikonomou P
AD  - Laboratory of Experimental Surgery, Department of General Surgery, Democritus 
      University of Thrace, Alexandroupolis, GRC.
FAU - Voultsos, Polychronis
AU  - Voultsos P
AD  - Laboratory of Forensic Medicine &amp; Toxicology (Medical Law and Ethics), School of 
      Medicine, Faculty of Health Sciences, Aristotle University of Thessaloniki, 
      Thessaloniki, GRC.
FAU - Tsaroucha, Aleka
AU  - Tsaroucha A
AD  - Department of General Surgery, Democritus University of Thrace, Alexandroupolis, 
      GRC.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20240223
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10961144
OTO - NOTNLM
OT  - artificial intelligence
OT  - chatgpt
OT  - education
OT  - ethical framework
OT  - healthcare research
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/03/25 06:44
MHDA- 2024/03/25 06:45
PMCR- 2024/02/23
CRDT- 2024/03/25 04:21
PHST- 2024/02/23 00:00 [accepted]
PHST- 2024/03/25 06:45 [medline]
PHST- 2024/03/25 06:44 [pubmed]
PHST- 2024/03/25 04:21 [entrez]
PHST- 2024/02/23 00:00 [pmc-release]
AID - 10.7759/cureus.54759 [doi]
PST - epublish
SO  - Cureus. 2024 Feb 23;16(2):e54759. doi: 10.7759/cureus.54759. eCollection 2024 
      Feb.

PMID- 38452222
OWN - NLM
STAT- MEDLINE
DCOM- 20240311
LR  - 20240311
IS  - 1460-2210 (Electronic)
IS  - 0141-5387 (Linking)
VI  - 46
IP  - 2
DP  - 2024 Apr 1
TI  - Enhancing systematic reviews in orthodontics: a comparative examination of 
      GPT-3.5 and GPT-4 for generating PICO-based queries with tailored prompts and 
      configurations.
LID - cjae011 [pii]
LID - 10.1093/ejo/cjae011 [doi]
AB  - OBJECTIVES: The rapid advancement of Large Language Models (LLMs) has prompted an 
      exploration of their efficacy in generating PICO-based (Patient, Intervention, 
      Comparison, Outcome) queries, especially in the field of orthodontics. This study 
      aimed to assess the usability of Large Language Models (LLMs), in aiding 
      systematic review processes, with a specific focus on comparing the performance 
      of ChatGPT 3.5 and ChatGPT 4 using a specialized prompt tailored for 
      orthodontics. MATERIALS/METHODS: Five databases were perused to curate a sample 
      of 77 systematic reviews and meta-analyses published between 2016 and 2021. 
      Utilizing prompt engineering techniques, the LLMs were directed to formulate PICO 
      questions, Boolean queries, and relevant keywords. The outputs were subsequently 
      evaluated for accuracy and consistency by independent researchers using 
      three-point and six-point Likert scales. Furthermore, the PICO records of 41 
      studies, which were compatible with the PROSPERO records, were compared with the 
      responses provided by the models. RESULTS: ChatGPT 3.5 and 4 showcased a 
      consistent ability to craft PICO-based queries. Statistically significant 
      differences in accuracy were observed in specific categories, with GPT-4 often 
      outperforming GPT-3.5. LIMITATIONS: The study's test set might not encapsulate 
      the full range of LLM application scenarios. Emphasis on specific question types 
      may also not reflect the complete capabilities of the models. 
      CONCLUSIONS/IMPLICATIONS: Both ChatGPT 3.5 and 4 can be pivotal tools for 
      generating PICO-driven queries in orthodontics when optimally configured. 
      However, the precision required in medical research necessitates a judicious and 
      critical evaluation of LLM-generated outputs, advocating for a circumspect 
      integration into scientific investigations.
CI  - © The Author(s) 2024. Published by Oxford University Press on behalf of the 
      European Orthodontic Society. All rights reserved. For permissions, please email: 
      journals.permissions@oup.com.
FAU - Demir, Gizem Boztaş
AU  - Demir GB
AD  - Department of Orthodontics, Gulhane Faculty of Dentistry, University of Health 
      Sciences, Ankara, Türkiye.
FAU - Süküt, Yağızalp
AU  - Süküt Y
AD  - Department of Orthodontics, Gulhane Faculty of Dentistry, University of Health 
      Sciences, Ankara, Türkiye.
FAU - Duran, Gökhan Serhat
AU  - Duran GS
AD  - Department of Orthodontics, Gulhane Faculty of Dentistry, University of Health 
      Sciences, Ankara, Türkiye.
FAU - Topsakal, Kübra Gülnur
AU  - Topsakal KG
AD  - Department of Orthodontics, Gulhane Faculty of Dentistry, University of Health 
      Sciences, Ankara, Türkiye.
FAU - Görgülü, Serkan
AU  - Görgülü S
AD  - Department of Orthodontics, Gulhane Faculty of Dentistry, University of Health 
      Sciences, Ankara, Türkiye.
LA  - eng
PT  - Journal Article
PL  - England
TA  - Eur J Orthod
JT  - European journal of orthodontics
JID - 7909010
SB  - IM
MH  - Humans
MH  - *Dental Care
OTO - NOTNLM
OT  - ChatGPT
OT  - PICO-based queries
OT  - artificial intelligence (AI)
OT  - database search
OT  - prompt engineering
EDAT- 2024/03/07 18:42
MHDA- 2024/03/11 06:43
CRDT- 2024/03/07 15:13
PHST- 2024/03/11 06:43 [medline]
PHST- 2024/03/07 18:42 [pubmed]
PHST- 2024/03/07 15:13 [entrez]
AID - 7624046 [pii]
AID - 10.1093/ejo/cjae011 [doi]
PST - ppublish
SO  - Eur J Orthod. 2024 Apr 1;46(2):cjae011. doi: 10.1093/ejo/cjae011.

PMID- 38162955
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240103
IS  - 2001-0370 (Print)
IS  - 2001-0370 (Electronic)
IS  - 2001-0370 (Linking)
VI  - 24
DP  - 2024 Dec
TI  - Beyond the Scalpel: Assessing ChatGPT's potential as an auxiliary intelligent 
      virtual assistant in oral surgery.
PG  - 46-52
LID - 10.1016/j.csbj.2023.11.058 [doi]
AB  - AI has revolutionized the way we interact with technology. Noteworthy advances in 
      AI algorithms and large language models (LLM) have led to the development of 
      natural generative language (NGL) systems such as ChatGPT. Although these LLM can 
      simulate human conversations and generate content in real time, they face 
      challenges related to the topicality and accuracy of the information they 
      generate. This study aimed to assess whether ChatGPT-4 could provide accurate and 
      reliable answers to general dentists in the field of oral surgery, and thus 
      explore its potential as an intelligent virtual assistant in clinical decision 
      making in oral surgery. Thirty questions related to oral surgery were posed to 
      ChatGPT4, each question repeated 30 times. Subsequently, a total of 900 responses 
      were obtained. Two surgeons graded the answers according to the guidelines of the 
      Spanish Society of Oral Surgery, using a three-point Likert scale (correct, 
      partially correct/incomplete, and incorrect). Disagreements were arbitrated by an 
      experienced oral surgeon, who provided the final grade Accuracy was found to be 
      71.7%, and consistency of the experts' grading across iterations, ranged from 
      moderate to almost perfect. ChatGPT-4, with its potential capabilities, will 
      inevitably be integrated into dental disciplines, including oral surgery. In the 
      future, it could be considered as an auxiliary intelligent virtual assistant, 
      though it would never replace oral surgery experts. Proper training and verified 
      information by experts will remain vital to the implementation of the technology. 
      More comprehensive research is needed to ensure the safe and successful 
      application of AI in oral surgery.
CI  - © 2023 The Authors.
FAU - Suárez, Ana
AU  - Suárez A
AD  - Department of Pre-Clinic Dentistry, Faculty of Biomedical and Health Sciences, 
      Universidad Europea de Madrid, Calle Tajo s/n, Villaviciosa de Odón, 28670 
      Madrid, Spain.
FAU - Jiménez, Jaime
AU  - Jiménez J
AD  - Department of Clinic Dentistry, Faculty of Biomedical and Health Sciences, 
      Universidad Europea de Madrid, Calle Tajo s/n, Villaviciosa de Odón, 28670 
      Madrid, Spain.
FAU - Llorente de Pedro, María
AU  - Llorente de Pedro M
AD  - Department of Pre-Clinic Dentistry, Faculty of Biomedical and Health Sciences, 
      Universidad Europea de Madrid, Calle Tajo s/n, Villaviciosa de Odón, 28670 
      Madrid, Spain.
FAU - Andreu-Vázquez, Cristina
AU  - Andreu-Vázquez C
AD  - Department of Veterinary Medicine, Faculty of Biomedical and Health Sciences, 
      Universidad Europea de Madrid, Calle Tajo s/n, Villaviciosa de Odón, 28670 
      Madrid, Spain.
FAU - Díaz-Flores García, Víctor
AU  - Díaz-Flores García V
AD  - Department of Pre-Clinic Dentistry, Faculty of Biomedical and Health Sciences, 
      Universidad Europea de Madrid, Calle Tajo s/n, Villaviciosa de Odón, 28670 
      Madrid, Spain.
FAU - Gómez Sánchez, Margarita
AU  - Gómez Sánchez M
AD  - Department of Pre-Clinic Dentistry, Faculty of Biomedical and Health Sciences, 
      Universidad Europea de Madrid, Calle Tajo s/n, Villaviciosa de Odón, 28670 
      Madrid, Spain.
FAU - Freire, Yolanda
AU  - Freire Y
AD  - Department of Pre-Clinic Dentistry, Faculty of Biomedical and Health Sciences, 
      Universidad Europea de Madrid, Calle Tajo s/n, Villaviciosa de Odón, 28670 
      Madrid, Spain.
LA  - eng
PT  - Journal Article
DEP - 20231206
PL  - Netherlands
TA  - Comput Struct Biotechnol J
JT  - Computational and structural biotechnology journal
JID - 101585369
PMC - PMC10755495
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - ChatGPT
OT  - Chatbot
OT  - Dentistry
OT  - Large language models
OT  - Natural generative language
OT  - Open AI
OT  - Oral surgery
COIS- The authors declare no competing interests.
EDAT- 2024/01/02 11:42
MHDA- 2024/01/02 11:43
PMCR- 2023/12/06
CRDT- 2024/01/01 04:32
PHST- 2023/11/03 00:00 [received]
PHST- 2023/11/28 00:00 [revised]
PHST- 2023/11/28 00:00 [accepted]
PHST- 2024/01/02 11:43 [medline]
PHST- 2024/01/02 11:42 [pubmed]
PHST- 2024/01/01 04:32 [entrez]
PHST- 2023/12/06 00:00 [pmc-release]
AID - S2001-0370(23)00473-7 [pii]
AID - 10.1016/j.csbj.2023.11.058 [doi]
PST - epublish
SO  - Comput Struct Biotechnol J. 2023 Dec 6;24:46-52. doi: 10.1016/j.csbj.2023.11.058. 
      eCollection 2024 Dec.

PMID- 38024047
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231201
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 11
DP  - 2023 Nov
TI  - The Future of Patient Education: AI-Driven Guide for Type 2 Diabetes.
PG  - e48919
LID - 10.7759/cureus.48919 [doi]
LID - e48919
AB  - Introduction and aim The surging incidence of type 2 diabetes has become a 
      growing concern for the healthcare sector. This chronic ailment, characterized by 
      its complex blend of genetic and lifestyle determinants, has witnessed a notable 
      increase in recent times, exerting substantial pressure on healthcare resources. 
      As more individuals turn to online platforms for health guidance and embrace the 
      utilization of Chat Generative Pre-trained Transformer (ChatGPT; San Francisco, 
      CA: OpenAI), a text-generating AI (TGAI), to get insights into their well-being, 
      evaluating its effectiveness and reliability becomes crucial.&nbsp;This research 
      primarily aimed to evaluate the correctness of TGAI responses to type 2 diabetes 
      (T2DM) inquiries via ChatGPT. Furthermore, this study aimed to examine the 
      consistency of TGAI in addressing common queries on T2DM complications for 
      patient education. Material and methods Questions on T2DM were formulated by 
      experienced physicians and screened by research personnel before querying 
      ChatGPT. Each question was posed thrice, and the collected answers were 
      summarized. Responses were then sorted into three distinct categories as follows: 
      (a) appropriate, (b) inappropriate, and (c) unreliable by two seasoned 
      physicians. In instances of differing opinions, a third physician was consulted 
      to achieve consensus. Results From the initial set of 110 T2DM questions, 40 were 
      dismissed by experts for relevance, resulting in a final count of 70. An 
      overwhelming 98.5% of the AI's answers were judged as appropriate, thus 
      underscoring its reliability over traditional online search engines. Nonetheless, 
      a 1.5% rate of inappropriate responses underlines the importance of ongoing AI 
      improvements and strict adherence to medical protocols. Conclusion TGAI provides 
      medical information of high quality and reliability. This study underscores 
      TGAI's impressive effectiveness in delivering reliable information about T2DM, 
      with 98.5% of responses aligning with the standard of care. These results hold 
      promise for integrating AI platforms as supplementary tools to enhance patient 
      education and outcomes.
CI  - Copyright © 2023, Hernandez et al.
FAU - Hernandez, Carlos A
AU  - Hernandez CA
AD  - Internal Medicine, Ross University School of Medicine, Bridgetown, BRB.
FAU - Vazquez Gonzalez, Andres E
AU  - Vazquez Gonzalez AE
AD  - Internal Medicine, Capital Health, Trenton, USA.
FAU - Polianovskaia, Anastasiia
AU  - Polianovskaia A
AD  - Internal Medicine, Capital Health, Trenton, USA.
FAU - Amoro Sanchez, Rafael
AU  - Amoro Sanchez R
AD  - Internal Medicine, Capital Health, Trenton, USA.
FAU - Muyolema Arce, Veronica
AU  - Muyolema Arce V
AD  - Internal Medicine, Capital Health, Trenton, USA.
FAU - Mustafa, Ahmed
AU  - Mustafa A
AD  - Internal Medicine, Capital Health, Trenton, USA.
FAU - Vypritskaya, Ekaterina
AU  - Vypritskaya E
AD  - Internal Medicine, Capital Health, Trenton, USA.
FAU - Perez Gutierrez, Oscar
AU  - Perez Gutierrez O
AD  - Internal Medicine, Capital Health, Trenton, USA.
FAU - Bashir, Muhammad
AU  - Bashir M
AD  - Internal Medicine, Capital Health, Trenton, USA.
FAU - Eighaei Sedeh, Ashkan
AU  - Eighaei Sedeh A
AD  - Internal Medicine, Capital Health, Trenton, USA.
LA  - eng
PT  - Journal Article
DEP - 20231116
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10654048
OTO - NOTNLM
OT  - artificial intelligence
OT  - chatgpt
OT  - diabetes
OT  - health education
OT  - openai
OT  - patient education
OT  - type 2 diabetes
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/11/29 18:42
MHDA- 2023/11/29 18:43
PMCR- 2023/11/16
CRDT- 2023/11/29 16:01
PHST- 2023/11/16 00:00 [accepted]
PHST- 2023/11/29 18:43 [medline]
PHST- 2023/11/29 18:42 [pubmed]
PHST- 2023/11/29 16:01 [entrez]
PHST- 2023/11/16 00:00 [pmc-release]
AID - 10.7759/cureus.48919 [doi]
PST - epublish
SO  - Cureus. 2023 Nov 16;15(11):e48919. doi: 10.7759/cureus.48919. eCollection 2023 
      Nov.

PMID- 37394309
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240327
IS  - 1878-4372 (Electronic)
IS  - 1360-1385 (Linking)
VI  - 29
IP  - 2
DP  - 2024 Feb
TI  - One hundred important questions facing plant science derived using a large 
      language model.
PG  - 210-218
LID - S1360-1385(23)00199-1 [pii]
LID - 10.1016/j.tplants.2023.06.008 [doi]
AB  - Artificial intelligence (AI) is advancing rapidly and continually evolving in 
      various fields. Recently, the release of ChatGPT has sparked significant public 
      interest. In this study, we revisit the '100 Important Questions Facing Plant 
      Science' by leveraging ChatGPT as a valuable tool for generating 
      thought-provoking questions relevant to plant science. These questions primarily 
      revolve around the utilization of plants in product development, understanding 
      plant mechanisms, plant-environment interactions, and enhancing plant traits, 
      with an emphasis on sustainable product development. While ChatGPT may not 
      capture certain crucial aspects highlighted by scientists, it offers valuable 
      insights into the questions generated by experts. Our analysis demonstrates that 
      ChatGPT can be cautiously employed as a supportive tool to facilitate, 
      streamline, and expedite specific tasks in plant science.
CI  - Copyright © 2023 Elsevier Ltd. All rights reserved.
FAU - Agathokleous, Evgenios
AU  - Agathokleous E
AD  - Key Laboratory of Ecosystem Carbon Source and Sink, China Meteorological 
      Administration (ECSS-CMA), School of Applied Meteorology, Nanjing University of 
      Information Science and Technology, Nanjing, 210044, China; Collaborative 
      Innovation Center on Forecast and Evaluation of Meteorological Disasters 
      (CIC-FEMD), Nanjing University of Information Science &amp; Technology, Nanjing, 
      China. Electronic address: evgenios@nuist.edu.cn.
FAU - Rillig, Matthias C
AU  - Rillig MC
AD  - Freie Universität Berlin, Institut für Biologie, Altensteinstr. 6, D-14195, 
      Berlin, Germany; Berlin-Brandenburg Institute of Advanced Biodiversity Research 
      (BBIB), D-14195, Berlin, Germany.
FAU - Peñuelas, Josep
AU  - Peñuelas J
AD  - CSIC, Global Ecology Unit CREAF-CSIC-UAB, Bellaterra, Catalonia 08193, Spain; 
      CREAF, Cerdanyola del Vallès, Catalonia 08193, Spain.
FAU - Yu, Zhen
AU  - Yu Z
AD  - Key Laboratory of Ecosystem Carbon Source and Sink, China Meteorological 
      Administration (ECSS-CMA), School of Applied Meteorology, Nanjing University of 
      Information Science and Technology, Nanjing, 210044, China; Collaborative 
      Innovation Center on Forecast and Evaluation of Meteorological Disasters 
      (CIC-FEMD), Nanjing University of Information Science &amp; Technology, Nanjing, 
      China. Electronic address: zyu@nuist.edu.cn.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20230630
PL  - England
TA  - Trends Plant Sci
JT  - Trends in plant science
JID - 9890299
SB  - IM
MH  - *Artificial Intelligence
MH  - *Language
MH  - Phenotype
OTO - NOTNLM
OT  - ChatGPT
OT  - Generative Pre-trained Transformer
OT  - artificial intelligence
OT  - biology
OT  - large language model
OT  - plant science
COIS- Declaration of interests No interests are declared.
EDAT- 2023/07/03 00:41
MHDA- 2024/02/10 14:48
CRDT- 2023/07/02 21:59
PHST- 2023/05/17 00:00 [received]
PHST- 2023/06/10 00:00 [revised]
PHST- 2023/06/12 00:00 [accepted]
PHST- 2024/02/10 14:48 [medline]
PHST- 2023/07/03 00:41 [pubmed]
PHST- 2023/07/02 21:59 [entrez]
AID - S1360-1385(23)00199-1 [pii]
AID - 10.1016/j.tplants.2023.06.008 [doi]
PST - ppublish
SO  - Trends Plant Sci. 2024 Feb;29(2):210-218. doi: 10.1016/j.tplants.2023.06.008. 
      Epub 2023 Jun 30.

PMID- 38559461
OWN - NLM
STAT- MEDLINE
DCOM- 20240403
LR  - 20240403
IS  - 1551-4056 (Electronic)
IS  - 0044-0086 (Print)
IS  - 0044-0086 (Linking)
VI  - 97
IP  - 1
DP  - 2024 Mar
TI  - Assessing the Efficacy of Large Language Models in Health Literacy: A 
      Comprehensive Cross-Sectional Study.
PG  - 17-27
LID - 10.59249/ZTOZ1966 [doi]
AB  - Enhanced health literacy in children has been empirically linked to better health 
      outcomes over the long term; however, few interventions have been shown to 
      improve health literacy. In this context, we investigate whether large language 
      models (LLMs) can serve as a medium to improve health literacy in children. We 
      tested pediatric conditions using 26 different prompts in ChatGPT-3.5, ChatGPT-4, 
      Microsoft Bing, and Google Bard (now known as Google Gemini). The primary outcome 
      measurement was the reading grade level (RGL) of output as assessed by Gunning 
      Fog, Flesch-Kincaid Grade Level, Automated Readability Index, and Coleman-Liau 
      indices. Word counts were also assessed. Across all models, output for basic 
      prompts such as "Explain" and "What is (are)," were at, or exceeded, the 
      tenth-grade RGL. When prompts were specified to explain conditions from the 
      first- to twelfth-grade level, we found that LLMs had varying abilities to tailor 
      responses based on grade level. ChatGPT-3.5 provided responses that ranged from 
      the seventh-grade to college freshmen RGL while ChatGPT-4 outputted responses 
      from the tenth-grade to the college senior RGL. Microsoft Bing provided responses 
      from the ninth- to eleventh-grade RGL while Google Bard provided responses from 
      the seventh- to tenth-grade RGL. LLMs face challenges in crafting outputs below a 
      sixth-grade RGL. However, their capability to modify outputs above this 
      threshold, provides a potential mechanism for adolescents to explore, understand, 
      and engage with information regarding their health conditions, spanning from 
      simple to complex terms. Future studies are needed to verify the accuracy and 
      efficacy of these tools.
CI  - Copyright ©2024, Yale Journal of Biology and Medicine.
FAU - Amin, Kanhai S
AU  - Amin KS
AD  - Yale College, New Haven, CT, USA.
FAU - Mayes, Linda C
AU  - Mayes LC
AD  - Yale Child Study Center, Yale School of Medicine, New Haven, CT, USA.
FAU - Khosla, Pavan
AU  - Khosla P
AD  - Yale School of Medicine, New Haven, CT, USA.
FAU - Doshi, Rushabh H
AU  - Doshi RH
AD  - Yale School of Medicine, New Haven, CT, USA.
LA  - eng
PT  - Journal Article
DEP - 20240329
PL  - United States
TA  - Yale J Biol Med
JT  - The Yale journal of biology and medicine
JID - 0417414
SB  - IM
MH  - Adolescent
MH  - Child
MH  - Humans
MH  - *Health Literacy
MH  - Cross-Sectional Studies
MH  - Comprehension
MH  - Reading
MH  - Language
PMC - PMC10964816
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - ChatGPT
OT  - Google Bard
OT  - Google Gemini
OT  - Health Literacy
OT  - Large Language Models
OT  - Microsoft Bing
OT  - Pediatrics
OT  - Reading Grade Level
EDAT- 2024/04/01 18:42
MHDA- 2024/04/03 06:44
PMCR- 2024/03/29
CRDT- 2024/04/01 17:14
PHST- 2024/04/03 06:44 [medline]
PHST- 2024/04/01 18:42 [pubmed]
PHST- 2024/04/01 17:14 [entrez]
PHST- 2024/03/29 00:00 [pmc-release]
AID - yjbm97117 [pii]
AID - 10.59249/ZTOZ1966 [doi]
PST - epublish
SO  - Yale J Biol Med. 2024 Mar 29;97(1):17-27. doi: 10.59249/ZTOZ1966. eCollection 
      2024 Mar.

PMID- 37356702
OWN - NLM
STAT- MEDLINE
DCOM- 20230717
LR  - 20231121
IS  - 1873-6246 (Electronic)
IS  - 0301-0511 (Linking)
VI  - 181
DP  - 2023 Jul
TI  - Editorial: Generative artificial intelligence as a plagiarism problem.
PG  - 108621
LID - S0301-0511(23)00138-2 [pii]
LID - 10.1016/j.biopsycho.2023.108621 [doi]
AB  - There is increasing concern and consternation about generative artificial 
      intelligence (AI) programs and its potential impact on academia. This editorial 
      addresses the potential impact of such programs on scientific publishing as it 
      relates to the journal Biological Psychology. Using chatGPT as an example, it 
      makes the case that a prime concern is its implications for facilitating 
      plagiarism. It briefly outlines what is known about the algorithm of the GPT text 
      model, and also the implications of its chatGPT front end, on being able to 
      establish appropriate credit for ideas in text that it outputs. It is concluded 
      that, at least for Biological Psychology, the expectation is that authors will be 
      transparent about AI usage, will declare when AI is the source of an idea, and 
      will redouble efforts to seek out and cite prior claims to ideas in the published 
      literature when AI is involved.
CI  - Copyright © 2023 Elsevier B.V. All rights reserved.
FAU - Dien, Joseph
AU  - Dien J
AD  - Department of Human Development and Quantitative Methodology, University of 
      Maryland, 3304 Benjamin Building, College Park, MD 20742, USA. Electronic 
      address: jdien07@mac.com.
LA  - eng
PT  - Editorial
DEP - 20230624
PL  - Netherlands
TA  - Biol Psychol
JT  - Biological psychology
JID - 0375566
SB  - IM
MH  - Humans
MH  - *Plagiarism
MH  - *Artificial Intelligence
MH  - Publishing
MH  - Algorithms
OTO - NOTNLM
OT  - Academic misconduct
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Large language models
OT  - Plagiarism
COIS- Declaration of Competing Interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2023/06/26 00:41
MHDA- 2023/07/17 06:42
CRDT- 2023/06/25 19:16
PHST- 2023/05/26 00:00 [received]
PHST- 2023/06/20 00:00 [revised]
PHST- 2023/06/22 00:00 [accepted]
PHST- 2023/07/17 06:42 [medline]
PHST- 2023/06/26 00:41 [pubmed]
PHST- 2023/06/25 19:16 [entrez]
AID - S0301-0511(23)00138-2 [pii]
AID - 10.1016/j.biopsycho.2023.108621 [doi]
PST - ppublish
SO  - Biol Psychol. 2023 Jul;181:108621. doi: 10.1016/j.biopsycho.2023.108621. Epub 
      2023 Jun 24.

PMID- 38375341
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240221
IS  - 2666-061X (Electronic)
IS  - 2666-061X (Linking)
VI  - 6
IP  - 2
DP  - 2024 Apr
TI  - ChatGPT Can Offer Satisfactory Responses to Common Patient Questions Regarding 
      Elbow Ulnar Collateral Ligament Reconstruction.
PG  - 100893
LID - 10.1016/j.asmr.2024.100893 [doi]
LID - 100893
AB  - PURPOSE: To determine whether ChatGPT effectively responds to 10 commonly asked 
      questions concerning ulnar collateral ligament (UCL) reconstruction. METHODS: A 
      comprehensive list of 90 UCL reconstruction questions was initially created, with 
      a final set of 10 "most commonly asked" questions ultimately selected. Questions 
      were presented to ChatGPT and its response was documented. Responses were 
      evaluated independently by 3 authors using an evidence-based methodology, 
      resulting in a grading system categorized as follows: (1) excellent response not 
      requiring clarification; (2) satisfactory requiring minimal clarification; (3) 
      satisfactory requiring moderate clarification; and (4) unsatisfactory requiring 
      substantial clarification. RESULTS: Six of 10 ten responses were rated as 
      "excellent" or "satisfactory." Of those 6 responses, 2 were determined to be 
      "excellent response not requiring clarification," 3 were "satisfactory requiring 
      minimal clarification," and 1 was "satisfactory requiring moderate 
      clarification." Four questions encompassing inquiries about "What are the 
      potential risks of UCL reconstruction surgery?" "Which type of graft should be 
      used for my UCL reconstruction?" and "Should I have UCL reconstruction or 
      repair?" were rated as "unsatisfactory requiring substantial clarification." 
      CONCLUSIONS: ChatGPT exhibited the potential to improve a patient's basic 
      understanding of UCL reconstruction and provided responses that were deemed 
      satisfactory to excellent for 60% of the most commonly asked questions. For the 
      other 40% of questions, ChatGPT gave unsatisfactory responses, primarily due to a 
      lack of relevant details or the need for further explanation. CLINICAL RELEVANCE: 
      ChatGPT can assist in patient education regarding UCL reconstruction; however, 
      its ability to appropriately answer more complex questions remains to be an area 
      of skepticism and future improvement.
CI  - © 2024 The Authors.
FAU - Johns, William L
AU  - Johns WL
AD  - Rothman Orthopaedic Institute at Thomas Jefferson University, Philadelphia, 
      Pennsylvania, U.S.A.
FAU - Kellish, Alec
AU  - Kellish A
AD  - Rothman Orthopaedic Institute at Thomas Jefferson University, Philadelphia, 
      Pennsylvania, U.S.A.
FAU - Farronato, Dominic
AU  - Farronato D
AD  - Sidney Kimmel Medical College at Thomas Jefferson University, Philadelphia, 
      Pennsylvania, U.S.A.
FAU - Ciccotti, Michael G
AU  - Ciccotti MG
AD  - Rothman Orthopaedic Institute at Thomas Jefferson University, Philadelphia, 
      Pennsylvania, U.S.A.
FAU - Hammoud, Sommer
AU  - Hammoud S
AD  - Rothman Orthopaedic Institute at Thomas Jefferson University, Philadelphia, 
      Pennsylvania, U.S.A.
LA  - eng
PT  - Journal Article
DEP - 20240213
PL  - United States
TA  - Arthrosc Sports Med Rehabil
JT  - Arthroscopy, sports medicine, and rehabilitation
JID - 101765256
PMC - PMC10875189
COIS- The authors declare the following financial interests/personal relationships 
      which may be considered as potential competing interests: M.G.C. reports board or 
      committee member, American Orthopaedic Society for Sports Medicine, Major League 
      Baseball Team Physicians Association, and Orthopaedic Learning Center, outside 
      the submitted work. S.H. reports board or committee member, American Orthopaedic 
      Society for Sports Medicine, Orthopaedic Learning Center, and Perry Initiative, 
      outside the submitted work; and paid consultant, Arthrex, outside the submitted 
      work. All other authors (W.L.J., A.K., D.F.) declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper. Full ICMJE author disclosure forms 
      are available for this article online, as supplementary material.
EDAT- 2024/02/20 11:50
MHDA- 2024/02/20 11:51
PMCR- 2024/02/13
CRDT- 2024/02/20 03:43
PHST- 2023/08/25 00:00 [received]
PHST- 2024/01/08 00:00 [accepted]
PHST- 2024/02/20 11:51 [medline]
PHST- 2024/02/20 11:50 [pubmed]
PHST- 2024/02/20 03:43 [entrez]
PHST- 2024/02/13 00:00 [pmc-release]
AID - S2666-061X(24)00011-7 [pii]
AID - 100893 [pii]
AID - 10.1016/j.asmr.2024.100893 [doi]
PST - epublish
SO  - Arthrosc Sports Med Rehabil. 2024 Feb 13;6(2):100893. doi: 
      10.1016/j.asmr.2024.100893. eCollection 2024 Apr.

PMID- 37407364
OWN - NLM
STAT- MEDLINE
DCOM- 20230922
LR  - 20240330
IS  - 2405-8025 (Electronic)
IS  - 2405-8025 (Linking)
VI  - 9
IP  - 10
DP  - 2023 Oct
TI  - Pearls and pitfalls of ChatGPT in medical oncology.
PG  - 788-790
LID - S2405-8033(23)00109-7 [pii]
LID - 10.1016/j.trecan.2023.06.007 [doi]
AB  - Recently, ChatGPT has drawn attention to the potential uses of artificial 
      intelligence (AI) in academia. Here, we discuss how ChatGPT can be of value to 
      medicine and medical oncology and the potential pitfalls that may be encountered.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Blum, Jacob
AU  - Blum J
AD  - The Johns Hopkins University School of Medicine. Baltimore, MD, USA.
FAU - Menta, Arjun K
AU  - Menta AK
AD  - The Johns Hopkins University School of Medicine. Baltimore, MD, USA.
FAU - Zhao, Xiyu
AU  - Zhao X
AD  - The Johns Hopkins University School of Medicine. Baltimore, MD, USA.
FAU - Yang, Victor B
AU  - Yang VB
AD  - The Johns Hopkins University School of Medicine. Baltimore, MD, USA.
FAU - Gouda, Mohamed A
AU  - Gouda MA
AD  - Department of Investigational Cancer Therapeutics, The University of Texas MD 
      Anderson Cancer Center, Houston, TX, USA.
FAU - Subbiah, Vivek
AU  - Subbiah V
AD  - Sarah Cannon Research Institute, Nashville, TN, USA. Electronic address: 
      Vivek.Subbiah@scri.com.
LA  - eng
GR  - T32 GM136577/GM/NIGMS NIH HHS/United States
PT  - Journal Article
DEP - 20230704
PL  - United States
TA  - Trends Cancer
JT  - Trends in cancer
JID - 101665956
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Medical Oncology
OTO - NOTNLM
OT  - artificial intelligence
OT  - oncology
OT  - practice
COIS- Declaration of interests V.S. is on the advisory board of Illumina, Labcorp, 
      Relay Therapeutics, Bayer, Jazz Pharmaceuticals, and Aadi Biosciences. No 
      interests are declared for the other authors.
EDAT- 2023/07/06 01:08
MHDA- 2023/09/22 06:42
CRDT- 2023/07/05 22:01
PHST- 2023/04/25 00:00 [received]
PHST- 2023/06/08 00:00 [revised]
PHST- 2023/06/13 00:00 [accepted]
PHST- 2023/09/22 06:42 [medline]
PHST- 2023/07/06 01:08 [pubmed]
PHST- 2023/07/05 22:01 [entrez]
AID - S2405-8033(23)00109-7 [pii]
AID - 10.1016/j.trecan.2023.06.007 [doi]
PST - ppublish
SO  - Trends Cancer. 2023 Oct;9(10):788-790. doi: 10.1016/j.trecan.2023.06.007. Epub 
      2023 Jul 4.

PMID- 37806782
OWN - NLM
STAT- MEDLINE
DCOM- 20231127
LR  - 20231127
IS  - 1471-1842 (Electronic)
IS  - 1471-1834 (Linking)
VI  - 40
IP  - 4
DP  - 2023 Dec
TI  - An overview of the capabilities of ChatGPT for medical writing and its 
      implications for academic integrity.
PG  - 440-446
LID - 10.1111/hir.12509 [doi]
AB  - The artificial intelligence (AI) tool ChatGPT, which is based on a large language 
      model (LLM), is gaining popularity in academic institutions, notably in the 
      medical field. This article provides a brief overview of the capabilities of 
      ChatGPT for medical writing and its implications for academic integrity. It 
      provides a list of AI generative tools, common use of AI generative tools for 
      medical writing, and provides a list of AI generative text detection tools. It 
      also provides recommendations for policymakers, information professionals, and 
      medical faculty for the constructive use of AI generative tools and related 
      technology. It also highlights the role of health sciences librarians and 
      educators in protecting students from generating text through ChatGPT in their 
      academic work.
CI  - © 2023 Health Libraries Group.
FAU - Liu, Huihui
AU  - Liu H
AD  - Shanxi University, Xiaodian District, Taiyuan, People's Republic of China.
FAU - Azam, Mehreen
AU  - Azam M
AD  - Department of Information Management, The Islamia University of Bahawalpur, 
      Bahawalpur, Pakistan.
FAU - Bin Naeem, Salman
AU  - Bin Naeem S
AD  - Department of Information Management, The Islamia University of Bahawalpur, 
      Bahawalpur, Pakistan.
FAU - Faiola, Anthony
AU  - Faiola A
AD  - Department of Health and Clinical Sciences, College of Health Sciences, 
      University of Kentucky, Lexington, Kentucky, USA.
LA  - eng
GR  - Shanxi Provincial Universities Philosophy and Social Science/
PT  - Journal Article
DEP - 20231008
PL  - England
TA  - Health Info Libr J
JT  - Health information and libraries journal
JID - 100970070
MH  - Humans
MH  - Artificial Intelligence
MH  - *Medical Writing
MH  - Schools
MH  - Language
MH  - *Librarians
OTO - NOTNLM
OT  - artificial intelligence (AI)
OT  - librarians, health science
OT  - libraries, academic
OT  - plagiarism
OT  - students, medical
EDAT- 2023/10/09 00:41
MHDA- 2023/11/27 12:43
CRDT- 2023/10/08 21:42
PHST- 2023/08/21 00:00 [received]
PHST- 2023/09/25 00:00 [accepted]
PHST- 2023/11/27 12:43 [medline]
PHST- 2023/10/09 00:41 [pubmed]
PHST- 2023/10/08 21:42 [entrez]
AID - 10.1111/hir.12509 [doi]
PST - ppublish
SO  - Health Info Libr J. 2023 Dec;40(4):440-446. doi: 10.1111/hir.12509. Epub 2023 Oct 
      8.

PMID- 37062612
OWN - NLM
STAT- MEDLINE
DCOM- 20230515
LR  - 20230515
IS  - 1768-3122 (Electronic)
IS  - 0248-8663 (Linking)
VI  - 44
IP  - 5
DP  - 2023 May
TI  - [Artificial intelligence and internal medicine: The example of hydroxychloroquine 
      according to ChatGPT].
PG  - 218-226
LID - S0248-8663(23)00121-2 [pii]
LID - 10.1016/j.revmed.2023.03.017 [doi]
AB  - Artificial intelligence (AI) using deep learning is revolutionizing several 
      fields, including medicine, with a wide range of applications. Available since 
      the end of 2022, ChatGPT is a conversational AI or "chatbot", using artificial 
      intelligence to dialogue with its users in all fields. Through the example of 
      hydroxychloroquine (HCQ), we discuss its use for patients, clinicians, or 
      researchers, and discuss its performance and limitations, particularly in 
      relation to algorithmic bias. If AI tools using deep learning do not dispense 
      with the expertise and experience of a clinician (at least, for the moment), they 
      have a potential to improve or simplify our daily practice.
CI  - Copyright © 2023 Société Nationale Française de Médecine Interne (SNFMI). 
      Published by Elsevier Masson SAS. All rights reserved.
FAU - Nguyen, Y
AU  - Nguyen Y
AD  - Service de médecine interne, hôpital Cochin, AP-HP centre, Université Paris cité, 
      75014 Paris, France; Centre de recherche en épidémiologie et statistiques 
      (CRESS), unité Inserm 1153, Université de Paris cité, Paris, France. Electronic 
      address: yann.nguyen2@aphp.fr.
FAU - Costedoat-Chalumeau, N
AU  - Costedoat-Chalumeau N
AD  - Service de médecine interne, hôpital Cochin, AP-HP centre, Université Paris cité, 
      75014 Paris, France; Centre de recherche en épidémiologie et statistiques 
      (CRESS), unité Inserm 1153, Université de Paris cité, Paris, France.
LA  - fre
PT  - English Abstract
PT  - Journal Article
TT  - Les intelligences artificielles conversationnelles en médecine interne&nbsp;: 
      l’exemple de l’hydroxychloroquine selon ChatGPT.
DEP - 20230414
PL  - France
TA  - Rev Med Interne
JT  - La Revue de medecine interne
JID - 8101383
RN  - 4QWG6N8QKH (Hydroxychloroquine)
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Hydroxychloroquine/therapeutic use
MH  - Internal Medicine
MH  - Software
MH  - Communication
OTO - NOTNLM
OT  - Artifical intelligence
OT  - ChatGPT
OT  - Hydroxychloroquine
OT  - Intelligence artificielle
OT  - Lupus érythémateux systémique
OT  - Systemic lupus erythematosus
EDAT- 2023/04/17 06:00
MHDA- 2023/05/15 06:42
CRDT- 2023/04/16 21:57
PHST- 2023/01/18 00:00 [received]
PHST- 2023/03/22 00:00 [revised]
PHST- 2023/03/31 00:00 [accepted]
PHST- 2023/05/15 06:42 [medline]
PHST- 2023/04/17 06:00 [pubmed]
PHST- 2023/04/16 21:57 [entrez]
AID - S0248-8663(23)00121-2 [pii]
AID - 10.1016/j.revmed.2023.03.017 [doi]
PST - ppublish
SO  - Rev Med Interne. 2023 May;44(5):218-226. doi: 10.1016/j.revmed.2023.03.017. Epub 
      2023 Apr 14.

PMID- 38433764
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240305
IS  - 2666-5204 (Electronic)
IS  - 2666-5204 (Linking)
VI  - 18
DP  - 2024 Jun
TI  - Prediction of outcomes after cardiac arrest by a generative artificial 
      intelligence model.
PG  - 100587
LID - 10.1016/j.resplu.2024.100587 [doi]
LID - 100587
AB  - AIMS: To investigate the prognostic accuracy of a non-medical generative 
      artificial intelligence model (Chat Generative Pre-Trained Transformer 4 - 
      ChatGPT-4) as a novel aspect in predicting death and poor neurological outcome at 
      hospital discharge based on real-life data from cardiac arrest patients. METHODS: 
      This prospective cohort study investigates the prognostic performance of 
      ChatGPT-4 to predict outcomes at hospital discharge of adult cardiac arrest 
      patients admitted to intensive care at a large Swiss tertiary academic medical 
      center (COMMUNICATE/PROPHETIC cohort study). We prompted ChatGPT-4 with sixteen 
      prognostic parameters derived from established post-cardiac arrest scores for 
      each patient. We compared the prognostic performance of ChatGPT-4 regarding the 
      area under the curve (AUC), sensitivity, specificity, positive and negative 
      predictive values, and likelihood ratios of three cardiac arrest scores 
      (Out-of-Hospital Cardiac Arrest [OHCA], Cardiac Arrest Hospital Prognosis [CAHP], 
      and PROgnostication using LOGistic regression model for Unselected adult cardiac 
      arrest patients in the Early stages [PROLOGUE score]) for in-hospital mortality 
      and poor neurological outcome. RESULTS: Mortality at hospital discharge was 43% 
      (n&nbsp;=&nbsp;309/713), 54% of patients (n&nbsp;=&nbsp;387/713) had a poor neurological outcome. 
      ChatGPT-4 showed good discrimination regarding in-hospital mortality with an AUC 
      of 0.85, similar to the OHCA, CAHP, and PROLOGUE (AUCs of 0.82, 0.83, and 0.84, 
      respectively) scores. For poor neurological outcome, ChatGPT-4 showed a similar 
      prediction to the post-cardiac arrest scores (AUC 0.83). CONCLUSIONS: ChatGPT-4 
      showed a similar performance in predicting mortality and poor neurological 
      outcome compared to validated post-cardiac arrest scores. However, more research 
      is needed regarding illogical answers for potential incorporation of an LLM in 
      the multimodal outcome prognostication after cardiac arrest.
CI  - © 2024 The Author(s).
FAU - Amacher, Simon A
AU  - Amacher SA
AD  - Intensive Care Medicine, Department of Acute Medical Care, University Hospital 
      Basel, Basel, Switzerland.
AD  - Medical Communication and Psychosomatic Medicine, University Hospital Basel, 
      Basel, Switzerland.
AD  - Emergency Medicine, Department of Acute Medical Care, University Hospital Basel, 
      Basel, Switzerland.
FAU - Arpagaus, Armon
AU  - Arpagaus A
AD  - Medical Communication and Psychosomatic Medicine, University Hospital Basel, 
      Basel, Switzerland.
FAU - Sahmer, Christian
AU  - Sahmer C
AD  - Medical Communication and Psychosomatic Medicine, University Hospital Basel, 
      Basel, Switzerland.
FAU - Becker, Christoph
AU  - Becker C
AD  - Medical Communication and Psychosomatic Medicine, University Hospital Basel, 
      Basel, Switzerland.
AD  - Emergency Medicine, Department of Acute Medical Care, University Hospital Basel, 
      Basel, Switzerland.
FAU - Gross, Sebastian
AU  - Gross S
AD  - Medical Communication and Psychosomatic Medicine, University Hospital Basel, 
      Basel, Switzerland.
FAU - Urben, Tabita
AU  - Urben T
AD  - Medical Communication and Psychosomatic Medicine, University Hospital Basel, 
      Basel, Switzerland.
FAU - Tisljar, Kai
AU  - Tisljar K
AD  - Intensive Care Medicine, Department of Acute Medical Care, University Hospital 
      Basel, Basel, Switzerland.
FAU - Sutter, Raoul
AU  - Sutter R
AD  - Intensive Care Medicine, Department of Acute Medical Care, University Hospital 
      Basel, Basel, Switzerland.
AD  - Medical Faculty, University of Basel, Basel, Switzerland.
AD  - Division of Neurophysiology, Department of Neurology, University Hospital Basel, 
      Basel, Switzerland.
FAU - Marsch, Stephan
AU  - Marsch S
AD  - Intensive Care Medicine, Department of Acute Medical Care, University Hospital 
      Basel, Basel, Switzerland.
AD  - Medical Faculty, University of Basel, Basel, Switzerland.
FAU - Hunziker, Sabina
AU  - Hunziker S
AD  - Medical Communication and Psychosomatic Medicine, University Hospital Basel, 
      Basel, Switzerland.
AD  - Medical Faculty, University of Basel, Basel, Switzerland.
AD  - Post-Intensive Care Clinic, University Hospital Basel, Basel, Switzerland.
LA  - eng
PT  - Journal Article
DEP - 20240222
PL  - Netherlands
TA  - Resusc Plus
JT  - Resuscitation plus
JID - 101774410
PMC - PMC10906512
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Cardiac arrest
OT  - Cardiopulmonary resuscitation
OT  - Mortality prediction
OT  - Neurological outcome
COIS- The authors declare that they have no known competing financial interests or 
      personal relationships that could have appeared to influence the work reported in 
      this paper.
EDAT- 2024/03/04 06:42
MHDA- 2024/03/04 06:43
PMCR- 2024/02/22
CRDT- 2024/03/04 04:35
PHST- 2023/12/27 00:00 [received]
PHST- 2024/02/01 00:00 [revised]
PHST- 2024/02/11 00:00 [accepted]
PHST- 2024/03/04 06:43 [medline]
PHST- 2024/03/04 06:42 [pubmed]
PHST- 2024/03/04 04:35 [entrez]
PHST- 2024/02/22 00:00 [pmc-release]
AID - S2666-5204(24)00038-9 [pii]
AID - 100587 [pii]
AID - 10.1016/j.resplu.2024.100587 [doi]
PST - epublish
SO  - Resusc Plus. 2024 Feb 22;18:100587. doi: 10.1016/j.resplu.2024.100587. 
      eCollection 2024 Jun.

PMID- 38304112
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240203
IS  - 2452-1094 (Print)
IS  - 2452-1094 (Electronic)
IS  - 2452-1094 (Linking)
VI  - 9
IP  - 3
DP  - 2024 Mar
TI  - Exploring Capabilities of Large Language Models such as ChatGPT in Radiation 
      Oncology.
PG  - 101400
LID - 10.1016/j.adro.2023.101400 [doi]
LID - 101400
AB  - PURPOSE: Technological progress of machine learning and natural language 
      processing has led to the development of large language models (LLMs), capable of 
      producing well-formed text responses and providing natural language access to 
      knowledge. Modern conversational LLMs such as ChatGPT have shown remarkable 
      capabilities across a variety of fields, including medicine. These models may 
      assess even highly specialized medical knowledge within specific disciplines, 
      such as radiation therapy. We conducted an exploratory study to examine the 
      capabilities of ChatGPT to answer questions in radiation therapy. METHODS AND 
      MATERIALS: A set of multiple-choice questions about clinical, physics, and 
      biology general knowledge in radiation oncology as well as a set of open-ended 
      questions were created. These were given as prompts to the LLM ChatGPT, and the 
      answers were collected and analyzed. For the multiple-choice questions, it was 
      checked how many of the answers of the model could be clearly assigned to one of 
      the allowed multiple-choice-answers, and the proportion of correct answers was 
      determined. For the open-ended questions, independent blinded radiation 
      oncologists evaluated the quality of the answers regarding correctness and 
      usefulness on a 5-point Likert scale. Furthermore, the evaluators were asked to 
      provide suggestions for improving the quality of the answers. RESULTS: For 70 
      multiple-choice questions, ChatGPT gave valid answers in 66 cases (94.3%). In 
      60.61% of the valid answers, the selected answer was correct (50.0% of clinical 
      questions, 78.6% of physics questions, and 58.3% of biology questions). For 25 
      open-ended questions, 12 answers of ChatGPT were considered as "acceptable," 
      "good," or "very good" regarding both correctness and helpfulness by all 6 
      participating radiation oncologists. Overall, the answers were considered "very 
      good" in 29.3% and 28%, "good" in 28% and 29.3%, "acceptable" in 19.3% and 19.3%, 
      "bad" in 9.3% and 9.3%, and "very bad" in 14% and 14% regarding 
      correctness/helpfulness. CONCLUSIONS: Modern conversational LLMs such as ChatGPT 
      can provide satisfying answers to many relevant questions in radiation therapy. 
      As they still fall short of consistently providing correct information, it is 
      problematic to use them for obtaining medical information. As LLMs will further 
      improve in the future, they are expected to have an increasing impact not only on 
      general society, but also on clinical practice, including radiation oncology.
CI  - © 2023 The Author(s).
FAU - Dennstädt, Fabio
AU  - Dennstädt F
AD  - Department of Radiation Oncology, Kantonsspital St. Gallen, St. Gallen, 
      Switzerland.
FAU - Hastings, Janna
AU  - Hastings J
AD  - School of Medicine, University of St. Gallen, St. Gallen, Switzerland.
AD  - Institute for Implementation Science in Health Care, University of Zurich, 
      Zurich, Switzerland.
FAU - Putora, Paul Martin
AU  - Putora PM
AD  - Department of Radiation Oncology, Kantonsspital St. Gallen, St. Gallen, 
      Switzerland.
AD  - Department of Radiation Oncology, Inselspital, Bern University Hospital and 
      University of Bern, Switzerland.
FAU - Vu, Erwin
AU  - Vu E
AD  - Department of Radiation Oncology, Kantonsspital St. Gallen, St. Gallen, 
      Switzerland.
FAU - Fischer, Galina F
AU  - Fischer GF
AD  - Department of Radiation Oncology, Kantonsspital St. Gallen, St. Gallen, 
      Switzerland.
FAU - Süveg, Krisztian
AU  - Süveg K
AD  - Department of Radiation Oncology, Kantonsspital St. Gallen, St. Gallen, 
      Switzerland.
FAU - Glatzer, Markus
AU  - Glatzer M
AD  - Department of Radiation Oncology, Kantonsspital St. Gallen, St. Gallen, 
      Switzerland.
FAU - Riggenbach, Elena
AU  - Riggenbach E
AD  - Department of Radiation Oncology, Inselspital, Bern University Hospital and 
      University of Bern, Switzerland.
FAU - Hà, Hông-Linh
AU  - Hà HL
AD  - Department of Radiation Oncology, Inselspital, Bern University Hospital and 
      University of Bern, Switzerland.
FAU - Cihoric, Nikola
AU  - Cihoric N
AD  - Department of Radiation Oncology, Inselspital, Bern University Hospital and 
      University of Bern, Switzerland.
LA  - eng
PT  - Journal Article
DEP - 20231104
PL  - United States
TA  - Adv Radiat Oncol
JT  - Advances in radiation oncology
JID - 101677247
PMC - PMC10831180
COIS- Nikola Cihoric is a technical lead for the SmartOncology project and medical 
      advisor for Wemedoo AG, Steinhausen AG, Switzerland.
EDAT- 2024/02/02 06:43
MHDA- 2024/02/02 06:44
PMCR- 2023/11/04
CRDT- 2024/02/02 04:01
PHST- 2023/08/24 00:00 [received]
PHST- 2023/10/16 00:00 [accepted]
PHST- 2024/02/02 06:44 [medline]
PHST- 2024/02/02 06:43 [pubmed]
PHST- 2024/02/02 04:01 [entrez]
PHST- 2023/11/04 00:00 [pmc-release]
AID - S2452-1094(23)00228-2 [pii]
AID - 101400 [pii]
AID - 10.1016/j.adro.2023.101400 [doi]
PST - epublish
SO  - Adv Radiat Oncol. 2023 Nov 4;9(3):101400. doi: 10.1016/j.adro.2023.101400. 
      eCollection 2024 Mar.

PMID- 37796786
OWN - NLM
STAT- MEDLINE
DCOM- 20231101
LR  - 20240210
IS  - 1932-6203 (Electronic)
IS  - 1932-6203 (Linking)
VI  - 18
IP  - 10
DP  - 2023
TI  - An exploratory survey about using ChatGPT in education, healthcare, and research.
PG  - e0292216
LID - 10.1371/journal.pone.0292216 [doi]
LID - e0292216
AB  - OBJECTIVE: ChatGPT is the first large language model (LLM) to reach a large, 
      mainstream audience. Its rapid adoption and exploration by the population at 
      large has sparked a wide range of discussions regarding its acceptable and 
      optimal integration in different areas. In a hybrid (virtual and in-person) panel 
      discussion event, we examined various perspectives regarding the use of ChatGPT 
      in education, research, and healthcare. MATERIALS AND METHODS: We surveyed 
      in-person and online attendees using an audience interaction platform (Slido). We 
      quantitatively analyzed received responses on questions about the use of ChatGPT 
      in various contexts. We compared pairwise categorical groups with a Fisher's 
      Exact. Furthermore, we used qualitative methods to analyze and code discussions. 
      RESULTS: We received 420 responses from an estimated 844 participants (response 
      rate 49.7%). Only 40% of the audience had tried ChatGPT. More trainees had tried 
      ChatGPT compared with faculty. Those who had used ChatGPT were more interested in 
      using it in a wider range of contexts going forwards. Of the three discussed 
      contexts, the greatest uncertainty was shown about using ChatGPT in education. 
      Pros and cons were raised during discussion for the use of this technology in 
      education, research, and healthcare. DISCUSSION: There was a range of 
      perspectives around the uses of ChatGPT in education, research, and healthcare, 
      with still much uncertainty around its acceptability and optimal uses. There were 
      different perspectives from respondents of different roles (trainee vs faculty vs 
      staff). More discussion is needed to explore perceptions around the use of LLMs 
      such as ChatGPT in vital sectors such as education, healthcare and research. 
      Given involved risks and unforeseen challenges, taking a thoughtful and measured 
      approach in adoption would reduce the likelihood of harm.
CI  - Copyright: © 2023 Hosseini et al. This is an open access article distributed 
      under the terms of the Creative Commons Attribution License, which permits 
      unrestricted use, distribution, and reproduction in any medium, provided the 
      original author and source are credited.
FAU - Hosseini, Mohammad
AU  - Hosseini M
AUID- ORCID: 0000-0002-2385-985X
AD  - Department of Preventive Medicine, Northwestern University Feinberg School of 
      Medicine, Chicago, Illinois, United States of America.
FAU - Gao, Catherine A
AU  - Gao CA
AUID- ORCID: 0000-0001-5576-3943
AD  - Division of Pulmonary and Critical Care, Department of Medicine, Northwestern 
      University Feinberg School of Medicine, Chicago, Illinois, United States of 
      America.
FAU - Liebovitz, David M
AU  - Liebovitz DM
AUID- ORCID: 0000-0002-2518-5940
AD  - Divisions of General Internal Medicine and Health and Biomedical Informatics, 
      Department of Medicine, Northwestern University Feinberg School of Medicine, 
      Chicago, Illinois, United States of America.
AD  - Center for Medical Education in Digital Health and Data Science, Northwestern 
      University Feinberg School of Medicine, Chicago, Illinois, United States of 
      America.
FAU - Carvalho, Alexandre M
AU  - Carvalho AM
AUID- ORCID: 0000-0002-6961-7004
AD  - Division of Infectious Diseases, Department of Medicine, Northwestern University 
      Feinberg School of Medicine, Chicago, Illinois, United States of America.
AD  - Center for Pathogen Genomics &amp; Microbial Evolution, Northwestern University 
      Feinberg School of Medicine, Chicago, Illinois, United States of America.
FAU - Ahmad, Faraz S
AU  - Ahmad FS
AUID- ORCID: 0000-0002-2613-2541
AD  - Department of Preventive Medicine, Northwestern University Feinberg School of 
      Medicine, Chicago, Illinois, United States of America.
AD  - Division of Cardiology, Department of Medicine, Northwestern University Feinberg 
      School of Medicine, Chicago, Illinois, United States of America.
AD  - Bluhm Cardiovascular Center for Artificial Intelligence, Northwestern Medicine, 
      Northwestern University Feinberg School of Medicine, Chicago, Illinois, United 
      States of America.
FAU - Luo, Yuan
AU  - Luo Y
AUID- ORCID: 0000-0003-0195-7456
AD  - Department of Preventive Medicine, Northwestern University Feinberg School of 
      Medicine, Chicago, Illinois, United States of America.
FAU - MacDonald, Ngan
AU  - MacDonald N
AUID- ORCID: 0000-0003-0268-7224
AD  - Institute for Artificial Intelligence in Medicine, Northwestern University 
      Feinberg School of Medicine, Chicago, Illinois, United States of America.
FAU - Holmes, Kristi L
AU  - Holmes KL
AUID- ORCID: 0000-0001-8420-5254
AD  - Department of Preventive Medicine, Northwestern University Feinberg School of 
      Medicine, Chicago, Illinois, United States of America.
AD  - Institute for Artificial Intelligence in Medicine, Northwestern University 
      Feinberg School of Medicine, Chicago, Illinois, United States of America.
AD  - Galter Health Sciences Library and Learning Center, Northwestern University 
      Feinberg School of Medicine, Chicago, Illinois, United States of America.
FAU - Kho, Abel
AU  - Kho A
AUID- ORCID: 0000-0003-1993-5634
AD  - Divisions of General Internal Medicine and Health and Biomedical Informatics, 
      Department of Medicine, Northwestern University Feinberg School of Medicine, 
      Chicago, Illinois, United States of America.
AD  - Institute for Artificial Intelligence in Medicine, Northwestern University 
      Feinberg School of Medicine, Chicago, Illinois, United States of America.
LA  - eng
GR  - F32 HL162377/HL/NHLBI NIH HHS/United States
GR  - K23 HL155970/HL/NHLBI NIH HHS/United States
GR  - UL1 TR001422/TR/NCATS NIH HHS/United States
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
PT  - Research Support, Non-U.S. Gov't
DEP - 20231005
PL  - United States
TA  - PLoS One
JT  - PloS one
JID - 101285081
SB  - IM
UOF - medRxiv. 2023 Apr 03;:. PMID: 37066228
MH  - Humans
MH  - Educational Status
MH  - *Faculty
MH  - *Mainstreaming, Education
MH  - Health Facilities
MH  - Probability
PMC - PMC10553335
COIS- The authors report no conflicting interests.
EDAT- 2023/10/05 18:42
MHDA- 2023/11/01 12:45
PMCR- 2023/10/05
CRDT- 2023/10/05 13:33
PHST- 2023/05/12 00:00 [received]
PHST- 2023/09/14 00:00 [accepted]
PHST- 2023/11/01 12:45 [medline]
PHST- 2023/10/05 18:42 [pubmed]
PHST- 2023/10/05 13:33 [entrez]
PHST- 2023/10/05 00:00 [pmc-release]
AID - PONE-D-23-13566 [pii]
AID - 10.1371/journal.pone.0292216 [doi]
PST - epublish
SO  - PLoS One. 2023 Oct 5;18(10):e0292216. doi: 10.1371/journal.pone.0292216. 
      eCollection 2023.

PMID- 38433238
OWN - NLM
STAT- MEDLINE
DCOM- 20240305
LR  - 20240307
IS  - 2045-2322 (Electronic)
IS  - 2045-2322 (Linking)
VI  - 14
IP  - 1
DP  - 2024 Mar 4
TI  - Bias of AI-generated content: an examination of news produced by large language 
      models.
PG  - 5224
LID - 10.1038/s41598-024-55686-2 [doi]
LID - 5224
AB  - Large language models (LLMs) have the potential to transform our lives and work 
      through the content they generate, known as AI-Generated Content (AIGC). To 
      harness this transformation, we need to understand the limitations of LLMs. Here, 
      we investigate the bias of AIGC produced by seven representative LLMs, including 
      ChatGPT and LLaMA. We collect news articles from The New York Times and Reuters, 
      both known for their dedication to provide unbiased news. We then apply each 
      examined LLM to generate news content with headlines of these news articles as 
      prompts, and evaluate the gender and racial biases of the AIGC produced by the 
      LLM by comparing the AIGC and the original news articles. We further analyze the 
      gender bias of each LLM under biased prompts by adding gender-biased messages to 
      prompts constructed from these news headlines. Our study reveals that the AIGC 
      produced by each examined LLM demonstrates substantial gender and racial biases. 
      Moreover, the AIGC generated by each LLM exhibits notable discrimination against 
      females and individuals of the Black race. Among the LLMs, the AIGC generated by 
      ChatGPT demonstrates the lowest level of bias, and ChatGPT is the sole model 
      capable of declining content generation when provided with biased prompts.
CI  - © 2024. The Author(s).
FAU - Fang, Xiao
AU  - Fang X
AD  - University of Delaware, Newark, USA. xfang@udel.edu.
FAU - Che, Shangkun
AU  - Che S
AD  - Tsinghua University, Beijing, China. csk19@mails.tsinghua.edu.cn.
FAU - Mao, Minjia
AU  - Mao M
AD  - University of Delaware, Newark, USA.
FAU - Zhang, Hongzhe
AU  - Zhang H
AD  - Chinese University of Hong Kong, Shenzhen, China.
FAU - Zhao, Ming
AU  - Zhao M
AD  - University of Delaware, Newark, USA.
FAU - Zhao, Xiaohang
AU  - Zhao X
AD  - Shanghai University of Finance and Economics, Shanghai, China.
LA  - eng
PT  - Journal Article
DEP - 20240304
PL  - England
TA  - Sci Rep
JT  - Scientific reports
JID - 101563288
SB  - IM
MH  - Humans
MH  - Female
MH  - Male
MH  - Animals
MH  - Sexism
MH  - Bias
MH  - *Aortic Valve Insufficiency
MH  - *Camelids, New World
MH  - Language
PMC - PMC10909834
OTO - NOTNLM
OT  - AI-generated content (AIGC)
OT  - Bias
OT  - ChatGPT
OT  - Gender bias
OT  - Generative AI
OT  - Large language model (LLM)
OT  - Prompt
OT  - Racial bias
COIS- The authors declare no competing interests.
EDAT- 2024/03/04 00:43
MHDA- 2024/03/05 06:43
PMCR- 2024/03/04
CRDT- 2024/03/03 23:16
PHST- 2023/10/27 00:00 [received]
PHST- 2024/02/26 00:00 [accepted]
PHST- 2024/03/05 06:43 [medline]
PHST- 2024/03/04 00:43 [pubmed]
PHST- 2024/03/03 23:16 [entrez]
PHST- 2024/03/04 00:00 [pmc-release]
AID - 10.1038/s41598-024-55686-2 [pii]
AID - 55686 [pii]
AID - 10.1038/s41598-024-55686-2 [doi]
PST - epublish
SO  - Sci Rep. 2024 Mar 4;14(1):5224. doi: 10.1038/s41598-024-55686-2.

PMID- 38545872
OWN - NLM
STAT- Publisher
LR  - 20240328
IS  - 1440-1665 (Electronic)
IS  - 1039-8562 (Linking)
DP  - 2024 Mar 28
TI  - ChatGPT in private practice: The opportunities and pitfalls of novel technology.
PG  - 10398562241241473
LID - 10.1177/10398562241241473 [doi]
AB  - OBJECTIVE: This article explores the transformative impact of OpenAI and ChatGPT 
      on Australian medical practitioners, particularly psychiatrists in the private 
      practice setting. It delves into the extensive benefits and limitations 
      associated with integrating ChatGPT into medical practice, summarising current 
      policies and scrutinising medicolegal implications. CONCLUSION: A careful 
      assessment is imperative to determine whether the benefits of AI integration 
      outweigh the associated risks. Practitioners are urged to review AI-generated 
      content to ensure its accuracy, recognising that liability likely resides with 
      them rather than with AI platforms, despite the lack of case law specific to 
      negligence and AI in the Australian context at present. It is important to employ 
      measures that ensure patient confidentiality is not breached and practitioners 
      are encouraged to seek counsel from their professional indemnity insurer. There 
      is considerable potential for future development of specialised AI software 
      tailored specifically for the medical profession, making the use of AI more 
      suitable for the medical field in the Australian legal landscape. Moving forward, 
      it is essential to embrace technology and actively address its challenges rather 
      than dismissing AI integration into medical practice. It is becoming increasingly 
      essential that both the psychiatric community, medical community at large and 
      policy makers develop comprehensive guidelines to fill existing policy gaps and 
      adapt to the evolving landscape of AI technologies in healthcare.
FAU - Lehman, Kirk
AU  - Lehman K
AUID- ORCID: 0000-0002-6622-7107
AD  - Bond University, Robina, QLD, Australia. RINGGOLD: 3555
FAU - Aroney, Emeil
AU  - Aroney E
AUID- ORCID: 0009-0007-6433-2791
AD  - Bond University, Robina, QLD, Australia. RINGGOLD: 3555
FAU - Wu, Isabella
AU  - Wu I
AD  - Bond University, Robina, QLD, Australia. RINGGOLD: 3555
LA  - eng
PT  - Journal Article
DEP - 20240328
PL  - England
TA  - Australas Psychiatry
JT  - Australasian psychiatry : bulletin of Royal Australian and New Zealand College of 
      Psychiatrists
JID - 9613603
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - confidentiality
OT  - medicolegal
OT  - psychiatry
COIS- DisclosureThe author(s) declared no potential conflicts of interest with respect 
      to the research, authorship, and/or publication of this article.
EDAT- 2024/03/28 12:53
MHDA- 2024/03/28 12:53
CRDT- 2024/03/28 06:13
PHST- 2024/03/28 12:53 [medline]
PHST- 2024/03/28 12:53 [pubmed]
PHST- 2024/03/28 06:13 [entrez]
AID - 10.1177/10398562241241473 [doi]
PST - aheadofprint
SO  - Australas Psychiatry. 2024 Mar 28:10398562241241473. doi: 
      10.1177/10398562241241473.

PMID- 37843604
OWN - NLM
STAT- MEDLINE
DCOM- 20231207
LR  - 20231207
IS  - 1248-9204 (Electronic)
IS  - 1248-9204 (Linking)
VI  - 27
IP  - 6
DP  - 2023 Dec
TI  - AI's deep dive into complex pediatric inguinal hernia issues: a challenge to 
      traditional guidelines?
PG  - 1587-1599
LID - 10.1007/s10029-023-02900-1 [doi]
AB  - OBJECTIVE: This study utilized ChatGPT, an artificial intelligence program based 
      on large language models, to explore controversial issues in pediatric inguinal 
      hernia surgery and compare its responses with the guidelines of the European 
      Association of Pediatric Surgeons (EUPSA). METHODS: Six contentious issues raised 
      by EUPSA were submitted to ChatGPT 4.0 for analysis, for which two independent 
      responses were generated for each issue. These generated answers were 
      subsequently compared with systematic reviews and guidelines. To ensure content 
      accuracy and reliability, a content analysis was conducted, and expert 
      evaluations were solicited for validation. Content analysis evaluated the 
      consistency or discrepancy between ChatGPT 4.0's responses and the guidelines. An 
      expert scoring method assess the quality, reliability, and applicability of 
      responses. The TF-IDF model tested the stability and consistency of the two 
      responses. RESULTS: The responses generated by ChatGPT 4.0 were mostly consistent 
      with the guidelines. However, some differences and contradictions were noted. The 
      average quality score was 3.33, reliability score was 2.75, and applicability 
      score was 3.46 (out of 5). The average similarity between the two responses was 
      0.72 (out of 1), Content analysis and expert ratings yielded consistent 
      conclusions, enhancing the credibility of our research. CONCLUSION: ChatGPT can 
      provide valuable responses to clinical questions, but it has limitations and 
      requires further improvement. It is recommended to combine ChatGPT with other 
      reliable data sources to improve clinical practice and decision-making.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag France SAS, 
      part of Springer Nature.
FAU - Wang, G
AU  - Wang G
AUID- ORCID: 0000-0002-8224-7706
AD  - Department of Pediatrics, Women's and Children's Hospital, Chongqing Medical 
      University, 120 Longshan Rd., Chongqing, 401147, People's Republic of China.
AD  - Department of Pediatrics, Children's Hospital, Chongqing Medical University, 
      Chongqing, People's Republic of China.
AD  - Department of Pediatric General Surgery, Chongqing Maternal and Child Health 
      Hospital, Chongqing Medical University, Chongqing, People's Republic of China.
FAU - Liu, Q
AU  - Liu Q
AD  - Department of Pediatrics, Women's and Children's Hospital, Chongqing Medical 
      University, 120 Longshan Rd., Chongqing, 401147, People's Republic of China.
AD  - Department of Fetus and Pediatrics, Chongqing Health Center for Women and 
      Children, Chongqing, People's Republic of China.
FAU - Chen, G
AU  - Chen G
AD  - Department of Pediatrics, Women's and Children's Hospital, Chongqing Medical 
      University, 120 Longshan Rd., Chongqing, 401147, People's Republic of China.
AD  - Department of Fetus and Pediatrics, Chongqing Health Center for Women and 
      Children, Chongqing, People's Republic of China.
FAU - Xia, B
AU  - Xia B
AD  - Department of Pediatrics, Women's and Children's Hospital, Chongqing Medical 
      University, 120 Longshan Rd., Chongqing, 401147, People's Republic of China.
AD  - Department of Fetus and Pediatrics, Chongqing Health Center for Women and 
      Children, Chongqing, People's Republic of China.
FAU - Zeng, D
AU  - Zeng D
AD  - Department of Pediatrics, Women's and Children's Hospital, Chongqing Medical 
      University, 120 Longshan Rd., Chongqing, 401147, People's Republic of China.
AD  - Department of Fetus and Pediatrics, Chongqing Health Center for Women and 
      Children, Chongqing, People's Republic of China.
FAU - Chen, G
AU  - Chen G
AD  - Department of Pediatrics, Women's and Children's Hospital, Chongqing Medical 
      University, 120 Longshan Rd., Chongqing, 401147, People's Republic of China. 
      38096550@qq.com.
AD  - Department of Fetus and Pediatrics, Chongqing Health Center for Women and 
      Children, Chongqing, People's Republic of China. 38096550@qq.com.
AD  - Department of Pediatric General Surgery, Chongqing Maternal and Child Health 
      Hospital, Chongqing Medical University, Chongqing, People's Republic of China. 
      38096550@qq.com.
AD  - Department of Obstetrics and Gynecology, Chongqing Health Center for Women and 
      Children, Women and Children's Hospital of Chongqing Medical University, 120 
      Longshan Rd., Chongqing, 401147, People's Republic of China. 38096550@qq.com.
FAU - Guo, C
AU  - Guo C
AD  - Department of Pediatrics, Women's and Children's Hospital, Chongqing Medical 
      University, 120 Longshan Rd., Chongqing, 401147, People's Republic of China. 
      guochunbao@foxmail.com.
AD  - Department of Fetus and Pediatrics, Chongqing Health Center for Women and 
      Children, Chongqing, People's Republic of China. guochunbao@foxmail.com.
AD  - Department of Pediatric General Surgery, Chongqing Maternal and Child Health 
      Hospital, Chongqing Medical University, Chongqing, People's Republic of China. 
      guochunbao@foxmail.com.
LA  - eng
GR  - No: 30973440/the National Natural Science Foundation of China/
GR  - 30770950/the National Natural Science Foundation of China/
GR  - No. YBRP-2021XX/the Youth Basic Research Project from the Ministry of Key 
      Laboratory of Child Development and Disorders/
GR  - cstc2020jcyj-msxmX0326/the Chongqing Natural Science Foundation/
GR  - CSTB2022NSCQ-MSX0819/the Chongqing Natural Science Foundation/
PT  - Journal Article
DEP - 20231016
PL  - France
TA  - Hernia
JT  - Hernia : the journal of hernias and abdominal wall surgery
JID - 9715168
SB  - IM
MH  - Humans
MH  - Child
MH  - *Artificial Intelligence
MH  - *Hernia, Inguinal/surgery
MH  - Reproducibility of Results
MH  - Herniorrhaphy
MH  - Research Design
OTO - NOTNLM
OT  - ChatGPT/ChatGPT 4.0
OT  - Controversial issues
OT  - Guidelines
OT  - Inguinal hernia
OT  - Systematic reviews
EDAT- 2023/10/16 12:42
MHDA- 2023/12/07 12:42
CRDT- 2023/10/16 11:05
PHST- 2023/07/04 00:00 [received]
PHST- 2023/09/19 00:00 [accepted]
PHST- 2023/12/07 12:42 [medline]
PHST- 2023/10/16 12:42 [pubmed]
PHST- 2023/10/16 11:05 [entrez]
AID - 10.1007/s10029-023-02900-1 [pii]
AID - 10.1007/s10029-023-02900-1 [doi]
PST - ppublish
SO  - Hernia. 2023 Dec;27(6):1587-1599. doi: 10.1007/s10029-023-02900-1. Epub 2023 Oct 
      16.

PMID- 38318564
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240207
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 16
IP  - 1
DP  - 2024 Jan
TI  - Assessing the Accuracy of Information on Medication Abortion: A Comparative 
      Analysis of ChatGPT and Google Bard AI.
PG  - e51544
LID - 10.7759/cureus.51544 [doi]
LID - e51544
AB  - Background and objective ChatGPT and Google Bard AI are widely used 
      conversational chatbots, even in healthcare. While they have several strengths, 
      they can generate seemingly correct&nbsp;but erroneous responses, warranting caution 
      in medical contexts. In an era where access to abortion care is diminishing, 
      patients may increasingly rely on online resources and AI-driven language models 
      for information on medication abortions. In light of this, this study&nbsp;aimed 
      to&nbsp;compare the accuracy and comprehensiveness of responses generated by ChatGPT 
      3.5 and Google Bard AI&nbsp;to medical queries about medication abortions. Methods 
      Fourteen open-ended questions about medication abortion were formulated based on 
      the Frequently Asked Questions (FAQs) from the National Abortion Federation (NAF) 
      and the Reproductive Health Access Project (RHAP) websites. These questions were 
      answered using ChatGPT version 3.5 and Google Bard AI on October 7, 2023. The 
      accuracy of the responses was analyzed by cross-referencing the generated answers 
      against the information provided by NAF and RHAP. Any discrepancies were further 
      verified against the guidelines from the American Congress of Obstetricians and 
      Gynecologists (ACOG). A rating scale used by Johnson et al. was employed for 
      assessment, utilizing a 6-point Likert scale [ranging from 1 (completely 
      incorrect) to 6&nbsp;(correct)] to evaluate accuracy and a 3-point scale [ranging from 
      1 (incomplete) to 3 (comprehensive)] to assess completeness. Questions that did 
      not yield answers were assigned a score of 0 and&nbsp;omitted from the correlation 
      analysis. Data analysis and visualization were done using R Software version 
      4.3.1. Statistical significance was determined by employing Spearman's R and 
      Mann-Whitney U tests. Results All questions were entered sequentially into both 
      chatbots by the same author. On the initial attempt, ChatGPT successfully 
      generated relevant responses for all questions, while Google Bard AI failed to 
      provide answers for five questions. Repeating the same question in Google Bard AI 
      yielded an answer for one; two were answered with different phrasing; and two 
      remained unanswered despite rephrasing. ChatGPT showed a median accuracy score of 
      5 (mean: 5.26, SD: 0.73) and a median completeness score of 3 (mean: 2.57, SD: 
      0.51). It showed the highest accuracy score in six responses and the highest 
      completeness score in eight responses. In contrast, Google Bard AI had a median 
      accuracy score of 5 (mean: 4.5, SD: 2.03) and a median completeness score of 2 
      (mean: 2.14, SD: 1.03). It achieved the highest accuracy score in five responses 
      and the highest completeness score in six responses. Spearman's correlation 
      coefficient revealed no correlation between accuracy and completeness for ChatGPT 
      (rs = -0.46771, p = 0.09171). However, Google Bard AI showed a marginally 
      significant correlation (rs = 0.5738, p = 0.05108). Mann-Whitney U test indicated 
      no statistically significant differences between ChatGPT and Google Bard AI 
      concerning accuracy (U = 82, p&gt;0.05) or completeness (U = 78, p&gt;0.05). Conclusion 
      While both chatbots showed similar levels of accuracy, minor errors&nbsp;were noted, 
      pertaining to finer aspects that demand specialized knowledge of abortion care. 
      This could explain the lack of a significant correlation between accuracy and 
      completeness. Ultimately, AI-driven language models have the potential to provide 
      information on medication abortions, but there is a need for continual refinement 
      and oversight.
CI  - Copyright © 2024, Mediboina et al.
FAU - Mediboina, Anjali
AU  - Mediboina A
AD  - Community Medicine, Alluri Sita Ramaraju Academy of Medical Sciences, Eluru, IND.
FAU - Badam, Rajani Kumari
AU  - Badam RK
AD  - Obstetrics and Gynaecology, Sri Venkateswara Medical College, Tirupathi, IND.
FAU - Chodavarapu, Sailaja
AU  - Chodavarapu S
AD  - Obstetrics and Gynaecology, Government Medical College, Rajamahendravaram, IND.
LA  - eng
PT  - Journal Article
DEP - 20240102
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10840059
OTO - NOTNLM
OT  - artificial intelligence
OT  - chatbots
OT  - chatgpt
OT  - ethics
OT  - google bardai
OT  - large language models
OT  - medication abortion
OT  - patient information
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/02/06 06:42
MHDA- 2024/02/06 06:43
PMCR- 2024/01/02
CRDT- 2024/02/06 04:02
PHST- 2024/01/01 00:00 [accepted]
PHST- 2024/02/06 06:43 [medline]
PHST- 2024/02/06 06:42 [pubmed]
PHST- 2024/02/06 04:02 [entrez]
PHST- 2024/01/02 00:00 [pmc-release]
AID - 10.7759/cureus.51544 [doi]
PST - epublish
SO  - Cureus. 2024 Jan 2;16(1):e51544. doi: 10.7759/cureus.51544. eCollection 2024 Jan.

PMID- 37857839
OWN - NLM
STAT- MEDLINE
DCOM- 20231023
LR  - 20240210
IS  - 2045-2322 (Electronic)
IS  - 2045-2322 (Linking)
VI  - 13
IP  - 1
DP  - 2023 Oct 19
TI  - A vignette-based evaluation of ChatGPT's ability to provide appropriate and 
      equitable medical advice across care contexts.
PG  - 17885
LID - 10.1038/s41598-023-45223-y [doi]
LID - 17885
AB  - ChatGPT is a large language model trained on text corpora and reinforced with 
      human supervision. Because ChatGPT can provide human-like responses to complex 
      questions, it could become an easily accessible source of medical advice for 
      patients. However, its ability to answer medical questions appropriately and 
      equitably remains unknown. We presented ChatGPT with 96 advice-seeking vignettes 
      that varied across clinical contexts, medical histories, and social 
      characteristics. We analyzed responses for clinical appropriateness by 
      concordance with guidelines, recommendation type, and consideration of social 
      factors. Ninety-three (97%) responses were appropriate and did not explicitly 
      violate clinical guidelines. Recommendations in response to advice-seeking 
      questions were completely absent (N = 34, 35%), general (N = 18, 18%), or 
      specific (N = 44, 46%). 53 (55%) explicitly considered social factors like race 
      or insurance status, which in some cases changed clinical recommendations. 
      ChatGPT consistently provided background information in response to medical 
      questions but did not reliably offer appropriate and personalized medical advice.
CI  - © 2023. Springer Nature Limited.
FAU - Nastasi, Anthony J
AU  - Nastasi AJ
AD  - Department of Emergency Medicine, University of Pennsylvania, 3400 Spruce Street, 
      Philadelphia, PA, 19104, USA. Anthony.Nastasi@pennmedicine.upenn.edu.
FAU - Courtright, Katherine R
AU  - Courtright KR
AD  - Perelman School of Medicine, Palliative and Advanced Illness Research (PAIR) 
      Center, University of Pennsylvania, Philadelphia, PA, USA.
AD  - Division of Pulmonary, Allergy, &amp; Critical Care Medicine, Perelman School of 
      Medicine, University of Pennsylvania, Philadelphia, PA, USA.
AD  - Perelman School of Medicine, Leonard Davis Institute of Health Economics, 
      University of Pennsylvania, Philadelphia, PA, USA.
AD  - Perelman School of Medicine, Penn Palliative Care Program, University of 
      Pennsylvania, Philadelphia, PA, USA.
FAU - Halpern, Scott D
AU  - Halpern SD
AD  - Perelman School of Medicine, Palliative and Advanced Illness Research (PAIR) 
      Center, University of Pennsylvania, Philadelphia, PA, USA.
AD  - Division of Pulmonary, Allergy, &amp; Critical Care Medicine, Perelman School of 
      Medicine, University of Pennsylvania, Philadelphia, PA, USA.
AD  - Perelman School of Medicine, Leonard Davis Institute of Health Economics, 
      University of Pennsylvania, Philadelphia, PA, USA.
FAU - Weissman, Gary E
AU  - Weissman GE
AD  - Perelman School of Medicine, Palliative and Advanced Illness Research (PAIR) 
      Center, University of Pennsylvania, Philadelphia, PA, USA.
AD  - Division of Pulmonary, Allergy, &amp; Critical Care Medicine, Perelman School of 
      Medicine, University of Pennsylvania, Philadelphia, PA, USA.
AD  - Perelman School of Medicine, Leonard Davis Institute of Health Economics, 
      University of Pennsylvania, Philadelphia, PA, USA.
AD  - Perelman School of Medicine, Penn Institute for Biomedical Informatics, 
      University of Pennsylvania, Philadelphia, PA, USA.
LA  - eng
GR  - K23 HL141639/HL/NHLBI NIH HHS/United States
GR  - R01AG073384/NH/NIH HHS/United States
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
DEP - 20231019
PL  - England
TA  - Sci Rep
JT  - Scientific reports
JID - 101563288
SB  - IM
MH  - Humans
MH  - Female
MH  - *Insurance Coverage
MH  - *Language
MH  - Social Factors
MH  - Uterus
PMC - PMC10587094
COIS- The authors declare no competing interests.
EDAT- 2023/10/20 00:43
MHDA- 2023/10/23 01:18
PMCR- 2023/10/19
CRDT- 2023/10/19 23:43
PHST- 2023/03/01 00:00 [received]
PHST- 2023/10/17 00:00 [accepted]
PHST- 2023/10/23 01:18 [medline]
PHST- 2023/10/20 00:43 [pubmed]
PHST- 2023/10/19 23:43 [entrez]
PHST- 2023/10/19 00:00 [pmc-release]
AID - 10.1038/s41598-023-45223-y [pii]
AID - 45223 [pii]
AID - 10.1038/s41598-023-45223-y [doi]
PST - epublish
SO  - Sci Rep. 2023 Oct 19;13(1):17885. doi: 10.1038/s41598-023-45223-y.

PMID- 38482764
OWN - NLM
STAT- MEDLINE
DCOM- 20240315
LR  - 20240315
IS  - 1660-9379 (Print)
IS  - 1660-9379 (Linking)
VI  - 20
IP  - 865
DP  - 2024 Mar 13
TI  - [Applications, techniques, and best practices for using ChatGPT].
PG  - 557-561
LID - 10.53738/REVMED.2024.20.865.557 [doi]
AB  - The future of a machine writing our reports for us could also lead to it carrying 
      out our consultations, a scenario whose relevance is open to debate. 
      Nevertheless, the present offers us new artificial intelligence tools that can 
      support us in our daily activities. The publication in 2017 of Transformers 
      initiated a disruptive revolution by enabling the emergence of major language 
      models, of which ChatGPT is the best known. In view of their growing adoption, 
      the authors felt it would be useful to offer some pragmatic advice on how to 
      improve the use of these tools. In this article, we first look at how ChatGPT 
      works and its potential applications in medicine, before providing a practical 
      guide to using it to get the best results.
FAU - Garin, Dorian
AU  - Garin D
AUID- ORCID: 0000-0002-3872-3496
AD  - Service de médecine interne, HFR Fribourg, Hôpital cantonal, 1752 
      Villars-Sur-Glâne.
AD  - Faculté de médecine, Université de Genève, 1211 Genève 4.
FAU - Lovis, Christian
AU  - Lovis C
AUID- ORCID: 0000-0002-2681-8076
AD  - Faculté de médecine, Université de Genève, 1211 Genève 4.
AD  - Service des sciences de l'information médicale, Département diagnostic, Hôpitaux 
      universitaires de Genève, 1211 Genève 14.
LA  - fre
PT  - English Abstract
PT  - Journal Article
TT  - Applications, techniques et bonnes pratiques d’utilisation de ChatGPT.
PL  - Switzerland
TA  - Rev Med Suisse
JT  - Revue medicale suisse
JID - 101219148
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Emotions
MH  - Language
MH  - *Medicine
MH  - Referral and Consultation
COIS- Les auteurs n’ont déclaré aucun conflit d’intérêts en relation avec cet article.
EDAT- 2024/03/14 06:46
MHDA- 2024/03/15 06:43
CRDT- 2024/03/14 05:43
PHST- 2024/03/15 06:43 [medline]
PHST- 2024/03/14 06:46 [pubmed]
PHST- 2024/03/14 05:43 [entrez]
AID - RMS0865-009 [pii]
AID - 10.53738/REVMED.2024.20.865.557 [doi]
PST - ppublish
SO  - Rev Med Suisse. 2024 Mar 13;20(865):557-561. doi: 
      10.53738/REVMED.2024.20.865.557.

PMID- 36926868
OWN - NLM
STAT- MEDLINE
DCOM- 20230328
LR  - 20230605
IS  - 1549-960X (Electronic)
IS  - 1549-9596 (Linking)
VI  - 63
IP  - 6
DP  - 2023 Mar 27
TI  - Do Large Language Models Understand Chemistry? A Conversation with ChatGPT.
PG  - 1649-1655
LID - 10.1021/acs.jcim.3c00285 [doi]
AB  - Large language models (LLMs) have promised a revolution in answering complex 
      questions using the ChatGPT model. Its application in chemistry is still in its 
      infancy. This viewpoint addresses the question of how well ChatGPT understands 
      chemistry by posing five simple tasks in different subareas of chemistry.
FAU - Castro Nascimento, Cayque Monteiro
AU  - Castro Nascimento CM
AUID- ORCID: 0000-0003-4712-0598
AD  - Departamento de Química, Pontifícia Universidade Católica do Rio de Janeiro, P.O. 
      Box 38097, Rio de Janeiro, RJ 22451-900, Brazil.
FAU - Pimentel, André Silva
AU  - Pimentel AS
AUID- ORCID: 0000-0002-1301-0561
AD  - Departamento de Química, Pontifícia Universidade Católica do Rio de Janeiro, P.O. 
      Box 38097, Rio de Janeiro, RJ 22451-900, Brazil.
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20230316
PL  - United States
TA  - J Chem Inf Model
JT  - Journal of chemical information and modeling
JID - 101230060
SB  - IM
MH  - *Communication
MH  - *Language
EDAT- 2023/03/18 06:00
MHDA- 2023/03/28 17:16
CRDT- 2023/03/17 07:56
PHST- 2023/03/28 17:16 [medline]
PHST- 2023/03/18 06:00 [pubmed]
PHST- 2023/03/17 07:56 [entrez]
AID - 10.1021/acs.jcim.3c00285 [doi]
PST - ppublish
SO  - J Chem Inf Model. 2023 Mar 27;63(6):1649-1655. doi: 10.1021/acs.jcim.3c00285. 
      Epub 2023 Mar 16.

PMID- 37485215
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230725
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 6
DP  - 2023 Jun
TI  - Artificial Intelligence in Ophthalmology: A Comparative Analysis of GPT-3.5, 
      GPT-4, and Human Expertise in Answering StatPearls Questions.
PG  - e40822
LID - 10.7759/cureus.40822 [doi]
LID - e40822
AB  - Importance Chat Generative Pre-Trained Transformer (ChatGPT) has shown promising 
      performance in various fields, including medicine, business, and law, but its 
      accuracy in specialty-specific medical questions, particularly in ophthalmology, 
      is still uncertain. Purpose This study evaluates the performance of two ChatGPT 
      models (GPT-3.5 and GPT-4) and human professionals in answering ophthalmology 
      questions from the StatPearls question bank, assessing their outcomes, and 
      providing insights into the integration of artificial intelligence (AI) 
      technology in ophthalmology. Methods ChatGPT's performance was evaluated using 
      467 ophthalmology questions from the StatPearls question bank. These questions 
      were stratified into 11 subcategories, four difficulty levels, and three 
      generalized anatomical categories. The answer accuracy of GPT-3.5, GPT-4, and 
      human participants was assessed. Statistical analysis was conducted via the 
      Kolmogorov-Smirnov test for normality, one-way analysis of variance (ANOVA) for 
      the statistical significance of GPT-3 versus GPT-4 versus human performance, and 
      repeated unpaired two-sample t-tests to compare the means of two groups. Results 
      GPT-4 outperformed both GPT-3.5 and human professionals on ophthalmology 
      StatPearls questions, except in the "Lens and Cataract" category. The performance 
      differences were statistically significant overall, with GPT-4 achieving higher 
      accuracy (73.2%) compared to GPT-3.5 (55.5%, p-value &lt; 0.001) and humans (58.3%, 
      p-value &lt; 0.001). There were variations in performance across difficulty levels 
      (rated one to four), but GPT-4 consistently performed better than both GPT-3.5 
      and humans on level-two, -three, and -four questions. On questions of level-four 
      difficulty, human performance significantly exceeded that of GPT-3.5 (p = 0.008). 
      Conclusion The study's findings demonstrate GPT-4's significant performance 
      improvements over GPT-3.5 and human professionals on StatPearls ophthalmology 
      questions. Our results highlight the potential of advanced conversational AI 
      systems to be utilized as important tools in the education and practice of 
      medicine.
CI  - Copyright © 2023, Moshirfar et al.
FAU - Moshirfar, Majid
AU  - Moshirfar M
AD  - Corneal and Refractive Surgery, Hoopes Vision Research Center, Draper, USA.
AD  - Ophthalmology, The University of Utah, Salt Lake City, USA.
AD  - Eye Banking and Corneal Transplantation, Utah Lions Eye Bank, Murray, USA.
FAU - Altaf, Amal W
AU  - Altaf AW
AD  - Medical School, University of Arizona College of Medicine Phoenix, Phoenix, USA.
FAU - Stoakes, Isabella M
AU  - Stoakes IM
AD  - Medical School, Pacific Northwest University of Health Science, Yakima, USA.
AD  - Ophthalmology, Hoopes Vision Research Center, Draper, USA.
FAU - Tuttle, Jared J
AU  - Tuttle JJ
AD  - Medical School, University of Texas Health Science Center at San Antonio, San 
      Antonio, USA.
FAU - Hoopes, Phillip C
AU  - Hoopes PC
AD  - Ophthalmology, Hoopes Vision Research Center, Draper, USA.
LA  - eng
PT  - Journal Article
DEP - 20230622
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10362981
OTO - NOTNLM
OT  - artificial intelligence
OT  - chatbot
OT  - chatgpt-3.5
OT  - chatgpt-4
OT  - clinical decision-making
OT  - conversational ai
OT  - conversational generative pre-trained transformer
OT  - cornea
OT  - ophthalmology
OT  - statpearls
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/07/24 06:42
MHDA- 2023/07/24 06:43
PMCR- 2023/06/22
CRDT- 2023/07/24 04:54
PHST- 2023/06/22 00:00 [accepted]
PHST- 2023/07/24 06:43 [medline]
PHST- 2023/07/24 06:42 [pubmed]
PHST- 2023/07/24 04:54 [entrez]
PHST- 2023/06/22 00:00 [pmc-release]
AID - 10.7759/cureus.40822 [doi]
PST - epublish
SO  - Cureus. 2023 Jun 22;15(6):e40822. doi: 10.7759/cureus.40822. eCollection 2023 
      Jun.

PMID- 38248771
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240129
IS  - 2075-4426 (Print)
IS  - 2075-4426 (Electronic)
IS  - 2075-4426 (Linking)
VI  - 14
IP  - 1
DP  - 2024 Jan 5
TI  - ChatGPT May Offer an Adequate Substitute for Informed Consent to Patients Prior 
      to Total Knee Arthroplasty-Yet Caution Is Needed.
LID - 10.3390/jpm14010069 [doi]
LID - 69
AB  - Prior to undergoing total knee arthroplasty (TKA), surgeons are often confronted 
      with patients with numerous questions regarding the procedure and the recovery 
      process. Due to limited staff resources and mounting individual workload, 
      increased efficiency, e.g., using artificial intelligence (AI), is of increasing 
      interest. We comprehensively evaluated ChatGPT's orthopedic responses using the 
      DISCERN instrument. Three independent orthopedic surgeons rated the responses 
      across various criteria. We found consistently high scores, predominantly 
      exceeding a score of three out of five in almost all categories, indicative of 
      the quality and accuracy of the information provided. Notably, the AI 
      demonstrated proficiency in conveying precise and reliable information on 
      orthopedic topics. However, a notable observation pertains to the generation of 
      non-existing references for certain claims. This study underscores the 
      significance of critically evaluating references provided by ChatGPT and 
      emphasizes the necessity of cross-referencing information from established 
      sources. Overall, the findings contribute valuable insights into the performance 
      of ChatGPT in delivering accurate orthopedic information for patients in clinical 
      use while shedding light on areas warranting further refinement. Future 
      iterations of natural language processing systems may be able to replace, in part 
      or in entirety, the preoperative interactions, thereby optimizing the efficiency, 
      accessibility, and standardization of patient communication.
FAU - Kienzle, Arne
AU  - Kienzle A
AUID- ORCID: 0000-0002-4556-2993
AD  - Center for Musculoskeletal Surgery, Clinic for Orthopedics, 
      Charité-Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin, 
      Humboldt-Universität zu Berlin, and Berlin Institute of Health, 10117 Berlin, 
      Germany.
AD  - Julius Wolff Institute and Center for Musculoskeletal Surgery, 
      Charité-Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin, 
      Humboldt-Universität zu Berlin, and Berlin Institute of Health, 13353 Berlin, 
      Germany.
AD  - Berlin Institute of Health at Charité-Universitätsmedizin Berlin, BIH Biomedical 
      Innovation Academy, BIH Charité Clinician Scientist Program, 10117 Berlin, 
      Germany.
FAU - Niemann, Marcel
AU  - Niemann M
AUID- ORCID: 0000-0001-7264-3089
AD  - Center for Musculoskeletal Surgery, Clinic for Orthopedics, 
      Charité-Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin, 
      Humboldt-Universität zu Berlin, and Berlin Institute of Health, 10117 Berlin, 
      Germany.
FAU - Meller, Sebastian
AU  - Meller S
AUID- ORCID: 0000-0002-9457-6187
AD  - Center for Musculoskeletal Surgery, Clinic for Orthopedics, 
      Charité-Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin, 
      Humboldt-Universität zu Berlin, and Berlin Institute of Health, 10117 Berlin, 
      Germany.
FAU - Gwinner, Clemens
AU  - Gwinner C
AD  - Center for Musculoskeletal Surgery, Clinic for Orthopedics, 
      Charité-Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin, 
      Humboldt-Universität zu Berlin, and Berlin Institute of Health, 10117 Berlin, 
      Germany.
LA  - eng
PT  - Journal Article
DEP - 20240105
PL  - Switzerland
TA  - J Pers Med
JT  - Journal of personalized medicine
JID - 101602269
PMC - PMC10821427
OTO - NOTNLM
OT  - ChatGPT
OT  - OpenAI
OT  - language processing system
OT  - preoperative management
OT  - total knee arthroplasty
COIS- Authors declare no conflicts of interest.
EDAT- 2024/01/22 06:42
MHDA- 2024/01/22 06:43
PMCR- 2024/01/05
CRDT- 2024/01/22 04:43
PHST- 2023/11/08 00:00 [received]
PHST- 2023/12/30 00:00 [revised]
PHST- 2024/01/03 00:00 [accepted]
PHST- 2024/01/22 06:43 [medline]
PHST- 2024/01/22 06:42 [pubmed]
PHST- 2024/01/22 04:43 [entrez]
PHST- 2024/01/05 00:00 [pmc-release]
AID - jpm14010069 [pii]
AID - jpm-14-00069 [pii]
AID - 10.3390/jpm14010069 [doi]
PST - epublish
SO  - J Pers Med. 2024 Jan 5;14(1):69. doi: 10.3390/jpm14010069.

PMID- 37349973
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230701
IS  - 2753-670X (Electronic)
IS  - 2753-670X (Linking)
VI  - 36
IP  - 6
DP  - 2023 Jun 1
TI  - ChatGPT: revolutionizing cardiothoracic surgery research through artificial 
      intelligence.
LID - 10.1093/icvts/ivad090 [doi]
LID - ivad090
FAU - Arjomandi Rad, Arian
AU  - Arjomandi Rad A
AUID- ORCID: 0000-0002-4931-4049
AD  - Medical Sciences Division, University of Oxford, Oxford, UK.
AD  - Department of Cardiothoracic Surgery, Maastricht University Medical Center, 
      Maastricht, Netherlands.
AD  - Department of Surgery and Cancer, Imperial College London, London, UK.
FAU - Sardari Nia, Peyman
AU  - Sardari Nia P
AUID- ORCID: 0000-0003-2296-1497
AD  - Department of Cardiothoracic Surgery, Maastricht University Medical Center, 
      Maastricht, Netherlands.
FAU - Athanasiou, Thanos
AU  - Athanasiou T
AUID- ORCID: 0000-0002-2525-5515
AD  - Department of Surgery and Cancer, Imperial College London, London, UK.
LA  - eng
PT  - Journal Article
PL  - England
TA  - Interdiscip Cardiovasc Thorac Surg
JT  - Interdisciplinary cardiovascular and thoracic surgery
JID - 9918540787006676
SB  - IM
PMC - PMC10287897
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Cardiothoracic surgery
OT  - ChatGPT
OT  - Research
EDAT- 2023/06/23 06:42
MHDA- 2023/06/23 06:43
PMCR- 2023/06/22
CRDT- 2023/06/23 01:23
PHST- 2023/04/21 00:00 [received]
PHST- 2023/05/28 00:00 [revised]
PHST- 2023/06/19 00:00 [accepted]
PHST- 2023/06/23 06:43 [medline]
PHST- 2023/06/23 06:42 [pubmed]
PHST- 2023/06/23 01:23 [entrez]
PHST- 2023/06/22 00:00 [pmc-release]
AID - 7205498 [pii]
AID - ivad090 [pii]
AID - 10.1093/icvts/ivad090 [doi]
PST - ppublish
SO  - Interdiscip Cardiovasc Thorac Surg. 2023 Jun 1;36(6):ivad090. doi: 
      10.1093/icvts/ivad090.

PMID- 37305993
OWN - NLM
STAT- MEDLINE
DCOM- 20230717
LR  - 20231229
IS  - 2042-8189 (Electronic)
IS  - 1478-2715 (Linking)
VI  - 53
IP  - 2
DP  - 2023 Jun
TI  - ChatGPT, artificial intelligence and scientific writing: What authors, peer 
      reviewers and editors should know.
PG  - 90-93
LID - 10.1177/14782715231181023 [doi]
FAU - Misra, Durga Prasanna
AU  - Misra DP
AUID- ORCID: 0000-0002-5035-7396
AD  - Department of Clinical Immunology and Rheumatology, Sanjay Gandhi Postgraduate 
      Institute of Medical Sciences, Lucknow, India.
FAU - Chandwar, Kunal
AU  - Chandwar K
AUID- ORCID: 0000-0002-0499-0641
AD  - Department of Clinical Immunology and Rheumatology, Sanjay Gandhi Postgraduate 
      Institute of Medical Sciences, Lucknow, India.
LA  - eng
PT  - Editorial
DEP - 20230612
PL  - England
TA  - J R Coll Physicians Edinb
JT  - The journal of the Royal College of Physicians of Edinburgh
JID - 101144324
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Peer Review, Research
MH  - Publishing
MH  - Writing
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - information science
OT  - peer review
OT  - publications
OT  - scientific publication
EDAT- 2023/06/12 06:42
MHDA- 2023/07/17 06:42
CRDT- 2023/06/12 05:25
PHST- 2023/07/17 06:42 [medline]
PHST- 2023/06/12 06:42 [pubmed]
PHST- 2023/06/12 05:25 [entrez]
AID - 10.1177/14782715231181023 [doi]
PST - ppublish
SO  - J R Coll Physicians Edinb. 2023 Jun;53(2):90-93. doi: 10.1177/14782715231181023. 
      Epub 2023 Jun 12.

PMID- 38238101
OWN - NLM
STAT- MEDLINE
DCOM- 20240315
LR  - 20240315
IS  - 1399-5618 (Electronic)
IS  - 1398-5647 (Linking)
VI  - 26
IP  - 2
DP  - 2024 Mar
TI  - Comment on published article "A chat about bipolar disorder".
PG  - 190
LID - 10.1111/bdi.13402 [doi]
AB  - To the Editor, we follow the topic "A chat about bipolar disorder(1) ". According 
      to the study's findings, ChatGPT proved its ability to deliver basic and 
      informative information on bipolar disorders.
CI  - © 2024 John Wiley &amp; Sons A/S. Published by John Wiley &amp; Sons Ltd.
FAU - Daungsupawong, Hinpetch
AU  - Daungsupawong H
AUID- ORCID: 0009-0002-5881-2709
AD  - Private Academic Consultant, Phonhong, Lao People's Democratic Republic.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AUID- ORCID: 0000-0003-1039-3728
AD  - Chandigarh University, Mohali, India.
LA  - eng
PT  - Letter
DEP - 20240118
PL  - Denmark
TA  - Bipolar Disord
JT  - Bipolar disorders
JID - 100883596
SB  - IM
MH  - Humans
MH  - *Bipolar Disorder/therapy
OTO - NOTNLM
OT  - bipolar
OT  - chatGPT
OT  - education
EDAT- 2024/01/19 00:42
MHDA- 2024/03/15 06:43
CRDT- 2024/01/18 21:39
PHST- 2024/03/15 06:43 [medline]
PHST- 2024/01/19 00:42 [pubmed]
PHST- 2024/01/18 21:39 [entrez]
AID - 10.1111/bdi.13402 [doi]
PST - ppublish
SO  - Bipolar Disord. 2024 Mar;26(2):190. doi: 10.1111/bdi.13402. Epub 2024 Jan 18.

PMID- 37399881
OWN - NLM
STAT- MEDLINE
DCOM- 20231120
LR  - 20231120
IS  - 1873-0442 (Electronic)
IS  - 1477-8939 (Linking)
VI  - 54
DP  - 2023 Jul-Aug
TI  - ChatGPT in travel medicine: A friend or foe?
PG  - 102615
LID - S1477-8939(23)00075-3 [pii]
LID - 10.1016/j.tmaid.2023.102615 [doi]
FAU - Choudhary, Om Prakash
AU  - Choudhary OP
AD  - Department of Veterinary Anatomy, College of Veterinary Science, Guru Angad Dev 
      Veterinary and Animal Sciences University (GADVASU), Rampura Phul, Bathinda, 
      151103, Punjab, India. Electronic address: dr.om.choudhary@gmail.com.
FAU - Priyanka
AU  - Priyanka
AD  - Department of Veterinary Microbiology, College of Veterinary Science, Guru Angad 
      Dev Veterinary and Animal Sciences University (GADVASU), Rampura Phul, Bathinda, 
      151103, Punjab, India.
LA  - eng
PT  - Letter
DEP - 20230701
PL  - Netherlands
TA  - Travel Med Infect Dis
JT  - Travel medicine and infectious disease
JID - 101230758
SB  - IM
MH  - Humans
MH  - *Travel Medicine
MH  - *Artificial Intelligence
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Pandemic
OT  - Travel medicine
OT  - Traveling
OT  - Vaccination
COIS- Declaration of competing interest All authors report no conflicts of interest 
      relevant to this article.
EDAT- 2023/07/04 01:05
MHDA- 2023/07/04 01:06
CRDT- 2023/07/03 19:19
PHST- 2023/06/27 00:00 [received]
PHST- 2023/06/27 00:00 [accepted]
PHST- 2023/07/04 01:06 [medline]
PHST- 2023/07/04 01:05 [pubmed]
PHST- 2023/07/03 19:19 [entrez]
AID - S1477-8939(23)00075-3 [pii]
AID - 10.1016/j.tmaid.2023.102615 [doi]
PST - ppublish
SO  - Travel Med Infect Dis. 2023 Jul-Aug;54:102615. doi: 10.1016/j.tmaid.2023.102615. 
      Epub 2023 Jul 1.

PMID- 36907556
OWN - NLM
STAT- MEDLINE
DCOM- 20240325
LR  - 20240325
IS  - 1097-6787 (Electronic)
IS  - 0190-9622 (Linking)
VI  - 90
IP  - 4
DP  - 2024 Apr
TI  - Consulting ChatGPT: Ethical dilemmas in language model artificial intelligence.
PG  - 879-880
LID - S0190-9622(23)00364-X [pii]
LID - 10.1016/j.jaad.2023.02.052 [doi]
FAU - Beltrami, Eric J
AU  - Beltrami EJ
AD  - University of Connecticut School of Medicine, Farmington, CT.
FAU - Grant-Kels, Jane Margaret
AU  - Grant-Kels JM
AD  - Department of Dermatology, University of Connecticut Health Center, Farmington, 
      CT; Department of Dermatology, University of Florida, Gainesville, FL. Electronic 
      address: grant@uchc.edu.
LA  - eng
PT  - Letter
DEP - 20230311
PL  - United States
TA  - J Am Acad Dermatol
JT  - Journal of the American Academy of Dermatology
JID - 7907132
SB  - IM
CIN - J Am Acad Dermatol. 2023 Oct;89(4):e157-e158. PMID: 37263382
MH  - Humans
MH  - *Artificial Intelligence
OTO - NOTNLM
OT  - ChatGPT
OT  - accessibility to dermatology
OT  - artificial intelligence
OT  - beneficence
OT  - clinical practice
OT  - ethics
OT  - justice
OT  - language models
OT  - nonmaleficence
COIS- Conflicts of interest Dr Grant-Kels is a stockholder and medical advisor for 
      DermaSensor. This company’s specific technology is not discussed in this 
      submission. Author Beltrami has no conflicts of interest to declare.
EDAT- 2023/03/13 06:00
MHDA- 2024/03/25 06:43
CRDT- 2023/03/12 20:32
PHST- 2023/01/12 00:00 [received]
PHST- 2023/02/14 00:00 [revised]
PHST- 2023/02/28 00:00 [accepted]
PHST- 2024/03/25 06:43 [medline]
PHST- 2023/03/13 06:00 [pubmed]
PHST- 2023/03/12 20:32 [entrez]
AID - S0190-9622(23)00364-X [pii]
AID - 10.1016/j.jaad.2023.02.052 [doi]
PST - ppublish
SO  - J Am Acad Dermatol. 2024 Apr;90(4):879-880. doi: 10.1016/j.jaad.2023.02.052. Epub 
      2023 Mar 11.

PMID- 38450060
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240308
IS  - 0972-978X (Print)
IS  - 0972-978X (Electronic)
IS  - 0972-978X (Linking)
VI  - 53
DP  - 2024 Jul
TI  - Can generative artificial intelligence pass the orthopaedic board examination?
PG  - 27-33
LID - 10.1016/j.jor.2023.10.026 [doi]
AB  - BACKGROUND: Resident training programs in the US use the Orthopaedic In-Training 
      Examination (OITE) developed by the American Academy of Orthopaedic Surgeons 
      (AAOS) to assess the current knowledge of their residents and to identify the 
      residents at risk of failing the Amerian Board of Orthopaedic Surgery (ABOS) 
      examination. Optimal strategies for OITE preparation are constantly being 
      explored. There may be a role for Large Language Models (LLMs) in orthopaedic 
      resident education. ChatGPT, an LLM launched in late 2022 has demonstrated the 
      ability to produce accurate, detailed answers, potentially enabling it to aid in 
      medical education and clinical decision-making. The purpose of this study is to 
      evaluate the performance of ChatGPT on Orthopaedic In-Training Examinations using 
      Self-Assessment Exams from the AAOS database and approved literature as a proxy 
      for the Orthopaedic Board Examination. METHODS: 301 SAE questions from the AAOS 
      database and associated AAOS literature were input into ChatGPT's interface in a 
      question and multiple-choice format and the answers were then analyzed to 
      determine which answer choice was selected. A new chat was used for every 
      question. All answers were recorded, categorized, and compared to the answer 
      given by the OITE and SAE exams, noting whether the answer was right or wrong. 
      RESULTS: Of the 301 questions asked, ChatGPT was able to correctly answer 183 
      (60.8%) of them. The subjects with the highest percentage of correct questions 
      were basic science (81%), oncology (72.7%, shoulder and elbow (71.9%), and sports 
      (71.4%). The questions were further subdivided into 3 groups: those about 
      management, diagnosis, or knowledge recall. There were 86 management questions 
      and 47 were correct (54.7%), 45 diagnosis questions with 32 correct (71.7%), and 
      168 knowledge recall questions with 102 correct (60.7%). CONCLUSIONS: ChatGPT has 
      the potential to provide orthopedic educators and trainees with accurate clinical 
      conclusions for the majority of board-style questions, although its reasoning 
      should be carefully analyzed for accuracy and clinical validity. As such, its 
      usefulness in a clinical educational context is currently limited but rapidly 
      evolving. CLINICAL RELEVANCE: ChatGPT can access a multitude of medical data and 
      may help provide accurate answers to clinical questions.
CI  - © 2023 Professor P K Surendran Memorial Education Foundation. Published by 
      Elsevier B.V. All rights reserved.
FAU - Isleem, Ula N
AU  - Isleem UN
AD  - Department of Orthopaedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA.
FAU - Zaidat, Bashar
AU  - Zaidat B
AD  - Department of Orthopaedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA.
FAU - Ren, Renee
AU  - Ren R
AD  - Department of Orthopaedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA.
FAU - Geng, Eric A
AU  - Geng EA
AD  - Department of Orthopaedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA.
FAU - Burapachaisri, Aonnicha
AU  - Burapachaisri A
AD  - Department of Orthopaedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA.
FAU - Tang, Justin E
AU  - Tang JE
AD  - Department of Orthopaedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA.
FAU - Kim, Jun S
AU  - Kim JS
AD  - Department of Orthopaedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA.
FAU - Cho, Samuel K
AU  - Cho SK
AD  - Department of Orthopaedic Surgery, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA.
LA  - eng
PT  - Journal Article
DEP - 20231105
PL  - India
TA  - J Orthop
JT  - Journal of orthopaedics
JID - 101233220
PMC - PMC10912220
EDAT- 2024/03/07 06:43
MHDA- 2024/03/07 06:44
PMCR- 2025/07/01
CRDT- 2024/03/07 04:14
PHST- 2023/06/18 00:00 [received]
PHST- 2023/10/24 00:00 [revised]
PHST- 2023/10/26 00:00 [accepted]
PHST- 2025/07/01 00:00 [pmc-release]
PHST- 2024/03/07 06:44 [medline]
PHST- 2024/03/07 06:43 [pubmed]
PHST- 2024/03/07 04:14 [entrez]
AID - S0972-978X(23)00259-3 [pii]
AID - 10.1016/j.jor.2023.10.026 [doi]
PST - epublish
SO  - J Orthop. 2023 Nov 5;53:27-33. doi: 10.1016/j.jor.2023.10.026. eCollection 2024 
      Jul.

PMID- 38466153
OWN - NLM
STAT- Publisher
LR  - 20240311
IS  - 1469-0756 (Electronic)
IS  - 0032-5473 (Linking)
DP  - 2024 Mar 9
TI  - Artificial intelligence tools in medical education beyond Chat Generative 
      Pre-trained Transformer (ChatGPT).
LID - qgae014 [pii]
LID - 10.1093/postmj/qgae014 [doi]
FAU - Tan, Li Feng
AU  - Tan LF
AUID- ORCID: 0000-0003-1232-9308
AD  - Department of Medicine, Alexandra Hospital, Singapore &nbsp;159964.
FAU - Ng, Isaac K S
AU  - Ng IKS
AD  - Department of Medicine, National University Hospital, Singapore &nbsp;119074.
FAU - Teo, Desmond
AU  - Teo D
AD  - Department of Medicine, Alexandra Hospital, Singapore &nbsp;159964.
LA  - eng
PT  - Journal Article
DEP - 20240309
PL  - England
TA  - Postgrad Med J
JT  - Postgraduate medical journal
JID - 0234135
SB  - IM
EDAT- 2024/03/11 12:42
MHDA- 2024/03/11 12:42
CRDT- 2024/03/11 10:33
PHST- 2023/12/29 00:00 [received]
PHST- 2024/01/17 00:00 [accepted]
PHST- 2024/03/11 12:42 [medline]
PHST- 2024/03/11 12:42 [pubmed]
PHST- 2024/03/11 10:33 [entrez]
AID - 7624812 [pii]
AID - 10.1093/postmj/qgae014 [doi]
PST - aheadofprint
SO  - Postgrad Med J. 2024 Mar 9:qgae014. doi: 10.1093/postmj/qgae014.

PMID- 38353690
OWN - NLM
STAT- Publisher
LR  - 20240214
IS  - 1432-1041 (Electronic)
IS  - 0031-6970 (Linking)
DP  - 2024 Feb 14
TI  - ChatGPT for generating multiple-choice questions: Evidence on the use of 
      artificial intelligence in automatic item generation for a rational 
      pharmacotherapy exam.
LID - 10.1007/s00228-024-03649-x [doi]
AB  - PURPOSE: Artificial intelligence, specifically large language models such as 
      ChatGPT, offers valuable potential benefits in question (item) writing. This 
      study aimed to determine the feasibility of generating case-based multiple-choice 
      questions using ChatGPT in terms of item difficulty and discrimination levels. 
      METHODS: This study involved 99 fourth-year medical students who participated in 
      a rational pharmacotherapy clerkship carried out based-on the WHO 6-Step Model. 
      In response to a prompt that we provided, ChatGPT generated ten case-based 
      multiple-choice questions on hypertension. Following an expert panel, two of 
      these multiple-choice questions were incorporated into a medical school exam 
      without making any changes in the questions. Based on the administration of the 
      test, we evaluated their psychometric properties, including item difficulty, item 
      discrimination (point-biserial correlation), and functionality of the options. 
      RESULTS: Both questions exhibited acceptable levels of point-biserial 
      correlation, which is higher than the threshold of 0.30 (0.41 and 0.39). However, 
      one question had three non-functional options (options chosen by fewer than 5% of 
      the exam participants) while the other question had none. CONCLUSIONS: The 
      findings showed that the questions can effectively differentiate between students 
      who perform at high and low levels, which also point out the potential of ChatGPT 
      as an artificial intelligence tool in test development. Future studies may use 
      the prompt to generate items in order for enhancing the external validity of the 
      results by gathering data from diverse institutions and settings.
CI  - © 2024. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Kıyak, Yavuz Selim
AU  - Kıyak YS
AUID- ORCID: 0000-0002-5026-3234
AD  - Department of Medical Education and Informatics, Faculty of Medicine, Gazi 
      University, Ankara, Turkey. yskiyak@gazi.edu.tr.
AD  - Gazi Üniversitesi Hastanesi E Blok 9, Kat 06500 Beşevler, Ankara, Turkey. 
      yskiyak@gazi.edu.tr.
FAU - Coşkun, Özlem
AU  - Coşkun Ö
AUID- ORCID: 0000-0001-8716-1584
AD  - Department of Medical Education and Informatics, Faculty of Medicine, Gazi 
      University, Ankara, Turkey.
FAU - Budakoğlu, Işıl İrem
AU  - Budakoğlu Iİ
AUID- ORCID: 0000-0003-1517-3169
AD  - Department of Medical Education and Informatics, Faculty of Medicine, Gazi 
      University, Ankara, Turkey.
FAU - Uluoğlu, Canan
AU  - Uluoğlu C
AUID- ORCID: 0000-0003-0682-5794
AD  - Department of Medical Pharmacology, Faculty of Medicine, Gazi University, Ankara, 
      Turkey.
LA  - eng
PT  - Journal Article
DEP - 20240214
PL  - Germany
TA  - Eur J Clin Pharmacol
JT  - European journal of clinical pharmacology
JID - 1256165
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Automatic item generation
OT  - ChatGPT
OT  - Medical education
OT  - Multiple-choice questions
OT  - Rational pharmacotherapy
EDAT- 2024/02/14 12:50
MHDA- 2024/02/14 12:50
CRDT- 2024/02/14 11:03
PHST- 2023/12/27 00:00 [received]
PHST- 2024/02/03 00:00 [accepted]
PHST- 2024/02/14 12:50 [medline]
PHST- 2024/02/14 12:50 [pubmed]
PHST- 2024/02/14 11:03 [entrez]
AID - 10.1007/s00228-024-03649-x [pii]
AID - 10.1007/s00228-024-03649-x [doi]
PST - aheadofprint
SO  - Eur J Clin Pharmacol. 2024 Feb 14. doi: 10.1007/s00228-024-03649-x.

PMID- 37823416
OWN - NLM
STAT- Publisher
LR  - 20231012
IS  - 1537-6591 (Electronic)
IS  - 1058-4838 (Linking)
DP  - 2023 Oct 12
TI  - Can Chatbot artificial intelligence replace infectious disease physicians in the 
      management of bloodstream infections? A prospective cohort study.
LID - ciad632 [pii]
LID - 10.1093/cid/ciad632 [doi]
AB  - BACKGROUND: The development of chatbot artificial intelligences (AIs) have raised 
      major questions about their use in healthcare. We assessed the quality and safety 
      of the management suggested by ChatGPT-4 in real-life practice for patients with 
      positive blood culture. METHODS: On a four-week period in a tertiary care 
      hospital, data from consecutive infectious diseases (ID) consultations for a 
      first positive blood-culture were prospectively provided to ChatGPT-4. It was 
      requested to propose a comprehensive management plan (suspected/confirmed 
      diagnosis, work-up, antibiotic therapy, source control, follow-up). We compared 
      the management plan suggested by ChatGPT-4 with the plan suggested by ID 
      consultants, based on literature and guidelines. Comparisons were performed by 
      two ID physicians not involved in the patient management. RESULTS: Forty-four 
      cases with a first episode of positive blood-culture were included. ChatGPT-4 
      provided detailed and well-written responses in all cases. AI's diagnoses were 
      identical to those of the consultant in 26 cases (59 %). Suggested diagnostic 
      workups were satisfactory (i.e., no missing important diagnostic tests) in 35 
      (80%) cases, empirical antimicrobial therapies were adequate in 28 (64%) case, 
      and harmful in one (2%) case. Source control plans were inadequate in four (9%) 
      cases. Definitive antibiotic therapies were optimal in 16 patients (36%), and 
      harmful in two (5 %). Overall, management plans were considered optimal in only 
      one patient, as satisfactory in 17 (39%), and as harmful in seven patients (16%). 
      CONCLUSIONS: The use of ChatGPT-4 without consultant input remains hazardous when 
      seeking expert medical advice in 2023, especially for severe infectious diseases.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of 
      Infectious Diseases Society of America. All rights reserved. For permissions, 
      please e-mail: journals.permissions@oup.com.
FAU - Maillard, Alexis
AU  - Maillard A
AUID- ORCID: 0000-0002-7165-4103
AD  - Paris Centre University Hospital, Infectious Diseases Stewardship team, AP-HP, 
      Paris, France.
FAU - Micheli, Giulia
AU  - Micheli G
AD  - Paris Centre University Hospital, Infectious Diseases Stewardship team, AP-HP, 
      Paris, France.
AD  - Dipartimento di Sicurezza e Bioetica - Sezione di Malattie Infettive, Università 
      Cattolica del Sacro Cuore, Rome, RM 00168 &nbsp;Italy.
FAU - Lefevre, Leila
AU  - Lefevre L
AD  - Paris Centre University Hospital, Infectious Diseases Stewardship team, AP-HP, 
      Paris, France.
FAU - Guyonnet, Céline
AU  - Guyonnet C
AD  - Paris Centre University Hospital, Microbiology department, AP-HP, Paris, France.
AD  - Université Paris Cité, Institut Cochin, INSERM, U1016, CNRS, UMR8104, Paris, 
      France.
FAU - Poyart, Claire
AU  - Poyart C
AD  - Paris Centre University Hospital, Microbiology department, AP-HP, Paris, France.
AD  - Université Paris Cité, Institut Cochin, INSERM, U1016, CNRS, UMR8104, Paris, 
      France.
FAU - Canouï, Etienne
AU  - Canouï E
AD  - Paris Centre University Hospital, Infectious Diseases Stewardship team, AP-HP, 
      Paris, France.
FAU - Belan, Martin
AU  - Belan M
AD  - Paris Centre University Hospital, Infectious Diseases Stewardship team, AP-HP, 
      Paris, France.
AD  - Université de Paris Cité, Paris, France.
FAU - Charlier, Caroline
AU  - Charlier C
AD  - Paris Centre University Hospital, Infectious Diseases Stewardship team, AP-HP, 
      Paris, France.
AD  - Université de Paris Cité, Paris, France.
AD  - Institut Pasteur, French National Reference Center and WHO Collaborating Center 
      Listeria, Paris, France.
AD  - Institut Pasteur, Biology of Infection Unit, Inserm U1117, Paris, France.
LA  - eng
PT  - Journal Article
DEP - 20231012
PL  - United States
TA  - Clin Infect Dis
JT  - Clinical infectious diseases : an official publication of the Infectious Diseases 
      Society of America
JID - 9203213
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - bacteremia
OT  - evaluation
OT  - infectious diseases
EDAT- 2023/10/12 12:43
MHDA- 2023/10/12 12:43
CRDT- 2023/10/12 06:55
PHST- 2023/05/25 00:00 [received]
PHST- 2023/09/03 00:00 [revised]
PHST- 2023/10/09 00:00 [accepted]
PHST- 2023/10/12 12:43 [medline]
PHST- 2023/10/12 12:43 [pubmed]
PHST- 2023/10/12 06:55 [entrez]
AID - 7308526 [pii]
AID - 10.1093/cid/ciad632 [doi]
PST - aheadofprint
SO  - Clin Infect Dis. 2023 Oct 12:ciad632. doi: 10.1093/cid/ciad632.

PMID- 37589944
OWN - NLM
STAT- MEDLINE
DCOM- 20240115
LR  - 20240115
IS  - 1432-5241 (Electronic)
IS  - 0364-216X (Linking)
VI  - 47
IP  - 6
DP  - 2023 Dec
TI  - Complications Following Facelift and Neck Lift: Implementation and Assessment of 
      Large Language Model and Artificial Intelligence (ChatGPT) Performance Across 16 
      Simulated Patient Presentations.
PG  - 2407-2414
LID - 10.1007/s00266-023-03538-1 [doi]
AB  - INTRODUCTION: ChatGPT represents a potential resource for patient guidance and 
      education, with the possibility for quality improvement in healthcare delivery. 
      The present study evaluates the role of ChatGPT as an interactive patient 
      resource, and assesses its performance in identifying, triaging, and guiding 
      patients with concerns of postoperative complications following facelift and neck 
      lift surgery. METHODS: Sixteen patient profiles were generated to simulate 
      postoperative patient presentations, with complications of varying acuity and 
      severity. ChatGPT was assessed for its accuracy in generating a differential 
      diagnosis, soliciting a history, providing the most-likely diagnosis, the 
      appropriate disposition, treatments/interventions to begin from home, and 
      red-flag symptoms necessitating an urgent presentation to the emergency 
      department. RESULTS: Overall accuracy in providing a complete differential 
      diagnosis in response to simulated presentations was 85%, with an accuracy of 88% 
      in identifying the most-likely diagnosis after history-taking. However, 
      appropriate patient dispositions were suggested in only 56% of cases. Relevant 
      home treatments/interventions were suggested with an 82% accuracy, and red-flag 
      symptoms with a 73% accuracy. A detailed analysis, stratified according to 
      latency of postoperative presentation (&lt;48&nbsp;h, 48&nbsp;h-1&nbsp;week, or &gt;1&nbsp;week), and 
      according to acuity of complications, is presented herein. CONCLUSIONS: ChatGPT 
      overestimated the urgency of indicated patient dispositions in 44% of cases, 
      concerning for potential unnecessary increase in healthcare resource utilization. 
      Imperfect performance, and the tool's tendency for overinclusion in its 
      responses, risk increasing patient anxiety and straining physician-patient 
      relationships. While artificial intelligence has great potential in triaging 
      postoperative patient concerns, and improving efficiency and resource 
      utilization, ChatGPT's performance, in its current form, demonstrates a need for 
      further refinement before its safe and effective implementation in facial 
      aesthetic surgical practice. LEVEL OF EVIDENCE IV: This journal requires that 
      authors assign a level of evidence to each article. For a full description of 
      these Evidence-Based Medicine ratings, please refer to the Table of Contents or 
      the online Instructions to Authors www.springer.com/00266 .
CI  - © 2023. Springer Science+Business Media, LLC, part of Springer Nature and 
      International Society of Aesthetic Plastic Surgery.
FAU - Abi-Rafeh, Jad
AU  - Abi-Rafeh J
AUID- ORCID: 0000-0002-7483-1515
AD  - Division of Plastic, Reconstructive, and Aesthetic Surgery, McGill University 
      Health Centre, Montreal, QC, Canada.
FAU - Hanna, Steven
AU  - Hanna S
AD  - Manhattan Eye, Ear and Throat Hospital, New York, NY, USA.
FAU - Bassiri-Tehrani, Brian
AU  - Bassiri-Tehrani B
AD  - Private Practice, Atlanta, GA, USA.
FAU - Kazan, Roy
AU  - Kazan R
AD  - Division of Plastic and Reconstructive Surgery, University of Pittsburgh, 
      Pittsburgh, PA, USA.
FAU - Nahai, Foad
AU  - Nahai F
AD  - Former Maurice J. Jurkiewicz Chair and Professor of Plastic Surgery, Department 
      of Surgery, Emory University, Atlanta, GA, USA. nahaimd@aol.com.
LA  - eng
PT  - Journal Article
DEP - 20230817
PL  - United States
TA  - Aesthetic Plast Surg
JT  - Aesthetic plastic surgery
JID - 7701756
SB  - IM
EIN - Aesthetic Plast Surg. 2023 Dec 11;:. PMID: 38078915
MH  - Humans
MH  - *Rhytidoplasty/adverse effects
MH  - Artificial Intelligence
MH  - Neck/surgery
MH  - Postoperative Complications/diagnosis
MH  - Face
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Complications
OT  - Facelift
EDAT- 2023/08/17 12:42
MHDA- 2024/01/15 12:43
CRDT- 2023/08/17 11:16
PHST- 2023/06/25 00:00 [received]
PHST- 2023/07/19 00:00 [accepted]
PHST- 2024/01/15 12:43 [medline]
PHST- 2023/08/17 12:42 [pubmed]
PHST- 2023/08/17 11:16 [entrez]
AID - 10.1007/s00266-023-03538-1 [pii]
AID - 10.1007/s00266-023-03538-1 [doi]
PST - ppublish
SO  - Aesthetic Plast Surg. 2023 Dec;47(6):2407-2414. doi: 10.1007/s00266-023-03538-1. 
      Epub 2023 Aug 17.

PMID- 38103488
OWN - NLM
STAT- MEDLINE
DCOM- 20240315
LR  - 20240315
IS  - 1532-818X (Electronic)
IS  - 0196-0709 (Linking)
VI  - 45
IP  - 2
DP  - 2024 Mar-Apr
TI  - A large language model's assessment of methodology reporting in head and neck 
      surgery.
PG  - 104145
LID - S0196-0709(23)00359-9 [pii]
LID - 10.1016/j.amjoto.2023.104145 [doi]
AB  - OBJECTIVE: The aim of this study was to assess the ability of a Large Language 
      Model - ChatGPT 3.5 to appraise the quality of scientific methodology reporting 
      in head and neck specific scientific literature. METHODS: Authors asked ChatGPT 
      3.5 to create a grading system for scientific reporting of research methods. The 
      language model produced a system with a max of 60 points. Individual scores were 
      provided for Study Design and Description, Data Collection and Measurement, 
      Statistical Analysis, Ethical Considerations, and Overall Clarity and 
      Transparency. Twenty articles were selected at random from The American Head and 
      Neck Society's (AHNS) fellowship curriculum 2.0 for interrogation and each 
      'Methods' section was input into ChatGPT 3.5 for scoring. Analysis of variance 
      (ANOVA) was performed between different scoring categories and a post-hoc tukey 
      HSD test was performed. RESULTS: Twenty articles were assessed, eight were 
      categorized as very good and nine as good based on cumulative score. Lowest mean 
      score was noted with category of statistical analysis (Mean&nbsp;=&nbsp;0.49, SD&nbsp;=&nbsp;0.02). 
      On ANOVA a significant difference between means of the different scoring 
      categories was noted, F(4, 95)&nbsp;=&nbsp;13.4, p&nbsp;≤&nbsp;0.05. On post-hoc Tukey HSD test, mean 
      scores for categories of data collection (Mean&nbsp;=&nbsp;0.58, SD&nbsp;=&nbsp;0.06) and statistical 
      analysis (Mean&nbsp;=&nbsp;0.49, SD&nbsp;=&nbsp;0.02) were significantly lower when compared to other 
      categories. CONCLUSION: This article showcases the feasibility of employing a 
      large language model such as ChatGPT 3.5 to assess the methods sections in head 
      and neck academic writing.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Dang, Rushil
AU  - Dang R
AD  - Maxillofacial Oncology and Reconstructive Surgery, Department of Oral and 
      Maxillofacial surgery, Boston Medical Center, Boston, MA, USA.
FAU - Hanba, Curtis
AU  - Hanba C
AD  - Department of Head and Neck Surgery, The University of Texas MD Anderson Cancer 
      Center, Houston, TX, USA. Electronic address: curtis.j.hanba@gmail.com.
LA  - eng
PT  - Journal Article
DEP - 20231206
PL  - United States
TA  - Am J Otolaryngol
JT  - American journal of otolaryngology
JID - 8000029
SB  - IM
MH  - Humans
MH  - *Research Design
MH  - Analysis of Variance
MH  - *Curriculum
MH  - Head
MH  - Language
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - Head and neck
OT  - Large language model
COIS- Declaration of competing interest None.
EDAT- 2023/12/17 09:44
MHDA- 2024/03/15 06:43
CRDT- 2023/12/16 18:11
PHST- 2023/11/22 00:00 [received]
PHST- 2023/12/03 00:00 [accepted]
PHST- 2024/03/15 06:43 [medline]
PHST- 2023/12/17 09:44 [pubmed]
PHST- 2023/12/16 18:11 [entrez]
AID - S0196-0709(23)00359-9 [pii]
AID - 10.1016/j.amjoto.2023.104145 [doi]
PST - ppublish
SO  - Am J Otolaryngol. 2024 Mar-Apr;45(2):104145. doi: 10.1016/j.amjoto.2023.104145. 
      Epub 2023 Dec 6.

PMID- 37660470
OWN - NLM
STAT- MEDLINE
DCOM- 20230905
LR  - 20231013
IS  - 2164-554X (Electronic)
IS  - 2164-5515 (Print)
IS  - 2164-5515 (Linking)
VI  - 19
IP  - 2
DP  - 2023 Aug 1
TI  - Chatting with ChatGPT to learn about safety of COVID-19 vaccines - A perspective.
PG  - 2235200
LID - 10.1080/21645515.2023.2235200 [doi]
LID - 2235200
AB  - Vaccine hesitancy is among the top 10 threats to global health, according to the 
      World Health Organization (WHO). In this exploration, we delve into ChatGPT 
      capacity to generate opinions on vaccine hesitancy by interrogating this AI 
      chatbot for the 50 most prevalent counterfait messages, false and true 
      contraindications, and myths circulating on the internet regarding vaccine 
      safety. Our results indicate that, while the present version of ChatGPT's default 
      responses may be incomplete, they are generally satisfactory. Although ChatGPT 
      cannot substitute an expert or the scientific evidence itself, this form of AI 
      has the potential to guide users toward information that aligns well with 
      scientific evidence.
FAU - Salas, Antonio
AU  - Salas A
AUID- ORCID: 0000-0002-2336-702X
AD  - Unidade de Xenética, Instituto de Ciencias Forenses, Facultade de Medicina, 
      Universidade de Santiago de Compostela, and GenPoB Research Group, Instituto de 
      Investigación Sanitaria (IDIS), Hospital Clínico Universitario de Santiago 
      (SERGAS), Santiago de Compostela, Spain.
AD  - Genetics, Vaccines and Infections Research Group (GENVIP), Instituto de 
      Investigación Sanitaria de Santiago, Universidade de Santiago de Compostela, 
      Santiago de Compostela, Spain.
AD  - Centro de Investigación Biomédica en Red de Enfermedades Respiratorias 
      (CIBER-ES), Madrid, Spain.
FAU - Rivero-Calle, Irene
AU  - Rivero-Calle I
AUID- ORCID: 0000-0002-3678-9264
AD  - Genetics, Vaccines and Infections Research Group (GENVIP), Instituto de 
      Investigación Sanitaria de Santiago, Universidade de Santiago de Compostela, 
      Santiago de Compostela, Spain.
AD  - Centro de Investigación Biomédica en Red de Enfermedades Respiratorias 
      (CIBER-ES), Madrid, Spain.
AD  - WHO Collaborating Centre for Vaccine Safety of Santiago de Compostela, Servizo 
      Galego de Saude, Santiago de Compostela, Spain.
AD  - Translational Pediatrics and Infectious Diseases, Department of Pediatrics, 
      Hospital Clínico Universitario de Santiago de Compostela, Santiago de Compostela, 
      Spain.
FAU - Martinón-Torres, Federico
AU  - Martinón-Torres F
AUID- ORCID: 0000-0002-9023-581X
AD  - Genetics, Vaccines and Infections Research Group (GENVIP), Instituto de 
      Investigación Sanitaria de Santiago, Universidade de Santiago de Compostela, 
      Santiago de Compostela, Spain.
AD  - Centro de Investigación Biomédica en Red de Enfermedades Respiratorias 
      (CIBER-ES), Madrid, Spain.
AD  - WHO Collaborating Centre for Vaccine Safety of Santiago de Compostela, Servizo 
      Galego de Saude, Santiago de Compostela, Spain.
AD  - Translational Pediatrics and Infectious Diseases, Department of Pediatrics, 
      Hospital Clínico Universitario de Santiago de Compostela, Santiago de Compostela, 
      Spain.
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PL  - United States
TA  - Hum Vaccin Immunother
JT  - Human vaccines &amp; immunotherapeutics
JID - 101572652
RN  - 0 (COVID-19 Vaccines)
SB  - IM
CIN - Hum Vaccin Immunother. 2023 Aug;19(2):2263982. PMID: 37799088
MH  - Humans
MH  - *COVID-19 Vaccines/adverse effects
MH  - *COVID-19/prevention &amp; control
MH  - Internet
MH  - Software
MH  - Vaccination Hesitancy
PMC - PMC10478732
OTO - NOTNLM
OT  - COVID-19
OT  - ChatGPT
OT  - artificial intelligence
OT  - chatbot
OT  - misinformation
OT  - vaccine safety
COIS- AS declares no competing interests. IRC has participated in advisory boards 
      organized by MSD, GSK, Sanofi and Pfizer. IRC has been involved in clinical 
      trials funded by Ablynx, Abbot, Seqirus, Sanofi Pasteur MSD, Cubist, Wyeth, 
      Merck, Pfizer, Roche, Regeneron, Jansen, Medimmune, Novavax, Novartis and GSK, 
      although the funds were awarded to the institution. FM-T has received honoraria 
      from GSK, Pfizer Inc, Moderna, Astra Zeneca, Sanofi Pasteur, MSD, Seqirus, 
      Biofabri and Janssen for taking part in advisory boards and expert meetings and 
      for acting as a speaker in congresses outside the scope of the submitted work. 
      Federico Martinón-Torres has also acted as principal investigator in randomized 
      controlled trials of the above-mentioned companies as well as Ablynx, Gilead, 
      Regeneron, Roche, Abbott, Novavax, and MedImmune, with honoraria paid to his 
      institution.
EDAT- 2023/09/04 00:41
MHDA- 2023/09/05 06:41
PMCR- 2023/09/03
CRDT- 2023/09/03 18:03
PHST- 2023/09/05 06:41 [medline]
PHST- 2023/09/04 00:41 [pubmed]
PHST- 2023/09/03 18:03 [entrez]
PHST- 2023/09/03 00:00 [pmc-release]
AID - 2235200 [pii]
AID - 10.1080/21645515.2023.2235200 [doi]
PST - ppublish
SO  - Hum Vaccin Immunother. 2023 Aug 1;19(2):2235200. doi: 
      10.1080/21645515.2023.2235200.

PMID- 37467871
OWN - NLM
STAT- MEDLINE
DCOM- 20231023
LR  - 20231025
IS  - 1558-349X (Electronic)
IS  - 1546-1440 (Linking)
VI  - 20
IP  - 9
DP  - 2023 Sep
TI  - Exploring the Clinical Translation of Generative Models Like ChatGPT: Promise and 
      Pitfalls in Radiology, From Patients to Population Health.
PG  - 877-885
LID - S1546-1440(23)00516-1 [pii]
LID - 10.1016/j.jacr.2023.07.007 [doi]
AB  - Generative artificial intelligence (AI) tools such as GPT-4, and the chatbot 
      interface ChatGPT, show promise for a variety of applications in radiology and 
      health care. However, like other AI tools, ChatGPT has limitations and potential 
      pitfalls that must be considered before adopting it for teaching, clinical 
      practice, and beyond. We summarize five major emerging use cases for ChatGPT and 
      generative AI in radiology across the levels of increasing data complexity, along 
      with pitfalls associated with each. As the use of AI in health care continues to 
      grow, it is crucial for radiologists (and all physicians) to stay informed and 
      ensure the safe translation of these new technologies.
CI  - Copyright © 2023 American College of Radiology. Published by Elsevier Inc. All 
      rights reserved.
FAU - Doo, Florence X
AU  - Doo FX
AD  - Director of Innovation, University of Maryland Medical Intelligent Imaging Center 
      (UM2ii), Baltimore, Maryland; Member, Committee on Economics in Academic 
      Radiology, under the ACR Commission on Economics. Electronic address: 
      fdoo@som.umaryland.edu.
FAU - Cook, Tessa S
AU  - Cook TS
AD  - Vice Chair for Practice Transformation, Department of Radiology, Perelman School 
      of Medicine at the University of Pennsylvania, Philadelphia, Pennsylvania; 
      Fellowship Director, Imaging Informatics, and Chief, 3-D and Advanced Imaging, 
      Department of Radiology, Penn Medicine, Philadelphia, Pennsylvania; Chair, 
      Society for Imaging Informatics in Medicine; and Vice Chair, ACR Commission on 
      Patient- and Family-Centered Care; Chair, RAHSR Affinity Group. Electronic 
      address: https://twitter.com/asset25.
FAU - Siegel, Eliot L
AU  - Siegel EL
AD  - Vice Chair, Research Information Systems, University of Maryland, Baltimore, 
      Maryland; Lead, Radiology and Nuclear Medicine Diagnostics, US Department of 
      Veterans Affairs Veterans Integrated Services Network; Chief, Imaging, US 
      Department of Veterans Affairs Maryland Healthcare System; Radiology AI Senior 
      Consultant. Electronic address: https://twitter.com/EliotSiegel.
FAU - Joshi, Anupam
AU  - Joshi A
AD  - Oros Family Professor and Chair, Computer Science and Electrical Engineering, 
      University of Maryland, Baltimore, Maryland; Director, University of Maryland, 
      Baltimore County, Center for Cybersecurity; Director, CyberScholars Program; 
      Associate Editor, IEEE Transactions on Dependable and Secure Computing.
FAU - Parekh, Vishwa
AU  - Parekh V
AD  - Technical Director, University of Maryland Medical Intelligent Imaging (UM2ii) 
      Center, Baltimore, Maryland; Review Editor, Frontiers in Oncology. Electronic 
      address: https://twitter.com/vishwa_parekh.
FAU - Elahi, Ameena
AU  - Elahi A
AD  - University of Pennsylvania, Philadelphia, Pennsylvania; Application Manager, 
      Information Services, Penn Medicine, Philadelphia, Pennsylvania; Informatics 
      Operations Director, RAD-AID International. Electronic address: 
      https://twitter.com/AmeenaElahi.
FAU - Yi, Paul H
AU  - Yi PH
AD  - Director, University of Maryland Medical Intelligent Imaging (UM2ii) Center, 
      Baltimore, Maryland; Vice Chair, Society of Imaging Informatics in Medicine 
      Program Planning Committee; Associate Editor, Radiology: Artificial Intelligence. 
      Electronic address: https://twitter.com/PaulYiMD.
LA  - eng
PT  - Journal Article
DEP - 20230717
PL  - United States
TA  - J Am Coll Radiol
JT  - Journal of the American College of Radiology : JACR
JID - 101190326
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Radiology
MH  - Radiography
MH  - Radiologists
MH  - *Population Health
OTO - NOTNLM
OT  - ChatGPT
OT  - generative artificial intelligence
OT  - large language models
OT  - limitations
OT  - radiology
EDAT- 2023/07/20 01:07
MHDA- 2023/10/23 00:42
CRDT- 2023/07/19 19:23
PHST- 2023/04/24 00:00 [received]
PHST- 2023/06/22 00:00 [revised]
PHST- 2023/07/05 00:00 [accepted]
PHST- 2023/10/23 00:42 [medline]
PHST- 2023/07/20 01:07 [pubmed]
PHST- 2023/07/19 19:23 [entrez]
AID - S1546-1440(23)00516-1 [pii]
AID - 10.1016/j.jacr.2023.07.007 [doi]
PST - ppublish
SO  - J Am Coll Radiol. 2023 Sep;20(9):877-885. doi: 10.1016/j.jacr.2023.07.007. Epub 
      2023 Jul 17.

PMID- 37931432
OWN - NLM
STAT- MEDLINE
DCOM- 20231124
LR  - 20231124
IS  - 1872-8243 (Electronic)
IS  - 1386-5056 (Linking)
VI  - 180
DP  - 2023 Dec
TI  - Influence on the accuracy in ChatGPT: Differences in the amount of information 
      per medical field.
PG  - 105283
LID - S1386-5056(23)00301-5 [pii]
LID - 10.1016/j.ijmedinf.2023.105283 [doi]
AB  - OBJECTIVES: Although ChatGPT was not developed for medical use, there is growing 
      interest in its use in medical fields. Understanding its capabilities and 
      precautions for its use in the medical field is an urgent matter. We hypothesized 
      that differences in the amounts of information published in different medical 
      fields would be proportionate to the amounts of training ChatGPT receives in 
      those fields, and hence its accuracy in providing answers. STUDY DESIGN: A 
      non-clinical experimental study. METHODS: We administered the Japanese National 
      Medical Examination to GPT-3.5 and GPT-4 to examine the rates of accuracy and 
      consistency in their responses. We counted the total number of documents in the 
      Web of Science Core Collection per medical field and assessed the relationship 
      with ChatGPT's accuracy. We also performed multivariate-adjusted models to 
      investigate the risk factors for incorrect answers. RESULTS: For GPT-4, we 
      confirmed an accuracy rate of 81.0&nbsp;% and a consistency rate of 88.8&nbsp;% on the 
      exam; both showed improvement compared to those for GPT-3.5. A positive 
      correlation was observed between the accuracy rate and consistency rate 
      (R&nbsp;=&nbsp;0.51, P&nbsp;&lt;&nbsp;0.001). The number of documents per medical field was 
      significantly correlated with the accuracy rate in that medical field (R&nbsp;=&nbsp;0.44, 
      P&nbsp;&lt;&nbsp;0.05), with relatively few publications being an independent risk factor for 
      incorrect answers. CONCLUSIONS: Checking consistency may help identify incorrect 
      answers when using ChatGPT. Users should be aware that the accuracy of the 
      answers by ChatGPT may decrease when it is asked about topics with limited 
      published information, such as new drugs and diseases.
CI  - Copyright © 2023 Elsevier B.V. All rights reserved.
FAU - Haze, Tatsuya
AU  - Haze T
AD  - Department of Medical Science and Cardiorenal Medicine, Yokohama City University 
      Graduate School of Medicine, Yokohama, Japan; Department of Nephrology and 
      Hypertension, Yokohama City University Medical Center, Yokohama, Japan; YCU 
      Center for Novel and Exploratory Clinical Trials (Y-NEXT), Yokohama City 
      University Hospital, Yokohama, Japan. Electronic address: 
      haze.tat.bg@yokohama-cu.ac.jp.
FAU - Kawano, Rina
AU  - Kawano R
AD  - Department of Medical Science and Cardiorenal Medicine, Yokohama City University 
      Graduate School of Medicine, Yokohama, Japan; Department of Nephrology and 
      Hypertension, Yokohama City University Medical Center, Yokohama, Japan.
FAU - Takase, Hajime
AU  - Takase H
AD  - YCU Center for Novel and Exploratory Clinical Trials (Y-NEXT), Yokohama City 
      University Hospital, Yokohama, Japan; Department of Neurosurgery, Yokohama City 
      University Graduate School of Medicine, Yokohama, Japan.
FAU - Suzuki, Shota
AU  - Suzuki S
AD  - Department of Medical Science and Cardiorenal Medicine, Yokohama City University 
      Graduate School of Medicine, Yokohama, Japan; Department of Nephrology and 
      Hypertension, Yokohama City University Medical Center, Yokohama, Japan.
FAU - Hirawa, Nobuhito
AU  - Hirawa N
AD  - Department of Medical Science and Cardiorenal Medicine, Yokohama City University 
      Graduate School of Medicine, Yokohama, Japan; Department of Nephrology and 
      Hypertension, Yokohama City University Medical Center, Yokohama, Japan; Clinical 
      Education and Training Center, Yokohama City University Medical Center, Yokohama, 
      Japan.
FAU - Tamura, Kouichi
AU  - Tamura K
AD  - Department of Medical Science and Cardiorenal Medicine, Yokohama City University 
      Graduate School of Medicine, Yokohama, Japan.
LA  - eng
PT  - Journal Article
DEP - 20231103
PL  - Ireland
TA  - Int J Med Inform
JT  - International journal of medical informatics
JID - 9711057
SB  - IM
MH  - Humans
MH  - Risk Factors
MH  - *Artificial Intelligence
MH  - Data Accuracy
OTO - NOTNLM
OT  - Accuracy
OT  - Artificial Intelligence
OT  - ChatGPT
OT  - Consistency
OT  - Risk factor
COIS- Declaration of competing interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2023/11/07 00:42
MHDA- 2023/11/20 06:55
CRDT- 2023/11/06 18:06
PHST- 2023/06/08 00:00 [received]
PHST- 2023/10/30 00:00 [revised]
PHST- 2023/10/31 00:00 [accepted]
PHST- 2023/11/20 06:55 [medline]
PHST- 2023/11/07 00:42 [pubmed]
PHST- 2023/11/06 18:06 [entrez]
AID - S1386-5056(23)00301-5 [pii]
AID - 10.1016/j.ijmedinf.2023.105283 [doi]
PST - ppublish
SO  - Int J Med Inform. 2023 Dec;180:105283. doi: 10.1016/j.ijmedinf.2023.105283. Epub 
      2023 Nov 3.

PMID- 37277096
OWN - NLM
STAT- MEDLINE
DCOM- 20231009
LR  - 20231018
IS  - 2468-6530 (Electronic)
IS  - 2468-6530 (Linking)
VI  - 7
IP  - 10
DP  - 2023 Oct
TI  - Appropriateness and Readability of ChatGPT-4-Generated Responses for Surgical 
      Treatment of Retinal Diseases.
PG  - 862-868
LID - S2468-6530(23)00246-4 [pii]
LID - 10.1016/j.oret.2023.05.022 [doi]
AB  - OBJECTIVE: To evaluate the appropriateness and readability of the medical 
      knowledge provided by ChatGPT-4, an artificial intelligence-powered 
      conversational search engine, regarding common vitreoretinal surgeries for 
      retinal detachments (RDs), macular holes (MHs), and epiretinal membranes (ERMs). 
      DESIGN: Retrospective cross-sectional study. SUBJECTS: This study did not involve 
      any human participants. METHODS: We created lists of common questions about the 
      definition, prevalence, visual impact, diagnostic methods, surgical and 
      nonsurgical treatment options, postoperative information, surgery-related 
      complications, and visual prognosis of RD, MH, and ERM, and asked each question 3 
      times on the online ChatGPT-4 platform. The data for this cross-sectional study 
      were recorded on April 25, 2023. Two independent retina specialists graded the 
      appropriateness of the responses. Readability was assessed using Readable, an 
      online readability tool. MAIN OUTCOME MEASURES: The "appropriateness" and 
      "readability" of the answers generated by ChatGPT-4 bot. RESULTS: Responses were 
      consistently appropriate in 84.6% (33/39), 92% (23/25), and 91.7% (22/24) of the 
      questions related to RD, MH, and ERM, respectively. Answers were inappropriate at 
      least once in 5.1% (2/39), 8% (2/25), and 8.3% (2/24) of the respective 
      questions. The average Flesch Kincaid Grade Level and Flesch Reading Ease Score 
      were 14.1 ± 2.6 and 32.3 ± 10.8 for RD, 14 ± 1.3 and 34.4 ± 7.7 for MH, and 14.8 
      ± 1.3 and 28.1 ± 7.5 for ERM. These scores indicate that the answers are 
      difficult or very difficult to read for the average lay person and college 
      graduation would be required to understand the material. CONCLUSIONS: Most of the 
      answers provided by ChatGPT-4 were consistently appropriate. However, ChatGPT and 
      other natural language models in their current form are not a source of factual 
      information. Improving the credibility and readability of responses, especially 
      in specialized fields, such as medicine, is a critical focus of research. 
      Patients, physicians, and laypersons should be advised of the limitations of 
      these tools for eye- and health-related counseling. FINANCIAL DISCLOSURE(S): 
      Proprietary or commercial disclosure may be found in the Footnotes and 
      Disclosures at the end of this article.
CI  - Copyright © 2023 American Academy of Ophthalmology. Published by Elsevier Inc. 
      All rights reserved.
FAU - Momenaei, Bita
AU  - Momenaei B
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Wakabayashi, Taku
AU  - Wakabayashi T
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Shahlaee, Abtin
AU  - Shahlaee A
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Durrani, Asad F
AU  - Durrani AF
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Pandit, Saagar A
AU  - Pandit SA
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Wang, Kristine
AU  - Wang K
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Mansour, Hana A
AU  - Mansour HA
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Abishek, Robert M
AU  - Abishek RM
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Xu, David
AU  - Xu D
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Sridhar, Jayanth
AU  - Sridhar J
AD  - Bascom Palmer Eye Institute, University of Miami Miller School of Medicine, 
      Miami, Florida.
FAU - Yonekawa, Yoshihiro
AU  - Yonekawa Y
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania.
FAU - Kuriyan, Ajay E
AU  - Kuriyan AE
AD  - Wills Eye Hospital, Mid Atlantic Retina, Thomas Jefferson University, 
      Philadelphia, Pennsylvania. Electronic address: ajay.kuriyan@gmail.com.
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20230603
PL  - United States
TA  - Ophthalmol Retina
JT  - Ophthalmology. Retina
JID - 101695048
SB  - IM
CIN - Ophthalmol Retina. 2023 Oct;7(10):e15. PMID: 37379882
MH  - Humans
MH  - *Health Literacy
MH  - Comprehension
MH  - Cross-Sectional Studies
MH  - Artificial Intelligence
MH  - Retrospective Studies
MH  - *Retinal Diseases/surgery
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Readability
OT  - Retina
OT  - Vitrectomy
EDAT- 2023/06/06 01:11
MHDA- 2023/10/09 06:41
CRDT- 2023/06/05 19:24
PHST- 2023/03/28 00:00 [received]
PHST- 2023/05/26 00:00 [revised]
PHST- 2023/05/30 00:00 [accepted]
PHST- 2023/10/09 06:41 [medline]
PHST- 2023/06/06 01:11 [pubmed]
PHST- 2023/06/05 19:24 [entrez]
AID - S2468-6530(23)00246-4 [pii]
AID - 10.1016/j.oret.2023.05.022 [doi]
PST - ppublish
SO  - Ophthalmol Retina. 2023 Oct;7(10):862-868. doi: 10.1016/j.oret.2023.05.022. Epub 
      2023 Jun 3.

PMID- 37758604
OWN - NLM
STAT- Publisher
LR  - 20230927
IS  - 1535-6302 (Electronic)
IS  - 0363-0188 (Linking)
DP  - 2023 Aug 30
TI  - Radiology Reading Room for the Future: Harnessing the Power of Large Language 
      Models Like ChatGPT.
LID - S0363-0188(23)00133-0 [pii]
LID - 10.1067/j.cpradiol.2023.08.018 [doi]
AB  - Radiology has usually been the field of medicine that has been at the forefront 
      of technological advances, often being the first to wholeheartedly embrace them. 
      Whether it's from digitization to cloud side architecture, radiology has led the 
      way for adopting the latest advances. With the advent of large language models 
      (LLMs), especially with the unprecedented explosion of freely available ChatGPT, 
      time is ripe for radiology and radiologists to find novel ways to use the 
      technology to improve their workflow. Towards this, we believe these LLMs have a 
      key role in the radiology reading room not only to expedite processes, simplify 
      mundane and archaic tasks, but also to increase the radiologist's and radiologist 
      trainee's knowledge base at a far faster pace. In this article, we discuss some 
      of the ways we believe ChatGPT, and the likes can be harnessed in the reading 
      room.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Tippareddy, Charit
AU  - Tippareddy C
AD  - Department of Radiology, University Hospitals Cleveland Medical Center, 
      Cleveland, OH.
FAU - Jiang, Sirui
AU  - Jiang S
AD  - Department of Radiology, University Hospitals Cleveland Medical Center, 
      Cleveland, OH.
FAU - Bera, Kaustav
AU  - Bera K
AD  - Department of Radiology, University Hospitals Cleveland Medical Center, 
      Cleveland, OH. Electronic address: kxb413@case.edu.
FAU - Ramaiya, Nikhil
AU  - Ramaiya N
AD  - Department of Radiology, University Hospitals Cleveland Medical Center, 
      Cleveland, OH.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20230830
PL  - United States
TA  - Curr Probl Diagn Radiol
JT  - Current problems in diagnostic radiology
JID - 7607123
SB  - IM
EDAT- 2023/09/28 00:42
MHDA- 2023/09/28 00:42
CRDT- 2023/09/27 21:56
PHST- 2023/08/09 00:00 [received]
PHST- 2023/08/28 00:00 [revised]
PHST- 2023/08/28 00:00 [accepted]
PHST- 2023/09/28 00:42 [medline]
PHST- 2023/09/28 00:42 [pubmed]
PHST- 2023/09/27 21:56 [entrez]
AID - S0363-0188(23)00133-0 [pii]
AID - 10.1067/j.cpradiol.2023.08.018 [doi]
PST - aheadofprint
SO  - Curr Probl Diagn Radiol. 2023 Aug 30:S0363-0188(23)00133-0. doi: 
      10.1067/j.cpradiol.2023.08.018.

PMID- 37099373
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230513
IS  - 2369-3762 (Print)
IS  - 2369-3762 (Electronic)
IS  - 2369-3762 (Linking)
VI  - 9
DP  - 2023 Apr 26
TI  - Performance of ChatGPT on UK Standardized Admission Tests: Insights From the 
      BMAT, TMUA, LNAT, and TSA Examinations.
PG  - e47737
LID - 10.2196/47737 [doi]
LID - e47737
AB  - BACKGROUND: Large language models, such as ChatGPT by OpenAI, have demonstrated 
      potential in various applications, including medical education. Previous studies 
      have assessed ChatGPT's performance in university or professional settings. 
      However, the model's potential in the context of standardized admission tests 
      remains unexplored. OBJECTIVE: This study evaluated ChatGPT's performance on 
      standardized admission tests in the United Kingdom, including the BioMedical 
      Admissions Test (BMAT), Test of Mathematics for University Admission (TMUA), Law 
      National Aptitude Test (LNAT), and Thinking Skills Assessment (TSA), to 
      understand its potential as an innovative tool for education and test 
      preparation. METHODS: Recent public resources (2019-2022) were used to compile a 
      data set of 509 questions from the BMAT, TMUA, LNAT, and TSA covering diverse 
      topics in aptitude, scientific knowledge and applications, mathematical thinking 
      and reasoning, critical thinking, problem-solving, reading comprehension, and 
      logical reasoning. This evaluation assessed ChatGPT's performance using the 
      legacy GPT-3.5 model, focusing on multiple-choice questions for consistency. The 
      model's performance was analyzed based on question difficulty, the proportion of 
      correct responses when aggregating exams from all years, and a comparison of test 
      scores between papers of the same exam using binomial distribution and 
      paired-sample (2-tailed) t tests. RESULTS: The proportion of correct responses 
      was significantly lower than incorrect ones in BMAT section 2 (P&lt;.001) and TMUA 
      paper 1 (P&lt;.001) and paper 2 (P&lt;.001). No significant differences were observed 
      in BMAT section 1 (P=.2), TSA section 1 (P=.7), or LNAT papers 1 and 2, section A 
      (P=.3). ChatGPT performed better in BMAT section 1 than section 2 (P=.047), with 
      a maximum candidate ranking of 73% compared to a minimum of 1%. In the TMUA, it 
      engaged with questions but had limited accuracy and no performance difference 
      between papers (P=.6), with candidate rankings below 10%. In the LNAT, it 
      demonstrated moderate success, especially in paper 2's questions; however, 
      student performance data were unavailable. TSA performance varied across years 
      with generally moderate results and fluctuating candidate rankings. Similar 
      trends were observed for easy to moderate difficulty questions (BMAT section 1, 
      P=.3; BMAT section 2, P=.04; TMUA paper 1, P&lt;.001; TMUA paper 2, P=.003; TSA 
      section 1, P=.8; and LNAT papers 1 and 2, section A, P&gt;.99) and hard to 
      challenging ones (BMAT section 1, P=.7; BMAT section 2, P&lt;.001; TMUA paper 1, 
      P=.007; TMUA paper 2, P&lt;.001; TSA section 1, P=.3; and LNAT papers 1 and 2, 
      section A, P=.2). CONCLUSIONS: ChatGPT shows promise as a supplementary tool for 
      subject areas and test formats that assess aptitude, problem-solving and critical 
      thinking, and reading comprehension. However, its limitations in areas such as 
      scientific and mathematical knowledge and applications highlight the need for 
      continuous development and integration with conventional learning strategies in 
      order to fully harness its potential.
CI  - ©Panagiotis Giannos, Orestis Delardas. Originally published in JMIR Medical 
      Education (https://mededu.jmir.org), 26.04.2023.
FAU - Giannos, Panagiotis
AU  - Giannos P
AUID- ORCID: 0000-0003-1037-1983
AD  - Department of Life Sciences, Faculty of Natural Sciences, Imperial College 
      London, London, United Kingdom.
AD  - Promotion of Emerging and Evaluative Research Society, London, United Kingdom.
FAU - Delardas, Orestis
AU  - Delardas O
AUID- ORCID: 0000-0003-2835-0400
AD  - Promotion of Emerging and Evaluative Research Society, London, United Kingdom.
LA  - eng
PT  - Journal Article
DEP - 20230426
PL  - Canada
TA  - JMIR Med Educ
JT  - JMIR medical education
JID - 101684518
PMC - PMC10173042
OTO - NOTNLM
OT  - BMAT
OT  - ChatGPT
OT  - GPT
OT  - LNAT
OT  - TMUA
OT  - TSA
OT  - law
OT  - medical education
OT  - medicine
OT  - natural language processing
OT  - standardized admissions tests
COIS- Conflicts of Interest: None declared.
EDAT- 2023/04/26 12:42
MHDA- 2023/04/26 12:43
PMCR- 2023/04/26
CRDT- 2023/04/26 11:54
PHST- 2023/03/30 00:00 [received]
PHST- 2023/04/09 00:00 [accepted]
PHST- 2023/04/09 00:00 [revised]
PHST- 2023/04/26 12:43 [medline]
PHST- 2023/04/26 12:42 [pubmed]
PHST- 2023/04/26 11:54 [entrez]
PHST- 2023/04/26 00:00 [pmc-release]
AID - v9i1e47737 [pii]
AID - 10.2196/47737 [doi]
PST - epublish
SO  - JMIR Med Educ. 2023 Apr 26;9:e47737. doi: 10.2196/47737.

PMID- 38354991
OWN - NLM
STAT- Publisher
LR  - 20240311
IS  - 1873-2607 (Electronic)
IS  - 0749-3797 (Linking)
DP  - 2024 Feb 12
TI  - Accuracy of Online Artificial Intelligence Models in Primary Care Settings.
LID - S0749-3797(24)00060-6 [pii]
LID - 10.1016/j.amepre.2024.02.006 [doi]
AB  - INTRODUCTION: The importance of preventive medicine and primary care in the 
      sphere of public health is expanding, yet a gap exists in the utilization of 
      recommended medical services. As patients increasingly turn to online resources 
      for supplementary advice, the role of artificial intelligence (AI) in providing 
      accurate and reliable information has emerged. The present study aimed to assess 
      ChatGPT-4's and Google Bard's capacity to deliver accurate recommendations in 
      preventive medicine and primary care. METHODS: Fifty-six questions were 
      formulated and presented to ChatGPT-4 in June 2023 and Google Bard in October 
      2023, and the responses were independently reviewed by two physicians, with each 
      answer being classified as "accurate," "inaccurate," or "accurate with missing 
      information." Disagreements were resolved by a third physician. RESULTS: Initial 
      inter-reviewer agreement on grading was substantial (Cohen's Kappa was 0.76, 
      95%CI [0.61-0.90] for ChatGPT-4 and 0.89, 95%CI [0.79-0.99] for Bard). After 
      reaching a consensus, 28.6% of ChatGPT-4-generated answers were deemed accurate, 
      28.6% inaccurate, and 42.8% accurate with missing information. In comparison, 
      53.6% of Bard-generated answers were deemed accurate, 17.8% inaccurate, and 28.6% 
      accurate with missing information. Responses to CDC and immunization-related 
      questions showed notable inaccuracies (80%) in both models. CONCLUSIONS: 
      ChatGPT-4 and Bard demonstrated potential in offering accurate information in 
      preventive care. It also brought to light the critical need for regular updates, 
      particularly in the rapidly evolving areas of medicine. A significant proportion 
      of the AI models' responses were deemed "accurate with missing information," 
      emphasizing the importance of viewing AI tools as complementary resources when 
      seeking medical information.
CI  - Copyright © 2024 American Journal of Preventive Medicine. Published by Elsevier 
      Inc. All rights reserved.
FAU - Kassab, Joseph
AU  - Kassab J
AD  - Research Institute, Cleveland Clinic Foundation, Cleveland, Ohio.
FAU - Hadi El Hajjar, Abdel
AU  - Hadi El Hajjar A
AD  - Department of Internal Medicine, Cleveland Clinic Foundation, Cleveland, Ohio.
FAU - Wardrop, Richard M 3rd
AU  - Wardrop RM 3rd
AD  - Department of Internal Medicine, Cleveland Clinic Foundation, Cleveland, Ohio.
FAU - Brateanu, Andrei
AU  - Brateanu A
AD  - Department of Internal Medicine, Cleveland Clinic Foundation, Cleveland, Ohio. 
      Electronic address: abratean@ccf.org.
LA  - eng
PT  - Journal Article
DEP - 20240212
PL  - Netherlands
TA  - Am J Prev Med
JT  - American journal of preventive medicine
JID - 8704773
SB  - IM
EDAT- 2024/02/15 00:42
MHDA- 2024/02/15 00:42
CRDT- 2024/02/14 19:18
PHST- 2023/07/25 00:00 [received]
PHST- 2024/02/06 00:00 [revised]
PHST- 2024/02/07 00:00 [accepted]
PHST- 2024/02/15 00:42 [pubmed]
PHST- 2024/02/15 00:42 [medline]
PHST- 2024/02/14 19:18 [entrez]
AID - S0749-3797(24)00060-6 [pii]
AID - 10.1016/j.amepre.2024.02.006 [doi]
PST - aheadofprint
SO  - Am J Prev Med. 2024 Feb 12:S0749-3797(24)00060-6. doi: 
      10.1016/j.amepre.2024.02.006.

PMID- 38043471
OWN - NLM
STAT- MEDLINE
DCOM- 20240110
LR  - 20240206
IS  - 1879-0534 (Electronic)
IS  - 0010-4825 (Linking)
VI  - 168
DP  - 2024 Jan
TI  - Evaluation of Large language model performance on the Multi-Specialty Recruitment 
      Assessment (MSRA) exam.
PG  - 107794
LID - S0010-4825(23)01259-3 [pii]
LID - 10.1016/j.compbiomed.2023.107794 [doi]
AB  - INTRODUCTION: AI-powered platforms have gained prominence in medical education 
      and training, offering diverse applications from surgical performance assessment 
      to exam preparation. This research paper examines the capabilities of Large 
      Language Models (LLMs), including Llama 2, Google Bard, Bing Chat, and 
      ChatGPT-3.5, in answering multiple-choice questions of the Clinical Problem 
      Solving (CPS) paper of the Multi-Specialty Recruitment Assessment (MSRA) exam. 
      METHODS: Using a dataset of 100 CPS questions from ten subject categories, we 
      assessed the LLMs' performance against medical doctors preparing for the exam. 
      RESULTS: Results showed that Bing Chat outperformed all other LLMs and even 
      surpassed human users from the Qbank question bank. Conversely, Llama 2's 
      performance was inferior to human users. Google Bard and ChatGPT 3.5 did not 
      exhibit statistically significant differences in correct response rates compared 
      to human candidates. Pairwise comparisons demonstrated Bing Chat's significant 
      superiority over Llama 2, Google Bard, and ChatGPT 3.5. However, no significant 
      differences were found between Llama 2 and Google Bard, Llama 2, and ChatGPT-3.5, 
      and Google Bard and ChatGPT-3.5. DISCUSSION: Freely available LLMs have already 
      demonstrated that they can perform as well or even outperform human users in 
      answering MSRA exam questions. Bing Chat emerged as a particularly strong 
      performer. The study also highlights the potential for enhancing LLMs' medical 
      knowledge acquisition through tailored fine-tuning. Medical knowledge tailored 
      LLMs such as Med-PaLM, have already shown promising results. CONCLUSION: We 
      provided valuable insights into LLMs' competence in answering medical MCQs and 
      their potential integration into medical education and assessment processes.
CI  - Copyright © 2023 Elsevier Ltd. All rights reserved.
FAU - Tsoutsanis, Panagiotis
AU  - Tsoutsanis P
AD  - Northern Care Alliance NHS Foundation Trust, Rochdale Eye Unit, Rochdale 
      infirmary, Greater Manchester, UK; Department of Education, University of Oxford, 
      Oxford, UK. Electronic address: tpanos02@gmail.com.
FAU - Tsoutsanis, Aristotelis
AU  - Tsoutsanis A
AD  - Department of Informatics, Technical University of Munich, Munich, Germany.
LA  - eng
PT  - Journal Article
DEP - 20231130
PL  - United States
TA  - Comput Biol Med
JT  - Computers in biology and medicine
JID - 1250250
SB  - IM
MH  - Humans
MH  - Animals
MH  - *Camelids, New World
MH  - *Medicine
MH  - *Education, Medical
MH  - Language
MH  - Problem Solving
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Large language models
OT  - Medical education
OT  - Medical exam
COIS- Declaration of competing interest The authors have declared that no competing 
      interests exist.
EDAT- 2023/12/04 00:42
MHDA- 2024/01/10 06:42
CRDT- 2023/12/03 18:14
PHST- 2023/08/30 00:00 [received]
PHST- 2023/11/27 00:00 [revised]
PHST- 2023/11/28 00:00 [accepted]
PHST- 2024/01/10 06:42 [medline]
PHST- 2023/12/04 00:42 [pubmed]
PHST- 2023/12/03 18:14 [entrez]
AID - S0010-4825(23)01259-3 [pii]
AID - 10.1016/j.compbiomed.2023.107794 [doi]
PST - ppublish
SO  - Comput Biol Med. 2024 Jan;168:107794. doi: 10.1016/j.compbiomed.2023.107794. Epub 
      2023 Nov 30.

PMID- 37017291
OWN - NLM
STAT- MEDLINE
DCOM- 20231023
LR  - 20231026
IS  - 1708-8240 (Electronic)
IS  - 1496-4155 (Linking)
VI  - 35
IP  - 7
DP  - 2023 Oct
TI  - Implications of large language models such as ChatGPT for dental medicine.
PG  - 1098-1102
LID - 10.1111/jerd.13046 [doi]
AB  - OBJECTIVE: This article provides an overview of the implications of ChatGPT and 
      other large language models (LLMs) for dental medicine. OVERVIEW: ChatGPT, a LLM 
      trained on massive amounts of textual data, is adept at fulfilling various 
      language-related tasks. Despite its impressive capabilities, ChatGPT has serious 
      limitations, such as occasionally giving incorrect answers, producing nonsensical 
      content, and presenting misinformation as fact. Dental practitioners, assistants, 
      and hygienists are not likely to be significantly impacted by LLMs. However, LLMs 
      could affect the work of administrative personnel and the provision of dental 
      telemedicine. LLMs offer potential for clinical decision support, text 
      summarization, efficient writing, and multilingual communication. As more people 
      seek health information from LLMs, it is crucial to safeguard against inaccurate, 
      outdated, and biased responses to health-related queries. LLMs pose challenges 
      for patient data confidentiality and cybersecurity that must be tackled. In 
      dental education, LLMs present fewer challenges than in other academic fields. 
      LLMs can enhance academic writing fluency, but acceptable usage boundaries in 
      science need to be established. CONCLUSIONS: While LLMs such as ChatGPT may have 
      various useful applications in dental medicine, they come with risks of malicious 
      use and serious limitations, including the potential for misinformation. CLINICAL 
      SIGNIFICANCE: Along with the potential benefits of using LLMs as an additional 
      tool in dental medicine, it is crucial to carefully consider the limitations and 
      potential risks inherent in such artificial intelligence technologies.
CI  - © 2023 The Authors. Journal of Esthetic and Restorative Dentistry published by 
      Wiley Periodicals LLC.
FAU - Eggmann, Florin
AU  - Eggmann F
AUID- ORCID: 0000-0001-6185-1480
AD  - Department of Preventive and Restorative Sciences, Penn Dental Medicine, Robert 
      Schattner Center, University of Pennsylvania, Philadelphia, Pennsylvania, USA.
AD  - Department of Periodontology, Endodontology, and Cariology, University Center for 
      Dental Medicine Basel UZB, University of Basel, Basel, Switzerland.
FAU - Weiger, Roland
AU  - Weiger R
AD  - Department of Periodontology, Endodontology, and Cariology, University Center for 
      Dental Medicine Basel UZB, University of Basel, Basel, Switzerland.
FAU - Zitzmann, Nicola U
AU  - Zitzmann NU
AD  - Department of Reconstructive Dentistry, University Center for Dental Medicine 
      Basel UZB, University of Basel, Basel, Switzerland.
FAU - Blatz, Markus B
AU  - Blatz MB
AUID- ORCID: 0000-0001-6341-7047
AD  - Department of Preventive and Restorative Sciences, Penn Dental Medicine, Robert 
      Schattner Center, University of Pennsylvania, Philadelphia, Pennsylvania, USA.
LA  - eng
GR  - Freiwillige Akademische Gesellschaft/
GR  - Gottfried und Julia Bangerter-Rhyner-Stiftung/
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PT  - Review
DEP - 20230405
PL  - England
TA  - J Esthet Restor Dent
JT  - Journal of esthetic and restorative dentistry : official publication of the 
      American Academy of Esthetic Dentistry ... [et al.]
JID - 101096515
RN  - 621BVT9M36 (Fenbendazole)
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Dentists
MH  - Professional Role
MH  - Language
MH  - Fenbendazole
OTO - NOTNLM
OT  - artificial intelligence
OT  - dental care
OT  - dental education
OT  - evidence-based dentistry
OT  - machine learning
EDAT- 2023/04/06 06:00
MHDA- 2023/10/23 00:43
CRDT- 2023/04/05 07:43
PHST- 2023/03/25 00:00 [revised]
PHST- 2023/03/07 00:00 [received]
PHST- 2023/03/28 00:00 [accepted]
PHST- 2023/10/23 00:43 [medline]
PHST- 2023/04/06 06:00 [pubmed]
PHST- 2023/04/05 07:43 [entrez]
AID - 10.1111/jerd.13046 [doi]
PST - ppublish
SO  - J Esthet Restor Dent. 2023 Oct;35(7):1098-1102. doi: 10.1111/jerd.13046. Epub 
      2023 Apr 5.

PMID- 38448291
OWN - NLM
STAT- Publisher
LR  - 20240306
IS  - 0219-3108 (Electronic)
IS  - 1015-9584 (Linking)
DP  - 2024 Mar 5
TI  - Is ChatGPT Proficient in extracting critical medical information from patient 
      records?
LID - S1015-9584(24)00385-3 [pii]
LID - 10.1016/j.asjsur.2024.02.123 [doi]
FAU - Qian, Chengxing
AU  - Qian C
AD  - Department of Neurosurgery, the Tongling People's Hospital, Tongling, China.
FAU - Wang, Renzhi
AU  - Wang R
AD  - Chinese University of Hong Kong (Shenzhen) School of Medicine, Shenzhen, 
      Guangdong, China; Department of Neurosurgery, Peking Union Medical College 
      Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, 
      Beijing, China.
FAU - Fang, Yi
AU  - Fang Y
AD  - Department of Neurosurgery, Peking Union Medical College Hospital, Chinese 
      Academy of Medical Sciences and Peking Union Medical College, Beijing, China. 
      Electronic address: fangyi@cuhk.edu.cn.
LA  - eng
PT  - Letter
DEP - 20240305
PL  - Netherlands
TA  - Asian J Surg
JT  - Asian journal of surgery
JID - 8900600
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Medical records
OT  - Neurosurgery
COIS- Declaration of competing interest The authors have no personal, financial, or 
      institutional interest in any of the drugs, materials, or devices described in 
      this article.
EDAT- 2024/03/07 00:42
MHDA- 2024/03/07 00:42
CRDT- 2024/03/06 21:54
PHST- 2024/01/05 00:00 [received]
PHST- 2024/02/21 00:00 [accepted]
PHST- 2024/03/07 00:42 [medline]
PHST- 2024/03/07 00:42 [pubmed]
PHST- 2024/03/06 21:54 [entrez]
AID - S1015-9584(24)00385-3 [pii]
AID - 10.1016/j.asjsur.2024.02.123 [doi]
PST - aheadofprint
SO  - Asian J Surg. 2024 Mar 5:S1015-9584(24)00385-3. doi: 
      10.1016/j.asjsur.2024.02.123.

PMID- 36943179
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20231218
LR  - 20231218
IS  - 1520-5851 (Electronic)
IS  - 0013-936X (Print)
IS  - 0013-936X (Linking)
VI  - 57
IP  - 46
DP  - 2023 Nov 21
TI  - ChatGPT and Environmental Research.
PG  - 17667-17670
LID - 10.1021/acs.est.3c01818 [doi]
FAU - Zhu, Jun-Jie
AU  - Zhu JJ
AUID- ORCID: 0000-0002-7546-2870
AD  - Department of Civil and Environmental Engineering, Princeton University, 
      Princeton, New Jersey 08544, United States.
AD  - Andlinger Center for Energy and the Environment, Princeton University, Princeton, 
      New Jersey 08544, United States.
FAU - Jiang, Jinyue
AU  - Jiang J
AD  - Department of Civil and Environmental Engineering, Princeton University, 
      Princeton, New Jersey 08544, United States.
AD  - Andlinger Center for Energy and the Environment, Princeton University, Princeton, 
      New Jersey 08544, United States.
FAU - Yang, Meiqi
AU  - Yang M
AUID- ORCID: 0000-0003-0913-6804
AD  - Department of Civil and Environmental Engineering, Princeton University, 
      Princeton, New Jersey 08544, United States.
AD  - Andlinger Center for Energy and the Environment, Princeton University, Princeton, 
      New Jersey 08544, United States.
FAU - Ren, Zhiyong Jason
AU  - Ren ZJ
AUID- ORCID: 0000-0001-7606-0331
AD  - Department of Civil and Environmental Engineering, Princeton University, 
      Princeton, New Jersey 08544, United States.
AD  - Andlinger Center for Energy and the Environment, Princeton University, Princeton, 
      New Jersey 08544, United States.
LA  - eng
PT  - Journal Article
DEP - 20230321
PL  - United States
TA  - Environ Sci Technol
JT  - Environmental science &amp; technology
JID - 0213155
SB  - IM
PMC - PMC10666266
OTO - NOTNLM
OT  - ChatGPT
OT  - benefits
OT  - opportunities
OT  - over-reliance
COIS- The authors declare no competing financial interest.
EDAT- 2023/03/22 06:00
MHDA- 2023/03/22 06:01
PMCR- 2023/11/23
CRDT- 2023/03/21 10:43
PHST- 2023/03/22 06:01 [medline]
PHST- 2023/03/22 06:00 [pubmed]
PHST- 2023/03/21 10:43 [entrez]
PHST- 2023/11/23 00:00 [pmc-release]
AID - 10.1021/acs.est.3c01818 [doi]
PST - ppublish
SO  - Environ Sci Technol. 2023 Nov 21;57(46):17667-17670. doi: 
      10.1021/acs.est.3c01818. Epub 2023 Mar 21.

PMID- 37578819
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230831
IS  - 1929-073X (Print)
IS  - 1929-073X (Electronic)
IS  - 1929-073X (Linking)
VI  - 12
DP  - 2023 Aug 14
TI  - Appropriateness and Comprehensiveness of Using ChatGPT for Perioperative Patient 
      Education in Thoracic Surgery in Different Language Contexts: Survey Study.
PG  - e46900
LID - 10.2196/46900 [doi]
LID - e46900
AB  - BACKGROUND: ChatGPT, a dialogue-based artificial intelligence language model, has 
      shown promise in assisting clinical workflows and patient-clinician 
      communication. However, there is a lack of feasibility assessments regarding its 
      use for perioperative patient education in thoracic surgery. OBJECTIVE: This 
      study aimed to assess the appropriateness and comprehensiveness of using ChatGPT 
      for perioperative patient education in thoracic surgery in both English and 
      Chinese contexts. METHODS: This pilot study was conducted in February 2023. A 
      total of 37 questions focused on perioperative patient education in thoracic 
      surgery were created based on guidelines and clinical experience. Two sets of 
      inquiries were made to ChatGPT for each question, one in English and the other in 
      Chinese. The responses generated by ChatGPT were evaluated separately by 
      experienced thoracic surgical clinicians for appropriateness and 
      comprehensiveness based on a hypothetical draft response to a patient's question 
      on the electronic information platform. For a response to be qualified, it 
      required at least 80% of reviewers to deem it appropriate and 50% to deem it 
      comprehensive. Statistical analyses were performed using the unpaired chi-square 
      test or Fisher exact test, with a significance level set at P&lt;.05. RESULTS: The 
      set of 37 commonly asked questions covered topics such as disease information, 
      diagnostic procedures, perioperative complications, treatment measures, disease 
      prevention, and perioperative care considerations. In both the English and 
      Chinese contexts, 34 (92%) out of 37 responses were qualified in terms of both 
      appropriateness and comprehensiveness. The remaining 3 (8%) responses were 
      unqualified in these 2 contexts. The unqualified responses primarily involved the 
      diagnosis of disease symptoms and surgical-related complications symptoms. The 
      reasons for determining the responses as unqualified were similar in both 
      contexts. There was no statistically significant difference (34/37, 92% vs 34/37, 
      92%; P=.99) in the qualification rate between the 2 language sets. CONCLUSIONS: 
      This pilot study demonstrates the potential feasibility of using ChatGPT for 
      perioperative patient education in thoracic surgery in both English and Chinese 
      contexts. ChatGPT is expected to enhance patient satisfaction, reduce anxiety, 
      and improve compliance during the perioperative period. In the future, there will 
      be remarkable potential application for using artificial intelligence, in 
      conjunction with human review, for patient education and health consultation 
      after patients have provided their informed consent.
CI  - ©Chen-ye Shao, Hui Li, Xiao-long Liu, Chang Li, Li-qin Yang, Yue-juan Zhang, Jing 
      Luo, Jun Zhao. Originally published in the Interactive Journal of Medical 
      Research (https://www.i-jmr.org/), 14.08.2023.
FAU - Shao, Chen-Ye
AU  - Shao CY
AUID- ORCID: 0009-0006-7288-816X
AD  - Department of Thoracic Surgery, The First Affiliated Hospital of Soochow 
      University, Suzhou, China.
FAU - Li, Hui
AU  - Li H
AUID- ORCID: 0000-0002-6190-6266
AD  - Department of Obstetrics and Gynecology, The First Affiliated Hospital of Soochow 
      University, Suzhou, China.
FAU - Liu, Xiao-Long
AU  - Liu XL
AUID- ORCID: 0000-0002-3969-1684
AD  - Department of Cardiothoracic Surgery, Jinling Hospital, Medical School of Nanjing 
      University, Nanjing, China.
FAU - Li, Chang
AU  - Li C
AUID- ORCID: 0000-0002-9469-7173
AD  - Department of Thoracic Surgery, The First Affiliated Hospital of Soochow 
      University, Suzhou, China.
FAU - Yang, Li-Qin
AU  - Yang LQ
AUID- ORCID: 0009-0007-2765-0977
AD  - Department of Thoracic Surgery, The First Affiliated Hospital of Soochow 
      University, Suzhou, China.
FAU - Zhang, Yue-Juan
AU  - Zhang YJ
AUID- ORCID: 0009-0006-9280-9393
AD  - Department of Thoracic Surgery, The First Affiliated Hospital of Soochow 
      University, Suzhou, China.
FAU - Luo, Jing
AU  - Luo J
AUID- ORCID: 0000-0002-7400-7462
AD  - Department of Thoracic Surgery, The First Affiliated Hospital of Soochow 
      University, Suzhou, China.
FAU - Zhao, Jun
AU  - Zhao J
AUID- ORCID: 0000-0001-8903-8205
AD  - Department of Thoracic Surgery, The First Affiliated Hospital of Soochow 
      University, Suzhou, China.
LA  - eng
PT  - Journal Article
DEP - 20230814
PL  - Canada
TA  - Interact J Med Res
JT  - Interactive journal of medical research
JID - 101598421
PMC - PMC10463083
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - Generative Pre-trained Transformer
OT  - artificial intelligence
OT  - clinical workflow
OT  - communication
OT  - education
OT  - evaluation
OT  - feasibility
OT  - language
OT  - language model
OT  - patient
OT  - patient education
OT  - surgery
OT  - thoracic
OT  - thoracic surgery
OT  - workflow
COIS- Conflicts of Interest: None declared.
EDAT- 2023/08/14 12:42
MHDA- 2023/08/14 12:43
PMCR- 2023/08/14
CRDT- 2023/08/14 11:52
PHST- 2023/03/01 00:00 [received]
PHST- 2023/07/27 00:00 [accepted]
PHST- 2023/07/22 00:00 [revised]
PHST- 2023/08/14 12:43 [medline]
PHST- 2023/08/14 12:42 [pubmed]
PHST- 2023/08/14 11:52 [entrez]
PHST- 2023/08/14 00:00 [pmc-release]
AID - v12i1e46900 [pii]
AID - 10.2196/46900 [doi]
PST - epublish
SO  - Interact J Med Res. 2023 Aug 14;12:e46900. doi: 10.2196/46900.

PMID- 38235988
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 2245-1919 (Electronic)
IS  - 2245-1919 (Linking)
VI  - 71
IP  - 1
DP  - 2023 Dec 18
TI  - Reply to "Correspondence on 'cover letters written by ChatGPT-4 or humans'".
PG  - 1
AB  - This is a reply to the: "Correspondence on "cover letters written by ChatGPT-4 or 
      humans"" Dan Med J 2024;71(1):A205177.
CI  - Published under Open Access CC-BY-NC-BD 4.0. 
      https://creativecommons.org/licenses/by-nc-nd/4.0/.
FAU - Deveci, Can Deniz
AU  - Deveci CD
AD  - Center for Perioperative Optimization, Department of Surgery, Copenhagen 
      University Hospital - Herlev Hospital, Denmark.
FAU - Baker, Jason Joe
AU  - Baker JJ
AD  - Center for Perioperative Optimization, Department of Surgery, Copenhagen 
      University Hospital - Herlev Hospital, Denmark.
FAU - Sikander, Binyamin
AU  - Sikander B
AD  - Center for Perioperative Optimization, Department of Surgery, Copenhagen 
      University Hospital - Herlev Hospital, Denmark.
FAU - Rosenberg, Jacob
AU  - Rosenberg J
AD  - Center for Perioperative Optimization, Department of Surgery, Copenhagen 
      University Hospital - Herlev Hospital, Denmark.
LA  - eng
PT  - Comment
PT  - Letter
DEP - 20231218
PL  - Denmark
TA  - Dan Med J
JT  - Danish medical journal
JID - 101576205
SB  - IM
CON - Dan Med J. 2023 Nov 23;70(12):. PMID: 38018708
EDAT- 2024/01/18 12:42
MHDA- 2024/01/18 12:43
CRDT- 2024/01/18 08:43
PHST- 2024/01/18 12:43 [medline]
PHST- 2024/01/18 12:42 [pubmed]
PHST- 2024/01/18 08:43 [entrez]
AID - A205178 [pii]
PST - epublish
SO  - Dan Med J. 2023 Dec 18;71(1):1.

PMID- 37903836
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231102
IS  - 2045-2322 (Electronic)
IS  - 2045-2322 (Linking)
VI  - 13
IP  - 1
DP  - 2023 Oct 30
TI  - A large-scale comparison of human-written versus ChatGPT-generated essays.
PG  - 18617
LID - 10.1038/s41598-023-45644-9 [doi]
LID - 18617
AB  - ChatGPT and similar generative AI models have attracted hundreds of millions of 
      users and have become part of the public discourse. Many believe that such models 
      will disrupt society and lead to significant changes in the education system and 
      information generation. So far, this belief is based on either colloquial 
      evidence or benchmarks from the owners of the models-both lack scientific rigor. 
      We systematically assess the quality of AI-generated content through a 
      large-scale study comparing human-written versus ChatGPT-generated argumentative 
      student essays. We use essays that were rated by a large number of human experts 
      (teachers). We augment the analysis by considering a set of linguistic 
      characteristics of the generated essays. Our results demonstrate that ChatGPT 
      generates essays that are rated higher regarding quality than human-written 
      essays. The writing style of the AI models exhibits linguistic characteristics 
      that are different from those of the human-written essays. Since the technology 
      is readily available, we believe that educators must act immediately. We must 
      re-invent homework and develop teaching concepts that utilize these AI models in 
      the same way as math utilizes the calculator: teach the general concepts first 
      and then use AI tools to free up time for other learning objectives.
CI  - © 2023. The Author(s).
FAU - Herbold, Steffen
AU  - Herbold S
AD  - Faculty of Computer Science and Mathematics, University of Passau, Passau, 
      Germany. steffen.herbold@uni-passau.de.
FAU - Hautli-Janisz, Annette
AU  - Hautli-Janisz A
AD  - Faculty of Computer Science and Mathematics, University of Passau, Passau, 
      Germany.
FAU - Heuer, Ute
AU  - Heuer U
AD  - Faculty of Computer Science and Mathematics, University of Passau, Passau, 
      Germany.
FAU - Kikteva, Zlata
AU  - Kikteva Z
AD  - Faculty of Computer Science and Mathematics, University of Passau, Passau, 
      Germany.
FAU - Trautsch, Alexander
AU  - Trautsch A
AD  - Faculty of Computer Science and Mathematics, University of Passau, Passau, 
      Germany.
LA  - eng
PT  - Journal Article
DEP - 20231030
PL  - England
TA  - Sci Rep
JT  - Scientific reports
JID - 101563288
SB  - IM
PMC - PMC10616290
COIS- The authors declare no competing interests.
EDAT- 2023/10/31 06:42
MHDA- 2023/10/31 06:43
PMCR- 2023/10/30
CRDT- 2023/10/31 00:24
PHST- 2023/06/01 00:00 [received]
PHST- 2023/10/22 00:00 [accepted]
PHST- 2023/10/31 06:43 [medline]
PHST- 2023/10/31 06:42 [pubmed]
PHST- 2023/10/31 00:24 [entrez]
PHST- 2023/10/30 00:00 [pmc-release]
AID - 10.1038/s41598-023-45644-9 [pii]
AID - 45644 [pii]
AID - 10.1038/s41598-023-45644-9 [doi]
PST - epublish
SO  - Sci Rep. 2023 Oct 30;13(1):18617. doi: 10.1038/s41598-023-45644-9.

PMID- 38184777
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20240108
LR  - 20240117
IS  - 2005-8330 (Electronic)
IS  - 1229-6929 (Print)
IS  - 1229-6929 (Linking)
VI  - 25
IP  - 1
DP  - 2024 Jan
TI  - Correspondence on 'Is ChatGPT a "Fire of Prometheus" for Non-Native 
      English-Speaking Researchers in Academic Writing?'.
PG  - 120-121
LID - 10.3348/kjr.2023.0971 [doi]
FAU - Wiwanitkit, Somsri
AU  - Wiwanitkit S
AUID- ORCID: 0000-0002-9486-9473
AD  - Private Academic and Editorial Consultant, Bangkok, Thailand.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AUID- ORCID: 0000-0003-1039-3728
AD  - Center for Global Health Research, Saveetha Medical College Saveetha Institute of 
      Medical and Technical Sciences, Tamil Nadu, India. wviroj@yahoo.com.
LA  - eng
PT  - Comment
PT  - Letter
PL  - Korea (South)
TA  - Korean J Radiol
JT  - Korean journal of radiology
JID - 100956096
SB  - IM
CON - Korean J Radiol. 2023 Oct;24(10):952-959. PMID: 37793668
PMC - PMC10788612
OTO - NOTNLM
OT  - Academic
OT  - ChatGPT
OT  - English
OT  - Speaking
OT  - Writinge
COIS- The authors have no potential conflicts of interest to disclose.
EDAT- 2024/01/07 06:45
MHDA- 2024/01/07 06:46
PMCR- 2024/01/01
CRDT- 2024/01/07 00:03
PHST- 2023/10/05 00:00 [received]
PHST- 2023/10/07 00:00 [revised]
PHST- 2023/10/10 00:00 [accepted]
PHST- 2024/01/07 06:46 [medline]
PHST- 2024/01/07 06:45 [pubmed]
PHST- 2024/01/07 00:03 [entrez]
PHST- 2024/01/01 00:00 [pmc-release]
AID - 25.120 [pii]
AID - 10.3348/kjr.2023.0971 [doi]
PST - ppublish
SO  - Korean J Radiol. 2024 Jan;25(1):120-121. doi: 10.3348/kjr.2023.0971.

PMID- 37901112
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231031
IS  - 2297-1769 (Print)
IS  - 2297-1769 (Electronic)
IS  - 2297-1769 (Linking)
VI  - 10
DP  - 2023
TI  - Can ChatGPT diagnose my collapsing dog?
PG  - 1245168
LID - 10.3389/fvets.2023.1245168 [doi]
LID - 1245168
FAU - Abani, Samira
AU  - Abani S
AD  - Department of Small Animal Medicine and Surgery, University of Veterinary 
      Medicine Hannover, Hannover, Germany.
AD  - Centre for Systems Neuroscience, University of Veterinary Medicine Hannover, 
      Hannover, Germany.
FAU - De Decker, Steven
AU  - De Decker S
AD  - Department of Veterinary Clinical Science and Services, Royal Veterinary College, 
      University of London, London, United Kingdom.
FAU - Tipold, Andrea
AU  - Tipold A
AD  - Department of Small Animal Medicine and Surgery, University of Veterinary 
      Medicine Hannover, Hannover, Germany.
AD  - Centre for Systems Neuroscience, University of Veterinary Medicine Hannover, 
      Hannover, Germany.
FAU - Nessler, Jasmin Nicole
AU  - Nessler JN
AD  - Department of Small Animal Medicine and Surgery, University of Veterinary 
      Medicine Hannover, Hannover, Germany.
FAU - Volk, Holger Andreas
AU  - Volk HA
AD  - Department of Small Animal Medicine and Surgery, University of Veterinary 
      Medicine Hannover, Hannover, Germany.
AD  - Centre for Systems Neuroscience, University of Veterinary Medicine Hannover, 
      Hannover, Germany.
LA  - eng
PT  - Journal Article
DEP - 20231010
PL  - Switzerland
TA  - Front Vet Sci
JT  - Frontiers in veterinary science
JID - 101666658
PMC - PMC10600474
OTO - NOTNLM
OT  - ChatGPT
OT  - GPT-3.5
OT  - artificial intelligence
OT  - diagnosis
OT  - generative AI
OT  - language model
OT  - machine learning
OT  - natural language processing
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2023/10/30 06:46
MHDA- 2023/10/30 06:47
PMCR- 2023/01/01
CRDT- 2023/10/30 04:44
PHST- 2023/06/23 00:00 [received]
PHST- 2023/09/19 00:00 [accepted]
PHST- 2023/10/30 06:47 [medline]
PHST- 2023/10/30 06:46 [pubmed]
PHST- 2023/10/30 04:44 [entrez]
PHST- 2023/01/01 00:00 [pmc-release]
AID - 10.3389/fvets.2023.1245168 [doi]
PST - epublish
SO  - Front Vet Sci. 2023 Oct 10;10:1245168. doi: 10.3389/fvets.2023.1245168. 
      eCollection 2023.

PMID- 37025464
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230424
IS  - 2052-2975 (Print)
IS  - 2052-2975 (Electronic)
IS  - 2052-2975 (Linking)
VI  - 53
DP  - 2023 Jun
TI  - Are the issues pointed out by ChatGPT can be applied to Japan? - Examining the 
      reasons behind high COVID-19 excess deaths in Japan.
PG  - 101116
LID - 10.1016/j.nmni.2023.101116 [doi]
LID - 101116
FAU - Kaneda, Yudai
AU  - Kaneda Y
AD  - School of Medicine, Hokkaido University, Hokkaido, Japan.
FAU - Tsubokura, Masaharu
AU  - Tsubokura M
AD  - Department of Radiation Health Management, Fukushima Medical University School of 
      Medicine, Fukushima, Japan.
FAU - Ozaki, Akihiko
AU  - Ozaki A
AD  - Department of Breast and Thyroid Surgery, Jyoban Hospital of Tokiwa Foundation, 
      Fukushima, Japan.
FAU - Saito, Hiroaki
AU  - Saito H
AD  - Department of Internal Medicine, Soma Central Hospital, Fukushima, Japan.
FAU - Tanimoto, Tetsuya
AU  - Tanimoto T
AD  - Department of Internal Medicine, Jyoban Hospital of Tokiwa Foundation, Fukushima, 
      Japan.
LA  - eng
PT  - Journal Article
DEP - 20230329
PL  - England
TA  - New Microbes New Infect
JT  - New microbes and new infections
JID - 101624750
PMC - PMC10052853
OTO - NOTNLM
OT  - COVID-19
OT  - ChatGPT
OT  - Excess death
OT  - Japan
COIS- Dr Ozaki reported personal fees from Medical Network Systems Inc. and 
      10.13039/501100004095Kyowa Kirin co. ltd. outside the submitted work. Dr Tanimoto 
      reported personal fees from Medical Network Systems Inc. and Bionics co. ltd., 
      outside the submitted work. No other disclosures were reported.
EDAT- 2023/04/08 06:00
MHDA- 2023/04/08 06:01
PMCR- 2023/03/29
CRDT- 2023/04/07 02:30
PHST- 2023/03/06 00:00 [received]
PHST- 2023/03/23 00:00 [accepted]
PHST- 2023/04/08 06:01 [medline]
PHST- 2023/04/07 02:30 [entrez]
PHST- 2023/04/08 06:00 [pubmed]
PHST- 2023/03/29 00:00 [pmc-release]
AID - S2052-2975(23)00035-5 [pii]
AID - 101116 [pii]
AID - 10.1016/j.nmni.2023.101116 [doi]
PST - ppublish
SO  - New Microbes New Infect. 2023 Jun;53:101116. doi: 10.1016/j.nmni.2023.101116. 
      Epub 2023 Mar 29.

PMID- 38290286
OWN - NLM
STAT- MEDLINE
DCOM- 20240219
LR  - 20240219
IS  - 1872-7123 (Electronic)
IS  - 0165-1781 (Linking)
VI  - 333
DP  - 2024 Mar
TI  - Beyond rating scales: With targeted evaluation, large language models are poised 
      for psychological assessment.
PG  - 115667
LID - S0165-1781(23)00617-0 [pii]
LID - 10.1016/j.psychres.2023.115667 [doi]
AB  - In this narrative review, we survey recent empirical evaluations of AI-based 
      language assessments and present a case for the technology of large language 
      models to be poised for changing standardized psychological assessment. 
      Artificial intelligence has been undergoing a purported "paradigm shift" 
      initiated by new machine learning models, large language models (e.g., BERT, 
      LAMMA, and that behind ChatGPT). These models have led to unprecedented accuracy 
      over most computerized language processing tasks, from web searches to automatic 
      machine translation and question answering, while their dialogue-based forms, 
      like ChatGPT have captured the interest of over a million users. The success of 
      the large language model is mostly attributed to its capability to numerically 
      represent words in their context, long a weakness of previous attempts to 
      automate psychological assessment from language. While potential applications for 
      automated therapy are beginning to be studied on the heels of chatGPT's success, 
      here we present evidence that suggests, with thorough validation of targeted 
      deployment scenarios, that AI's newest technology can move mental health 
      assessment away from rating scales and to instead use how people naturally 
      communicate, in language.
CI  - Copyright © 2023 The Author(s). Published by Elsevier B.V. All rights reserved.
FAU - Kjell, Oscar N E
AU  - Kjell ONE
AD  - Psychology Department, Lund University, Sweden; Computer Science Department, 
      Stony Brook University, United States. Electronic address: oscar.kjell@psy.lu.se.
FAU - Kjell, Katarina
AU  - Kjell K
AD  - Psychology Department, Lund University, Sweden.
FAU - Schwartz, H Andrew
AU  - Schwartz HA
AD  - Psychology Department, Lund University, Sweden; Computer Science Department, 
      Stony Brook University, United States.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20231210
PL  - Ireland
TA  - Psychiatry Res
JT  - Psychiatry research
JID - 7911385
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Language
MH  - Machine Learning
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Assessment
OT  - Large language models
OT  - Psychology
OT  - Transformers
COIS- Declaration of competing interest Oscar Kjell, and Katarina Kjell have co-founded 
      and hold shares in a start-up using computational language assessments to 
      diagnose mental health problems. Open data, code and material: 
      https://osf.io/7aenc/?view_only=5e9eafcd2afe4b92a8ae74d371f88314.
EDAT- 2024/01/31 00:42
MHDA- 2024/02/19 06:43
CRDT- 2024/01/30 18:07
PHST- 2023/02/26 00:00 [received]
PHST- 2023/12/03 00:00 [revised]
PHST- 2023/12/05 00:00 [accepted]
PHST- 2024/02/19 06:43 [medline]
PHST- 2024/01/31 00:42 [pubmed]
PHST- 2024/01/30 18:07 [entrez]
AID - S0165-1781(23)00617-0 [pii]
AID - 10.1016/j.psychres.2023.115667 [doi]
PST - ppublish
SO  - Psychiatry Res. 2024 Mar;333:115667. doi: 10.1016/j.psychres.2023.115667. Epub 
      2023 Dec 10.

PMID- 38146711
OWN - NLM
STAT- Publisher
LR  - 20240305
IS  - 1466-187X (Electronic)
IS  - 0142-159X (Linking)
DP  - 2023 Dec 26
TI  - Twelve tips to leverage AI for efficient and effective medical question 
      generation: A guide for educators using Chat GPT.
PG  - 1-6
LID - 10.1080/0142159X.2023.2294703 [doi]
AB  - BACKGROUND: Crafting quality assessment questions in medical education is a 
      crucial yet time-consuming, expertise-driven undertaking that calls for 
      innovative solutions. Large language models (LLMs), such as ChatGPT (Chat 
      Generative Pre-Trained Transformer), present a promising yet underexplored avenue 
      for such innovations. AIMS: This study explores the utility of ChatGPT to 
      generate diverse, high-quality medical questions, focusing on multiple-choice 
      questions (MCQs) as an illustrative example, to increase educator's productivity 
      and enable self-directed learning for students. DESCRIPTION: Leveraging 12 
      strategies, we demonstrate how ChatGPT can be effectively used to generate 
      assessment questions aligned with Bloom's taxonomy and core knowledge domains 
      while promoting best practices in assessment design. CONCLUSION: Integrating LLM 
      tools like ChatGPT into generating medical assessment questions like MCQs 
      augments but does not replace human expertise. With continual instruction 
      refinement, AI can produce high-standard questions. Yet, the onus of ensuring 
      ultimate quality and accuracy remains with subject matter experts, affirming the 
      irreplaceable value of human involvement in the artificial intelligence-driven 
      education paradigm.
FAU - Indran, Inthrani Raja
AU  - Indran IR
AUID- ORCID: 0000-0002-3487-3948
AD  - Department of Pharmacology, National University of Singapore, Yong Loo Lin School 
      of Medicine, Singapore, Singapore.
FAU - Paranthaman, Priya
AU  - Paranthaman P
AD  - Department of Pharmacology, National University of Singapore, Yong Loo Lin School 
      of Medicine, Singapore, Singapore.
FAU - Gupta, Neelima
AU  - Gupta N
AUID- ORCID: 0009-0002-0810-3226
AD  - Department of Pharmacology, National University of Singapore, Yong Loo Lin School 
      of Medicine, Singapore, Singapore.
FAU - Mustafa, Nurulhuda
AU  - Mustafa N
AD  - Department of Pharmacology, National University of Singapore, Yong Loo Lin School 
      of Medicine, Singapore, Singapore.
LA  - eng
PT  - Journal Article
DEP - 20231226
PL  - England
TA  - Med Teach
JT  - Medical teacher
JID - 7909593
SB  - IM
OTO - NOTNLM
OT  - AI
OT  - Chat GPT
OT  - medical assessment
OT  - questions
EDAT- 2023/12/26 12:42
MHDA- 2023/12/26 12:42
CRDT- 2023/12/26 07:31
PHST- 2023/12/26 12:42 [pubmed]
PHST- 2023/12/26 12:42 [medline]
PHST- 2023/12/26 07:31 [entrez]
AID - 10.1080/0142159X.2023.2294703 [doi]
PST - aheadofprint
SO  - Med Teach. 2023 Dec 26:1-6. doi: 10.1080/0142159X.2023.2294703.

PMID- 37603843
OWN - NLM
STAT- MEDLINE
DCOM- 20240314
LR  - 20240314
IS  - 1873-1953 (Electronic)
IS  - 1474-5151 (Linking)
VI  - 23
IP  - 2
DP  - 2024 Mar 12
TI  - Using ChatGPT and Google Bard to improve the readability of written patient 
      information: a proof of concept.
PG  - 122-126
LID - 10.1093/eurjcn/zvad087 [doi]
AB  - Patient information materials often tend to be written at a reading level that is 
      too advanced for patients. In this proof-of-concept study, we used ChatGPT and 
      Google Bard to reduce the reading level of three selected patient information 
      sections from scientific journals. ChatGPT successfully improved readability. 
      However, it could not achieve the recommended 6th-grade reading level. Bard 
      reached the reading level of 6th graders but oversimplified the texts by omitting 
      up to 83% of the content. Despite the present limitations, developers of patient 
      information are encouraged to employ large language models, preferably ChatGPT, 
      to optimize their materials.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of the 
      European Society of Cardiology.
FAU - Moons, Philip
AU  - Moons P
AUID- ORCID: 0000-0002-8609-4516
AD  - KU Leuven Department of Public Health and Primary Care, KU Leuven-University of 
      Leuven, Kapucijnenvoer 35 PB7001, 3000 Leuven, Belgium.
AD  - Institute of Health and Care Sciences, University of Gothenburg, Arvid Wallgrens 
      backe 1, 413 46 Gothenburg, Sweden.
AD  - Department of Paediatrics and Child Health, University of Cape Town, Klipfontein 
      Rd, Rondebosch, 7700 Cape Town, South Africa.
FAU - Van Bulck, Liesbet
AU  - Van Bulck L
AUID- ORCID: 0000-0001-8975-4455
AD  - KU Leuven Department of Public Health and Primary Care, KU Leuven-University of 
      Leuven, Kapucijnenvoer 35 PB7001, 3000 Leuven, Belgium.
AD  - Research Foundation Flanders (FWO), Leuvenseweg 38, 1000 Brussels, Belgium.
LA  - eng
GR  - 1159522N to L.V.B./Research Foundation Flanders/
PT  - Journal Article
PL  - England
TA  - Eur J Cardiovasc Nurs
JT  - European journal of cardiovascular nursing
JID - 101128793
SB  - IM
MH  - Humans
MH  - *Comprehension
MH  - *Search Engine
MH  - Educational Status
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Chatbot
OT  - Language models
OT  - Natural language processing
OT  - Nurses
OT  - Nursing
OT  - Patient education
OT  - Patient information
OT  - Reading level
COIS- Conflict of interest: none declared.
EDAT- 2023/08/21 18:42
MHDA- 2024/03/14 06:46
CRDT- 2023/08/21 16:33
PHST- 2023/07/14 00:00 [received]
PHST- 2023/08/16 00:00 [revised]
PHST- 2023/08/17 00:00 [accepted]
PHST- 2024/03/14 06:46 [medline]
PHST- 2023/08/21 18:42 [pubmed]
PHST- 2023/08/21 16:33 [entrez]
AID - 7246857 [pii]
AID - 10.1093/eurjcn/zvad087 [doi]
PST - ppublish
SO  - Eur J Cardiovasc Nurs. 2024 Mar 12;23(2):122-126. doi: 10.1093/eurjcn/zvad087.

PMID- 38441078
OWN - NLM
STAT- Publisher
LR  - 20240305
IS  - 1557-900X (Electronic)
IS  - 0892-7790 (Linking)
DP  - 2024 Mar 5
TI  - Evaluation of the Current Status of Artificial Intelligence for Endourology 
      Patient Education: A Blind Comparison of ChatGPT and Google Bard against 
      Traditional Information Resources.
LID - 10.1089/end.2023.0696 [doi]
AB  - Introduction Artificial intelligence (AI) platforms such as ChatGPT and Bard are 
      increasingly utilized to answer patient healthcare questions. We present the 
      first study to blindly evaluate AI-generated responses to common endourology 
      patient questions against official patient education materials. Methods 32 
      questions and answers spanning kidney stones, ureteral stents, BPH, and UTUC were 
      extracted from official Urology Care Foundation (UCF) patient education 
      documents. The same questions were input into ChatGPT 4.0 and Bard, limiting 
      responses to within  10% of the word count of the corresponding UCF response to 
      ensure fair comparison. Six endourologists blindly evaluated responses from each 
      platform using Likert scales for accuracy, clarity, comprehensiveness, and 
      patient utility. Reviewers identified which response they believed was not 
      AI-generated. Lastly, Flesch-Kincaid Reading Grade Level formulas assessed the 
      readability of each platform response. Ratings were compared using ANOVA and 
      Chi-Square tests. Results ChatGPT responses were rated the highest across all 
      categories including accuracy, comprehensiveness, clarity, and patient utility 
      while UCF answers were consistently scored the lowest, all p&lt;0.01. Sub-analysis 
      revealed that this trend was consistent across question categories (i.e., kidney 
      stones, BPH, etc.). However, AI-generated responses were more likely to be 
      classified at an advanced reading level while UCF responses showed improved 
      readability (college or higher reading level: ChatGPT = 100%, Bard = 66%, UCF = 
      19%), p&lt;0.001. When asked to identify which answer was not AI-generated, 54.2% of 
      responses indicated ChatGPT, 26.6% indicated Bard, and only 19.3% correctly 
      identified it as the UCF response. Conclusions In a blind evaluation, 
      AI-generated responses from ChatGPT and Bard surpassed the quality of official 
      patient education materials in endourology, suggesting that current AI platforms 
      are already a reliable resource for basic urologic care information. AI-generated 
      responses do, however, tend to require a higher reading level, which may limit 
      their applicability to a broader audience.
FAU - Connors, Christopher
AU  - Connors C
AD  - Icahn School of Medicine at Mount Sinai, 5925, Urology, New York, New York, 
      United States; christopher.connors@icahn.mssm.edu.
FAU - Gupta, Kavita
AU  - Gupta K
AD  - Icahn School of Medicine at Mount Sinai, 5925, Urology, New York, New York, 
      United States; Kavita.Gupta2@mountsinai.org.
FAU - Khusid, Johnathan Alexander
AU  - Khusid JA
AD  - Icahn School of Medicine at Mount Sinai, 5925, Urology, 1 Gustave Levy Pl., New 
      York, New York, United States, 10029-6574; johnathan.khusid@mountsinai.org.
FAU - Khargi, Raymond
AU  - Khargi R
AD  - Icahn School of Medicine at Mount Sinai, 5925, Urology, New York, New York, 
      United States; raymond.khargi2@mountsinai.org.
FAU - Yaghoubian, Alan
AU  - Yaghoubian A
AD  - University of California Los Angeles David Geffen School of Medicine, 12222, 
      Urology, Los Angeles, California, United States; alanyaghoubian@gmail.com.
FAU - Levy, Micah
AU  - Levy M
AD  - Icahn School of Medicine at Mount Sinai, 5925, Urology, 1 Gustave L. Levy Place, 
      New York, New York, United States, 10029; micah.levy@icahn.mssm.edu.
FAU - Gallante, Blair
AU  - Gallante B
AD  - Icahn School of Medicine at Mount Sinai, 5925, Urology, 425 W. 59th Street, Suite 
      4F, New York, New York, United States, 10019; blair.gallante@mountsinai.org.
FAU - Atallah, William
AU  - Atallah W
AD  - Icahn School of Medicine at Mount Sinai, 5925, Urology, New York, New York, 
      United States; william.atallah@mountsinai.org.
FAU - Gupta, Mantu
AU  - Gupta M
AD  - Icahn School of Medicine at Mount Sinai, 5925, Urology, New York, New York, 
      United States; mantu.gupta@mountsinai.org.
LA  - eng
PT  - Journal Article
DEP - 20240305
PL  - United States
TA  - J Endourol
JT  - Journal of endourology
JID - 8807503
SB  - IM
EDAT- 2024/03/05 12:47
MHDA- 2024/03/05 12:47
CRDT- 2024/03/05 09:49
PHST- 2024/03/05 12:47 [medline]
PHST- 2024/03/05 12:47 [pubmed]
PHST- 2024/03/05 09:49 [entrez]
AID - 10.1089/end.2023.0696 [doi]
PST - aheadofprint
SO  - J Endourol. 2024 Mar 5. doi: 10.1089/end.2023.0696.

PMID- 37811189
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231010
IS  - 0971-3026 (Print)
IS  - 1998-3808 (Electronic)
IS  - 0970-2016 (Linking)
VI  - 33
IP  - 4
DP  - 2023 Oct
TI  - Online for On Call: A Study Assessing the Use of Internet Resources Including 
      ChatGPT among On-Call Radiology Residents in India.
PG  - 440-449
LID - 10.1055/s-0043-1772465 [doi]
AB  - Background  The information-seeking behavior of the radiology residents on call 
      has undergone modernization in the recent times given the advent of easy to 
      access, reliable online resources, and robust artificial intelligence chatbots 
      such as Chat Generative Pre-Trained Transformer (ChatGPT). Purpose  The aim of 
      this study was to conduct a baseline analysis among the residents to understand 
      the best way to meet information needs in the future, spread awareness about the 
      existing resources, and narrow down to the most preferred online resource. 
      Methods and Materials  A prospective, descriptive study was performed using an 
      online survey instrument and was conducted among radiology residents in India. 
      They were questioned on their demographics, frequency of on call, fatigue 
      experienced on call, and preferred information resources and reasons for choosing 
      them. Results  A total of 286 residents participated in the survey. All residents 
      had used the Internet radiology resources during on-call duties. The most 
      preferred resource material was Radiopaedia followed by Radiology Assistant. 
      IMAIOS e-Anatomy was the most preferred anatomy resource. There was significant ( 
      p  &lt; 0.05) difference in relation to the use of closed edit peer-reviewed 
      literature among the two batches with it being used almost exclusively by third 
      year residents. In the artificial intelligence-aided ChatGPT section, 61.8% had 
      used the software at least once while being on call, of them 57.6% responded that 
      the information was inaccurate, 67.2% responded that the information was 
      insufficient to aid in diagnosis, 100% felt that the lack of images in the 
      software made it an unlikely resource that would be used by them in the future, 
      and 85.8% agreed that they would use it for providing reporting templates in the 
      future. In the suggestions for upcoming versions, 100% responded that images 
      should be included in the description provide by the chatbot, and 74.5% felt that 
      references for the information being provided should be included as it reaffirms 
      the reliability of the information. Conclusions  Presently, we find that 
      Radiopaedia met most of the requirements as an ideal online radiology resource 
      according to the residents. In the present-day scenario, ChatGPT is not 
      considered as an important on-call radiology education resource first because it 
      lacks images which is quintessential for a budding radiologist, and second, it 
      does not have any reference or proof for the information that it is providing. 
      However, it may be of help to nonmedical professionals who need to understand 
      radiology in layman's terms and to radiologists for patient report preparation 
      and research writing.
CI  - Indian Radiological Association. This is an open access article published by 
      Thieme under the terms of the Creative Commons 
      Attribution-NonDerivative-NonCommercial License, permitting copying and 
      reproduction so long as the original work is given appropriate credit. Contents 
      may not be used for commercial purposes, or adapted, remixed, transformed or 
      built upon. ( https://creativecommons.org/licenses/by-nc-nd/4.0/ ).
FAU - Sethi, Humsheer Singh
AU  - Sethi HS
AUID- ORCID: 0000-0002-2722-0148
AD  - Department of Radiodiagnosis, Institute of Medical Sciences and SUM Hospital, 
      Siksha 'O' Anusandhan deemed to be University, Odisha, India.
FAU - Mohapatra, Satya
AU  - Mohapatra S
AD  - Department of Radiodiagnosis, Institute of Medical Sciences and SUM Hospital, 
      Siksha 'O' Anusandhan deemed to be University, Odisha, India.
FAU - Mali, Chayasmita
AU  - Mali C
AD  - Department of Pathology, Kalinga Institute of Medical Sciences, Bhubaneswar, 
      Odisha, India.
FAU - Dubey, Roopak
AU  - Dubey R
AD  - Department of Radiodiagnosis, Kalinga Institute of Medical Sciences, Bhubaneswar, 
      Odisha, India.
LA  - eng
PT  - Journal Article
DEP - 20230821
PL  - Germany
TA  - Indian J Radiol Imaging
JT  - The Indian journal of radiology &amp; imaging
JID - 8503873
PMC - PMC10556306
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - on-call radiology
OT  - resident education
COIS- Conflict of Interest None declared.
EDAT- 2023/10/09 12:42
MHDA- 2023/10/09 12:43
PMCR- 2023/08/01
CRDT- 2023/10/09 06:14
PHST- 2023/10/09 12:43 [medline]
PHST- 2023/10/09 12:42 [pubmed]
PHST- 2023/10/09 06:14 [entrez]
PHST- 2023/08/01 00:00 [pmc-release]
AID - IJRI-23-5-2558 [pii]
AID - 10.1055/s-0043-1772465 [doi]
PST - epublish
SO  - Indian J Radiol Imaging. 2023 Aug 21;33(4):440-449. doi: 10.1055/s-0043-1772465. 
      eCollection 2023 Oct.

PMID- 37767452
OWN - NLM
STAT- Publisher
LR  - 20230929
IS  - 2249-4863 (Print)
IS  - 2278-7135 (Electronic)
IS  - 2249-4863 (Linking)
VI  - 12
IP  - 8
DP  - 2023 Aug
TI  - A pilot study on the capability of artificial intelligence in preparation of 
      patients' educational materials for Indian public health issues.
PG  - 1659-1662
LID - 10.4103/jfmpc.jfmpc_262_23 [doi]
AB  - BACKGROUND: Patient education is an essential component of improving public 
      health as it empowers individuals with the knowledge and skills necessary for 
      making informed decisions about their health and well-being. Primary care 
      physicians play a crucial role in patients' education as they are the first 
      contact between the patients and the healthcare system. However, they may not get 
      adequate time to prepare educational material for their patients. An artificial 
      intelligence-based writer like ChatGPT can help write the material for 
      physicians. AIM: This study aimed to ascertain the capability of ChatGPT for 
      generating patients' educational materials for common public health issues in 
      India. MATERIALS AND METHODS: This observational study was conducted on the 
      internet using the free research version of ChatGPT, a conversational artificial 
      intelligence that can generate human-like text output. We conversed with the 
      program with the question - "prepare a patients' education material for X in 
      India." In the X, we used the following words or phrases - "air pollution," 
      "malnutrition," "maternal and child health," "mental health," "noncommunicable 
      diseases," "road traffic accidents," "tuberculosis," and "water-borne diseases." 
      The textual response in the conversation was collected and stored for further 
      analysis. The text was analyzed for readability, grammatical errors, and text 
      similarity. RESULT: We generated a total of eight educational documents with a 
      median of 26 (Q1-Q3: 21.5-34) sentences with a median of 349 (Q1-Q3: 329-450.5) 
      words. The median Flesch Reading Ease Score was 48.2 (Q1-Q3: 39-50.65). It 
      indicates that the text can be understood by a college student. The text was 
      grammatically correct with very few (seven errors in 3415 words) errors. The text 
      was very clear in the majority (8 out of 9) of documents with a median score of 
      85 (Q1-Q3: 82.5-85) in 100. The overall text similarity index was 18% (Q1-Q3: 
      7.5-26). CONCLUSION: The research version of the ChatGPT (January 30, 2023 
      version) is capable of generating patients' educational materials for common 
      public health issues in India with a difficulty level ideal for college students 
      with high grammatical accuracy. However, the text similarity should be checked 
      before using it. Primary care physicians can take the help of ChatGPT for 
      generating text for materials used for patients' education.
CI  - Copyright: © 2023 Journal of Family Medicine and Primary Care.
FAU - Mondal, Himel
AU  - Mondal H
AD  - Department of Physiology, All India Institute of Medical Sciences, Deoghar, 
      Jharkhand, India.
FAU - Panigrahi, Muralidhar
AU  - Panigrahi M
AD  - Department of Pharmacology, Bhima Bhoi Medical College and Hospital, Balangir, 
      Odisha, India.
FAU - Mishra, Baidyanath
AU  - Mishra B
AD  - Department of Physiology, Sri Jagannath Medical College and Hospital, Puri, 
      Odisha, India.
FAU - Behera, Joshil K
AU  - Behera JK
AD  - Department of Physiology, Nagaland Institute of Medical Science and Research, 
      Nagaland, India.
FAU - Mondal, Shaikat
AU  - Mondal S
AD  - Department of Physiology, Raiganj Government Medical College and Hospital, West 
      Bengal, India.
LA  - eng
PT  - Journal Article
DEP - 20230829
PL  - India
TA  - J Family Med Prim Care
JT  - Journal of family medicine and primary care
JID - 101610082
PMC - PMC10521817
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - education
OT  - health awareness
OT  - intelligence
OT  - internet
OT  - patient
OT  - physicians
OT  - publication
COIS- There are no conflicts of interest.
EDAT- 2023/09/28 06:42
MHDA- 2023/09/28 06:42
PMCR- 2023/08/01
CRDT- 2023/09/28 04:16
PHST- 2023/02/08 00:00 [received]
PHST- 2023/06/09 00:00 [revised]
PHST- 2023/06/10 00:00 [accepted]
PHST- 2023/09/28 06:42 [medline]
PHST- 2023/09/28 06:42 [pubmed]
PHST- 2023/09/28 04:16 [entrez]
PHST- 2023/08/01 00:00 [pmc-release]
AID - JFMPC-12-1659 [pii]
AID - 10.4103/jfmpc.jfmpc_262_23 [doi]
PST - ppublish
SO  - J Family Med Prim Care. 2023 Aug;12(8):1659-1662. doi: 
      10.4103/jfmpc.jfmpc_262_23. Epub 2023 Aug 29.

PMID- 38235987
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 2245-1919 (Electronic)
IS  - 2245-1919 (Linking)
VI  - 71
IP  - 1
DP  - 2023 Dec 18
TI  - Correspondence on "cover letters written by ChatGPT-4 or humans".
PG  - 1
AB  - This is a letter to the editor on the article "A comparison of cover letters 
      written by ChatGPT-4 or humans" Dan Med J 2023;70(12):A06230412.
CI  - Published under Open Access CC-BY-NC-BD 4.0. 
      https://creativecommons.org/licenses/by-nc-nd/4.0/.
FAU - Daungsupawong, Hinpetch
AU  - Daungsupawong H
AD  - Private Academic Consultant, Phonhong, Lao People's Democratic Republic.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AD  - Department of Research Analytics, Saveetha Dental College and Hospitals, Saveetha 
      Institute of Medical and Technical Sciences Saveetha University India.
LA  - eng
PT  - Comment
PT  - Letter
DEP - 20231218
PL  - Denmark
TA  - Dan Med J
JT  - Danish medical journal
JID - 101576205
SB  - IM
CON - Dan Med J. 2023 Nov 23;70(12):. PMID: 38018708
EDAT- 2024/01/18 12:42
MHDA- 2024/01/18 12:43
CRDT- 2024/01/18 08:43
PHST- 2024/01/18 12:43 [medline]
PHST- 2024/01/18 12:42 [pubmed]
PHST- 2024/01/18 08:43 [entrez]
AID - A205177 [pii]
PST - epublish
SO  - Dan Med J. 2023 Dec 18;71(1):1.

PMID- 37937071
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231109
IS  - 2632-1297 (Electronic)
IS  - 2632-1297 (Linking)
VI  - 5
IP  - 6
DP  - 2023
TI  - To ChatGPT or not to ChatGPT: the use of artificial intelligence in writing 
      scientific papers.
PG  - fcad266
LID - 10.1093/braincomms/fcad266 [doi]
LID - fcad266
AB  - Our Scientific Editor discusses the current use of artificial intelligence in 
      writing academic papers and reports the updated guidelines for Brain 
      Communications on the use of this tool in scientific writing.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of the 
      Guarantors of Brain.
FAU - Marescotti, Manuela
AU  - Marescotti M
AUID- ORCID: 0000-0003-3398-3560
AD  - Edinburgh, UK.
LA  - eng
PT  - Editorial
DEP - 20231101
PL  - England
TA  - Brain Commun
JT  - Brain communications
JID - 101755125
PMC - PMC10627518
COIS- The author reports no competing interests.
EDAT- 2023/11/08 06:43
MHDA- 2023/11/08 06:44
PMCR- 2023/11/01
CRDT- 2023/11/08 04:00
PHST- 2023/10/06 00:00 [received]
PHST- 2023/10/06 00:00 [revised]
PHST- 2023/10/17 00:00 [accepted]
PHST- 2023/11/08 06:44 [medline]
PHST- 2023/11/08 06:43 [pubmed]
PHST- 2023/11/08 04:00 [entrez]
PHST- 2023/11/01 00:00 [pmc-release]
AID - fcad266 [pii]
AID - 10.1093/braincomms/fcad266 [doi]
PST - epublish
SO  - Brain Commun. 2023 Nov 1;5(6):fcad266. doi: 10.1093/braincomms/fcad266. 
      eCollection 2023.

PMID- 37485676
OWN - NLM
STAT- MEDLINE
DCOM- 20231204
LR  - 20240321
IS  - 1550-9397 (Electronic)
IS  - 1550-9389 (Print)
IS  - 1550-9389 (Linking)
VI  - 19
IP  - 12
DP  - 2023 Dec 1
TI  - Evaluating ChatGPT responses on obstructive sleep apnea for patient education.
PG  - 1989-1995
LID - 10.5664/jcsm.10728 [doi]
AB  - STUDY OBJECTIVES: We evaluated the quality of ChatGPT responses to questions on 
      obstructive sleep apnea for patient education and assessed how prompting the 
      chatbot influences correctness, estimated grade level, and references of answers. 
      METHODS: ChatGPT was queried 4 times with 24 identical questions. Queries 
      differed by initial prompting: no prompting, patient-friendly prompting, 
      physician-level prompting, and prompting for statistics/references. Answers were 
      scored on a hierarchical scale: incorrect, partially correct, correct, correct 
      with either statistic or referenced citation ("correct+"), or correct with both a 
      statistic and citation ("perfect"). Flesch-Kincaid grade level and citation 
      publication years were recorded for answers. Proportions of responses at 
      incremental score thresholds were compared by prompt type using chi-squared 
      analysis. The relationship between prompt type and grade level was assessed using 
      analysis of variance. RESULTS: Across all prompts (n = 96 questions), 69 answers 
      (71.9%) were at least correct. Proportions of responses that were at least 
      partially correct (P = .387) or correct (P = .453) did not differ by prompt; 
      responses that were at least correct+ (P &lt; .001) or perfect (P &lt; .001) did. 
      Statistics/references prompting provided 74/77 (96.1%) references. Responses from 
      patient-friendly prompting had a lower mean grade level (12.45 ± 2.32) than no 
      prompting (14.15 ± 1.59), physician-level prompting (14.27 ± 2.09), and 
      statistics/references prompting (15.00 ± 2.26) (P &lt; .0001). CONCLUSIONS: ChatGPT 
      overall provides appropriate answers to most questions on obstructive sleep apnea 
      regardless of prompting. While prompting decreases response grade level, all 
      responses remained above accepted recommendations for presenting medical 
      information to patients. Given ChatGPT's rapid implementation, sleep experts may 
      seek to further scrutinize its medical literacy and utility for patients. 
      CITATION: Campbell DJ, Estephan LE, Mastrolonardo EV, Amin DR, Huntley CT, Boon 
      MS. Evaluating ChatGPT responses on obstructive sleep apnea for patient 
      education. J Clin Sleep Med. 2023;19(12):1989-1995.
CI  - © 2023 American Academy of Sleep Medicine.
FAU - Campbell, Daniel J
AU  - Campbell DJ
AD  - Department of Otolaryngology - Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania.
FAU - Estephan, Leonard E
AU  - Estephan LE
AD  - Department of Otolaryngology - Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania.
FAU - Mastrolonardo, Eric V
AU  - Mastrolonardo EV
AD  - Department of Otolaryngology - Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania.
FAU - Amin, Dev R
AU  - Amin DR
AD  - Department of Otolaryngology - Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania.
FAU - Huntley, Colin T
AU  - Huntley CT
AD  - Department of Otolaryngology - Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania.
FAU - Boon, Maurits S
AU  - Boon MS
AD  - Department of Otolaryngology - Head and Neck Surgery, Thomas Jefferson University 
      Hospitals, Philadelphia, Pennsylvania.
LA  - eng
PT  - Journal Article
PL  - United States
TA  - J Clin Sleep Med
JT  - Journal of clinical sleep medicine : JCSM : official publication of the American 
      Academy of Sleep Medicine
JID - 101231977
SB  - IM
MH  - Humans
MH  - Patient Education as Topic
MH  - *Sleep Apnea, Obstructive/therapy
MH  - Sleep
MH  - *Physicians
MH  - Software
PMC - PMC10692937
OTO - NOTNLM
OT  - artificial intelligence
OT  - obstructive sleep apnea
OT  - patient education
OT  - sleep surgery
COIS- All authors have seen and approved the manuscript. Work for this study was 
      performed at Department of Otolaryngology – Head and Neck Surgery, Thomas 
      Jefferson University Hospitals. The authors report no conflicts of interest.
EDAT- 2023/07/24 06:42
MHDA- 2023/12/04 12:42
PMCR- 2024/12/01
CRDT- 2023/07/24 05:26
PHST- 2024/12/01 00:00 [pmc-release]
PHST- 2023/12/04 12:42 [medline]
PHST- 2023/07/24 06:42 [pubmed]
PHST- 2023/07/24 05:26 [entrez]
AID - jcsm.10728 [pii]
AID - JC2300220 [pii]
AID - 10.5664/jcsm.10728 [doi]
PST - ppublish
SO  - J Clin Sleep Med. 2023 Dec 1;19(12):1989-1995. doi: 10.5664/jcsm.10728.

PMID- 38064244
OWN - NLM
STAT- Publisher
LR  - 20231208
IS  - 1049-7323 (Print)
IS  - 1049-7323 (Linking)
DP  - 2023 Dec 8
TI  - Artificial Intelligence Augmented Qualitative Analysis: The Way of the Future?
PG  - 10497323231217392
LID - 10.1177/10497323231217392 [doi]
AB  - The artificial intelligence (AI) revolution is here and gathering momentum, 
      thanks to new models of natural language processing (NLP) and rapidly increasing 
      adoption by the public. NLP technology uses statistical analysis of language 
      structures to analyse and generate human language, using text or speech as its 
      source material. It can also be applied to visual mediums like images and videos. 
      A few qualitative research early adopters are beginning to adopt this technology 
      into their work, but our understanding of its potential remains in its infancy. 
      This article will define and describe NLP-based AI and discuss its benefits and 
      limitations for reflexive thematic analysis in health research. While there are 
      many platforms available, ChatGPT is the most well-known and accessible. A worked 
      example using ChatGPT to augment reflexive thematic analysis is provided to 
      illustrate potential application in practice. This article is intended to inspire 
      further conversation around the role of AI in qualitative research and offer 
      practical guidance for researchers seeking to adopt this technology.
FAU - Hitch, Danielle
AU  - Hitch D
AUID- ORCID: 0000-0003-2798-2246
AD  - Deakin University, Geelong, VIC, Australia. RINGGOLD: 2104
AD  - Western Health, Sunshine Hospital, St Albans, VIC, Australia.
LA  - eng
PT  - Journal Article
DEP - 20231208
PL  - United States
TA  - Qual Health Res
JT  - Qualitative health research
JID - 9202144
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - qualitative analysis
OT  - reflexive thematic analysis
COIS- Declaration of Conflicting InterestsThe author(s) declared no potential conflicts 
      of interest with respect to the research, authorship, and/or publication of this 
      article.
EDAT- 2023/12/08 12:42
MHDA- 2023/12/08 12:42
CRDT- 2023/12/08 11:43
PHST- 2023/12/08 12:42 [medline]
PHST- 2023/12/08 12:42 [pubmed]
PHST- 2023/12/08 11:43 [entrez]
AID - 10.1177/10497323231217392 [doi]
PST - aheadofprint
SO  - Qual Health Res. 2023 Dec 8:10497323231217392. doi: 10.1177/10497323231217392.

PMID- 37181985
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230516
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 4
DP  - 2023 Apr
TI  - Skin Metastasis of Low-Grade Ovarian Serous Carcinoma: A Case Report.
PG  - e37401
LID - 10.7759/cureus.37401 [doi]
LID - e37401
AB  - This case report, written with the assistance of ChatGPT, describes a rare 
      manifestation of ovarian serous carcinoma that metastasized to the skin. A 
      30-year-old female with a history of stage IV low-grade serous ovarian carcinoma 
      presented for evaluation of a painful nodule on her back. Physical examination 
      demonstrated a round, firm, mobile subcutaneous nodule on the left upper back. An 
      excisional biopsy was performed, and histopathologic examination was consistent 
      with metastatic ovarian serous carcinoma. This case highlights the clinical 
      presentation, histopathology, and treatment of cutaneous metastasis of serous 
      ovarian carcinoma. Additionally, this case highlights the value and technique of 
      using ChatGPT to assist in writing medical case reports including&nbsp;outlining, 
      referencing, summarizing studies, and formatting citations.
CI  - Copyright © 2023, Ching et al.
FAU - Ching, Lauren M
AU  - Ching LM
AD  - Department of Dermatology, Georgetown University School of Medicine, Washington, 
      DC, USA.
FAU - Tran, Benjamin A
AU  - Tran BA
AD  - Department of Dermatology, MedStar Georgetown University Hospital, Washington, 
      DC, USA.
FAU - Russomanno, Kristen L
AU  - Russomanno KL
AD  - Department of Dermatology, MedStar Georgetown University Hospital, Washington, 
      DC, USA.
FAU - Cardis, Michael A
AU  - Cardis MA
AD  - Department of Dermatology, MedStar Washington Hospital Center, Washington, DC, 
      USA.
LA  - eng
PT  - Case Reports
DEP - 20230410
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10171884
OTO - NOTNLM
OT  - chatgpt
OT  - cutaneous metastasis
OT  - low-grade serous carcinoma
OT  - ovarian carcinoma
OT  - serous carcinoma
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/05/14 19:13
MHDA- 2023/05/14 19:14
PMCR- 2023/04/10
CRDT- 2023/05/14 12:26
PHST- 2023/04/10 00:00 [accepted]
PHST- 2023/05/14 19:14 [medline]
PHST- 2023/05/14 19:13 [pubmed]
PHST- 2023/05/14 12:26 [entrez]
PHST- 2023/04/10 00:00 [pmc-release]
AID - 10.7759/cureus.37401 [doi]
PST - epublish
SO  - Cureus. 2023 Apr 10;15(4):e37401. doi: 10.7759/cureus.37401. eCollection 2023 
      Apr.
</pre>
      
    </div>
  </main>


  
  


  


</body></html>