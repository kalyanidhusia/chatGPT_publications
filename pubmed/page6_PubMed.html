<!DOCTYPE html>
<!-- saved from url=(0075)https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&page=6&format=pubmed&size=200 -->
<html lang="en"><head itemscope="" itemtype="http://schema.org/WebPage" prefix="og: http://ogp.me/ns#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Mobile properties -->
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov/">
  <link rel="preconnect" href="https://www.ncbi.nlm.nih.gov/">
  <link rel="preconnect" href="https://www.google-analytics.com/">

  
  
    <link rel="stylesheet" href="./page6_PubMed_files/output.5ecf62baa0fa.css" type="text/css">
  

  <link rel="stylesheet" href="./page6_PubMed_files/output.452c70ce66f7.css" type="text/css">

  
    
  

  
    <link rel="stylesheet" href="./page6_PubMed_files/output.97c300a159d1.css" type="text/css">
  

  


    <title>chatGPT - Search Results - PubMed</title>

  
  
  <!-- Favicons -->
  <link rel="shortcut icon" type="image/ico" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.ico">
  <link rel="icon" type="image/png" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon.png">

  <!-- 192x192, as recommended for Android
  http://updates.html5rocks.com/2014/11/Support-for-theme-color-in-Chrome-39-for-Android
  -->
  <link rel="icon" type="image/png" sizes="192x192" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-192.png">

  <!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
  <link rel="apple-touch-icon-precomposed" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-57.png">
  <!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-72.png">
  <!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-114.png">
  <!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/favicons/favicon-144.png">


  <!-- For Pinger + Google Optimize integration (NS-820) -->
  <meta name="ncbi_sg_optimize_id" content="">

  <!-- Mobile browser address bar color -->
  <meta name="theme-color" content="#20558a">

  <!-- Preserve the Referrer when going from HTTPS to HTTP -->
  <meta name="referrer" content="origin-when-cross-origin">

  <meta name="ncbi_pinger_gtm_track" content="true">
<!-- Logging params: Pinger defaults -->

  
    <meta name="ncbi_app" content="pubmed">
  

  
    <meta name="ncbi_db" content="pubmed">
  

  
    <meta name="ncbi_phid" content="8F280001DD391CC5000033912EBCEED8.1.m_7">
  

  
    <meta name="ncbi_pinger_stat_url" content="https://www.ncbi.nlm.nih.gov/stat">
  

  
    <meta name="log_category" content="literature">
  

  
    <meta name="ncbi_cost_center" content="pubmed">
  



  <!-- Logging params: Pinger custom -->
  
    <meta name="log_op" content="search">
  
    <meta name="log_query" content="chatGPT">
  
    <meta name="ncbi_pdid" content="searchresult">
  
    <meta name="ncbi_pageno" content="6">
  
    <meta name="log_resultcount" content="2844">
  
    <meta name="log_userterm" content="chatGPT">
  
    <meta name="log_processedquery" content="&quot;chatGPT&quot;[All Fields]">
  
    <meta name="log_filtersactive" content="False">
  
    <meta name="log_filters" content="">
  
    <meta name="ncbi_log_query" content="chatGPT">
  
    <meta name="log_proximity_search_active" content="False">
  
    <meta name="log_format" content="pubmed">
  
    <meta name="log_sortorder" content="relevance">
  
    <meta name="log_pagesize" content="200">
  
    <meta name="log_displayeduids" content="38489133,37065337,37406864,38133626,37779171,37654681,38443209,37615692,38300696,38239498,36934624,38189213,37038572,38029273,38353076,37198001,38065864,37823708,37077800,37398259,38201418,37602755,37515033,38383555,38267726,38563415,37328703,37952568,38126285,38358993,38034065,37301435,38227351,38081504,38181885,37380584,38180493,37428540,38112589,37401180,37104260,37082496,38447265,38440018,37747487,37596194,37975899,37792871,37162073,37643186,38558009,37866949,37097229,38271674,38096831,37915603,37776670,37223340,37536678,37794649,38054196,37815084,37465170,37179029,38188345,37880412,37181995,37025739,37190006,37851495,37731897,37940756,38502607,37041067,37651677,38384621,37492313,37378043,36763148,38024074,38145486,38163049,37392002,37827724,38315183,38165624,37821756,37999958,38483451,38361810,37675304,37950763,37056538,38078148,37598727,37426542,37972444,37083166,37261590,37099761,37094207,38520470,38312244,38300168,38277081,37985815,38318684,37972974,38300581,38170691,37474421,38046723,37489920,37129631,38321995,38078915,37103921,38250148,38076046,38486198,36968864,36909565,36960444,38294466,38162975,37958050,38371109,36945641,37431972,38545679,38319619,38157785,36925365,38427906,37606629,36895547,38270616,38224925,38188855,38341517,38289662,38239542,38305909,38161881,37638368,37286898,37270457,37889368,37428357,38545764,38353523,37726551,37851468,36915398,36875254,36840450,37898341,37396972,37436486,37046397,38166323,38495947,38227177,37995379,37090271,38353783,37523010,37378876,38095605,37572695,37379558,37904927,37548259,37831553,37743152,38509182,37949663,38466827,38147494,37257860,38464946,38056135,37332007,38415145,38337430,37609457,38052322,37361298,37713254,38042556,37073196,37257586,38093584,38046053,37998447,37528548,36872153,38130802,38312403,37890505">
  
    <meta name="ncbi_search_id" content="R3Cs2GC7H0x0KE8U7UASSA:deef7cbfe7e81e7152f8e0e261e5ab1a">
  
    <meta name="ncbi_adj_nav_search_id" content="1zjIYrqMaHLHq046cM1ZlQ:fbd847f89c96214ac23b7c62d7cb721a">
  



  <!-- Social meta tags for unfurling urls -->
  
<meta name="description" content="chatGPT - Search Results - PubMed"><meta name="robots" content="noindex,follow,noarchive"><meta property="og:title" content="chatGPT - Search Results - PubMed"><meta property="og:url" content="https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&amp;page=6&amp;format=pubmed&amp;size=200"><meta property="og:description" content="chatGPT - Search Results - PubMed"><meta property="og:image" content="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/pubmed-meta-image-v2.jpg"><meta property="og:image:secure_url" content="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/pubmed-meta-image-v2.jpg"><meta property="og:type" content="website"><meta property="og:site_name" content="PubMed"><meta name="twitter:domain" content="pubmed.ncbi.nlm.nih.gov"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="chatGPT - Search Results - PubMed"><meta name="twitter:url" content="https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&amp;page=6&amp;format=pubmed&amp;size=200"><meta name="twitter:description" content="chatGPT - Search Results - PubMed"><meta name="twitter:image" content="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/pubmed-meta-image-v2.jpg">


  <!-- OpenSearch XML -->
  <link rel="search" type="application/opensearchdescription+xml" href="https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/opensearch.xml" title="PubMed search">

  <!-- Disables severely broken elements when no JS -->
  <noscript>
    <link rel="stylesheet" type="text/css" href="https://cdn.ncbi.nlm.nih.gov/pubmed/09ad9aad-98d9-47ec-b2ea-fb4dba3d550d/core/no-script.css">
  </noscript>

  
    <link rel="canonical" href="https://pubmed.ncbi.nlm.nih.gov/?term=chatGPT&amp;page=6&amp;format=pubmed&amp;size=200">
  


</head>
<body>

  
  <main class="search-page" id="search-page">
    <div class="search-results" id="search-results">
      
        <pre class="search-results-chunk">PMID- 38489133
OWN - NLM
STAT- Publisher
LR  - 20240315
IS  - 1559-0100 (Electronic)
IS  - 1355-008X (Linking)
DP  - 2024 Mar 15
TI  - Assessment of ChatGPT's adherence to ETA-thyroid nodule management guideline over 
      two different time intervals 14 days apart: in binary and multiple-choice 
      queries.
LID - 10.1007/s12020-024-03750-2 [doi]
AB  - OBJECTIVE: Artificial intelligence (AI) has significant potential in healthcare, 
      particularly in providing decision-support in specialized domains like thyroid 
      nodule management. This study assesses the effectiveness of ChatGPT-v4, an 
      advanced AI model, in aligning with the European Thyroid Association (ETA) - 2023 
      guidelines. METHODS: The study utilized a structured questionnaire comprising 100 
      questions, divided into true/false and multiple-choice formats, reflecting 
      real-world clinical scenarios in thyroid nodule management. These questions 
      encompassed diagnostic criteria, treatment options, follow-up protocols, and 
      patient counseling. ChatGPT response was evaluated for accuracy, consistency, and 
      comprehensiveness using a six-point Likert scale. The assessment occurred 
      initially and was repeated after 14 days. RESULTS: In the binary queries, the AI 
      model showed an ability to correct some initially incorrect responses. However, 
      there was a noticeable regression in certain responses. 8 of the 11 previously 
      non-compliant responses remained unchanged, while 3 non-compliant responses were 
      rectified. Conversely, 6 initially compliant answers transitioned to 
      non-compliance after 14 days. In multiple-choice queries, the AI's performance 
      was more consistent. A majority of the responses, 43 (86% of the total), were 
      initially correct and maintained their correctness upon re-assessment. However, 4 
      responses that were initially incorrect remained unchanged, and 3 correct 
      responses shifted to non-compliance over time. CONCLUSION: ChatGPT exhibited 
      improving potential as a clinical support tool in thyroid nodule management 
      altgouh it showed varied performance for binary and multiple-choice questions. 
      CLINICAL TRIAL REGISTRATION: N/A.
CI  - © 2024. The Author(s), under exclusive licence to Springer Science+Business 
      Media, LLC, part of Springer Nature.
FAU - Deniz, Muzaffer Serdar
AU  - Deniz MS
AUID- ORCID: 0000-0002-8905-3955
AD  - Department of Endocrinology, Sincan Education and Research Hospital, Ankara, 
      Turkey. md.mserdardeniz@gmail.com.
FAU - Guler, Bagdagul Yuksel
AU  - Guler BY
AUID- ORCID: 0000-0002-8023-2666
AD  - Department of Endocrinology, Sincan Education and Research Hospital, Ankara, 
      Turkey.
LA  - eng
PT  - Journal Article
DEP - 20240315
PL  - United States
TA  - Endocrine
JT  - Endocrine
JID - 9434444
SB  - IM
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - ChatGPT-v4
OT  - European Thyroid Association Guidelines
OT  - Thyroid Nodule Management
EDAT- 2024/03/15 18:42
MHDA- 2024/03/15 18:42
CRDT- 2024/03/15 12:19
PHST- 2024/01/06 00:00 [received]
PHST- 2024/02/15 00:00 [accepted]
PHST- 2024/03/15 18:42 [medline]
PHST- 2024/03/15 18:42 [pubmed]
PHST- 2024/03/15 12:19 [entrez]
AID - 10.1007/s12020-024-03750-2 [pii]
AID - 10.1007/s12020-024-03750-2 [doi]
PST - aheadofprint
SO  - Endocrine. 2024 Mar 15. doi: 10.1007/s12020-024-03750-2.

PMID- 37065337
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230418
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - Langerhans Cell Histiocytosis Presenting With Clinical Features of Hidradenitis 
      Suppurativa.
PG  - e36201
LID - 10.7759/cureus.36201 [doi]
LID - e36201
AB  - Langerhans cell histiocytosis (LCH) is a rare neoplastic disease of myeloid 
      dendritic cells with a widely variable presentation of organ system involvement 
      and severity. In this case report, we share the details of a rare case of 
      cutaneous LCH resembling hidradenitis suppurativa (HS).
CI  - Copyright © 2023, Yousif et al.
FAU - Yousif, Miranda L
AU  - Yousif ML
AD  - College of Medicine, University of Arizona College of Medicine - Phoenix, 
      Phoenix, USA.
FAU - Faulkner, Claire S
AU  - Faulkner CS
AD  - College of Medicine, University of Arizona College of Medicine - Phoenix, 
      Phoenix, USA.
FAU - Harper, Lise
AU  - Harper L
AD  - Internal Medicine, Banner University Medical Center, Phoenix, USA.
FAU - Ackerman, Lindsay
AU  - Ackerman L
AD  - Dermatology, Banner University Medical Center, Phoenix, USA.
LA  - eng
PT  - Case Reports
DEP - 20230315
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10104688
OTO - NOTNLM
OT  - chatgpt
OT  - chatgpt improved case report
OT  - hidradenitis suppurative
OT  - langerhans cell histiocytosis(lch)
OT  - neoplastic disease
OT  - rare cancers of female genital tract
OT  - rare skin disease
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/04/18 06:00
MHDA- 2023/04/18 06:01
PMCR- 2023/03/15
CRDT- 2023/04/17 03:54
PHST- 2023/03/15 00:00 [accepted]
PHST- 2023/04/18 06:01 [medline]
PHST- 2023/04/17 03:54 [entrez]
PHST- 2023/04/18 06:00 [pubmed]
PHST- 2023/03/15 00:00 [pmc-release]
AID - 10.7759/cureus.36201 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 15;15(3):e36201. doi: 10.7759/cureus.36201. eCollection 2023 
      Mar.

PMID- 37406864
OWN - NLM
STAT- Publisher
LR  - 20231011
IS  - 1527-9995 (Electronic)
IS  - 0090-4295 (Linking)
VI  - 180
DP  - 2023 Oct
TI  - Can ChatGPT, an Artificial Intelligence Language Model, Provide Accurate and 
      High-quality Patient Information on Prostate Cancer?
PG  - 35-58
LID - S0090-4295(23)00570-8 [pii]
LID - 10.1016/j.urology.2023.05.040 [doi]
AB  - OBJECTIVE: To evaluate the performance of ChatGPT, an artificial intelligence 
      (AI) language model, in providing patient information on prostate cancer, and to 
      compare the accuracy, similarity, and quality of the information to a reference 
      source. METHODS: Patient information material on prostate cancer was used as a 
      reference source from the website of the European Association of Urology Patient 
      Information. This was used to generate 59 queries. The accuracy of the model's 
      content was determined with F1, precision, and recall scores. The similarity was 
      assessed with cosine similarity, and the quality was evaluated using a 5-point 
      Likert scale named General Quality Score (GQS). RESULTS: ChatGPT was able to 
      respond to all prostate cancer-related queries. The average F1 score was 0.426 
      (range: 0-1), precision score was 0.349 (range: 0-1), recall score was 0.549 
      (range: 0-1), and cosine similarity was 0.609 (range: 0-1). The average GQS was 
      3.62 ± 0.49 (range: 1-5), with no answers achieving the maximum GQS of 5. While 
      ChatGPT produced a larger amount of information compared to the reference, the 
      accuracy and quality of the content were not optimal, with all scores indicating 
      need for improvement in the model's performance. CONCLUSION: Caution should be 
      exercised when using ChatGPT as a patient information source for prostate cancer 
      due to limitations in its performance, which may lead to inaccuracies and 
      potential misunderstandings. Further studies, using different topics and language 
      models, are needed to fully understand the capabilities and limitations of 
      AI-generated patient information.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Coskun, Burhan
AU  - Coskun B
AD  - Bursa Uludag University, Department of Urology, Nilüfer, Bursa, Turkey. 
      Electronic address: burhanc@uludag.edu.tr.
FAU - Ocakoglu, Gokhan
AU  - Ocakoglu G
AD  - Bursa Uludag University, Department of Biostatistics, Nilüfer, Bursa, Turkey.
FAU - Yetemen, Melih
AU  - Yetemen M
AD  - Bursa Uludag University, Department of Urology, Nilüfer, Bursa, Turkey.
FAU - Kaygisiz, Onur
AU  - Kaygisiz O
AD  - Bursa Uludag University, Department of Urology, Nilüfer, Bursa, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20230704
PL  - United States
TA  - Urology
JT  - Urology
JID - 0366151
SB  - IM
COIS- Declaration of Competing Interest The authors declare that they have no conflict 
      of interests.
EDAT- 2023/07/06 01:08
MHDA- 2023/07/06 01:08
CRDT- 2023/07/05 19:28
PHST- 2023/03/11 00:00 [received]
PHST- 2023/04/24 00:00 [revised]
PHST- 2023/05/23 00:00 [accepted]
PHST- 2023/07/06 01:08 [pubmed]
PHST- 2023/07/06 01:08 [medline]
PHST- 2023/07/05 19:28 [entrez]
AID - S0090-4295(23)00570-8 [pii]
AID - 10.1016/j.urology.2023.05.040 [doi]
PST - ppublish
SO  - Urology. 2023 Oct;180:35-58. doi: 10.1016/j.urology.2023.05.040. Epub 2023 Jul 4.

PMID- 38133626
OWN - NLM
STAT- Publisher
LR  - 20231222
IS  - 1537-2677 (Electronic)
IS  - 0740-9303 (Linking)
DP  - 2023 Dec 19
TI  - Generating Informed Consent Documents Related to Blepharoplasty Using ChatGPT.
LID - 10.1097/IOP.0000000000002574 [doi]
AB  - PURPOSE: This study aimed to demonstrate the performance of the popular 
      artificial intelligence (AI) language model, Chat Generative Pre-trained 
      Transformer (ChatGPT) (OpenAI, San Francisco, CA, U.S.A.), in generating the 
      informed consent (IC) document of blepharoplasty. METHODS: A total of 2 prompts 
      were provided to ChatGPT to generate IC documents. Four board-certified plastic 
      surgeons and 4 nonmedical staff members evaluated the AI-generated IC documents 
      and the original IC document currently used in the clinical setting. They 
      assessed these documents in terms of accuracy, informativeness, and 
      accessibility. RESULTS: Among board-certified plastic surgeons, the initial 
      AI-generated IC document scored significantly lower than the original IC document 
      in accuracy (p &lt; 0.001), informativeness (p = 0.005), and accessibility (p = 
      0.021), while the revised AI-generated IC document scored lower compared with the 
      original document in accuracy (p = 0.03) and accessibility (p = 0.021). Among 
      nonmedical staff members, no statistical significance of 2 AI-generated IC 
      documents was observed compared with the original document in terms of accuracy, 
      informativeness, and accessibility. CONCLUSIONS: Our results showed that current 
      ChatGPT cannot be used as a distinct patient education resource. However, it has 
      the potential to make better IC documents when improving the professional 
      terminology. This AI technology will eventually transform ophthalmic plastic 
      surgery healthcare systematics by enhancing patient education and decision-making 
      via IC documents.
CI  - Copyright © 2023 The American Society of Ophthalmic Plastic and Reconstructive 
      Surgery, Inc.
FAU - Shiraishi, Makoto
AU  - Shiraishi M
AUID- ORCID: 0000-0002-3734-2085
AD  - Department of Plastic and Reconstructive Surgery, The University of Tokyo 
      Hospital, Tokyo, Japan.
FAU - Tomioka, Yoko
AU  - Tomioka Y
FAU - Miyakuni, Ami
AU  - Miyakuni A
FAU - Moriwaki, Yuta
AU  - Moriwaki Y
FAU - Yang, Rui
AU  - Yang R
FAU - Oba, Jun
AU  - Oba J
FAU - Okazaki, Mutsumi
AU  - Okazaki M
LA  - eng
PT  - Journal Article
DEP - 20231219
PL  - United States
TA  - Ophthalmic Plast Reconstr Surg
JT  - Ophthalmic plastic and reconstructive surgery
JID - 8508431
SB  - IM
COIS- The authors have no financial or conflicts of interest to disclose.
EDAT- 2023/12/22 12:41
MHDA- 2023/12/22 12:41
CRDT- 2023/12/22 10:34
PHST- 2023/12/22 12:41 [medline]
PHST- 2023/12/22 12:41 [pubmed]
PHST- 2023/12/22 10:34 [entrez]
AID - 00002341-990000000-00306 [pii]
AID - 10.1097/IOP.0000000000002574 [doi]
PST - aheadofprint
SO  - Ophthalmic Plast Reconstr Surg. 2023 Dec 19. doi: 10.1097/IOP.0000000000002574.

PMID- 37779171
OWN - NLM
STAT- MEDLINE
DCOM- 20231003
LR  - 20231121
IS  - 2045-2322 (Electronic)
IS  - 2045-2322 (Linking)
VI  - 13
IP  - 1
DP  - 2023 Oct 1
TI  - Comparing ChatGPT and GPT-4 performance in USMLE soft skill assessments.
PG  - 16492
LID - 10.1038/s41598-023-43436-9 [doi]
LID - 16492
AB  - The United States Medical Licensing Examination (USMLE) has been a subject of 
      performance study for artificial intelligence (AI) models. However, their 
      performance on questions involving USMLE soft skills remains unexplored. This 
      study aimed to evaluate ChatGPT and GPT-4 on USMLE questions involving 
      communication skills, ethics, empathy, and professionalism. We used 80 
      USMLE-style questions involving soft skills, taken from the USMLE website and the 
      AMBOSS question bank. A follow-up query was used to assess the models' 
      consistency. The performance of the AI models was compared to that of previous 
      AMBOSS users. GPT-4 outperformed ChatGPT, correctly answering 90% compared to 
      ChatGPT's 62.5%. GPT-4 showed more confidence, not revising any responses, while 
      ChatGPT modified its original answers 82.5% of the time. The performance of GPT-4 
      was higher than that of AMBOSS's past users. Both AI models, notably GPT-4, 
      showed capacity for empathy, indicating AI's potential to meet the complex 
      interpersonal, ethical, and professional demands intrinsic to the practice of 
      medicine.
CI  - © 2023. Springer Nature Limited.
FAU - Brin, Dana
AU  - Brin D
AD  - Department of Diagnostic Imaging, Chaim Sheba Medical Center, Ramat Gan, Israel. 
      dannabrin@gmail.com.
AD  - Faculty of Medicine, Tel-Aviv University, Tel-Aviv, Israel. dannabrin@gmail.com.
FAU - Sorin, Vera
AU  - Sorin V
AD  - Department of Diagnostic Imaging, Chaim Sheba Medical Center, Ramat Gan, Israel.
AD  - Faculty of Medicine, Tel-Aviv University, Tel-Aviv, Israel.
FAU - Vaid, Akhil
AU  - Vaid A
AD  - The Charles Bronfman Institute of Personalized Medicine, Icahn School of Medicine 
      at Mount Sinai, New York, NY, USA.
FAU - Soroush, Ali
AU  - Soroush A
AD  - Division of Data-Driven and Digital Medicine (D3M), Icahn School of Medicine at 
      Mount Sinai, New York, NY, USA.
FAU - Glicksberg, Benjamin S
AU  - Glicksberg BS
AD  - Hasso Plattner Institute for Digital Health, Icahn School of Medicine at Mount 
      Sinai, New York, NY, USA.
FAU - Charney, Alexander W
AU  - Charney AW
AD  - The Charles Bronfman Institute of Personalized Medicine, Icahn School of Medicine 
      at Mount Sinai, New York, NY, USA.
FAU - Nadkarni, Girish
AU  - Nadkarni G
AD  - Division of Data-Driven and Digital Medicine (D3M), The Charles Bronfman 
      Institute of Personalized Medicine, Icahn School of Medicine at Mount Sinai, New 
      York, NY, USA.
FAU - Klang, Eyal
AU  - Klang E
AD  - Department of Diagnostic Imaging, Chaim Sheba Medical Center, Ramat Gan, Israel.
AD  - Faculty of Medicine, Tel-Aviv University, Tel-Aviv, Israel.
LA  - eng
PT  - Journal Article
DEP - 20231001
PL  - England
TA  - Sci Rep
JT  - Scientific reports
JID - 101563288
SB  - IM
MH  - *Artificial Intelligence
MH  - *Medicine
MH  - Empathy
MH  - Mental Processes
PMC - PMC10543445
COIS- The authors declare no competing interests.
EDAT- 2023/10/02 00:41
MHDA- 2023/10/03 06:47
PMCR- 2023/10/01
CRDT- 2023/10/01 23:09
PHST- 2023/07/26 00:00 [received]
PHST- 2023/09/23 00:00 [accepted]
PHST- 2023/10/03 06:47 [medline]
PHST- 2023/10/02 00:41 [pubmed]
PHST- 2023/10/01 23:09 [entrez]
PHST- 2023/10/01 00:00 [pmc-release]
AID - 10.1038/s41598-023-43436-9 [pii]
AID - 43436 [pii]
AID - 10.1038/s41598-023-43436-9 [doi]
PST - epublish
SO  - Sci Rep. 2023 Oct 1;13(1):16492. doi: 10.1038/s41598-023-43436-9.

PMID- 37654681
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230902
IS  - 2169-7574 (Print)
IS  - 2169-7574 (Electronic)
IS  - 2169-7574 (Linking)
VI  - 11
IP  - 8
DP  - 2023 Aug
TI  - Evaluation of Artificial Intelligence-generated Responses to Common Plastic 
      Surgery Questions.
PG  - e5226
LID - 10.1097/GOX.0000000000005226 [doi]
LID - e5226
AB  - BACKGROUND: Artificial intelligence (AI) is increasingly used to answer 
      questions, yet the accuracy and validity of current tools are uncertain. In 
      contrast to internet queries, AI generates summary responses as definitive. The 
      internet is rife with inaccuracies, and plastic surgery management guidelines 
      evolve, making verifiable information important. METHODS: We posed 10 questions 
      about breast implant-associated illness, anaplastic large lymphoma, and squamous 
      carcinoma to Bing, using the "more balanced" option, and to ChatGPT. Answers were 
      reviewed by two plastic surgeons for accuracy and fidelity to information on the 
      Food and Drug Administration (FDA) and American Society of Plastic Surgeons 
      (ASPS) websites. We also presented 10 multiple-choice questions from the 2022 
      plastic surgery in-service examination to Bing, using the "more precise" option, 
      and ChatGPT. Questions were repeated three times over consecutive weeks, and 
      answers were evaluated for accuracy and stability. RESULTS: Compared with answers 
      from the FDA and ASPS, Bing and ChatGPT were accurate. Bing answered 10 of the 30 
      multiple-choice questions correctly, nine incorrectly, and did not answer 11. 
      ChatGPT correctly answered 16 and incorrectly answered 14. In both parts, 
      responses from Bing were shorter, less detailed, and referred to verified and 
      unverified sources; ChatGPT did not provide citations. CONCLUSIONS: These AI 
      tools provided accurate information from the FDA and ASPS websites, but neither 
      consistently answered questions requiring nuanced decision-making correctly. 
      Advances in applications to plastic surgery will require algorithms that 
      selectively identify, evaluate, and exclude information to enhance the accuracy, 
      precision, validity, reliability, and utility of AI-generated responses.
CI  - Copyright © 2023 The Authors. Published by Wolters Kluwer Health, Inc. on behalf 
      of The American Society of Plastic Surgeons.
FAU - Copeland-Halperin, Libby R
AU  - Copeland-Halperin LR
AD  - From Northwell Health, New York, N.Y.
FAU - O'Brien, Lauren
AU  - O'Brien L
AD  - Michelle Copeland DMD MD PC, New York, N.Y.
FAU - Copeland, Michelle
AU  - Copeland M
AD  - Icahn School of Medicine at Mount Sinai, New York, N.Y.
LA  - eng
PT  - Journal Article
DEP - 20230830
PL  - United States
TA  - Plast Reconstr Surg Glob Open
JT  - Plastic and reconstructive surgery. Global open
JID - 101622231
PMC - PMC10468106
COIS- The authors have no financial interest to declare in relation to the content of 
      this article. Disclosure statements are at the end of this article, following the 
      correspondence information.
EDAT- 2023/09/01 06:43
MHDA- 2023/09/01 06:44
PMCR- 2023/08/30
CRDT- 2023/09/01 03:59
PHST- 2023/05/31 00:00 [received]
PHST- 2023/07/13 00:00 [accepted]
PHST- 2023/09/01 06:44 [medline]
PHST- 2023/09/01 06:43 [pubmed]
PHST- 2023/09/01 03:59 [entrez]
PHST- 2023/08/30 00:00 [pmc-release]
AID - 10.1097/GOX.0000000000005226 [doi]
PST - epublish
SO  - Plast Reconstr Surg Glob Open. 2023 Aug 30;11(8):e5226. doi: 
      10.1097/GOX.0000000000005226. eCollection 2023 Aug.

PMID- 38443209
OWN - NLM
STAT- MEDLINE
DCOM- 20240325
LR  - 20240325
IS  - 1476-5411 (Electronic)
IS  - 1367-0484 (Linking)
VI  - 47
IP  - 2
DP  - 2024 Apr
TI  - Assessing the proficiency of artificial intelligence programs in the diagnosis 
      and treatment of cornea, conjunctiva, and eyelid diseases and exploring the 
      advantages of each other benefits.
PG  - 102125
LID - S1367-0484(24)00008-0 [pii]
LID - 10.1016/j.clae.2024.102125 [doi]
AB  - PURPOSE: It was aimed to determine the knowledge level of ChatGPT, Bing, and Bard 
      artificial intelligence programs related to corneal, conjunctival, and eyelid 
      diseases and treatment modalities, to examine their reliability and superiority 
      to each other. METHODS: Forty-one questions related to corneal, conjunctival, and 
      eyelid diseases and treatment modalities were asked to the ChatGPT, Bing, and 
      Bard chatbots. The answers to the questions were compared with the answer keys 
      and grouped as correct or incorrect. Accuracy rates were compared. RESULTS: 
      ChatGPT gave the correct answer to 51.2&nbsp;% of the questions asked, Bing gave the 
      correct answer to 53.7&nbsp;%, and Bard gave the correct answer to 68.3&nbsp;%. There was 
      no significant difference in the rate of correct or incorrect answers to the 
      questions asked for the 3 artificial intelligence chatbots (p&nbsp;=&nbsp;0.208, Pearson's 
      chi-square test). CONCLUSION: Although information about the cornea, conjunctiva, 
      and eyelid diseases and treatment modalities can be accessed quickly and 
      accurately using up-to-date artificial intelligence programs, the answers may not 
      always be accurate and up-to-date. Care should be taken when evaluating this 
      information.
CI  - Copyright © 2024 British Contact Lens Association. Published by Elsevier Ltd. All 
      rights reserved.
FAU - Sensoy, Eyupcan
AU  - Sensoy E
AD  - Ankara Etlik City Hospital, Ankara, Turkey. Electronic address: 
      dreyupcansensoy@yahoo.com.
FAU - Citirik, Mehmet
AU  - Citirik M
AD  - Ankara Etlik City Hospital, Ankara, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20240304
PL  - England
TA  - Cont Lens Anterior Eye
JT  - Contact lens &amp; anterior eye : the journal of the British Contact Lens Association
JID - 9712714
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Reproducibility of Results
MH  - Conjunctiva
MH  - Cornea
MH  - *Eyelid Diseases/diagnosis/therapy
OTO - NOTNLM
OT  - Bard
OT  - Bing
OT  - ChatGPT
OT  - Conjunctiva
OT  - Cornea
OT  - Eyelid
COIS- Declaration of Competing Interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2024/03/06 00:42
MHDA- 2024/03/25 06:42
CRDT- 2024/03/05 21:55
PHST- 2023/07/21 00:00 [received]
PHST- 2023/12/19 00:00 [revised]
PHST- 2024/02/05 00:00 [accepted]
PHST- 2024/03/25 06:42 [medline]
PHST- 2024/03/06 00:42 [pubmed]
PHST- 2024/03/05 21:55 [entrez]
AID - S1367-0484(24)00008-0 [pii]
AID - 10.1016/j.clae.2024.102125 [doi]
PST - ppublish
SO  - Cont Lens Anterior Eye. 2024 Apr;47(2):102125. doi: 10.1016/j.clae.2024.102125. 
      Epub 2024 Mar 4.

PMID- 37615692
OWN - NLM
STAT- MEDLINE
DCOM- 20230831
LR  - 20230907
IS  - 2731-7056 (Electronic)
IS  - 2731-7048 (Linking)
VI  - 63
IP  - 9
DP  - 2023 Sep
TI  - [Large language models such as ChatGPT and GPT-4 for patient-centered care in 
      radiology].
PG  - 665-671
LID - 10.1007/s00117-023-01187-8 [doi]
AB  - BACKGROUND: With the introduction of ChatGPT in late November 2022, large 
      language models based on artificial intelligence have gained worldwide 
      recognition. These language models are trained on vast amounts of data, enabling 
      them to process complex tasks in seconds and provide detailed, high-level 
      text-based responses. OBJECTIVE: To provide an overview of the most widely 
      discussed large language models, ChatGPT and GPT‑4, with a&nbsp;focus on potential 
      applications for patient-centered radiology. MATERIALS AND METHODS: A&nbsp;PubMed 
      search of both large language models was performed using the terms "ChatGPT" and 
      "GPT-4", with subjective selection and completion in the form of a&nbsp;narrative 
      review. RESULTS: The generic nature of language models holds great promise for 
      radiology, enabling both patients and referrers to facilitate understanding of 
      radiological findings, overcome language barriers, and improve the quality of 
      informed consent discussions. This could represent a&nbsp;significant step towards 
      patient-centered or person-centered radiology. CONCLUSION: Large language models 
      represent a&nbsp;promising tool for improving the communication of findings, 
      interdisciplinary collaboration, and workflow in radiology. However, important 
      privacy issues and the reliable applicability of these models in medicine remain 
      to be addressed.
CI  - © 2023. The Author(s), under exclusive licence to Springer Medizin Verlag GmbH, 
      ein Teil von Springer Nature.
FAU - Fink, Matthias A
AU  - Fink MA
AD  - Klinik für Diagnostische und Interventionelle Radiologie, Universitätsklinikum 
      Heidelberg, Im Neuenheimer Feld&nbsp;420, 69120, Heidelberg, Deutschland. 
      matthias.fink@uni-heidelberg.de.
LA  - ger
PT  - English Abstract
PT  - Journal Article
PT  - Review
TT  - Goße Sprachmodelle wie ChatGPT und GPT-4 für eine patientenzentrierte Radiologie.
DEP - 20230824
PL  - Germany
TA  - Radiologie (Heidelb)
JT  - Radiologie (Heidelberg, Germany)
JID - 9918384887306676
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Radiology
MH  - Radiography
MH  - Language
MH  - Patient-Centered Care
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - Deep Learning
OT  - Machine Learning
OT  - Natural Language Processing
OT  - Patient-centred approach
EDAT- 2023/08/24 13:44
MHDA- 2023/08/31 06:42
CRDT- 2023/08/24 11:05
PHST- 2023/07/14 00:00 [accepted]
PHST- 2023/08/31 06:42 [medline]
PHST- 2023/08/24 13:44 [pubmed]
PHST- 2023/08/24 11:05 [entrez]
AID - 10.1007/s00117-023-01187-8 [pii]
AID - 10.1007/s00117-023-01187-8 [doi]
PST - ppublish
SO  - Radiologie (Heidelb). 2023 Sep;63(9):665-671. doi: 10.1007/s00117-023-01187-8. 
      Epub 2023 Aug 24.

PMID- 38300696
OWN - NLM
STAT- MEDLINE
DCOM- 20240202
LR  - 20240218
IS  - 2369-3762 (Electronic)
IS  - 2369-3762 (Linking)
VI  - 10
DP  - 2024 Feb 1
TI  - Increasing Realism and Variety of Virtual Patient Dialogues for Prenatal 
      Counseling Education Through a Novel Application of ChatGPT: Exploratory 
      Observational Study.
PG  - e50705
LID - 10.2196/50705 [doi]
LID - e50705
AB  - BACKGROUND: Using virtual patients, facilitated by natural language processing, 
      provides a valuable educational experience for learners. Generating a large, 
      varied sample of realistic and appropriate responses for virtual patients is 
      challenging. Artificial intelligence (AI) programs can be a viable source for 
      these responses, but their utility for this purpose has not been explored. 
      OBJECTIVE: In this study, we explored the effectiveness of generative AI 
      (ChatGPT) in developing realistic virtual standardized patient dialogues to teach 
      prenatal counseling skills. METHODS: ChatGPT was prompted to generate a list of 
      common areas of concern and questions that families expecting preterm delivery at 
      24 weeks gestation might ask during prenatal counseling. ChatGPT was then 
      prompted to generate 2 role-plays with dialogues between a parent expecting a 
      potential preterm delivery at 24 weeks and their counseling physician using each 
      of the example questions. The prompt was repeated for 2 unique role-plays: one 
      parent was characterized as anxious and the other as having low trust in the 
      medical system. Role-play scripts were exported verbatim and independently 
      reviewed by 2 neonatologists with experience in prenatal counseling, using a 
      scale of 1-5 on realism, appropriateness, and utility for virtual standardized 
      patient responses. RESULTS: ChatGPT generated 7 areas of concern, with 35 example 
      questions used to generate role-plays. The 35 role-play transcripts generated 176 
      unique parent responses (median 5, IQR 4-6, per role-play) with 268 unique 
      sentences. Expert review identified 117 (65%) of the 176 responses as indicating 
      an emotion, either directly or indirectly. Approximately half (98/176, 56%) of 
      the responses had 2 or more sentences, and half (88/176, 50%) included at least 1 
      question. More than half (104/176, 58%) of the responses from role-played parent 
      characters described a feeling, such as being scared, worried, or concerned. The 
      role-plays of parents with low trust in the medical system generated many unique 
      sentences (n=50). Most of the sentences in the responses were found to be 
      reasonably realistic (214/268, 80%), appropriate for variable prenatal counseling 
      conversation paths (233/268, 87%), and usable without more than a minimal 
      modification in a virtual patient program (169/268, 63%). CONCLUSIONS: Generative 
      AI programs, such as ChatGPT, may provide a viable source of training materials 
      to expand virtual patient programs, with careful attention to the concerns and 
      questions of patients and families. Given the potential for unrealistic or 
      inappropriate statements and questions, an expert should review AI chat outputs 
      before deploying them in an educational program.
CI  - ©Megan Gray, Austin Baird, Taylor Sawyer, Jasmine James, Thea DeBroux, Michelle 
      Bartlett, Jeanne Krick, Rachel Umoren. Originally published in JMIR Medical 
      Education (https://mededu.jmir.org), 01.02.2024.
FAU - Gray, Megan
AU  - Gray M
AUID- ORCID: 0000-0002-3272-9432
AD  - Division of Neonatology, University of Washington, Seattle, WA, United States.
FAU - Baird, Austin
AU  - Baird A
AUID- ORCID: 0000-0002-4711-3016
AD  - Division of Healthcare Simulation Sciences, Department of Surgery, University of 
      Washington, Seattle, WA, United States.
FAU - Sawyer, Taylor
AU  - Sawyer T
AUID- ORCID: 0000-0002-3908-8957
AD  - Division of Neonatology, University of Washington, Seattle, WA, United States.
FAU - James, Jasmine
AU  - James J
AUID- ORCID: 0000-0001-9907-906X
AD  - Department of Family Medicine, Providence St Peter, Olympia, WA, United States.
FAU - DeBroux, Thea
AU  - DeBroux T
AUID- ORCID: 0009-0005-5475-8233
AD  - Division of Neonatology, University of Washington, Seattle, WA, United States.
FAU - Bartlett, Michelle
AU  - Bartlett M
AUID- ORCID: 0000-0002-9119-6779
AD  - Department of Pediatrics, Children's Hospital of Philadelphia, Philadelphia, PA, 
      United States.
FAU - Krick, Jeanne
AU  - Krick J
AUID- ORCID: 0000-0002-6461-3776
AD  - Department of Pediatrics, San Antonio Uniformed Services Health Education 
      Consortium, San Antonio, TX, United States.
FAU - Umoren, Rachel
AU  - Umoren R
AUID- ORCID: 0000-0003-2356-9278
AD  - Division of Neonatology, University of Washington, Seattle, WA, United States.
LA  - eng
PT  - Journal Article
PT  - Observational Study
DEP - 20240201
PL  - Canada
TA  - JMIR Med Educ
JT  - JMIR medical education
JID - 101684518
SB  - IM
MH  - Female
MH  - Pregnancy
MH  - Infant, Newborn
MH  - Humans
MH  - Artificial Intelligence
MH  - *Premature Birth
MH  - Educational Status
MH  - Counseling
MH  - *Prenatal Education
PMC - PMC10870212
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - artificial intelligence
OT  - neonatology
OT  - prenatal counseling
OT  - simulation
OT  - virtual health
OT  - virtual patient
COIS- Conflicts of Interest: None declared.
EDAT- 2024/02/01 12:42
MHDA- 2024/02/02 06:43
PMCR- 2024/02/01
CRDT- 2024/02/01 11:53
PHST- 2023/07/17 00:00 [received]
PHST- 2023/12/11 00:00 [accepted]
PHST- 2023/10/18 00:00 [revised]
PHST- 2024/02/02 06:43 [medline]
PHST- 2024/02/01 12:42 [pubmed]
PHST- 2024/02/01 11:53 [entrez]
PHST- 2024/02/01 00:00 [pmc-release]
AID - v10i1e50705 [pii]
AID - 10.2196/50705 [doi]
PST - epublish
SO  - JMIR Med Educ. 2024 Feb 1;10:e50705. doi: 10.2196/50705.

PMID- 38239498
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240121
IS  - 2624-8212 (Electronic)
IS  - 2624-8212 (Linking)
VI  - 6
DP  - 2023
TI  - AI and its consequences for the written word.
PG  - 1326166
LID - 10.3389/frai.2023.1326166 [doi]
LID - 1326166
AB  - The latest developments of chatbots driven by Large Language Models (LLMs), more 
      specifically ChatGPT, have shaken the foundations of how text is created, and may 
      drastically reduce and change the need, ability, and valuation of human writing. 
      Furthermore, our trust in the written word is likely to decrease, as an 
      increasing proportion of all written text will be AI-generated - and potentially 
      incorrect. In this essay, I discuss these implications and possible scenarios for 
      us humans, and for AI itself.
CI  - Copyright © 2024 Hellström.
FAU - Hellström, Thomas
AU  - Hellström T
AD  - Department of Computing Science, Umeå University, Umeå, Sweden.
LA  - eng
PT  - Journal Article
DEP - 20240104
PL  - Switzerland
TA  - Front Artif Intell
JT  - Frontiers in artificial intelligence
JID - 101770551
PMC - PMC10794589
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - LLM
OT  - Large Language Models
OT  - human writing
OT  - societal impact
OT  - the written word
COIS- The author declares that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2024/01/19 06:42
MHDA- 2024/01/19 06:43
PMCR- 2024/01/04
CRDT- 2024/01/19 03:40
PHST- 2023/10/22 00:00 [received]
PHST- 2023/12/07 00:00 [accepted]
PHST- 2024/01/19 06:43 [medline]
PHST- 2024/01/19 06:42 [pubmed]
PHST- 2024/01/19 03:40 [entrez]
PHST- 2024/01/04 00:00 [pmc-release]
AID - 10.3389/frai.2023.1326166 [doi]
PST - epublish
SO  - Front Artif Intell. 2024 Jan 4;6:1326166. doi: 10.3389/frai.2023.1326166. 
      eCollection 2023.

PMID- 36934624
OWN - NLM
STAT- MEDLINE
DCOM- 20230414
LR  - 20231031
IS  - 1532-2793 (Electronic)
IS  - 0260-6917 (Linking)
VI  - 125
DP  - 2023 Jun
TI  - Chatting or cheating? The impacts of ChatGPT and other artificial intelligence 
      language models on nurse education.
PG  - 105796
LID - S0260-6917(23)00090-4 [pii]
LID - 10.1016/j.nedt.2023.105796 [doi]
FAU - Choi, Edmond Pui Hang
AU  - Choi EPH
AD  - School of Nursing, LKS Faculty of Medicine, The University of Hong Kong, Hong 
      Kong. Electronic address: h0714919@connect.hku.hk.
FAU - Lee, Jung Jae
AU  - Lee JJ
AD  - School of Nursing, LKS Faculty of Medicine, The University of Hong Kong, Hong 
      Kong.
FAU - Ho, Mu-Hsing
AU  - Ho MH
AD  - School of Nursing, LKS Faculty of Medicine, The University of Hong Kong, Hong 
      Kong.
FAU - Kwok, Jojo Yan Yan
AU  - Kwok JYY
AD  - School of Nursing, LKS Faculty of Medicine, The University of Hong Kong, Hong 
      Kong.
FAU - Lok, Kris Yuet Wan
AU  - Lok KYW
AD  - School of Nursing, LKS Faculty of Medicine, The University of Hong Kong, Hong 
      Kong.
LA  - eng
PT  - Journal Article
DEP - 20230315
PL  - Scotland
TA  - Nurse Educ Today
JT  - Nurse education today
JID - 8511379
MH  - Humans
MH  - *Artificial Intelligence
MH  - Educational Status
MH  - Language
MH  - *Education, Nursing
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Chatbot
OT  - Nurse Education
OT  - Nursing
COIS- Declaration of competing interest We declare that there are no conflicts of 
      interest related to this manuscript, and we take full responsibility for its 
      completion and attest to its validity.
EDAT- 2023/03/20 06:00
MHDA- 2023/04/14 06:41
CRDT- 2023/03/19 19:06
PHST- 2023/02/20 00:00 [received]
PHST- 2023/03/02 00:00 [revised]
PHST- 2023/03/09 00:00 [accepted]
PHST- 2023/04/14 06:41 [medline]
PHST- 2023/03/20 06:00 [pubmed]
PHST- 2023/03/19 19:06 [entrez]
AID - S0260-6917(23)00090-4 [pii]
AID - 10.1016/j.nedt.2023.105796 [doi]
PST - ppublish
SO  - Nurse Educ Today. 2023 Jun;125:105796. doi: 10.1016/j.nedt.2023.105796. Epub 2023 
      Mar 15.

PMID- 38189213
OWN - NLM
STAT- MEDLINE
DCOM- 20240115
LR  - 20240115
IS  - 1744-8298 (Electronic)
IS  - 1479-6678 (Linking)
VI  - 19
IP  - 16
DP  - 2023 Dec
TI  - Cardiology in the digital era: from artificial intelligence to Metaverse, paving 
      the way for future advancements.
PG  - 755-758
LID - 10.2217/fca-2023-0106 [doi]
AB  - Tweetable abstract Cardiology's digital revolution: AI diagnoses, ChatGPT 
      consults, Metaverse educates. Challenges &amp; promises explored. #CardiologyTech 
      #DigitalHealth.
FAU - Skalidis, Ioannis
AU  - Skalidis I
AUID- ORCID: 0000-0002-4374-0389
AD  - Department of Cardiology, University Hospital of Heraklion &amp; University of Crete, 
      Heraklion, 71500, Greece.
AD  - Department of Cardiology, University Hospital of Lausanne (CHUV), Lausanne, 1005, 
      Switzerland.
FAU - Kachrimanidis, Ioannis
AU  - Kachrimanidis I
AD  - Departement of Cardiology, Hippokration General Hospital, 11527, Athens, Greece.
FAU - Koliastasis, Leonidas
AU  - Koliastasis L
AD  - Departement of Cardiology, Hippokration General Hospital, 11527, Athens, Greece.
FAU - Arangalage, Dimitri
AU  - Arangalage D
AD  - Department of Cardiology, University Hospital of Lausanne (CHUV), Lausanne, 1005, 
      Switzerland.
AD  - Department of Cardiology, Bichat Hospital, AP-HP, 75018, Paris, France.
FAU - Antiochos, Panagiotis
AU  - Antiochos P
AD  - Department of Cardiology, University Hospital of Lausanne (CHUV), Lausanne, 1005, 
      Switzerland.
FAU - Maurizi, Niccolo
AU  - Maurizi N
AD  - Department of Cardiology, University Hospital of Lausanne (CHUV), Lausanne, 1005, 
      Switzerland.
FAU - Muller, Olivier
AU  - Muller O
AD  - Department of Cardiology, University Hospital of Lausanne (CHUV), Lausanne, 1005, 
      Switzerland.
FAU - Fournier, Stephane
AU  - Fournier S
AD  - Department of Cardiology, University Hospital of Lausanne (CHUV), Lausanne, 1005, 
      Switzerland.
FAU - Hamilos, Michalis
AU  - Hamilos M
AD  - Department of Cardiology, University Hospital of Heraklion &amp; University of Crete, 
      Heraklion, 71500, Greece.
FAU - Skalidis, Emmanouil
AU  - Skalidis E
AD  - Department of Cardiology, University Hospital of Heraklion &amp; University of Crete, 
      Heraklion, 71500, Greece.
LA  - eng
PT  - Editorial
DEP - 20240108
PL  - England
TA  - Future Cardiol
JT  - Future cardiology
JID - 101239345
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Cardiology
OTO - NOTNLM
OT  - ChatGPT
OT  - Metaverse
OT  - artificial intelligence
OT  - digital health
OT  - virtual reality
EDAT- 2024/01/08 06:42
MHDA- 2024/01/15 12:42
CRDT- 2024/01/08 05:52
PHST- 2024/01/15 12:42 [medline]
PHST- 2024/01/08 06:42 [pubmed]
PHST- 2024/01/08 05:52 [entrez]
AID - 10.2217/fca-2023-0106 [doi]
PST - ppublish
SO  - Future Cardiol. 2023 Dec;19(16):755-758. doi: 10.2217/fca-2023-0106. Epub 2024 
      Jan 8.

PMID- 37038572
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230412
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - Anesthetic Management of a Patient With Juvenile Hyaline Fibromatosis: A Case 
      Report Written With the Assistance of the Large Language Model ChatGPT.
PG  - e35946
LID - 10.7759/cureus.35946 [doi]
LID - e35946
AB  - This case report was written with the assistance of the large language model 
      known as ChatGPT, a form of generative artificial intelligence that can write 
      grammatically correct and semantically meaningful prose on a multitude of 
      topics.&nbsp;Here, it has assisted us in presenting a case of anesthetic management 
      for a case of Juvenile Hyaline Fibromatosis (JHF), an extremely rare genetic 
      disorder that is part of a spectrum of diseases presently characterized as 
      Hyaline Fibromatosis Syndrome (HFS), which also includes a more severe variant 
      presenting in infancy. HFS is caused by autosomal recessive mutations in the 
      ANTXR2 (anthrax toxin receptor cell adhesion molecule 2) gene, which binds 
      collagen IV and laminin, suggesting that it may be involved in extracellular 
      matrix adhesion. Defects in this molecule lead to abnormal deposition of hyaline 
      material in perivascular areas, presenting as cutaneous lesions, joint 
      contractures, and in some cases internal organ dysfunction. Anesthetic management 
      of patients with JHF may present difficulties with patient positioning and airway 
      management. Most reports of anesthetic management concern children with severe 
      disease and adult reports are uncommon. We present a case of JHF in a 39-year-old 
      woman managed for resection of a lower extremity cutaneous lesion. The anesthetic 
      management of this relatively minor case was uneventful, but the process of 
      drafting this report with the assistance of the new software tool ChatGPT was 
      informative of both its strengths and limitations.
CI  - Copyright © 2023, Segal et al.
FAU - Segal, Scott
AU  - Segal S
AD  - Anesthesiology, Wake Forest School of Medicine, Winston-Salem, USA.
FAU - Khanna, Ashish K
AU  - Khanna AK
AD  - Anesthesiology, Wake Forest School of Medicine, Winston Salem, USA.
LA  - eng
PT  - Case Reports
DEP - 20230309
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10082625
OTO - NOTNLM
OT  - anesthetic management
OT  - chatgpt
OT  - hyaline fibromatosis syndrome
OT  - juvenile hyaline fibromatosis
OT  - large language model
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/04/12 06:00
MHDA- 2023/04/12 06:01
PMCR- 2023/03/09
CRDT- 2023/04/11 01:48
PHST- 2023/03/09 00:00 [accepted]
PHST- 2023/04/12 06:01 [medline]
PHST- 2023/04/11 01:48 [entrez]
PHST- 2023/04/12 06:00 [pubmed]
PHST- 2023/03/09 00:00 [pmc-release]
AID - 10.7759/cureus.35946 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 9;15(3):e35946. doi: 10.7759/cureus.35946. eCollection 2023 Mar.

PMID- 38029273
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231201
IS  - 2052-4374 (Print)
IS  - 2052-4374 (Electronic)
IS  - 2052-4374 (Linking)
VI  - 35
DP  - 2023
TI  - The use of ChatGPT in occupational medicine: opportunities and threats.
PG  - e42
LID - 10.35371/aoem.2023.35.e42 [doi]
LID - e42
AB  - ChatGPT has the potential to revolutionize occupational medicine by providing a 
      powerful tool for analyzing data, improving communication, and increasing 
      efficiency. It can help identify patterns and trends in workplace health and 
      safety, act as a virtual assistant for workers, employers, and occupational 
      health professionals, and automate certain tasks. However, caution is required 
      due to ethical concerns, the need to maintain confidentiality, and the risk of 
      inconsistent or inaccurate results. ChatGPT cannot replace the crucial role of 
      the occupational health professional in the medical surveillance of workers and 
      the analysis of data on workers' health.
CI  - Copyright © 2023 Korean Society of Occupational &amp; Environmental Medicine.
FAU - Sridi, Chayma
AU  - Sridi C
AUID- ORCID: 0000-0003-1718-0952
AD  - Department of Occupational Medicine, Sahloul University Hospital, Sousse, 
      Tunisia.
FAU - Brigui, Salem
AU  - Brigui S
AUID- ORCID: 0009-0004-2627-7734
AD  - Hospital of Tela, Kasserine, Tunisia.
LA  - eng
PT  - Journal Article
DEP - 20231023
PL  - Korea (South)
TA  - Ann Occup Environ Med
JT  - Annals of occupational and environmental medicine
JID - 101609244
PMC - PMC10654530
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Occupational medicine
OT  - Workplace
COIS- Competing interests: The authors declare that they have no competing interests.
EDAT- 2023/11/29 18:41
MHDA- 2023/11/29 18:42
PMCR- 2023/10/23
CRDT- 2023/11/29 17:23
PHST- 2023/08/07 00:00 [received]
PHST- 2023/09/30 00:00 [revised]
PHST- 2023/10/06 00:00 [accepted]
PHST- 2023/11/29 18:42 [medline]
PHST- 2023/11/29 18:41 [pubmed]
PHST- 2023/11/29 17:23 [entrez]
PHST- 2023/10/23 00:00 [pmc-release]
AID - 10.35371/aoem.2023.35.e42 [doi]
PST - epublish
SO  - Ann Occup Environ Med. 2023 Oct 23;35:e42. doi: 10.35371/aoem.2023.35.e42. 
      eCollection 2023.

PMID- 38353076
OWN - NLM
STAT- MEDLINE
DCOM- 20240215
LR  - 20240215
IS  - 1873-4626 (Electronic)
IS  - 1091-255X (Linking)
VI  - 28
IP  - 1
DP  - 2024 Jan
TI  - Online artificial intelligence platforms and their applicability to 
      gastrointestinal surgical operations.
PG  - 64-69
LID - S1091-255X(23)00649-2 [pii]
LID - 10.1016/j.gassur.2023.11.019 [doi]
AB  - BACKGROUND: The internet is a common source of health information for patients. 
      Interactive online artificial intelligence (AI) may be a more reliable source of 
      health-related information than traditional search engines. This study aimed to 
      assess the quality and perceived utility of chat-based AI responses related to 3 
      common gastrointestinal (GI) surgical procedures. METHODS: A survey of 24 
      questions covering general perioperative information on cholecystectomy, 
      pancreaticoduodenectomy (PD), and colectomy was created. Each question was posed 
      to Chat Generative Pre-trained Transformer (ChatGPT) in June 2023, and the 
      generated responses were recorded. The quality and perceived utility of responses 
      were independently and subjectively graded by expert respondents specific to each 
      surgical field. Grades were classified as "poor," "fair," "good," "very good," or 
      "excellent." RESULTS: Among the 45 respondents (general surgeon [n&nbsp;=&nbsp;13], 
      surgical oncologist [n&nbsp;=&nbsp;18], colorectal surgeon [n&nbsp;=&nbsp;13], and transplant surgeon 
      [n&nbsp;=&nbsp;1]), most practiced at an academic facility (95.6%). Respondents had been in 
      practice for a mean of 12.3 years (general surgeon, 14.5&nbsp;±&nbsp;7.2; surgical 
      oncologist, 12.1&nbsp;±&nbsp;8.2; colorectal surgeon, 10.2&nbsp;±&nbsp;8.0) and performed a mean 53 
      index operations annually (cholecystectomy, 47&nbsp;±&nbsp;28; PD, 28&nbsp;±&nbsp;27; colectomy, 
      81&nbsp;±&nbsp;44). Overall, the most commonly assigned quality grade was "fair" or "good" 
      for most responses (n&nbsp;=&nbsp;622/1080, 57.6%). Most of the 1080 total utility grades 
      were "fair" (n&nbsp;=&nbsp;279, 25.8%) or "good" (n&nbsp;=&nbsp;344, 31.9%), whereas only 129 utility 
      grades (11.9%) were "poor." Of note, ChatGPT responses related to cholecystectomy 
      (45.3% ["very good"/"excellent"] vs 18.1% ["poor"/"fair"]) were deemed to be 
      better quality than AI responses about PD (18.9% ["very good"/"excellent"] vs 
      46.9% ["poor"/"fair"]) or colectomy (31.4% ["very good"/"excellent"] vs 38.3% 
      ["poor"/"fair"]). Overall, only 20.0% of the experts deemed ChatGPT to be an 
      accurate source of information, whereas 15.6% of the experts found it unreliable. 
      Moreover, 1 in 3 surgeons deemed ChatGPT responses as not likely to reduce 
      patient-physician correspondence (31.1%) or not comparable to in-person surgeon 
      responses (35.6%). CONCLUSIONS: Although a potential resource for patient 
      education, ChatGPT responses to common GI perioperative questions were deemed to 
      be of only modest quality and utility to patients. In addition, the relative 
      quality of AI responses varied markedly on the basis of procedure type.
CI  - Copyright © 2023 Society for Surgery of the Alimentary Tract. Published by 
      Elsevier Inc. All rights reserved.
FAU - Munir, Muhammad Musaab
AU  - Munir MM
AD  - Division of Surgical Oncology, Department of Surgery, The Ohio State University 
      Wexner Medical Center and James Comprehensive Cancer Center, Columbus, Ohio, 
      United States.
FAU - Endo, Yutaka
AU  - Endo Y
AD  - Division of Surgical Oncology, Department of Surgery, The Ohio State University 
      Wexner Medical Center and James Comprehensive Cancer Center, Columbus, Ohio, 
      United States.
FAU - Ejaz, Aslam
AU  - Ejaz A
AD  - Division of Surgical Oncology, Department of Surgery, The Ohio State University 
      Wexner Medical Center and James Comprehensive Cancer Center, Columbus, Ohio, 
      United States.
FAU - Dillhoff, Mary
AU  - Dillhoff M
AD  - Division of Surgical Oncology, Department of Surgery, The Ohio State University 
      Wexner Medical Center and James Comprehensive Cancer Center, Columbus, Ohio, 
      United States.
FAU - Cloyd, Jordan M
AU  - Cloyd JM
AD  - Division of Surgical Oncology, Department of Surgery, The Ohio State University 
      Wexner Medical Center and James Comprehensive Cancer Center, Columbus, Ohio, 
      United States.
FAU - Pawlik, Timothy M
AU  - Pawlik TM
AD  - Division of Surgical Oncology, Department of Surgery, The Ohio State University 
      Wexner Medical Center and James Comprehensive Cancer Center, Columbus, Ohio, 
      United States. Electronic address: Tim.Pawlik@osumc.edu.
LA  - eng
PT  - Journal Article
PL  - Netherlands
TA  - J Gastrointest Surg
JT  - Journal of gastrointestinal surgery : official journal of the Society for Surgery 
      of the Alimentary Tract
JID - 9706084
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Surgeons
MH  - Colectomy
MH  - Pancreaticoduodenectomy
MH  - *Colorectal Neoplasms
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Informational resource
OT  - Large language models
OT  - Surgical care
EDAT- 2024/02/14 06:43
MHDA- 2024/02/15 06:43
CRDT- 2024/02/14 05:14
PHST- 2023/08/24 00:00 [received]
PHST- 2023/10/28 00:00 [revised]
PHST- 2023/11/19 00:00 [accepted]
PHST- 2024/02/15 06:43 [medline]
PHST- 2024/02/14 06:43 [pubmed]
PHST- 2024/02/14 05:14 [entrez]
AID - S1091-255X(23)00649-2 [pii]
AID - 10.1016/j.gassur.2023.11.019 [doi]
PST - ppublish
SO  - J Gastrointest Surg. 2024 Jan;28(1):64-69. doi: 10.1016/j.gassur.2023.11.019.

PMID- 37198001
OWN - NLM
STAT- MEDLINE
DCOM- 20230705
LR  - 20230828
IS  - 1460-9584 (Electronic)
IS  - 1268-7731 (Linking)
VI  - 29
IP  - 5
DP  - 2023 Jul
TI  - Foot and Ankle Surgery declares use of generative artificial intelligence like 
      Chat Generative Pre-trained Transformer (ChatGPT) for scientific publications.
PG  - 385-386
LID - S1268-7731(23)00080-2 [pii]
LID - 10.1016/j.fas.2023.05.002 [doi]
FAU - Richter, Martinus
AU  - Richter M
AD  - Department for Foot and Ankle Surgery Nuremberg and Rummelsberg, Location 
      Hospital Rummelsberg, Rummelsberg 71, 90592 Schwarzenbruck, Germany. Electronic 
      address: martinus.richter@sana.de.
LA  - eng
PT  - Comment
PT  - Editorial
DEP - 20230511
PL  - France
TA  - Foot Ankle Surg
JT  - Foot and ankle surgery : official journal of the European Society of Foot and 
      Ankle Surgeons
JID - 9609647
SB  - IM
CON - Cureus. 2023 Feb 4;15(2):e34616. PMID: 36895547
MH  - Humans
MH  - *Ankle
MH  - *Artificial Intelligence
MH  - Endoscopy
MH  - Lower Extremity
MH  - Publications
EDAT- 2023/05/18 01:07
MHDA- 2023/07/05 06:42
CRDT- 2023/05/17 21:56
PHST- 2023/07/05 06:42 [medline]
PHST- 2023/05/18 01:07 [pubmed]
PHST- 2023/05/17 21:56 [entrez]
AID - S1268-7731(23)00080-2 [pii]
AID - 10.1016/j.fas.2023.05.002 [doi]
PST - ppublish
SO  - Foot Ankle Surg. 2023 Jul;29(5):385-386. doi: 10.1016/j.fas.2023.05.002. Epub 
      2023 May 11.

PMID- 38065864
OWN - NLM
STAT- MEDLINE
DCOM- 20231216
LR  - 20240214
IS  - 1536-5964 (Electronic)
IS  - 0025-7974 (Print)
IS  - 0025-7974 (Linking)
VI  - 102
IP  - 49
DP  - 2023 Dec 8
TI  - Evaluating cluster analysis techniques in ChatGPT versus R-language with 
      visualizations of author collaborations and keyword cooccurrences on articles in 
      the Journal of Medicine (Baltimore) 2023: Bibliometric analysis.
PG  - e36154
LID - 10.1097/MD.0000000000036154 [doi]
LID - e36154
AB  - BACKGROUND: Analyses of author collaborations and keyword co-occurrences are 
      frequently used in bibliographic research. However, no studies have introduced a 
      straightforward yet effective approach, such as utilizing ChatGPT with Code 
      Interpreter (ChatGPT_CI) or the R language, for creating cluster-oriented 
      networks. This research aims to compare cluster analysis methods in ChatGPT_CI 
      and R, visualize country-specific author collaborations, and then demonstrate the 
      most effective approach. METHODS: The research focused on articles and review 
      pieces from Medicine (Baltimore) published in 2023. By August 20, 2023, we had 
      gathered metadata for 1976 articles using the Web of Science core collections. 
      The efficiency and effectiveness of cluster displays between ChatGPT_CI and R 
      were compared by evaluating their time consumption. The best method was then 
      employed to present a series of visualizations of country-specific author 
      collaborations, rooted in social network and cluster analyses. Visualization 
      techniques incorporating network charts, chord diagrams, circle bar plots, circle 
      packing plots, heat dendrograms, dendrograms, and word clouds were demonstrated. 
      We further highlighted the research profiles of 2 prolific authors using timeline 
      visuals. RESULTS: The research findings include that (1) the most active 
      contributors were China, Nanjing Medical University (China), the Medical School 
      Department, and Dr Chou from Taiwan when considering countries, institutions, 
      departments, and individual authors, respectively; (2) the highest cited articles 
      originated from Medicine (Baltimore) accounting for 4.53%: New England Journal of 
      Medicine, PLOS ONE, LANCET, and The Journal of the American Medical Association, 
      with respective contributions of 3.25%, 2.7%, 2.52%, and 1.54%; (3) visual 
      cluster analysis in R proved to be more efficient and effective than ChatGPT_CI, 
      reducing the time taken from 1 hour to just 3 minutes; (4) 7 cluster-focused 
      networks were crafted using R on a custom platform; and (5) the research 
      trajectories of 2 prominent authors (Dr Brin from the United States and Dr Chow 
      from Taiwan) and articles themes in Medicine 2023 were depicted using timeline 
      visuals. CONCLUSIONS: This research highlighted the efficient and effective 
      methods for conducting cluster analyses of author collaborations using R. For 
      future related studies, such as keyword co-occurrence analysis, R is recommended 
      as a viable alternative for bibliographic research.
CI  - Copyright © 2023 the Author(s). Published by Wolters Kluwer Health, Inc.
FAU - Cheng, Yung-Ze
AU  - Cheng YZ
AD  - Department of Emergency Medicine, Chi-Mei Medical Center, Tainan, Taiwan.
FAU - Lai, Tzu-Han
AU  - Lai TH
AD  - Grade Two in Senior High School, National Tainan Second Senior High School, 
      Tainan, Taiwan.
FAU - Chien, Tsair-Wei
AU  - Chien TW
AD  - Department of Medical Research, Chi-Mei Medical Center, Tainan, Taiwan.
FAU - Chou, Willy
AU  - Chou W
AUID- ORCID: 0000-0002-1132-9341
AD  - Department of Physical Medicine and Rehabilitation, Chiali Chi-Mei Hospital, 
      Tainan, Taiwan.
AD  - Department of Physical Medicine and Rehabilitation, Chung San Medical University 
      Hospital, Taichung, Taiwan.
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Medicine (Baltimore)
JT  - Medicine
JID - 2985248R
SB  - IM
MH  - Humans
MH  - United States
MH  - *Bibliometrics
MH  - Publications
MH  - *Medicine
MH  - Cluster Analysis
MH  - China
PMC - PMC10713138
COIS- The authors have no funding and conflicts of interest to disclose.
EDAT- 2023/12/09 05:42
MHDA- 2023/12/17 09:45
PMCR- 2023/12/08
CRDT- 2023/12/08 22:33
PHST- 2023/12/17 09:45 [medline]
PHST- 2023/12/09 05:42 [pubmed]
PHST- 2023/12/08 22:33 [entrez]
PHST- 2023/12/08 00:00 [pmc-release]
AID - 00005792-202312080-00022 [pii]
AID - 10.1097/MD.0000000000036154 [doi]
PST - ppublish
SO  - Medicine (Baltimore). 2023 Dec 8;102(49):e36154. doi: 
      10.1097/MD.0000000000036154.

PMID- 37823708
OWN - NLM
STAT- MEDLINE
DCOM- 20231228
LR  - 20240228
IS  - 1537-453X (Electronic)
IS  - 0277-3732 (Print)
IS  - 0277-3732 (Linking)
VI  - 47
IP  - 1
DP  - 2024 Jan 1
TI  - Physician Assessment of ChatGPT and Bing Answers to American Cancer Society's 
      Questions to Ask About Your Cancer.
PG  - 17-21
LID - 10.1097/COC.0000000000001050 [doi]
AB  - OBJECTIVES: Artificial intelligence (AI) chatbots are a new, publicly available 
      tool for patients to access health care-related information with unknown 
      reliability related to cancer-related questions. This study assesses the quality 
      of responses to common questions for patients with cancer. METHODS: From February 
      to March 2023, we queried chat generative pretrained transformer (ChatGPT) from 
      OpenAI and Bing AI from Microsoft questions from the American Cancer Society's 
      recommended "Questions to Ask About Your Cancer" customized for all stages of 
      breast, colon, lung, and prostate cancer. Questions were, in addition, grouped by 
      type (prognosis, treatment, or miscellaneous). The quality of AI chatbot 
      responses was assessed by an expert panel using the validated DISCERN criteria. 
      RESULTS: Of the 117 questions presented to ChatGPT and Bing, the average score 
      for all questions were 3.9 and 3.2, respectively ( P &lt; 0.001) and the overall 
      DISCERN scores were 4.1 and 4.4, respectively. By disease site, the average score 
      for ChatGPT and Bing, respectively, were 3.9 and 3.6 for prostate cancer ( P = 
      0.02), 3.7 and 3.3 for lung cancer ( P &lt; 0.001), 4.1 and 2.9 for breast cancer ( 
      P &lt; 0.001), and 3.8 and 3.0 for colorectal cancer ( P &lt; 0.001). By type of 
      question, the average score for ChatGPT and Bing, respectively, were 3.6 and 3.4 
      for prognostic questions ( P = 0.12), 3.9 and 3.1 for treatment questions ( P &lt; 
      0.001), and 4.2 and 3.3 for miscellaneous questions ( P = 0.001). For 3 responses 
      (3%) by ChatGPT and 18 responses (15%) by Bing, at least one panelist rated them 
      as having serious or extensive shortcomings. CONCLUSIONS: AI chatbots provide 
      multiple opportunities for innovating health care. This analysis suggests a 
      critical need, particularly around cancer prognostication, for continual 
      refinement to limit misleading counseling, confusion, and emotional distress to 
      patients and families.
CI  - Copyright © 2023 Wolters Kluwer Health, Inc. All rights reserved.
FAU - Janopaul-Naylor, James R
AU  - Janopaul-Naylor JR
AD  - Department of Radiation Oncology, Emory University.
AD  - Department of Radiation Oncology, Memorial Sloan Kettering Cancer Center.
FAU - Koo, Andee
AU  - Koo A
AD  - Department of Radiation Oncology, Emory University.
FAU - Qian, David C
AU  - Qian DC
AD  - Department of Radiation Oncology, Emory University.
FAU - McCall, Neal S
AU  - McCall NS
AD  - Department of Radiation Oncology, Emory University.
FAU - Liu, Yuan
AU  - Liu Y
AD  - Department of Biostatistics and Bioinformatics, Rollins School of Public Health, 
      Emory University.
FAU - Patel, Sagar A
AU  - Patel SA
AD  - Department of Radiation Oncology, Emory University.
LA  - eng
GR  - P30 CA008748/CA/NCI NIH HHS/United States
GR  - P30 CA138292/CA/NCI NIH HHS/United States
PT  - Journal Article
DEP - 20231012
PL  - United States
TA  - Am J Clin Oncol
JT  - American journal of clinical oncology
JID - 8207754
SB  - IM
MH  - United States
MH  - Male
MH  - Humans
MH  - American Cancer Society
MH  - Artificial Intelligence
MH  - Reproducibility of Results
MH  - *Prostatic Neoplasms/therapy
MH  - *Physicians
PMC - PMC10841271
MID - NIHMS1931594
COIS- The authors declare no conflicts of interest.
EDAT- 2023/10/12 12:44
MHDA- 2023/12/28 06:42
PMCR- 2025/01/01
CRDT- 2023/10/12 09:14
PHST- 2025/01/01 00:00 [pmc-release]
PHST- 2023/12/28 06:42 [medline]
PHST- 2023/10/12 12:44 [pubmed]
PHST- 2023/10/12 09:14 [entrez]
AID - 00000421-990000000-00137 [pii]
AID - 10.1097/COC.0000000000001050 [doi]
PST - ppublish
SO  - Am J Clin Oncol. 2024 Jan 1;47(1):17-21. doi: 10.1097/COC.0000000000001050. Epub 
      2023 Oct 12.

PMID- 37077800
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230421
IS  - 0860-021X (Print)
IS  - 2083-1862 (Electronic)
IS  - 0860-021X (Linking)
VI  - 40
IP  - 2
DP  - 2023 Apr
TI  - From human writing to artificial intelligence generated text: examining the 
      prospects and potential threats of ChatGPT in academic writing.
PG  - 615-622
LID - 10.5114/biolsport.2023.125623 [doi]
AB  - Natural language processing (NLP) has been studied in computing for decades. 
      Recent technological advancements have led to the development of sophisticated 
      artificial intelligence (AI) models, such as Chat Generative Pre-trained 
      Transformer (ChatGPT). These models can perform a range of language tasks and 
      generate human-like responses, which offers exciting prospects for academic 
      efficiency. This manuscript aims at (i) exploring the potential benefits and 
      threats of ChatGPT and other NLP technologies in academic writing and research 
      publications; (ii) highlights the ethical considerations involved in using these 
      tools, and (iii) consider the impact they may have on the authenticity and 
      credibility of academic work. This study involved a literature review of relevant 
      scholarly articles published in peer-reviewed journals indexed in Scopus as 
      quartile 1. The search used keywords such as "ChatGPT," "AI-generated text," 
      "academic writing," and "natural language processing." The analysis was carried 
      out using a quasi-qualitative approach, which involved reading and critically 
      evaluating the sources and identifying relevant data to support the research 
      questions. The study found that ChatGPT and other NLP technologies have the 
      potential to enhance academic writing and research efficiency. However, their use 
      also raises concerns about the impact on the authenticity and credibility of 
      academic work. The study highlights the need for comprehensive discussions on the 
      potential use, threats, and limitations of these tools, emphasizing the 
      importance of ethical and academic principles, with human intelligence and 
      critical thinking at the forefront of the research process. This study highlights 
      the need for comprehensive debates and ethical considerations involved in their 
      use. The study also recommends that academics exercise caution when using these 
      tools and ensure transparency in their use, emphasizing the importance of human 
      intelligence and critical thinking in academic work.
CI  - Copyright © Biology of Sport 2023.
FAU - Dergaa, Ismail
AU  - Dergaa I
AD  - Primary Health Care Corporation (PHCC), Doha, Qatar.
AD  - Research Unit Physical Activity, Sport, and Health, UR18JS01, National 
      Observatory of Sport, Tunis 1003, Tunisia.
AD  - High Institute of Sport and Physical Education, University of Sfax, Sfax, 
      Tunisia.
FAU - Chamari, Karim
AU  - Chamari K
AD  - Aspetar, Orthopaedic and Sports Medicine Hospital, FIFA Medical Centre of 
      Excellence, Doha, Qatar.
FAU - Zmijewski, Piotr
AU  - Zmijewski P
AD  - Jozef Pilsudski University of Physical Education in Warsaw, Warsaw, Poland.
FAU - Ben Saad, Helmi
AU  - Ben Saad H
AD  - University of Sousse, Farhat HACHED hospital, Service of Physiology and 
      Functional Explorations, Sousse, Tunisia.
AD  - University of Sousse, Farhat HACHED hospital, Research Laboratory LR12SP09 «Heart 
      Failure», Sousse, Tunisia.
AD  - University of Sousse, Faculty of Medicine of Sousse, Laboratory of Physiology, 
      Sousse, Tunisia.
LA  - eng
PT  - Journal Article
DEP - 20230315
PL  - Poland
TA  - Biol Sport
JT  - Biology of sport
JID - 8700872
PMC - PMC10108763
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - Chatbot
OT  - Deep Learning
OT  - Google Bard
OT  - Higher Education
OT  - LLM
OT  - LLaMA
OT  - Machine Learning
OT  - NLM
OT  - NLP
OT  - Natural Language Processing
OT  - Paperpal
OT  - Peer Review
OT  - QuillBot
OT  - Rayyan
OT  - Research
OT  - Sports Medicine
COIS- We, hereby declare that we have no financial or personal relationships that could 
      potentially influence or bias the content of this paper. Specifically, none of 
      the authors holds any financial interests or conflicts of interest associated 
      with the ChatGPT or NLM technologies discussed in this paper. Furthermore, none 
      of the authors has affiliations with any organizations that might have a 
      financial interest in the research or its outcomes. Moreover, we confirm that we 
      have no personal or professional relationships that could potentially affect the 
      research or its findings. None of the authors has collaborated or consulted with 
      any individuals or organizations that have a financial or other interest in the 
      ChatGPT or NLM technologies. Additionally, we have not received any funding or 
      other types of support from any sources that could influence the research or its 
      findings. We affirm that the research presented in this paper is entirely based 
      on our own analysis and interpretation of the facts/data. We assure that there 
      are no conflicts of interest that could impact the objectivity or integrity of 
      the research. We make this declaration of no conflict of interest to ensure 
      transparency and maintain the credibility of the research manuscript presented.
EDAT- 2023/04/20 06:42
MHDA- 2023/04/20 06:43
PMCR- 2023/04/01
CRDT- 2023/04/20 02:28
PHST- 2023/02/26 00:00 [received]
PHST- 2023/03/04 00:00 [revised]
PHST- 2023/03/09 00:00 [accepted]
PHST- 2023/04/20 06:43 [medline]
PHST- 2023/04/20 06:42 [pubmed]
PHST- 2023/04/20 02:28 [entrez]
PHST- 2023/04/01 00:00 [pmc-release]
AID - 125623 [pii]
AID - 10.5114/biolsport.2023.125623 [doi]
PST - ppublish
SO  - Biol Sport. 2023 Apr;40(2):615-622. doi: 10.5114/biolsport.2023.125623. Epub 2023 
      Mar 15.

PMID- 37398259
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231019
DP  - 2023 Jun 12
TI  - From Answers to Insights: Unveiling the Strengths and Limitations of ChatGPT and 
      Biomedical Knowledge Graphs.
LID - 2023.06.09.23291208 [pii]
LID - 10.1101/2023.06.09.23291208 [doi]
AB  - Large Language Models (LLMs) have demonstrated exceptional performance in various 
      natural language processing tasks, utilizing their language generation 
      capabilities and knowledge acquisition potential from unstructured text. However, 
      when applied to the biomedical domain, LLMs encounter limitations, resulting in 
      erroneous and inconsistent answers. Knowledge Graphs (KGs) have emerged as 
      valuable resources for structured information representation and organization. 
      Specifically, Biomedical Knowledge Graphs (BKGs) have attracted significant 
      interest in managing large-scale and heterogeneous biomedical knowledge. This 
      study evaluates the capabilities of ChatGPT and existing BKGs in question 
      answering, knowledge discovery, and reasoning. Results indicate that while 
      ChatGPT with GPT-4.0 surpasses both GPT-3.5 and BKGs in providing existing 
      information, BKGs demonstrate superior information reliability. Additionally, 
      ChatGPT exhibits limitations in performing novel discoveries and reasoning, 
      particularly in establishing structured links between entities compared to BKGs. 
      To overcome these limitations, future research should focus on integrating LLMs 
      and BKGs to leverage their respective strengths. Such an integrated approach 
      would optimize task performance and mitigate potential risks, thereby advancing 
      knowledge in the biomedical field and contributing to overall well-being.
FAU - Hou, Yu
AU  - Hou Y
AD  - Department of Surgery, University of Minnesota, Minneapolis, MN, USA.
FAU - Yeung, Jeremy
AU  - Yeung J
AD  - Department of Surgery, University of Minnesota, Minneapolis, MN, USA.
FAU - Xu, Hua
AU  - Xu H
AD  - Section of Biomedical Informatics and Data Science, Yale University, New Haven, 
      Connecticut, USA.
FAU - Su, Chang
AU  - Su C
AD  - Department of Health Service Administration and Policy, Temple University, 
      Philadelphia, PA, USA.
FAU - Wang, Fei
AU  - Wang F
AD  - Department of Population Health Sciences, Weill Cornell Medicine, New York, NY, 
      USA.
FAU - Zhang, Rui
AU  - Zhang R
AD  - Department of Surgery, University of Minnesota, Minneapolis, MN, USA.
LA  - eng
GR  - R01 AG078154/AG/NIA NIH HHS/United States
GR  - R01 AT009457/AT/NCCIH NIH HHS/United States
PT  - Preprint
DEP - 20230612
PL  - United States
TA  - medRxiv
JT  - medRxiv : the preprint server for health sciences
JID - 101767986
PMC - PMC10312889
EDAT- 2023/07/03 13:05
MHDA- 2023/07/03 13:06
PMCR- 2023/06/30
CRDT- 2023/07/03 11:44
PHST- 2023/07/03 13:05 [pubmed]
PHST- 2023/07/03 13:06 [medline]
PHST- 2023/07/03 11:44 [entrez]
PHST- 2023/06/30 00:00 [pmc-release]
AID - 2023.06.09.23291208 [pii]
AID - 10.1101/2023.06.09.23291208 [doi]
PST - epublish
SO  - medRxiv [Preprint]. 2023 Jun 12:2023.06.09.23291208. doi: 
      10.1101/2023.06.09.23291208.

PMID- 38201418
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240124
IS  - 2075-4418 (Print)
IS  - 2075-4418 (Electronic)
IS  - 2075-4418 (Linking)
VI  - 14
IP  - 1
DP  - 2024 Jan 4
TI  - A Systematic Review and Meta-Analysis of Artificial Intelligence Tools in 
      Medicine and Healthcare: Applications, Considerations, Limitations, Motivation 
      and Challenges.
LID - 10.3390/diagnostics14010109 [doi]
LID - 109
AB  - Artificial intelligence (AI) has emerged as a transformative force in various 
      sectors, including medicine and healthcare. Large language models like ChatGPT 
      showcase AI's potential by generating human-like text through prompts. ChatGPT's 
      adaptability holds promise for reshaping medical practices, improving patient 
      care, and enhancing interactions among healthcare professionals, patients, and 
      data. In pandemic management, ChatGPT rapidly disseminates vital information. It 
      serves as a virtual assistant in surgical consultations, aids dental practices, 
      simplifies medical education, and aids in disease diagnosis. A total of 82 papers 
      were categorised into eight major areas, which are G1: treatment and medicine, 
      G2: buildings and equipment, G3: parts of the human body and areas of the 
      disease, G4: patients, G5: citizens, G6: cellular imaging, radiology, pulse and 
      medical images, G7: doctors and nurses, and G8: tools, devices and 
      administration. Balancing AI's role with human judgment remains a challenge. A 
      systematic literature review using the PRISMA approach explored AI's 
      transformative potential in healthcare, highlighting ChatGPT's versatile 
      applications, limitations, motivation, and challenges. In conclusion, ChatGPT's 
      diverse medical applications demonstrate its potential for innovation, serving as 
      a valuable resource for students, academics, and researchers in healthcare. 
      Additionally, this study serves as a guide, assisting students, academics, and 
      researchers in the field of medicine and healthcare alike.
FAU - Younis, Hussain A
AU  - Younis HA
AUID- ORCID: 0000-0002-5042-4990
AD  - College of Education for Women, University of Basrah, Basrah 61004, Iraq.
FAU - Eisa, Taiseer Abdalla Elfadil
AU  - Eisa TAE
AD  - Department of Information Systems-Girls Section, King Khalid University, Mahayil 
      62529, Saudi Arabia.
FAU - Nasser, Maged
AU  - Nasser M
AD  - Computer &amp; Information Sciences Department, Universiti Teknologi PETRONAS, Seri 
      Iskandar 32610, Malaysia.
FAU - Sahib, Thaeer Mueen
AU  - Sahib TM
AD  - Kufa Technical Institute, Al-Furat Al-Awsat Technical University, Kufa 54001, 
      Iraq.
FAU - Noor, Ameen A
AU  - Noor AA
AD  - Computer Science Department, College of Education, University of Almustansirya, 
      Baghdad 10045, Iraq.
FAU - Alyasiri, Osamah Mohammed
AU  - Alyasiri OM
AUID- ORCID: 0000-0002-2345-2443
AD  - Karbala Technical Institute, Al-Furat Al-Awsat Technical University, Karbala 
      56001, Iraq.
FAU - Salisu, Sani
AU  - Salisu S
AUID- ORCID: 0000-0001-9196-4507
AD  - Department of Information Technology, Federal University Dutse, Dutse 720101, 
      Nigeria.
FAU - Hayder, Israa M
AU  - Hayder IM
AD  - Qurna Technique Institute, Southern Technical University, Basrah 61016, Iraq.
FAU - Younis, Hameed AbdulKareem
AU  - Younis HA
AUID- ORCID: 0000-0003-4580-9287
AD  - Department of Cybersecurity, College of Computer Science and Information 
      Technology, University of Basrah, Basrah 61016, Iraq.
LA  - eng
GR  - RGP2/52/44/King Khalid University/
PT  - Journal Article
PT  - Review
DEP - 20240104
PL  - Switzerland
TA  - Diagnostics (Basel)
JT  - Diagnostics (Basel, Switzerland)
JID - 101658402
PMC - PMC10802884
OTO - NOTNLM
OT  - ChatGPT
OT  - cellular imaging
OT  - dental
OT  - disease
OT  - healthcare
OT  - image
OT  - medicine
OT  - pharmaceutical
OT  - radiology and sonar
COIS- The authors declare no conflict of interest.
EDAT- 2024/01/11 07:42
MHDA- 2024/01/11 07:43
PMCR- 2024/01/04
CRDT- 2024/01/11 01:05
PHST- 2023/10/29 00:00 [received]
PHST- 2023/12/02 00:00 [revised]
PHST- 2023/12/04 00:00 [accepted]
PHST- 2024/01/11 07:43 [medline]
PHST- 2024/01/11 07:42 [pubmed]
PHST- 2024/01/11 01:05 [entrez]
PHST- 2024/01/04 00:00 [pmc-release]
AID - diagnostics14010109 [pii]
AID - diagnostics-14-00109 [pii]
AID - 10.3390/diagnostics14010109 [doi]
PST - epublish
SO  - Diagnostics (Basel). 2024 Jan 4;14(1):109. doi: 10.3390/diagnostics14010109.

PMID- 37602755
OWN - NLM
STAT- MEDLINE
DCOM- 20240227
LR  - 20240227
IS  - 1445-2197 (Electronic)
IS  - 1445-1433 (Linking)
VI  - 94
IP  - 1-2
DP  - 2024 Feb
TI  - Investigating the impact of innovative AI chatbot on post-pandemic medical 
      education and clinical assistance: a comprehensive analysis.
PG  - 68-77
LID - 10.1111/ans.18666 [doi]
AB  - BACKGROUND: The COVID-19 pandemic has significantly disrupted clinical experience 
      and exposure of medical students and junior doctors. Artificial Intelligence (AI) 
      integration in medical education has the potential to enhance learning and 
      improve patient care. This study aimed to evaluate the effectiveness of three 
      popular large language models (LLMs) in serving as clinical decision-making 
      support tools for junior doctors. METHODS: A series of increasingly complex 
      clinical scenarios were presented to ChatGPT, Google's Bard and Bing's AI. Their 
      responses were evaluated against standard guidelines, and for reliability by the 
      Flesch Reading Ease Score, Flesch-Kincaid Grade Level, the Coleman-Liau Index, 
      and the modified DISCERN score for assessing suitability. Lastly, the LLMs 
      outputs were assessed by using the Likert scale for accuracy, informativeness, 
      and accessibility by three experienced specialists. RESULTS: In terms of 
      readability and reliability, ChatGPT stood out among the three LLMs, recording 
      the highest scores in Flesch Reading Ease (31.2 ± 3.5), Flesch-Kincaid Grade 
      Level (13.5 ± 0.7), Coleman-Lau Index (13) and DISCERN (62 ± 4.4). These results 
      suggest statistically significant superior comprehensibility and alignment with 
      clinical guidelines in the medical advice given by ChatGPT. Bard followed closely 
      behind, with BingAI trailing in all categories. The only non-significant 
      statistical differences (P &gt; 0.05) were found between ChatGPT and Bard's 
      readability indices, and between the Flesch Reading Ease scores of ChatGPT/Bard 
      and BingAI. CONCLUSION: This study demonstrates the potential utility of LLMs in 
      fostering self-directed and personalized learning, as well as bolstering clinical 
      decision-making support for junior doctors. However further development is needed 
      for its integration into education.
CI  - © 2023 The Authors. ANZ Journal of Surgery published by John Wiley &amp; Sons 
      Australia, Ltd on behalf of Royal Australasian College of Surgeons.
FAU - Xie, Yi
AU  - Xie Y
AD  - Department of Surgery, Peninsula Health, Melbourne, Victoria, Australia.
FAU - Seth, Ishith
AU  - Seth I
AUID- ORCID: 0000-0001-5444-8925
AD  - Department of Surgery, Peninsula Health, Melbourne, Victoria, Australia.
AD  - Faculty of Science, Medicine, and Health, Monash University, Melbourne, Victoria, 
      Australia.
FAU - Hunter-Smith, David J
AU  - Hunter-Smith DJ
AUID- ORCID: 0000-0001-5660-0572
AD  - Department of Surgery, Peninsula Health, Melbourne, Victoria, Australia.
AD  - Faculty of Science, Medicine, and Health, Monash University, Melbourne, Victoria, 
      Australia.
FAU - Rozen, Warren M
AU  - Rozen WM
AUID- ORCID: 0000-0002-4092-182X
AD  - Department of Surgery, Peninsula Health, Melbourne, Victoria, Australia.
AD  - Faculty of Science, Medicine, and Health, Monash University, Melbourne, Victoria, 
      Australia.
FAU - Seifman, Marc A
AU  - Seifman MA
AUID- ORCID: 0000-0002-5989-4809
AD  - Department of Surgery, Peninsula Health, Melbourne, Victoria, Australia.
AD  - Faculty of Science, Medicine, and Health, Monash University, Melbourne, Victoria, 
      Australia.
LA  - eng
PT  - Journal Article
DEP - 20230821
PL  - Australia
TA  - ANZ J Surg
JT  - ANZ journal of surgery
JID - 101086634
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - Pandemics
MH  - Reproducibility of Results
MH  - *Health Literacy
MH  - Comprehension
MH  - *Education, Medical
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - junior doctor
OT  - large language model
OT  - surgical education
EDAT- 2023/08/21 12:43
MHDA- 2024/02/27 06:45
CRDT- 2023/08/21 09:26
PHST- 2023/08/04 00:00 [revised]
PHST- 2023/03/24 00:00 [received]
PHST- 2023/08/07 00:00 [accepted]
PHST- 2024/02/27 06:45 [medline]
PHST- 2023/08/21 12:43 [pubmed]
PHST- 2023/08/21 09:26 [entrez]
AID - 10.1111/ans.18666 [doi]
PST - ppublish
SO  - ANZ J Surg. 2024 Feb;94(1-2):68-77. doi: 10.1111/ans.18666. Epub 2023 Aug 21.

PMID- 37515033
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230801
IS  - 2076-393X (Print)
IS  - 2076-393X (Electronic)
IS  - 2076-393X (Linking)
VI  - 11
IP  - 7
DP  - 2023 Jul 7
TI  - Artificial Intelligence and Public Health: Evaluating ChatGPT Responses to 
      Vaccination Myths and Misconceptions.
LID - 10.3390/vaccines11071217 [doi]
LID - 1217
AB  - Artificial intelligence (AI) tools, such as ChatGPT, are the subject of intense 
      debate regarding their possible applications in contexts such as health care. 
      This study evaluates the Correctness, Clarity, and Exhaustiveness of the answers 
      provided by ChatGPT on the topic of vaccination. The World Health Organization's 
      11 "myths and misconceptions" about vaccinations were administered to both the 
      free (GPT-3.5) and paid version (GPT-4.0) of ChatGPT. The AI tool's responses 
      were evaluated qualitatively and quantitatively, in reference to those myth and 
      misconceptions provided by WHO, independently by two expert Raters. The agreement 
      between the Raters was significant for both versions (p of K &lt; 0.05). Overall, 
      ChatGPT responses were easy to understand and 85.4% accurate although one of the 
      questions was misinterpreted. Qualitatively, the GPT-4.0 responses were superior 
      to the GPT-3.5 responses in terms of Correctness, Clarity, and Exhaustiveness (Δ 
      = 5.6%, 17.9%, 9.3%, respectively). The study shows that, if appropriately 
      questioned, AI tools can represent a useful aid in the health care field. 
      However, when consulted by non-expert users, without the support of expert 
      medical advice, these tools are not free from the risk of eliciting misleading 
      responses. Moreover, given the existing social divide in information access, the 
      improved accuracy of answers from the paid version raises further ethical issues.
FAU - Deiana, Giovanna
AU  - Deiana G
AUID- ORCID: 0000-0003-1151-6830
AD  - Department of Biomedical Sciences, University of Sassari, 07100 Sassari, Italy.
AD  - Department of Medical, Surgical and Experimental Sciences, University Hospital of 
      Sassari, 07100 Sassari, Italy.
FAU - Dettori, Marco
AU  - Dettori M
AUID- ORCID: 0000-0002-4901-2067
AD  - Department of Medical, Surgical and Experimental Sciences, University Hospital of 
      Sassari, 07100 Sassari, Italy.
AD  - Department of Medicine, Surgery and Pharmacy, University of Sassari, 07100 
      Sassari, Italy.
AD  - Department of Restorative, Pediatric and Preventive Dentistry, University of 
      Bern, 3012 Bern, Switzerland.
FAU - Arghittu, Antonella
AU  - Arghittu A
AUID- ORCID: 0000-0001-9257-1422
AD  - Department of Medicine, Surgery and Pharmacy, University of Sassari, 07100 
      Sassari, Italy.
FAU - Azara, Antonio
AU  - Azara A
AUID- ORCID: 0000-0001-9089-0817
AD  - Department of Medical, Surgical and Experimental Sciences, University Hospital of 
      Sassari, 07100 Sassari, Italy.
AD  - Department of Medicine, Surgery and Pharmacy, University of Sassari, 07100 
      Sassari, Italy.
FAU - Gabutti, Giovanni
AU  - Gabutti G
AUID- ORCID: 0000-0002-7259-4923
AD  - Working Group "Vaccines and Immunization Policies", Italian Society of Hygiene, 
      Preventive Medicine and Public Health, 16030 Cogorno, Italy.
FAU - Castiglia, Paolo
AU  - Castiglia P
AUID- ORCID: 0000-0002-0972-4828
AD  - Department of Medical, Surgical and Experimental Sciences, University Hospital of 
      Sassari, 07100 Sassari, Italy.
AD  - Department of Medicine, Surgery and Pharmacy, University of Sassari, 07100 
      Sassari, Italy.
AD  - Working Group "Vaccines and Immunization Policies", Italian Society of Hygiene, 
      Preventive Medicine and Public Health, 16030 Cogorno, Italy.
LA  - eng
PT  - Journal Article
DEP - 20230707
PL  - Switzerland
TA  - Vaccines (Basel)
JT  - Vaccines
JID - 101629355
PMC - PMC10386180
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - immunization
OT  - myths and misconceptions
OT  - public health
OT  - vaccines
COIS- The authors declare no conflict of interest directly relating to this manuscript. 
      GG declares outside this paper having received personal fees for advisory board 
      membership and consultancy from Emergent BioSolutions, the GSK group of 
      companies, Merck Sharp &amp; Dohme, Pfizer, Sanofi Pasteur Italy, Moderna and 
      Seqirus, as well as personal fees for lectures from Merck Sharp &amp; Dohme, Moderna, 
      Pfizer, and Seqirus. PC declares outside this paper having received personal fees 
      for advisory board membership and travel expenses for lectures from the GSK group 
      of companies, Merck Sharp &amp; Dohme, Pfizer, Sanofi Pasteur Italy, Moderna and 
      Seqirus.
EDAT- 2023/07/29 11:49
MHDA- 2023/07/29 11:50
PMCR- 2023/07/07
CRDT- 2023/07/29 01:44
PHST- 2023/06/01 00:00 [received]
PHST- 2023/07/04 00:00 [revised]
PHST- 2023/07/05 00:00 [accepted]
PHST- 2023/07/29 11:50 [medline]
PHST- 2023/07/29 11:49 [pubmed]
PHST- 2023/07/29 01:44 [entrez]
PHST- 2023/07/07 00:00 [pmc-release]
AID - vaccines11071217 [pii]
AID - vaccines-11-01217 [pii]
AID - 10.3390/vaccines11071217 [doi]
PST - epublish
SO  - Vaccines (Basel). 2023 Jul 7;11(7):1217. doi: 10.3390/vaccines11071217.

PMID- 38383555
OWN - NLM
STAT- MEDLINE
DCOM- 20240223
LR  - 20240224
IS  - 2041-1723 (Electronic)
IS  - 2041-1723 (Linking)
VI  - 15
IP  - 1
DP  - 2024 Feb 21
TI  - Large language models streamline automated machine learning for clinical studies.
PG  - 1603
LID - 10.1038/s41467-024-45879-8 [doi]
LID - 1603
AB  - A knowledge gap persists between machine learning (ML) developers (e.g., data 
      scientists) and practitioners (e.g., clinicians), hampering the full utilization 
      of ML for clinical data analysis. We investigated the potential of the ChatGPT 
      Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and 
      perform ML analyses efficiently. Real-world clinical datasets and study details 
      from large trials across various medical specialties were presented to ChatGPT 
      ADA without specific guidance. ChatGPT ADA autonomously developed 
      state-of-the-art ML models based on the original study's training data to predict 
      clinical outcomes such as cancer development, cancer progression, disease 
      complications, or biomarkers such as pathogenic gene sequences. Following the 
      re-implementation and optimization of the published models, the head-to-head 
      comparison of the ChatGPT ADA-crafted ML models and their respective manually 
      crafted counterparts revealed no significant differences in traditional 
      performance metrics (p ≥ 0.072). Strikingly, the ChatGPT ADA-crafted ML models 
      often outperformed their counterparts. In conclusion, ChatGPT ADA offers a 
      promising avenue to democratize ML in medicine by simplifying complex data 
      analyses, yet should enhance, not replace, specialized training and resources, to 
      promote broader applications in medical research and practice.
CI  - © 2024. The Author(s).
FAU - Tayebi Arasteh, Soroosh
AU  - Tayebi Arasteh S
AUID- ORCID: 0000-0003-1015-7733
AD  - Department of Diagnostic and Interventional Radiology, University Hospital RWTH 
      Aachen, Aachen, Germany. soroosh.arasteh@rwth-aachen.de.
FAU - Han, Tianyu
AU  - Han T
AUID- ORCID: 0000-0002-8636-6462
AD  - Department of Diagnostic and Interventional Radiology, University Hospital RWTH 
      Aachen, Aachen, Germany. than@ukaachen.de.
FAU - Lotfinia, Mahshad
AU  - Lotfinia M
AUID- ORCID: 0000-0001-7605-7992
AD  - Department of Diagnostic and Interventional Radiology, University Hospital RWTH 
      Aachen, Aachen, Germany.
AD  - Institute of Heat and Mass Transfer, RWTH Aachen University, Aachen, Germany.
FAU - Kuhl, Christiane
AU  - Kuhl C
AD  - Department of Diagnostic and Interventional Radiology, University Hospital RWTH 
      Aachen, Aachen, Germany.
FAU - Kather, Jakob Nikolas
AU  - Kather JN
AUID- ORCID: 0000-0002-3730-5348
AD  - Else Kroener Fresenius Center for Digital Health, Medical Faculty Carl Gustav 
      Carus, Technical University Dresden, Dresden, Germany.
AD  - Medical Oncology, National Center for Tumor Diseases (NCT), University Hospital 
      Heidelberg, Heidelberg, Germany.
FAU - Truhn, Daniel
AU  - Truhn D
AUID- ORCID: 0000-0002-9605-0728
AD  - Department of Diagnostic and Interventional Radiology, University Hospital RWTH 
      Aachen, Aachen, Germany.
FAU - Nebelung, Sven
AU  - Nebelung S
AUID- ORCID: 0000-0002-5267-9962
AD  - Department of Diagnostic and Interventional Radiology, University Hospital RWTH 
      Aachen, Aachen, Germany.
LA  - eng
PT  - Journal Article
DEP - 20240221
PL  - England
TA  - Nat Commun
JT  - Nature communications
JID - 101528555
SB  - IM
MH  - Humans
MH  - *Algorithms
MH  - Benchmarking
MH  - Language
MH  - Machine Learning
MH  - *Neoplasms
PMC - PMC10881983
COIS- J.N.K. declares consulting services for Owkin, France; DoMore Diagnostics, 
      Norway, and Panakeia, UK. Furthermore, J.N.K. holds shares in StratifAI GmbH and 
      has received honoraria for lectures by Bayer, Eisai, MSD, BMS, Roche, Pfizer, and 
      Fresenius. D.T. holds shares in StraifAI GmbH, Germany, and received honoraria 
      for lectures by Bayer. The other authors declare no competing interests.
EDAT- 2024/02/22 00:43
MHDA- 2024/02/23 06:43
PMCR- 2024/02/21
CRDT- 2024/02/21 23:46
PHST- 2023/10/10 00:00 [received]
PHST- 2024/02/06 00:00 [accepted]
PHST- 2024/02/23 06:43 [medline]
PHST- 2024/02/22 00:43 [pubmed]
PHST- 2024/02/21 23:46 [entrez]
PHST- 2024/02/21 00:00 [pmc-release]
AID - 10.1038/s41467-024-45879-8 [pii]
AID - 45879 [pii]
AID - 10.1038/s41467-024-45879-8 [doi]
PST - epublish
SO  - Nat Commun. 2024 Feb 21;15(1):1603. doi: 10.1038/s41467-024-45879-8.

PMID- 38267726
OWN - NLM
STAT- MEDLINE
DCOM- 20240325
LR  - 20240325
IS  - 1573-2568 (Electronic)
IS  - 0163-2116 (Linking)
VI  - 69
IP  - 3
DP  - 2024 Mar
TI  - Applicability of Online Chat-Based Artificial Intelligence Models to Colorectal 
      Cancer Screening.
PG  - 791-797
LID - 10.1007/s10620-024-08274-3 [doi]
AB  - BACKGROUND: Over the past year, studies have shown potential in the applicability 
      of ChatGPT in various medical specialties including cardiology and oncology. 
      However, the application of ChatGPT and other online chat-based AI models to 
      patient education and patient-physician communication on colorectal cancer 
      screening has not been critically evaluated which is what we aimed to do in this 
      study. METHODS: We posed 15 questions on important colorectal cancer screening 
      concepts and 5 common questions asked by patients to the 3 most commonly used 
      freely available artificial intelligence (AI) models. The responses provided by 
      the AI models were graded for appropriateness and reliability using American 
      College of Gastroenterology guidelines. The responses to each question provided 
      by an AI model were graded as reliably appropriate (RA), reliably inappropriate 
      (RI) and unreliable. Grader assessments were validated by the joint probability 
      of agreement for two raters. RESULTS: ChatGPT and YouChat™ provided RA responses 
      to the questions posed more often than BingChat. There were two questions 
      that &gt; 1 AI model provided unreliable responses to. ChatGPT did not provide 
      references. BingChat misinterpreted some of the information it referenced. The 
      age of CRC screening provided by YouChat™ was not consistently up-to-date. 
      Inter-rater reliability for 2 raters was 89.2%. CONCLUSION: Most responses 
      provided by AI models on CRC screening were appropriate. Some limitations exist 
      in their ability to correctly interpret medical literature and provide updated 
      information in answering queries. Patients should consult their physicians for 
      context on the recommendations made by these AI models.
CI  - © 2024. The Author(s), under exclusive licence to Springer Science+Business 
      Media, LLC, part of Springer Nature.
FAU - Atarere, Joseph
AU  - Atarere J
AUID- ORCID: 0000-0002-7224-2194
AD  - Department of Medicine, MedStar Health, 201 East University Pkwy, Baltimore, MD, 
      21218, USA. joseph.o.atarere@medstar.net.
AD  - Department of Biostatistics and Epidemiology, Harvard T.H. Chan School of Public 
      Health, Boston, MA, USA. joseph.o.atarere@medstar.net.
FAU - Naqvi, Haider
AU  - Naqvi H
AD  - Department of Medicine, MedStar Health, 201 East University Pkwy, Baltimore, MD, 
      21218, USA.
FAU - Haas, Christopher
AU  - Haas C
AD  - Department of Medicine, MedStar Health, 201 East University Pkwy, Baltimore, MD, 
      21218, USA.
FAU - Adewunmi, Comfort
AU  - Adewunmi C
AD  - Division of Geriatrics and Gerontology, Emory University School of Medicine, 
      Atlanta, GA, USA.
FAU - Bandaru, Sumanth
AU  - Bandaru S
AD  - Department of Medicine, MedStar Health, 201 East University Pkwy, Baltimore, MD, 
      21218, USA.
FAU - Allamneni, Rakesh
AU  - Allamneni R
AD  - Department of Medicine, MedStar Health, 201 East University Pkwy, Baltimore, MD, 
      21218, USA.
FAU - Ugonabo, Onyinye
AU  - Ugonabo O
AD  - Department of Medicine, Marshall University Joan C. Edwards School of Medicine, 
      Huntington, WV, USA.
FAU - Egbo, Olachi
AU  - Egbo O
AD  - Department of Medicine, Aurora Medical Center, Oshkosh, WI, USA.
FAU - Umoren, Mfoniso
AU  - Umoren M
AD  - Division of Gastroenterology, Georgetown University Hospital, Washington, DC, 
      USA.
FAU - Kanth, Priyanka
AU  - Kanth P
AD  - Division of Gastroenterology, Georgetown University Hospital, Washington, DC, 
      USA.
LA  - eng
PT  - Journal Article
DEP - 20240124
PL  - United States
TA  - Dig Dis Sci
JT  - Digestive diseases and sciences
JID - 7902782
SB  - IM
MH  - Humans
MH  - *Early Detection of Cancer
MH  - Artificial Intelligence
MH  - Reproducibility of Results
MH  - Communication
MH  - *Colorectal Neoplasms/diagnosis
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Colorectal cancer screening
OT  - Health education
EDAT- 2024/01/25 00:42
MHDA- 2024/03/25 06:43
CRDT- 2024/01/24 23:29
PHST- 2023/10/21 00:00 [received]
PHST- 2024/01/02 00:00 [accepted]
PHST- 2024/03/25 06:43 [medline]
PHST- 2024/01/25 00:42 [pubmed]
PHST- 2024/01/24 23:29 [entrez]
AID - 10.1007/s10620-024-08274-3 [pii]
AID - 10.1007/s10620-024-08274-3 [doi]
PST - ppublish
SO  - Dig Dis Sci. 2024 Mar;69(3):791-797. doi: 10.1007/s10620-024-08274-3. Epub 2024 
      Jan 24.

PMID- 38563415
OWN - NLM
STAT- Publisher
LR  - 20240402
IS  - 1531-4995 (Electronic)
IS  - 0023-852X (Linking)
DP  - 2024 Apr 2
TI  - The Comparative Diagnostic Capability of Large Language Models in Otolaryngology.
LID - 10.1002/lary.31434 [doi]
AB  - OBJECTIVES: Evaluate and compare the ability of large language models (LLMs) to 
      diagnose various ailments in otolaryngology. METHODS: We collected all 100 
      clinical vignettes from the second edition of Otolaryngology Cases-The University 
      of Cincinnati Clinical Portfolio by Pensak et al. With the addition of the prompt 
      "Provide a diagnosis given the following history," we prompted ChatGPT-3.5, 
      Google Bard, and Bing-GPT4 to provide a diagnosis for each vignette. These 
      diagnoses were compared to the portfolio for accuracy and recorded. All queries 
      were run in June 2023. RESULTS: ChatGPT-3.5 was the most accurate model (89% 
      success rate), followed by Google Bard (82%) and Bing GPT (74%). A chi-squared 
      test revealed a significant difference between the three LLMs in providing 
      correct diagnoses (p = 0.023). Of the 100 vignettes, seven require additional 
      testing results (i.e., biopsy, non-contrast CT) for accurate clinical diagnosis. 
      When omitting these vignettes, the revised success rates were 95.7% for 
      ChatGPT-3.5, 88.17% for Google Bard, and 78.72% for Bing-GPT4 (p = 0.002). 
      CONCLUSIONS: ChatGPT-3.5 offers the most accurate diagnoses when given 
      established clinical vignettes as compared to Google Bard and Bing-GPT4. LLMs may 
      accurately offer assessments for common otolaryngology conditions but currently 
      require detailed prompt information and critical supervision from clinicians. 
      There is vast potential in the clinical applicability of LLMs; however, 
      practitioners should be wary of possible "hallucinations" and misinformation in 
      responses. LEVEL OF EVIDENCE: 3 Laryngoscope, 2024.
CI  - © 2024 The Authors. The Laryngoscope published by Wiley Periodicals LLC on behalf 
      of The American Laryngological, Rhinological and Otological Society, Inc.
FAU - Warrier, Akshay
AU  - Warrier A
AUID- ORCID: 0000-0003-4841-6366
AD  - Department of Otolaryngology-Head and Neck Surgery, Rutgers New Jersey Medical 
      School, Newark, New Jersey, U.S.A.
FAU - Singh, Rohan
AU  - Singh R
AD  - Department of Otolaryngology-Head and Neck Surgery, Rutgers New Jersey Medical 
      School, Newark, New Jersey, U.S.A.
FAU - Haleem, Afash
AU  - Haleem A
AUID- ORCID: 0000-0001-7268-6142
AD  - Department of Otolaryngology-Head and Neck Surgery, Rutgers New Jersey Medical 
      School, Newark, New Jersey, U.S.A.
FAU - Zaki, Haider
AU  - Zaki H
AD  - Department of Otolaryngology-Head and Neck Surgery, Rutgers New Jersey Medical 
      School, Newark, New Jersey, U.S.A.
FAU - Eloy, Jean Anderson
AU  - Eloy JA
AUID- ORCID: 0000-0003-2893-7818
AD  - Department of Otolaryngology-Head and Neck Surgery, Rutgers New Jersey Medical 
      School, Newark, New Jersey, U.S.A.
AD  - Center for Skull Base and Pituitary Surgery, Neurological Institute of New 
      Jersey, Rutgers New Jersey Medical School, Newark, New Jersey, U.S.A.
LA  - eng
PT  - Journal Article
DEP - 20240402
PL  - United States
TA  - Laryngoscope
JT  - The Laryngoscope
JID - 8607378
SB  - IM
OTO - NOTNLM
OT  - Bing AI
OT  - ChatGPT
OT  - Google Bard
OT  - artificial intelligence
OT  - diagnostic accuracy
OT  - large language models
OT  - otolaryngology
EDAT- 2024/04/02 12:49
MHDA- 2024/04/02 12:49
CRDT- 2024/04/02 07:43
PHST- 2024/03/05 00:00 [revised]
PHST- 2024/01/19 00:00 [received]
PHST- 2024/03/21 00:00 [accepted]
PHST- 2024/04/02 12:49 [medline]
PHST- 2024/04/02 12:49 [pubmed]
PHST- 2024/04/02 07:43 [entrez]
AID - 10.1002/lary.31434 [doi]
PST - aheadofprint
SO  - Laryngoscope. 2024 Apr 2. doi: 10.1002/lary.31434.

PMID- 37328703
OWN - NLM
STAT- MEDLINE
DCOM- 20231109
LR  - 20231109
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 51
IP  - 12
DP  - 2023 Dec
TI  - ChatGPT, Bard, and Large Language Models for Biomedical Research: Opportunities 
      and Pitfalls.
PG  - 2647-2651
LID - 10.1007/s10439-023-03284-0 [doi]
AB  - Large Language Models (LLMs) such as ChatGPT and Bard have emerged as 
      groundbreaking interactive chatbots, capturing significant attention and 
      transforming the biomedical research landscape. These powerful tools offer 
      immense potential for advancing scientific inquiry, but they also present 
      challenges and pitfalls. Leveraging large language models, researchers can 
      streamline literature reviews, summarize complex findings, and even generate 
      novel hypotheses, enabling the exploration of uncharted scientific territories. 
      However, the inherent risk of misinformation and misleading interpretations 
      underscores the critical importance of rigorous validation and verification 
      processes. This article provides a comprehensive overview of the current 
      landscape and delves into the opportunities and pitfalls associated with 
      employing LLMs in biomedical research. Furthermore, it sheds light on strategies 
      to enhance the utility of LLMs in biomedical research, offering recommendations 
      to ensure their responsible and effective implementation in this domain. The 
      findings presented in this article contribute to the advancement of biomedical 
      engineering by harnessing the potential of LLMs while addressing their 
      limitations.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Thapa, Surendrabikram
AU  - Thapa S
AUID- ORCID: 0000-0003-4119-8239
AD  - Virginia Tech, Blacksburg, VA, 24060, USA. surendrabikram@vt.edu.
FAU - Adhikari, Surabhi
AU  - Adhikari S
AD  - Columbia University, New York, NY, 10027, USA.
LA  - eng
PT  - Letter
PT  - Review
DEP - 20230616
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - *Language
MH  - Bioengineering
MH  - Biomedical Engineering
MH  - *Biomedical Research
OTO - NOTNLM
OT  - Bard
OT  - Biomedical research
OT  - ChatGPT
OT  - Large language models
OT  - Natural language processing
EDAT- 2023/06/17 05:11
MHDA- 2023/11/09 06:41
CRDT- 2023/06/16 23:31
PHST- 2023/06/10 00:00 [received]
PHST- 2023/06/13 00:00 [accepted]
PHST- 2023/11/09 06:41 [medline]
PHST- 2023/06/17 05:11 [pubmed]
PHST- 2023/06/16 23:31 [entrez]
AID - 10.1007/s10439-023-03284-0 [pii]
AID - 10.1007/s10439-023-03284-0 [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2023 Dec;51(12):2647-2651. doi: 10.1007/s10439-023-03284-0. Epub 
      2023 Jun 16.

PMID- 37952568
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240313
IS  - 1097-6787 (Electronic)
IS  - 0190-9622 (Linking)
VI  - 90
IP  - 3
DP  - 2024 Mar
TI  - ChatRx: ChatGPT's potential to educate patients on medication adverse effects.
PG  - 669-670
LID - S0190-9622(23)03199-7 [pii]
LID - 10.1016/j.jaad.2023.11.008 [doi]
FAU - Chen, Annie I
AU  - Chen AI
AD  - Department of Dermatology, University of Pittsburgh School of Medicine, 
      Pittsburgh, Pennsylvania.
FAU - Ferris, Laura K
AU  - Ferris LK
AD  - Department of Dermatology, University of Pittsburgh School of Medicine, 
      Pittsburgh, Pennsylvania.
FAU - Nambudiri, Vinod E
AU  - Nambudiri VE
AD  - Department of Dermatology, Brigham and Women's Hospital, Boston, Massachusetts; 
      Harvard Medical School, Boston, Massachusetts.
FAU - Piette, Evan W
AU  - Piette EW
AD  - Department of Dermatology, Brigham and Women's Hospital, Boston, Massachusetts; 
      Harvard Medical School, Boston, Massachusetts. Electronic address: 
      epiette@bwh.harvard.edu.
LA  - eng
PT  - Journal Article
DEP - 20231111
PL  - United States
TA  - J Am Acad Dermatol
JT  - Journal of the American Academy of Dermatology
JID - 7907132
SB  - IM
MH  - *Patient Education as Topic
MH  - *Artificial Intelligence
MH  - *Drug-Related Side Effects and Adverse Reactions
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - clinical research
OT  - dermatology
OT  - drug side effects
OT  - general dermatology
OT  - innovation
OT  - large language models
OT  - medical dermatology
OT  - medical education
OT  - patients
OT  - technology
COIS- Conflicts of interest None disclosed.
EDAT- 2023/11/13 00:42
MHDA- 2024/02/12 15:42
CRDT- 2023/11/12 19:12
PHST- 2023/08/24 00:00 [received]
PHST- 2023/10/23 00:00 [revised]
PHST- 2023/11/08 00:00 [accepted]
PHST- 2024/02/12 15:42 [medline]
PHST- 2023/11/13 00:42 [pubmed]
PHST- 2023/11/12 19:12 [entrez]
AID - S0190-9622(23)03199-7 [pii]
AID - 10.1016/j.jaad.2023.11.008 [doi]
PST - ppublish
SO  - J Am Acad Dermatol. 2024 Mar;90(3):669-670. doi: 10.1016/j.jaad.2023.11.008. Epub 
      2023 Nov 11.

PMID- 38126285
OWN - NLM
STAT- MEDLINE
DCOM- 20231222
LR  - 20231222
IS  - 2724-6442 (Electronic)
IS  - 2724-6051 (Linking)
VI  - 75
IP  - 6
DP  - 2023 Dec
TI  - Evaluating the performance of ChatGPT in answering questions related to benign 
      prostate hyperplasia and prostate cancer.
PG  - 729-733
LID - 10.23736/S2724-6051.23.05450-2 [doi]
AB  - BACKGROUND: The aim of this study was to evaluate the accuracy and 
      reproducibility of ChatGPT's answers to frequently asked questions about benign 
      prostate hyperplasia (BPH) and prostate cancer. METHODS: Frequently asked 
      questions on the websites of urology associations, hospitals, and social media 
      about prostate cancer and BPH were evaluated. Also, strong recommendation-level 
      data were noted in the recommendations tables of the European Urology Association 
      (EAU) 2022 Guidelines on Prostate Cancer and Management of Non-neurogenic Male 
      Lower Urinary Tract Symptoms sections. All questions were asked in order in 
      ChatGPT Mar 23 Version. All answers were evaluated separately by two specialist 
      urologists and scored between 1-4. RESULTS: Forty questions about BPH and 86 
      questions about prostate cancer were included in the study. The answers to all 
      BPH-related questions resulted in 90.0% completely correct. This rate for 
      questions about prostate cancer was 94.2%. The completely correct rate in the 
      questions prepared according to the strong recommendations of the EAU guideline 
      was 77.8% for BPH and 76.2% for prostate cancer. The similarity rates of the 
      answers to the repeated questions were 90.0% and 93% for questions related to BPH 
      and prostate cancer, respectively. CONCLUSIONS: ChatGPT has given satisfactory 
      answers to questions about BPH and prostate cancer. Although it has limitations, 
      it can be predicted that it will take an important place in the health sector in 
      the future, as it is a constantly evolving platform. ChatGPT was able to provide 
      helpful information about BPH and prostate cancer, although it is not perfect. It 
      is constantly getting better, and may become an important resource in the 
      healthcare field in the future.
FAU - Caglar, Ufuk
AU  - Caglar U
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Türkiye - 
      ufukcglr@gmail.com.
FAU - Yildiz, Oguzhan
AU  - Yildiz O
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Türkiye.
FAU - Meric, Arda
AU  - Meric A
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Türkiye.
FAU - Ayranci, Ali
AU  - Ayranci A
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Türkiye.
FAU - Yusuf, Resit
AU  - Yusuf R
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Türkiye.
FAU - Sarilar, Omer
AU  - Sarilar O
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Türkiye.
FAU - Ozgor, Faruk
AU  - Ozgor F
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Türkiye.
LA  - eng
PT  - Journal Article
PL  - Italy
TA  - Minerva Urol Nephrol
JT  - Minerva urology and nephrology
JID - 101777299
SB  - IM
MH  - Humans
MH  - Male
MH  - *Prostatic Hyperplasia/diagnosis
MH  - Prostate
MH  - Hyperplasia
MH  - Reproducibility of Results
MH  - *Prostatic Neoplasms/diagnosis
EDAT- 2023/12/21 12:42
MHDA- 2023/12/22 06:42
CRDT- 2023/12/21 06:50
PHST- 2023/12/22 06:42 [medline]
PHST- 2023/12/21 12:42 [pubmed]
PHST- 2023/12/21 06:50 [entrez]
AID - S2724-6051.23.05450-2 [pii]
AID - 10.23736/S2724-6051.23.05450-2 [doi]
PST - ppublish
SO  - Minerva Urol Nephrol. 2023 Dec;75(6):729-733. doi: 
      10.23736/S2724-6051.23.05450-2.

PMID- 38358993
OWN - NLM
STAT- MEDLINE
DCOM- 20240219
LR  - 20240219
IS  - 1932-6203 (Electronic)
IS  - 1932-6203 (Linking)
VI  - 19
IP  - 2
DP  - 2024
TI  - Study protocol for factors influencing the adoption of ChatGPT technology by 
      startups: Perceptions and attitudes of entrepreneurs.
PG  - e0298427
LID - 10.1371/journal.pone.0298427 [doi]
LID - e0298427
AB  - BACKGROUND: Generative Artificial Intelligence (AI) technology, for instance Chat 
      Generative Pre-trained Transformer (ChatGPT), is continuously evolving, and its 
      userbase is growing. These technologies are now being experimented by the 
      businesses to leverage their potential and minimise their risks in business 
      operations. The continuous adoption of the emerging Generative AI technologies 
      will help startups gain more and more experience with adoptions, helping them to 
      leverage continuously evolving technological innovation landscape. However, there 
      is a dearth of prior research on ChatGPT adoption in the startup context, 
      especially from Entrepreneur perspective, highlights the urgent need for a 
      thorough investigation to identify the variables influencing this technological 
      adoption. The primary objective of this study is to ascertain the factors that 
      impact the uptake of ChatGPT technology by startups, anticipate their influence 
      on the triumph of companies, and offer pragmatic suggestions for various 
      stakeholders, including entrepreneurs, and policymakers. METHOD AND ANALYSIS: 
      This study attempts to explore the variables impacting startups' adoption of 
      ChatGPT technology, with an emphasis on comprehending entrepreneurs' attitudes 
      and perspectives. To identify and then empirically validate the Generative AI 
      technology adoption framework, the study uses a two-stage methodology that 
      includes experience-based research, and survey research. The research method 
      design is descriptive and Correlational design. Stage one of the research study 
      is descriptive and involves adding practical insights, and real-world context to 
      the model by drawing from the professional consulting experiences of the 
      researchers with the SMEs. The outcome of this stage is the adoption model (also 
      called as research framework), building Upon Technology Adoption Model (TAM), 
      that highlight the technology adoption factors (also called as latent variables) 
      connected with subset of each other and finally to the technology adoption factor 
      (or otherwise). Further, the latent variables and their relationships with other 
      latent variables as graphically highlighted by the adoption model will be 
      translated into the structured questionnaire. Stage two involves survey based 
      research. In this stage, structured questionnaire is tested with small group of 
      entrepreneurs (who has provided informed consent) and finally to be distributed 
      among startup founders to further validate the relationships between these 
      factors and the level of influence individual factors have on overall technology 
      adoption. Partial Least Squares Structural Equation Modeling (PLS-SEM) will be 
      used to analyze the gathered data. This multifaceted approach allows for a 
      comprehensive analysis of the adoption process, with an emphasis on 
      understanding, describing, and correlating the key elements at play. DISCUSSION: 
      This is the first study to investigate the factors that impact the adoption of 
      Generative AI, for instance ChatGPT technology by startups from the Entrepreneurs 
      perspectives. The study's findings will give Entrepreneurs, Policymakers, 
      technology providers, researchers, and Institutions offering support for 
      entrepreneurs like Academia, Incubators and Accelerators, University libraries, 
      public libraries, chambers of commerce, and foreign embassies important new 
      information that will help them better understand the factors that encourage and 
      hinder ChatGPT adoption. This will allow them to make well-informed strategic 
      decisions about how to apply and use this technology in startup settings thereby 
      improving their services for businesses.
CI  - Copyright: © 2024 Gupta, Yang. This is an open access article distributed under 
      the terms of the Creative Commons Attribution License, which permits unrestricted 
      use, distribution, and reproduction in any medium, provided the original author 
      and source are credited.
FAU - Gupta, Varun
AU  - Gupta V
AUID- ORCID: 0000-0003-2824-3402
AD  - School of Computing and Mathematical Sciences, Leicester University, Leicester, 
      England.
AD  - Multidisciplinary Research Centre for Innovations in SMEs (MrciS), Gisma 
      University of Applied Sciences, Potsdam, Germany.
AD  - Department of Economics and Business Administration, University of Alcala, Alcalá 
      de Henares (Madrid), Madrid, Spain.
FAU - Yang, Hongji
AU  - Yang H
AUID- ORCID: 0000-0001-6561-3631
AD  - School of Computing and Mathematical Sciences, Leicester University, Leicester, 
      England.
LA  - eng
PT  - Journal Article
DEP - 20240215
PL  - United States
TA  - PLoS One
JT  - PloS one
JID - 101285081
RN  - EC 2.3.1.6 (Choline O-Acetyltransferase)
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Technology
MH  - Biological Transport
MH  - Choline O-Acetyltransferase
MH  - Commerce
PMC - PMC10868733
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/02/15 18:42
MHDA- 2024/02/19 06:42
PMCR- 2024/02/15
CRDT- 2024/02/15 13:35
PHST- 2023/11/01 00:00 [received]
PHST- 2024/01/21 00:00 [accepted]
PHST- 2024/02/19 06:42 [medline]
PHST- 2024/02/15 18:42 [pubmed]
PHST- 2024/02/15 13:35 [entrez]
PHST- 2024/02/15 00:00 [pmc-release]
AID - PONE-D-23-35737 [pii]
AID - 10.1371/journal.pone.0298427 [doi]
PST - epublish
SO  - PLoS One. 2024 Feb 15;19(2):e0298427. doi: 10.1371/journal.pone.0298427. 
      eCollection 2024.

PMID- 38034065
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231202
IS  - 2473-974X (Electronic)
IS  - 2473-974X (Linking)
VI  - 7
IP  - 4
DP  - 2023 Oct-Dec
TI  - Assessment of Artificial Intelligence Performance on the Otolaryngology Residency 
      In-Service Exam.
PG  - e98
LID - 10.1002/oto2.98 [doi]
LID - e98
AB  - OBJECTIVES: This study seeks to determine the potential use and reliability of a 
      large language learning model for answering questions in a sub-specialized area 
      of medicine, specifically practice exam questions in otolaryngology-head and neck 
      surgery and assess its current efficacy for surgical trainees and learners. STUDY 
      DESIGN AND SETTING: All available questions from a public, paid-access question 
      bank were manually input through ChatGPT. METHODS: Outputs from ChatGPT were 
      compared against the benchmark of the answers and explanations from the question 
      bank. Questions were assessed in 2 domains: accuracy and comprehensiveness of 
      explanations. RESULTS: Overall, our study demonstrates a ChatGPT correct answer 
      rate of 53% and a correct explanation rate of 54%. We find that with increasing 
      difficulty of questions there is a decreasing rate of answer and explanation 
      accuracy. CONCLUSION: Currently, artificial intelligence-driven learning 
      platforms are not robust enough to be reliable medical education resources to 
      assist learners in sub-specialty specific patient decision making scenarios.
CI  - © 2023 The Authors. OTO Open published by Wiley Periodicals LLC on behalf of 
      American Academy of Otolaryngology–Head and Neck Surgery Foundation.
FAU - Mahajan, Arushi P
AU  - Mahajan AP
AUID- ORCID: 0009-0007-6948-0689
AD  - University of Michigan Medical School Ann Arbor Michigan USA.
FAU - Shabet, Christina L
AU  - Shabet CL
AD  - University of Michigan Medical School Ann Arbor Michigan USA.
FAU - Smith, Joshua
AU  - Smith J
AD  - Department of Otolaryngology-Head and Neck Surgery University of Michigan School 
      of Medicine Ann Arbor Michigan USA.
FAU - Rudy, Shannon F
AU  - Rudy SF
AD  - Department of Otolaryngology-Head and Neck Surgery University of Michigan School 
      of Medicine Ann Arbor Michigan USA.
FAU - Kupfer, Robbi A
AU  - Kupfer RA
AD  - Department of Otolaryngology-Head and Neck Surgery University of Michigan School 
      of Medicine Ann Arbor Michigan USA.
FAU - Bohm, Lauren A
AU  - Bohm LA
AUID- ORCID: 0000-0003-3775-8424
AD  - Department of Otolaryngology-Head and Neck Surgery University of Michigan School 
      of Medicine Ann Arbor Michigan USA.
LA  - eng
PT  - Journal Article
DEP - 20231129
PL  - United States
TA  - OTO Open
JT  - OTO open
JID - 101717942
PMC - PMC10687376
OTO - NOTNLM
OT  - BoardVitals
OT  - ChatGPT
OT  - artificial intelligence
OT  - in‐service exams
OT  - large language models
OT  - otolaryngology residency training
COIS- None.
EDAT- 2023/11/30 18:45
MHDA- 2023/11/30 18:46
PMCR- 2023/11/29
CRDT- 2023/11/30 17:28
PHST- 2023/08/09 00:00 [received]
PHST- 2023/09/19 00:00 [revised]
PHST- 2023/11/04 00:00 [accepted]
PHST- 2023/11/30 18:46 [medline]
PHST- 2023/11/30 18:45 [pubmed]
PHST- 2023/11/30 17:28 [entrez]
PHST- 2023/11/29 00:00 [pmc-release]
AID - OTO298 [pii]
AID - 10.1002/oto2.98 [doi]
PST - epublish
SO  - OTO Open. 2023 Nov 29;7(4):e98. doi: 10.1002/oto2.98. eCollection 2023 Oct-Dec.

PMID- 37301435
OWN - NLM
STAT- MEDLINE
DCOM- 20230911
LR  - 20231102
IS  - 2213-2201 (Electronic)
VI  - 11
IP  - 9
DP  - 2023 Sep
TI  - Artificial Intelligence Chatbots in Allergy and Immunology Practice: Where Have 
      We Been and Where Are We Going?
PG  - 2697-2700
LID - S2213-2198(23)00641-4 [pii]
LID - 10.1016/j.jaip.2023.05.042 [doi]
AB  - Artificial intelligence (AI) is rapidly becoming a valuable tool in healthcare, 
      providing clinicians with a new AI lens perspective for patient care, diagnosis, 
      and treatment. This article explores the potential applications, benefits, and 
      challenges of AI chatbots in clinical settings, with a particular emphasis on 
      ChatGPT 4.0 (OpenAI - Chat generative pretrained transformer 4.0), especially in 
      the field of allergy and immunology. AI chatbots have shown considerable promise 
      in various medical domains, including radiology and dermatology, by improving 
      patient engagement, diagnostic accuracy, and personalized treatment plans. 
      ChatGPT 4.0, developed by OpenAI, is good at understanding and replying to 
      prompts in a way that makes sense. However, it is critical to address the 
      potential biases, data privacy issues, ethical considerations, and the need for 
      verification of AI-generated findings. When used responsibly, AI chatbots can 
      significantly enhance clinical practice in allergy and immunology. However, there 
      are still challenges in using this technology that require ongoing research and 
      collaboration between AI developers and medical specialists. To this end, the 
      ChatGPT 4.0 platform has the potential to enhance patient engagement, improve 
      diagnostic accuracy, and provide personalized treatment plans in allergy and 
      immunology practice. However, limitations and risks must be addressed to ensure 
      their safe and effective use in clinical practice.
CI  - Copyright © 2023 The Authors. Published by Elsevier Inc. All rights reserved.
FAU - Goktas, Polat
AU  - Goktas P
AD  - UCD School of Computer Science, University College Dublin, Belfield, Dublin, 
      Ireland; CeADAR: Ireland's Centre for Applied Artificial Intelligence, 
      Clonskeagh, Dublin, Ireland. Electronic address: polat.goktas@ucd.ie.
FAU - Karakaya, Gul
AU  - Karakaya G
AD  - School of Medicine, Department of Chest Diseases, Division of Allergy and 
      Clinical Immunology, Hacettepe University, Ankara, Turkey.
FAU - Kalyoncu, Ali Fuat
AU  - Kalyoncu AF
AD  - School of Medicine, Department of Chest Diseases, Division of Allergy and 
      Clinical Immunology, Hacettepe University, Ankara, Turkey.
FAU - Damadoglu, Ebru
AU  - Damadoglu E
AD  - School of Medicine, Department of Chest Diseases, Division of Allergy and 
      Clinical Immunology, Hacettepe University, Ankara, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20230608
PL  - United States
TA  - J Allergy Clin Immunol Pract
JT  - The journal of allergy and clinical immunology. In practice
JID - 101597220
SB  - IM
CIN - J Allergy Clin Immunol Pract. 2023 Oct;11(10):3285-3286. PMID: 37805232
CIN - J Allergy Clin Immunol Pract. 2023 Oct;11(10):3286-3287. PMID: 37805233
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Hypersensitivity/diagnosis/therapy
MH  - Patient Participation
MH  - Technology
OTO - NOTNLM
OT  - AI chatbots
OT  - Allergy
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Ethical considerations
OT  - Generative pretrained transformer
OT  - Healthcare
OT  - Natural language processing
EDAT- 2023/06/11 01:06
MHDA- 2023/09/11 06:43
CRDT- 2023/06/10 19:28
PHST- 2023/04/30 00:00 [received]
PHST- 2023/05/22 00:00 [revised]
PHST- 2023/05/25 00:00 [accepted]
PHST- 2023/09/11 06:43 [medline]
PHST- 2023/06/11 01:06 [pubmed]
PHST- 2023/06/10 19:28 [entrez]
AID - S2213-2198(23)00641-4 [pii]
AID - 10.1016/j.jaip.2023.05.042 [doi]
PST - ppublish
SO  - J Allergy Clin Immunol Pract. 2023 Sep;11(9):2697-2700. doi: 
      10.1016/j.jaip.2023.05.042. Epub 2023 Jun 8.

PMID- 38227351
OWN - NLM
STAT- MEDLINE
DCOM- 20240117
LR  - 20240202
IS  - 2369-3762 (Electronic)
IS  - 2369-3762 (Linking)
VI  - 10
DP  - 2024 Jan 16
TI  - A Novel Evaluation Model for Assessing ChatGPT on Otolaryngology-Head and Neck 
      Surgery Certification Examinations: Performance Study.
PG  - e49970
LID - 10.2196/49970 [doi]
LID - e49970
AB  - BACKGROUND: ChatGPT is among the most popular large language models (LLMs), 
      exhibiting proficiency in various standardized tests, including multiple-choice 
      medical board examinations. However, its performance on otolaryngology-head and 
      neck surgery (OHNS) certification examinations and open-ended medical board 
      certification examinations has not been reported. OBJECTIVE: We aimed to evaluate 
      the performance of ChatGPT on OHNS board examinations and propose a novel method 
      to assess an AI model's performance on open-ended medical board examination 
      questions. METHODS: Twenty-one open-ended questions were adopted from the Royal 
      College of Physicians and Surgeons of Canada's sample examination to query 
      ChatGPT on April 11, 2023, with and without prompts. A new model, named 
      Concordance, Validity, Safety, Competency (CVSC), was developed to evaluate its 
      performance. RESULTS: In an open-ended question assessment, ChatGPT achieved a 
      passing mark (an average of 75% across 3 trials) in the attempts and demonstrated 
      higher accuracy with prompts. The model demonstrated high concordance (92.06%) 
      and satisfactory validity. While demonstrating considerable consistency in 
      regenerating answers, it often provided only partially correct responses. 
      Notably, concerning features such as hallucinations and self-conflicting answers 
      were observed. CONCLUSIONS: ChatGPT achieved a passing score in the sample 
      examination and demonstrated the potential to pass the OHNS certification 
      examination of the Royal College of Physicians and Surgeons of Canada. Some 
      concerns remain due to its hallucinations, which could pose risks to patient 
      safety. Further adjustments are necessary to yield safer and more accurate 
      answers for clinical implementation.
CI  - ©Cai Long, Kayle Lowe, Jessica Zhang, André dos Santos, Alaa Alanazi, Daniel 
      O'Brien, Erin D Wright, David Cote. Originally published in JMIR Medical 
      Education (https://mededu.jmir.org), 16.01.2024.
FAU - Long, Cai
AU  - Long C
AUID- ORCID: 0000-0002-5311-7355
AD  - Division of Otolaryngology-Head and Neck Surgery, University of Alberta, 
      Edmonton, AB, Canada.
FAU - Lowe, Kayle
AU  - Lowe K
AUID- ORCID: 0009-0006-7940-333X
AD  - Faculty of Medicine, University of Alberta, Edmonton, AB, Canada.
FAU - Zhang, Jessica
AU  - Zhang J
AUID- ORCID: 0000-0002-1578-8529
AD  - Faculty of Medicine, University of Alberta, Edmonton, AB, Canada.
FAU - Santos, André Dos
AU  - Santos AD
AUID- ORCID: 0009-0000-0393-3082
AD  - Alberta Machine Intelligence Institute, Edmonton, AB, Canada.
FAU - Alanazi, Alaa
AU  - Alanazi A
AUID- ORCID: 0000-0001-8096-9118
AD  - Division of Otolaryngology-Head and Neck Surgery, University of Alberta, 
      Edmonton, AB, Canada.
FAU - O'Brien, Daniel
AU  - O'Brien D
AUID- ORCID: 0000-0002-8394-9902
AD  - Department of Surgery, Creighton University, Omaha, NE, United States.
FAU - Wright, Erin D
AU  - Wright ED
AUID- ORCID: 0000-0001-5601-2754
AD  - Division of Otolaryngology-Head and Neck Surgery, University of Alberta, 
      Edmonton, AB, Canada.
FAU - Cote, David
AU  - Cote D
AUID- ORCID: 0000-0001-8971-6969
AD  - Division of Otolaryngology-Head and Neck Surgery, University of Alberta, 
      Edmonton, AB, Canada.
LA  - eng
PT  - Journal Article
DEP - 20240116
PL  - Canada
TA  - JMIR Med Educ
JT  - JMIR medical education
JID - 101684518
SB  - IM
MH  - Humans
MH  - Canada
MH  - Certification
MH  - Hallucinations
MH  - *Otolaryngology
MH  - *Surgeons
PMC - PMC10828939
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - ENT
OT  - LLM
OT  - LLMs
OT  - NLP
OT  - OHNS
OT  - answer
OT  - answers
OT  - artificial intelligence
OT  - chatbot
OT  - chatbots
OT  - clinical implementation
OT  - ear
OT  - exam
OT  - examination
OT  - examinations
OT  - exams
OT  - language model
OT  - large language models
OT  - laryngology
OT  - machine learning
OT  - medical education
OT  - medical examination
OT  - medical licensing
OT  - natural language processing
OT  - nose
OT  - otolaryngology
OT  - otolaryngology/head and neck surgery
OT  - otology
OT  - patient safety
OT  - response
OT  - responses
OT  - safety
OT  - surgery
OT  - surgical
OT  - throat
OT  - wide range information
COIS- Conflicts of Interest: None declared.
EDAT- 2024/01/16 13:41
MHDA- 2024/01/17 06:43
PMCR- 2024/01/16
CRDT- 2024/01/16 11:52
PHST- 2023/06/16 00:00 [received]
PHST- 2023/11/07 00:00 [accepted]
PHST- 2023/11/04 00:00 [revised]
PHST- 2024/01/17 06:43 [medline]
PHST- 2024/01/16 13:41 [pubmed]
PHST- 2024/01/16 11:52 [entrez]
PHST- 2024/01/16 00:00 [pmc-release]
AID - v10i1e49970 [pii]
AID - 10.2196/49970 [doi]
PST - epublish
SO  - JMIR Med Educ. 2024 Jan 16;10:e49970. doi: 10.2196/49970.

PMID- 38081504
OWN - NLM
STAT- MEDLINE
DCOM- 20240116
LR  - 20240116
IS  - 1873-1570 (Electronic)
IS  - 0300-9572 (Linking)
VI  - 194
DP  - 2024 Jan
TI  - Testing ChatGPT ability to answer laypeople questions about cardiac arrest and 
      cardiopulmonary resuscitation.
PG  - 110077
LID - S0300-9572(23)00813-4 [pii]
LID - 10.1016/j.resuscitation.2023.110077 [doi]
AB  - INTRODUCTION: Cardiac arrest leaves witnesses, survivors, and their relatives 
      with a multitude of questions. When a young or a public figure is affected, 
      interest around cardiac arrest and cardiopulmonary resuscitation (CPR) increases. 
      ChatGPT allows everyone to obtain human-like responses on any topic. Due to the 
      risks of accessing incorrect information, we assessed ChatGPT accuracy in 
      answering laypeople questions about cardiac arrest and CPR. METHODS: We 
      co-produced a list of 40 questions with members of Sudden Cardiac Arrest UK 
      covering all aspects of cardiac arrest and CPR. Answers provided by ChatGPT to 
      each question were evaluated by professionals for their accuracy, by 
      professionals and laypeople for their relevance, clarity, comprehensiveness, and 
      overall value on a scale from 1 (poor) to 5 (excellent), and for readability. 
      RESULTS: ChatGPT answers received an overall positive evaluation (4.3&nbsp;±&nbsp;0.7) by 
      14 professionals and 16 laypeople. Also, clarity (4.4&nbsp;±&nbsp;0.6), relevance 
      (4.3&nbsp;±&nbsp;0.6), accuracy (4.0&nbsp;±&nbsp;0.6), and comprehensiveness (4.2&nbsp;±&nbsp;0.7) of answers 
      was rated high. Professionals, however, rated overall value (4.0&nbsp;±&nbsp;0.5 vs 
      4.6&nbsp;±&nbsp;0.7; p&nbsp;=&nbsp;0.02) and comprehensiveness (3.9&nbsp;±&nbsp;0.6 vs 4.5&nbsp;±&nbsp;0.7; p&nbsp;=&nbsp;0.02) 
      lower compared to laypeople. CPR-related answers consistently received a lower 
      score across all parameters by professionals and laypeople. Readability was 
      'difficult' (median Flesch reading ease score of 34 [IQR 26-42]). CONCLUSIONS: 
      ChatGPT provided largely accurate, relevant, and comprehensive answers to 
      questions about cardiac arrest commonly asked by survivors, their relatives, and 
      lay rescuers, except CPR-related answers that received the lowest scores. Large 
      language model will play a significant role in the future and healthcare-related 
      content generated should be monitored.
CI  - Copyright © 2023 Elsevier B.V. All rights reserved.
FAU - Scquizzato, Tommaso
AU  - Scquizzato T
AD  - Department of Anesthesia and Intensive Care, IRCCS San Raffaele Scientific 
      Institute, Milan, Italy.
FAU - Semeraro, Federico
AU  - Semeraro F
AD  - Department of Anaesthesia, Intensive Care and Emergency Medical Services, 
      Ospedale Maggiore, Bologna, Italy.
FAU - Swindell, Paul
AU  - Swindell P
AD  - Sudden Cardiac Arrest UK, United Kingdom.
FAU - Simpson, Rupert
AU  - Simpson R
AD  - Essex Cardiothoracic Centre, Mid and South Essex NHS Foundation Trust, Basildon, 
      United Kingdom; Medical Technology Research Centre, Anglia Ruskin School of 
      Medicine, Chelmsford, United Kingdom.
FAU - Angelini, Matteo
AU  - Angelini M
AD  - Department of Anesthesia and Intensive Care, IRCCS San Raffaele Scientific 
      Institute, Milan, Italy.
FAU - Gazzato, Arianna
AU  - Gazzato A
AD  - Department of Anesthesia and Intensive Care, IRCCS San Raffaele Scientific 
      Institute, Milan, Italy.
FAU - Sajjad, Uzma
AU  - Sajjad U
AD  - Essex Cardiothoracic Centre, Mid and South Essex NHS Foundation Trust, Basildon, 
      United Kingdom; Medical Technology Research Centre, Anglia Ruskin School of 
      Medicine, Chelmsford, United Kingdom.
FAU - Bignami, Elena G
AU  - Bignami EG
AD  - Anesthesiology, Critical Care and Pain Medicine Division, Department of Medicine 
      and Surgery, University of Parma, Parma, Italy.
FAU - Landoni, Giovanni
AU  - Landoni G
AD  - Department of Anesthesia and Intensive Care, IRCCS San Raffaele Scientific 
      Institute, Milan, Italy; School of Medicine, Vita-Salute San Raffaele University, 
      Milan, Italy.
FAU - Keeble, Thomas R
AU  - Keeble TR
AD  - Essex Cardiothoracic Centre, Mid and South Essex NHS Foundation Trust, Basildon, 
      United Kingdom; Medical Technology Research Centre, Anglia Ruskin School of 
      Medicine, Chelmsford, United Kingdom.
FAU - Mion, Marco
AU  - Mion M
AD  - Essex Cardiothoracic Centre, Mid and South Essex NHS Foundation Trust, Basildon, 
      United Kingdom; Medical Technology Research Centre, Anglia Ruskin School of 
      Medicine, Chelmsford, United Kingdom. Electronic address: m.mion@nhs.net.
LA  - eng
PT  - Journal Article
DEP - 20231209
PL  - Ireland
TA  - Resuscitation
JT  - Resuscitation
JID - 0332173
SB  - IM
MH  - Humans
MH  - *Cardiopulmonary Resuscitation
MH  - *Out-of-Hospital Cardiac Arrest
MH  - Death, Sudden, Cardiac
MH  - Health Facilities
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Cardiopulmonary resuscitation
OT  - ChatGPT
OT  - Large language model
OT  - Out-of-hospital cardiac arrest
COIS- Declaration of competing interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2023/12/12 00:42
MHDA- 2024/01/16 06:42
CRDT- 2023/12/11 19:26
PHST- 2023/10/28 00:00 [received]
PHST- 2023/11/15 00:00 [accepted]
PHST- 2024/01/16 06:42 [medline]
PHST- 2023/12/12 00:42 [pubmed]
PHST- 2023/12/11 19:26 [entrez]
AID - S0300-9572(23)00813-4 [pii]
AID - 10.1016/j.resuscitation.2023.110077 [doi]
PST - ppublish
SO  - Resuscitation. 2024 Jan;194:110077. doi: 10.1016/j.resuscitation.2023.110077. 
      Epub 2023 Dec 9.

PMID- 38181885
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20240215
LR  - 20240215
IS  - 1097-6787 (Electronic)
IS  - 0190-9622 (Linking)
VI  - 90
IP  - 3
DP  - 2024 Mar
TI  - This month in JAAD International: March 2024: ChatGPT and large language models 
      in the clinic.
PG  - 482
LID - S0190-9622(24)00001-X [pii]
LID - 10.1016/j.jaad.2024.01.001 [doi]
FAU - Kantor, Jonathan
AU  - Kantor J
AD  - Department of Dermatology, Center for Global Health, and Center for Clinical 
      Epidemiology and Biostatistics, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania; Florida Center for Dermatology, St 
      Augustine, Florida. Electronic address: jonkantor@gmail.com.
LA  - eng
PT  - Editorial
DEP - 20240104
PL  - United States
TA  - J Am Acad Dermatol
JT  - Journal of the American Academy of Dermatology
JID - 7907132
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - GPT-4
OT  - Google Bard
OT  - Microsoft Bing
OT  - QDA
OT  - actor analysis
OT  - after-visit summary
OT  - annotation
OT  - artificial intelligence
OT  - focus group
OT  - large language models
OT  - medical education
OT  - natural language processing
OT  - practice management
OT  - prompt engineering
OT  - qualitative data analysis software
OT  - qualitative research
COIS- Conflicts of interest None disclosed.
EDAT- 2024/01/06 10:42
MHDA- 2024/02/12 05:43
CRDT- 2024/01/05 19:19
PHST- 2024/01/02 00:00 [received]
PHST- 2024/01/02 00:00 [accepted]
PHST- 2024/02/12 05:43 [medline]
PHST- 2024/01/06 10:42 [pubmed]
PHST- 2024/01/05 19:19 [entrez]
AID - S0190-9622(24)00001-X [pii]
AID - 10.1016/j.jaad.2024.01.001 [doi]
PST - ppublish
SO  - J Am Acad Dermatol. 2024 Mar;90(3):482. doi: 10.1016/j.jaad.2024.01.001. Epub 
      2024 Jan 4.

PMID- 37380584
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230731
LR  - 20230731
IS  - 1532-8171 (Electronic)
IS  - 0735-6757 (Linking)
VI  - 70
DP  - 2023 Aug
TI  - ChatGPT and conversational artificial intelligence: Ethics in the eye of the 
      beholder.
PG  - 191
LID - S0735-6757(23)00317-0 [pii]
LID - 10.1016/j.ajem.2023.06.023 [doi]
FAU - Gottlieb, Michael
AU  - Gottlieb M
AD  - Department of Emergency Medicine, Rush University Medical Center, Chicago, IL, 
      United States of America. Electronic address: MichaelGottliebMD@gmail.com.
FAU - Kline, Jeffrey A
AU  - Kline JA
AD  - Department of Emergency Medicine, Wayne State University School of Medicine, 
      Detroit, MI, United States of America. Electronic address: jkline@wayne.edu.
FAU - Schneider, Alexander J
AU  - Schneider AJ
AD  - NantGames, Inc, Los Angeles, CA, United States of America. Electronic address: 
      publications@schneids.me.
FAU - Coates, Wendy C
AU  - Coates WC
AD  - Department of Emergency Medicine, University of California, Los Angeles, David 
      Geffen School of Medicine, Los Angeles, CA, United States of America. Electronic 
      address: balletmd@gmail.com.
LA  - eng
PT  - Comment
PT  - Letter
DEP - 20230614
PL  - United States
TA  - Am J Emerg Med
JT  - The American journal of emergency medicine
JID - 8309942
SB  - IM
CON - Am J Emerg Med. 2023 Aug;70:190. PMID: 37349234
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Ethics
OT  - Publication
OT  - Research
COIS- Declaration of Competing Interest We have no conflicts of interest to declare and 
      this manuscript has not been submitted elsewhere.
EDAT- 2023/06/29 01:08
MHDA- 2023/07/25 06:43
CRDT- 2023/06/28 21:58
PHST- 2023/06/05 00:00 [received]
PHST- 2023/06/13 00:00 [accepted]
PHST- 2023/07/25 06:43 [medline]
PHST- 2023/06/29 01:08 [pubmed]
PHST- 2023/06/28 21:58 [entrez]
AID - S0735-6757(23)00317-0 [pii]
AID - 10.1016/j.ajem.2023.06.023 [doi]
PST - ppublish
SO  - Am J Emerg Med. 2023 Aug;70:191. doi: 10.1016/j.ajem.2023.06.023. Epub 2023 Jun 
      14.

PMID- 38180493
OWN - NLM
STAT- Publisher
LR  - 20240105
IS  - 1439-099X (Electronic)
IS  - 0179-7158 (Linking)
DP  - 2024 Jan 5
TI  - The role of artificial intelligence in informed patient consent for radiotherapy 
      treatments-a&nbsp;case report.
LID - 10.1007/s00066-023-02190-7 [doi]
AB  - Recent advancements in large language models (LMM; e.g., ChatGPT (OpenAI, San 
      Francisco, California, USA)) have seen widespread use in various fields, 
      including healthcare. This case study reports on the first use of LMM in 
      a&nbsp;pretreatment discussion and in obtaining informed consent for a&nbsp;radiation 
      oncology treatment. Further, the reproducibility of the replies by ChatGPT&nbsp;3.5 
      was analyzed. A&nbsp;breast cancer patient, following legal consultation, engaged in 
      a&nbsp;conversation with ChatGPT&nbsp;3.5 regarding her radiotherapy treatment. The patient 
      posed questions about side effects, prevention, activities, medications, and late 
      effects. While some answers contained inaccuracies, responses closely resembled 
      doctors' replies. In a&nbsp;final evaluation discussion, the patient, however, stated 
      that she preferred the presence of a&nbsp;physician and expressed concerns about the 
      source of the provided information. The reproducibility was tested in ten 
      iterations. Future guidelines for using such models in radiation oncology should 
      be driven by medical professionals. While artificial intelligence (AI) supports 
      essential tasks, human interaction remains crucial.
CI  - © 2024. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany.
FAU - Moll, M
AU  - Moll M
AUID- ORCID: 0000-0001-5523-8866
AD  - Department of Radiation Oncology, Comprehensive Cancer Center Vienna, Medical 
      University Vienna, Vienna, Austria. matthias.moll@meduniwien.ac.at.
FAU - Heilemann, G
AU  - Heilemann G
AUID- ORCID: 0000-0002-7461-3956
AD  - Department of Radiation Oncology, Comprehensive Cancer Center Vienna, Medical 
      University Vienna, Vienna, Austria.
FAU - Georg, Dietmar
AU  - Georg D
AUID- ORCID: 0000-0002-8327-3877
AD  - Department of Radiation Oncology, Comprehensive Cancer Center Vienna, Medical 
      University Vienna, Vienna, Austria.
FAU - Kauer-Dorner, D
AU  - Kauer-Dorner D
AD  - Department of Radiation Oncology, Comprehensive Cancer Center Vienna, Medical 
      University Vienna, Vienna, Austria.
FAU - Kuess, P
AU  - Kuess P
AUID- ORCID: 0000-0003-2961-1692
AD  - Department of Radiation Oncology, Comprehensive Cancer Center Vienna, Medical 
      University Vienna, Vienna, Austria.
LA  - eng
PT  - Journal Article
DEP - 20240105
PL  - Germany
TA  - Strahlenther Onkol
JT  - Strahlentherapie und Onkologie : Organ der Deutschen Rontgengesellschaft ... [et 
      al]
JID - 8603469
SB  - IM
OTO - NOTNLM
OT  - AI
OT  - Artificial intelligence
OT  - Breast cancer
OT  - ChatGPT
OT  - OpenAI
EDAT- 2024/01/05 12:42
MHDA- 2024/01/05 12:42
CRDT- 2024/01/05 11:04
PHST- 2023/09/21 00:00 [received]
PHST- 2023/12/03 00:00 [accepted]
PHST- 2024/01/05 12:42 [medline]
PHST- 2024/01/05 12:42 [pubmed]
PHST- 2024/01/05 11:04 [entrez]
AID - 10.1007/s00066-023-02190-7 [pii]
AID - 10.1007/s00066-023-02190-7 [doi]
PST - aheadofprint
SO  - Strahlenther Onkol. 2024 Jan 5. doi: 10.1007/s00066-023-02190-7.

PMID- 37428540
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230727
IS  - 2369-3762 (Print)
IS  - 2369-3762 (Electronic)
IS  - 2369-3762 (Linking)
VI  - 9
DP  - 2023 Jul 10
TI  - Putting ChatGPT's Medical Advice to the (Turing) Test: Survey Study.
PG  - e46939
LID - 10.2196/46939 [doi]
LID - e46939
AB  - BACKGROUND: Chatbots are being piloted to draft responses to patient questions, 
      but patients' ability to distinguish between provider and chatbot responses and 
      patients' trust in chatbots' functions are not well established. OBJECTIVE: This 
      study aimed to assess the feasibility of using ChatGPT (Chat Generative 
      Pre-trained Transformer) or a similar artificial intelligence-based chatbot for 
      patient-provider communication. METHODS: A survey study was conducted in January 
      2023. Ten representative, nonadministrative patient-provider interactions were 
      extracted from the electronic health record. Patients' questions were entered 
      into ChatGPT with a request for the chatbot to respond using approximately the 
      same word count as the human provider's response. In the survey, each patient 
      question was followed by a provider- or ChatGPT-generated response. Participants 
      were informed that 5 responses were provider generated and 5 were chatbot 
      generated. Participants were asked-and incentivized financially-to correctly 
      identify the response source. Participants were also asked about their trust in 
      chatbots' functions in patient-provider communication, using a Likert scale from 
      1-5. RESULTS: A US-representative sample of 430 study participants aged 18 and 
      older were recruited on Prolific, a crowdsourcing platform for academic studies. 
      In all, 426 participants filled out the full survey. After removing participants 
      who spent less than 3 minutes on the survey, 392 respondents remained. Overall, 
      53.3% (209/392) of respondents analyzed were women, and the average age was 47.1 
      (range 18-91) years. The correct classification of responses ranged between 49% 
      (192/392) to 85.7% (336/392) for different questions. On average, chatbot 
      responses were identified correctly in 65.5% (1284/1960) of the cases, and human 
      provider responses were identified correctly in 65.1% (1276/1960) of the cases. 
      On average, responses toward patients' trust in chatbots' functions were weakly 
      positive (mean Likert score 3.4 out of 5), with lower trust as the health-related 
      complexity of the task in the questions increased. CONCLUSIONS: ChatGPT responses 
      to patient questions were weakly distinguishable from provider responses. 
      Laypeople appear to trust the use of chatbots to answer lower-risk health 
      questions. It is important to continue studying patient-chatbot interaction as 
      chatbots move from administrative to more clinical roles in health care.
CI  - ©Oded Nov, Nina Singh, Devin Mann. Originally published in JMIR Medical Education 
      (https://mededu.jmir.org), 10.07.2023.
FAU - Nov, Oded
AU  - Nov O
AUID- ORCID: 0000-0001-6410-2995
AD  - Department of Technology Management, Tandon School of Engineering, New York 
      University, New York, NY, United States.
FAU - Singh, Nina
AU  - Singh N
AUID- ORCID: 0000-0002-4623-2451
AD  - Department of Population Health, Grossman School of Medicine, New York 
      University, New York, NY, United States.
FAU - Mann, Devin
AU  - Mann D
AUID- ORCID: 0000-0002-2099-0852
AD  - Department of Population Health, Grossman School of Medicine, New York 
      University, New York, NY, United States.
AD  - Medical Center Information Technology, Langone Health, New York University, New 
      York, NY, United States.
LA  - eng
PT  - Journal Article
DEP - 20230710
PL  - Canada
TA  - JMIR Med Educ
JT  - JMIR medical education
JID - 101684518
PMC - PMC10366957
OTO - NOTNLM
OT  - AI
OT  - ChatGPT, Chat Generative Pre-trained Transformer
OT  - artificial intelligence
OT  - chatbot
OT  - ethics
OT  - feasibility
OT  - language model
OT  - large language model
OT  - machine learning
OT  - patient-provider interaction
OT  - privacy
COIS- Conflicts of Interest: None declared.
EDAT- 2023/07/10 13:06
MHDA- 2023/07/10 13:07
PMCR- 2023/07/10
CRDT- 2023/07/10 11:54
PHST- 2023/03/02 00:00 [received]
PHST- 2023/06/14 00:00 [accepted]
PHST- 2023/05/26 00:00 [revised]
PHST- 2023/07/10 13:07 [medline]
PHST- 2023/07/10 13:06 [pubmed]
PHST- 2023/07/10 11:54 [entrez]
PHST- 2023/07/10 00:00 [pmc-release]
AID - v9i1e46939 [pii]
AID - 10.2196/46939 [doi]
PST - epublish
SO  - JMIR Med Educ. 2023 Jul 10;9:e46939. doi: 10.2196/46939.

PMID- 38112589
OWN - NLM
STAT- MEDLINE
DCOM- 20240318
LR  - 20240318
IS  - 1537-7385 (Electronic)
IS  - 0894-9115 (Linking)
VI  - 103
IP  - 4
DP  - 2024 Apr 1
TI  - (How) ChatGPT-Artificial Intelligence Thinks It Can Help/Harm Physiatry.
PG  - 346-349
LID - 10.1097/PHM.0000000000002370 [doi]
AB  - ChatGPT is a chatbot that is based on the generative pretrained transformer 
      architecture as an artificial inteligence-based large language model. Its 
      widespread use in healthcare practice, research, and education seems to be 
      (increasingly) inevitable. Also considering the relevant limitations regarding 
      privacy, ethics, bias, legal, and validity, in this article, its use as a 
      supplement (for sure not as a substitute for physicians) is discussed in light of 
      the recent literature. Particularly, the "opinion" of ChatGPT about how it can 
      help/harm physiatry is exemplified.
CI  - Copyright © 2023 Wolters Kluwer Health, Inc. All rights reserved.
FAU - Jačisko, Jakub
AU  - Jačisko J
AD  - From the Department of Rehabilitation and Sports Medicine, Second Faculty of 
      Medicine, Charles University and University Hospital Motol, Prague, Czech 
      Republic (JJ, VV); Department of Physical Medicine and Rehabilitation, National 
      Taiwan University Hospital, Bei-Hu Branch, Taipei, Taiwan (K-VC); and Department 
      of Physical and Rehabilitation Medicine, Hacettepe University Medical School, 
      Ankara, Turkey (LO).
FAU - Veselý, Viktor
AU  - Veselý V
FAU - Chang, Ke-Vin
AU  - Chang KV
FAU - Özçakar, Levent
AU  - Özçakar L
LA  - eng
PT  - Journal Article
DEP - 20231017
PL  - United States
TA  - Am J Phys Med Rehabil
JT  - American journal of physical medicine &amp; rehabilitation
JID - 8803677
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Dietary Supplements
MH  - Educational Status
MH  - Language
MH  - *Physical and Rehabilitation Medicine
COIS- Financial disclosure statements have been obtained, and no conflicts of interest 
      have been reported by the authors or by any individuals in control of the content 
      of this article.
EDAT- 2023/12/19 19:52
MHDA- 2024/03/18 06:44
CRDT- 2023/12/19 11:05
PHST- 2024/03/18 06:44 [medline]
PHST- 2023/12/19 19:52 [pubmed]
PHST- 2023/12/19 11:05 [entrez]
AID - 00002060-990000000-00338 [pii]
AID - 10.1097/PHM.0000000000002370 [doi]
PST - ppublish
SO  - Am J Phys Med Rehabil. 2024 Apr 1;103(4):346-349. doi: 
      10.1097/PHM.0000000000002370. Epub 2023 Oct 17.

PMID- 37401180
OWN - NLM
STAT- MEDLINE
DCOM- 20230705
LR  - 20230908
IS  - 0975-5691 (Electronic)
IS  - 0974-8466 (Linking)
VI  - VIII
IP  - 2
DP  - 2023 Apr-Jun
TI  - ChatGPT: Impact of an artificial author on bibliometrics.
PG  - 93-94
LID - 10.20529/IJME.2023.029 [doi]
AB  - The use of Artificial Intelligence (AI) in medical science has been widely 
      discussed and debated. Topol foresaw that AI, particularly deep learning, would 
      be used in a variety of applications, with users ranging from specialty doctors 
      to paramedics [1]. He discussed how deep neural networks (DNNs) of AI can help 
      interpret medical scans, pathology slides, skin lesions, retinal images, 
      electrocardiograms, endoscopy, faces, and vital signs. He has described its 
      application in radiology, pathology, dermatology, ophthalmology, cardiology, 
      mental health, and other fields [1]. Among several other AI applications used in 
      our daily life, the next generation breakthrough, AI model ChatGPT-3 
      (https://chat.openai.com/) was launched on November 30, 2022 by OpenAI, 
      California, which is well-known for its innovations in automated text generation. 
      ChatGPT converses with the user, ascertains the user's needs, and responds 
      accordingly. It can write a poem, a diet plan, recipes, letters, computer 
      programmes, a eulogy, do copy editing, and so on.
FAU - Nagarkar, Shubhada
AU  - Nagarkar S
AD  - Department of Library and Information Science and Centre for Publication Ethics, 
      Savitribai Phule Pune University, Pune, INDIA.
LA  - eng
PT  - Comment
PT  - Editorial
PL  - India
TA  - Indian J Med Ethics
JT  - Indian journal of medical ethics
JID - 101214913
SB  - IM
CON - Nat Med. 2019 Jan;25(1):44-56. PMID: 30617339
MH  - Male
MH  - Humans
MH  - *Artificial Intelligence
MH  - Bibliometrics
MH  - *Medicine
MH  - Mental Health
MH  - Paramedics
EDAT- 2023/07/04 06:42
MHDA- 2023/07/05 06:42
CRDT- 2023/07/04 02:33
PHST- 2023/07/05 06:42 [medline]
PHST- 2023/07/04 06:42 [pubmed]
PHST- 2023/07/04 02:33 [entrez]
AID - 10.20529/IJME.2023.029 [doi]
PST - ppublish
SO  - Indian J Med Ethics. 2023 Apr-Jun;VIII(2):93-94. doi: 10.20529/IJME.2023.029.

PMID- 37104260
OWN - NLM
STAT- MEDLINE
DCOM- 20230501
LR  - 20230501
IS  - 1540-9597 (Electronic)
IS  - 0276-3869 (Linking)
VI  - 42
IP  - 2
DP  - 2023 Apr-Jun
TI  - ChatGPT, an Opportunity to Understand More About Language Models.
PG  - 194-201
LID - 10.1080/02763869.2023.2194149 [doi]
AB  - ChatGPT, a leading large language model, has achieved some success beyond 
      previous language models and caught the world's attention since its release in 
      late 2022. Businesses and healthcare professional fields have raised strong 
      interests in investing in large language models to assist various kinds of 
      information searching in their domain of expertise. Under the influence of 
      ChatGPT, searched information may be received in a new personalized chat format, 
      in contrast to the traditional search engines with pages of results for users to 
      evaluate and open. Large language models and generative AI present new 
      opportunities for librarians to understand more about language models' 
      development as well as the future directions of the language models that are 
      developed behind the user interfaces. Being aware of how language models impact 
      the communication of information will enrich librarians' abilities to examine the 
      quality of AI outputs and awareness of users' rights and data curation policies, 
      to better assist patrons' research activities that involve using language models 
      in the foreseeable future.
FAU - Zhang, Borui
AU  - Zhang B
AD  - Academic Research Consulting and Services Department, Smathers Libraries, 
      University of Florida, Gainesville, USA.
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Med Ref Serv Q
JT  - Medical reference services quarterly
JID - 8219208
MH  - Humans
MH  - *Search Engine
MH  - Communication
MH  - Language
MH  - *Librarians
EDAT- 2023/04/27 18:41
MHDA- 2023/05/01 06:42
CRDT- 2023/04/27 13:33
PHST- 2023/05/01 06:42 [medline]
PHST- 2023/04/27 18:41 [pubmed]
PHST- 2023/04/27 13:33 [entrez]
AID - 10.1080/02763869.2023.2194149 [doi]
PST - ppublish
SO  - Med Ref Serv Q. 2023 Apr-Jun;42(2):194-201. doi: 10.1080/02763869.2023.2194149.

PMID- 37082496
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230422
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - Neuroblastoma Masquerading as a Septic Hip Infection in a Three-Year-Old.
PG  - e36350
LID - 10.7759/cureus.36350 [doi]
LID - e36350
AB  - Metastatic neuroblastoma to the bone and septic joint shares the same incidence 
      in age and clinical symptomology.&nbsp;Here we discuss a three-year-old male who 
      presented with anemia, persistent hip pain, and a refusal to bear weight. A 
      thorough evaluation based on a broad differential diagnosis allowed for an 
      expedient diagnosis of metastatic neuroblastoma. The timely diagnosis allowed for 
      rapid enrolment in a children's oncology group (COG) clinical trial for advanced 
      neuroblastoma. The patient tolerated the therapy without adverse events and 
      remains in remission.
CI  - Copyright © 2023, Lynch et al.
FAU - Lynch, Joseph D
AU  - Lynch JD
AD  - Pediatric Hospital Medicine, West Virginia University School of Medicine, 
      Morgantown, USA.
FAU - Tomboc, Patrick J
AU  - Tomboc PJ
AD  - Pediatric Hematology-Oncology, West Virginia University School of Medicine, 
      Morgantown, USA.
LA  - eng
PT  - Case Reports
DEP - 20230319
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10111876
OTO - NOTNLM
OT  - chatgpt
OT  - chatgpt improved case report
OT  - inpatient pediatrics
OT  - neuroblastoma
OT  - neuroblastoma metastases
OT  - pediatric clinical conundrum
OT  - pediatric hematology-oncology
OT  - pediatric hospital medicine
OT  - pediatric septic joint
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/04/21 06:41
MHDA- 2023/04/21 06:42
PMCR- 2023/03/19
CRDT- 2023/04/21 02:26
PHST- 2023/03/18 00:00 [accepted]
PHST- 2023/04/21 06:42 [medline]
PHST- 2023/04/21 06:41 [pubmed]
PHST- 2023/04/21 02:26 [entrez]
PHST- 2023/03/19 00:00 [pmc-release]
AID - 10.7759/cureus.36350 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 19;15(3):e36350. doi: 10.7759/cureus.36350. eCollection 2023 
      Mar.

PMID- 38447265
OWN - NLM
STAT- Publisher
LR  - 20240306
IS  - 1872-8464 (Electronic)
IS  - 0165-5876 (Linking)
VI  - 179
DP  - 2024 Feb 29
TI  - The utility and accuracy of ChatGPT in providing post-operative instructions 
      following tonsillectomy: A pilot study.
PG  - 111901
LID - S0165-5876(24)00055-7 [pii]
LID - 10.1016/j.ijporl.2024.111901 [doi]
AB  - OBJECTIVE: To investigate the utility of answers generated by ChatGPT, a large 
      language model, to common questions parents have for their children following 
      tonsillectomy. METHODS: Twenty Otolaryngology residents anonymously submitted 
      common questions asked by parents of pediatric patients following tonsillectomy. 
      After identifying the 16 most common questions via consensus-based approach, we 
      asked ChatGPT to generate responses to these queries. Satisfaction with the 
      AI-generated answers was rated from 1 (Worst) to 5 (Best) by an expert panel of 3 
      pediatric Otolaryngologists. RESULTS: The distribution of questions across the 
      five most common domains, their mean satisfaction scores, and their Krippendorf's 
      interrater reliability coefficient were: Pain management [6, (3.67), (0.434)], 
      Complications [4, (3.58), (-0.267)], Diet [3, (4.33), (-0.357)], Physical 
      Activity [2, (4.33), (-0.318)], and Follow-up [1, (2.67), (-0.250)]. The panel 
      noted that answers for diet, bleeding complications, and return to school were 
      thorough. Pain management and follow-up recommendations were inaccurate, 
      including a recommendation to prescribe codeine to children despite a black-box 
      warning, and a suggested post-operative follow-up at 1 week, rather than the 
      customary 2-4 weeks for our panel. CONCLUSION: Although ChatGPT can provide 
      accurate answers for common patient questions following tonsillectomy, it 
      sometimes provides eloquently written inaccurate information. This may lead to 
      patients using AI-generated medical advice contrary to physician advice. The 
      inaccuracy in pain management answers likely reflects regional practice 
      variability. If trained appropriately, ChatGPT could be an excellent resource for 
      Otolaryngologists and patients to answer questions in the postoperative period. 
      Future research should investigate if Otolaryngologist-trained models can 
      increase the accuracy of responses.
CI  - Copyright © 2024 Elsevier B.V. All rights reserved.
FAU - Dhar, Sarit
AU  - Dhar S
AD  - Department of Otolaryngology Head &amp; Neck Surgery, University of Tennessee Health 
      Science Center, 910 Madison Ave, Memphis, TN, 38163, USA.
FAU - Kothari, Dhruv
AU  - Kothari D
AD  - Department of Otolaryngology Head &amp; Neck Surgery, University of Tennessee Health 
      Science Center, 910 Madison Ave, Memphis, TN, 38163, USA; Department of 
      Otolaryngology Head &amp; Neck Surgery, Cedars-Sinai Medical Center, 8700 Beverly 
      Blvd, Los Angeles, CA, 90048, USA.
FAU - Vasquez, Missael
AU  - Vasquez M
AD  - Department of Otolaryngology Head &amp; Neck Surgery, Cedars-Sinai Medical Center, 
      8700 Beverly Blvd, Los Angeles, CA, 90048, USA.
FAU - Clarke, Travis
AU  - Clarke T
AD  - Department of Otolaryngology Head &amp; Neck Surgery, University of Tennessee Health 
      Science Center, 910 Madison Ave, Memphis, TN, 38163, USA.
FAU - Maroda, Andrew
AU  - Maroda A
AD  - Department of Otolaryngology Head &amp; Neck Surgery, University of Tennessee Health 
      Science Center, 910 Madison Ave, Memphis, TN, 38163, USA.
FAU - McClain, Wade G
AU  - McClain WG
AD  - Department of Otolaryngology Head &amp; Neck Surgery, University of Tennessee Health 
      Science Center, 910 Madison Ave, Memphis, TN, 38163, USA.
FAU - Sheyn, Anthony
AU  - Sheyn A
AD  - Department of Otolaryngology Head &amp; Neck Surgery, University of Tennessee Health 
      Science Center, 910 Madison Ave, Memphis, TN, 38163, USA.
FAU - Tuliszewski, Robert M
AU  - Tuliszewski RM
AD  - Department of Otolaryngology Head &amp; Neck Surgery, University of Tennessee Health 
      Science Center, 910 Madison Ave, Memphis, TN, 38163, USA.
FAU - Tang, Dennis M
AU  - Tang DM
AD  - Department of Otolaryngology Head &amp; Neck Surgery, Cedars-Sinai Medical Center, 
      8700 Beverly Blvd, Los Angeles, CA, 90048, USA.
FAU - Rangarajan, Sanjeet V
AU  - Rangarajan SV
AD  - Department of Otolaryngology-Head and Neck Surgery, University Hospitals 
      Cleveland Medical Center, Case Western Reserve University School of Medicine, 
      11100 Euclid Ave, Cleveland, OH, 44106, USA. Electronic address: 
      sanjeet.rangarajan@uhhospitals.org.
LA  - eng
PT  - Journal Article
DEP - 20240229
PL  - Ireland
TA  - Int J Pediatr Otorhinolaryngol
JT  - International journal of pediatric otorhinolaryngology
JID - 8003603
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Machine learning
OT  - Otolaryngology
OT  - Post-operative instructions
OT  - Tonsillectomy
COIS- Declaration of competing interest No conflict.
EDAT- 2024/03/07 00:42
MHDA- 2024/03/07 00:42
CRDT- 2024/03/06 18:02
PHST- 2023/11/30 00:00 [received]
PHST- 2024/02/20 00:00 [revised]
PHST- 2024/02/24 00:00 [accepted]
PHST- 2024/03/07 00:42 [medline]
PHST- 2024/03/07 00:42 [pubmed]
PHST- 2024/03/06 18:02 [entrez]
AID - S0165-5876(24)00055-7 [pii]
AID - 10.1016/j.ijporl.2024.111901 [doi]
PST - aheadofprint
SO  - Int J Pediatr Otorhinolaryngol. 2024 Feb 29;179:111901. doi: 
      10.1016/j.ijporl.2024.111901.

PMID- 38440018
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240306
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 16
IP  - 2
DP  - 2024 Feb
TI  - Rathke's Cleft Cyst Leads to Stunted Growth in Children: A Case Report Written 
      With the Help of ChatGPT.
PG  - e53384
LID - 10.7759/cureus.53384 [doi]
LID - e53384
AB  - In recent times, ChatGPT has become a globally renowned AI tool, revolutionizing 
      academic research by offering innovative methods and opportunities. The 
      integration of AI into various domains is a prevailing topic, focusing on 
      optimizing its utility. This article presents a case study of a child with 
      Rathke's cyst, primarily exhibiting symptoms of growth and developmental delay. 
      The patient's self-perception of stunted growth, coupled with previous 
      assessments indicating partial growth hormone deficiency, prompted further 
      investigation. Laboratory assessments revealed low growth hormone and 
      insulin-like growth factor levels, while imaging disclosed a pituitary lesion. 
      Rathke's cyst was postulated as the probable cause of the growth hormone 
      deficiency. Rathke's cyst remains a rare medical condition with substantial 
      research knowledge gaps. In this article, we synergize ChatGPT responses with a 
      comprehensive case report of a child with Rathke's cyst as the primary 
      symptom-growth and developmental delay. We explore the methods and feasibility of 
      employing ChatGPT within this case report.
CI  - Copyright © 2024, Huang et al.
FAU - Huang, Huanxiang
AU  - Huang H
AD  - Department of Neurosurgery, Oriental Hospital Affiliated to Xiamen University, 
      Fuzhou, CHN.
FAU - Li, Jun
AU  - Li J
AD  - Department of Neurosurgery, Fuzhou 900th Hospital, Fuzong Clinical Medical 
      College of Fujian Medical University, Fuzhou, CHN.
FAU - Li, Ziqi
AU  - Li Z
AD  - Department of Neurosurgery, Oriental Hospital Affiliated to Xiamen University, 
      Fuzhou, CHN.
FAU - Xiao, Yong
AU  - Xiao Y
AD  - Department of Neurological Surgery, Central Institute for Mental Health, 
      University of Heidelberg, Heidelberg, DEU.
FAU - Wang, Shousen
AU  - Wang S
AD  - Department of Neurosurgery, Oriental Hospital Affiliated to Xiamen University, 
      Fuzhou, CHN.
LA  - eng
PT  - Case Reports
DEP - 20240201
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10911638
OTO - NOTNLM
OT  - chat gpt
OT  - children
OT  - growth and development
OT  - pituitary function
OT  - rathke’s cleft cyst
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/03/05 06:45
MHDA- 2024/03/05 06:46
PMCR- 2024/02/01
CRDT- 2024/03/05 03:47
PHST- 2024/01/31 00:00 [accepted]
PHST- 2024/03/05 06:46 [medline]
PHST- 2024/03/05 06:45 [pubmed]
PHST- 2024/03/05 03:47 [entrez]
PHST- 2024/02/01 00:00 [pmc-release]
AID - 10.7759/cureus.53384 [doi]
PST - epublish
SO  - Cureus. 2024 Feb 1;16(2):e53384. doi: 10.7759/cureus.53384. eCollection 2024 Feb.

PMID- 37747487
OWN - NLM
STAT- MEDLINE
DCOM- 20231130
LR  - 20231217
IS  - 1559-713X (Electronic)
IS  - 1559-2332 (Linking)
VI  - 18
IP  - 6
DP  - 2023 Dec 1
TI  - Artificial Intelligence and the Simulationists.
PG  - 395-399
LID - 10.1097/SIH.0000000000000747 [doi]
AB  - The recent introduction of ChatGPT, an advanced, easy-to-use, and freely 
      available artificial intelligence (AI) program, created new possibilities across 
      many industries and professions including healthcare simulation. ChatGPT has the 
      potential to streamline healthcare simulation-based education while also 
      providing insights for the scenario development process that conventional case 
      development may miss. However, there are issues related to accuracy, relevance, 
      and structure of the products produced by the ChatGPT AI program. This article 
      examines 2 AI-generated simulation case examples highlighting strengths and 
      weaknesses while providing guidance on the use of ChatGPT as a simulation 
      resource.
CI  - Copyright © 2023 Society for Simulation in Healthcare.
FAU - Rodgers, David L
AU  - Rodgers DL
AD  - From the Department of Medicine (D.L.R.) Indiana University School of Medicine; 
      Interprofessional Simulation Center (D.L.R., M.N., A.R.), Indiana University 
      School of Medicine, Bloomington; IU Health, Simulation Center at Fairbanks Hall 
      (A.R., J.P.), Indianapolis; Indiana University School of Nursing (R.B.), 
      Bloomington, IN; Penn State Health Milton S. Hershey Medical Center, Clinical 
      Simulation Center (T.B.), Hershey, PA; UT Southwestern Medical Center (J.H.), 
      Dallas, TX; Department of Emergency Medicine (P.V.K.), Indiana University School 
      of Medicine, Bloomington, IN; and Department of Emergency Medicine, Division of 
      Simulation (R.A.), Indiana University School of Medicine, Indianapolis, IN.
FAU - Needler, Mathew
AU  - Needler M
FAU - Robinson, Alexander
AU  - Robinson A
FAU - Barnes, Roxie
AU  - Barnes R
FAU - Brosche, Theresa
AU  - Brosche T
FAU - Hernandez, Jessica
AU  - Hernandez J
FAU - Poore, Julie
AU  - Poore J
FAU - VandeKoppel, Paul
AU  - VandeKoppel P
FAU - Ahmed, Rami
AU  - Ahmed R
LA  - eng
PT  - Journal Article
DEP - 20230920
PL  - United States
TA  - Simul Healthc
JT  - Simulation in healthcare : journal of the Society for Simulation in Healthcare
JID - 101264408
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Computer Simulation
COIS- The authors declare no conflict of interest.
EDAT- 2023/09/25 12:42
MHDA- 2023/11/30 06:42
CRDT- 2023/09/25 11:04
PHST- 2023/11/30 06:42 [medline]
PHST- 2023/09/25 12:42 [pubmed]
PHST- 2023/09/25 11:04 [entrez]
AID - 01266021-990000000-00086 [pii]
AID - 10.1097/SIH.0000000000000747 [doi]
PST - ppublish
SO  - Simul Healthc. 2023 Dec 1;18(6):395-399. doi: 10.1097/SIH.0000000000000747. Epub 
      2023 Sep 20.

PMID- 37596194
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240314
IS  - 1873-4898 (Electronic)
IS  - 1477-5131 (Linking)
VI  - 20
IP  - 1
DP  - 2024 Feb
TI  - Evaluating the performance of ChatGPT in answering questions related to pediatric 
      urology.
PG  - 26.e1-26.e5
LID - S1477-5131(23)00318-2 [pii]
LID - 10.1016/j.jpurol.2023.08.003 [doi]
AB  - INTRODUCTION: Artificial intelligence is advancing in various domains, including 
      medicine, and its progress is expected to continue in the future. OBJECTIVE: This 
      research aimed to assess the precision and consistency of ChatGPT's responses to 
      commonly asked inquiries related to pediatric urology. MATERIALS AND METHODS: We 
      examined commonly posed inquiries regarding pediatric urology found on urology 
      association websites, hospitals, and social media platforms. Additionally, we 
      referenced the recommendations tables in the European Urology Association's (EAU) 
      2022 Guidelines on Pediatric Urology, which contained robust data at the strong 
      recommendation level. All questions were systematically presented to ChatGPT's 
      May 23 Version, and two expert urologists independently assessed and assigned 
      scores ranging from 1 to 4 to each response. RESULTS: A hundred thirty seven 
      questions about pediatric urology were included in the study. The answers to 
      questions resulted in 92.0% completely correct. The completely correct rate in 
      the questions prepared according to the strong recommendations of the EAU 
      guideline was 93.6%. No question was answered completely wrong. The similarity 
      rates of the answers to the repeated questions were between 93.8% and 100%. 
      CONCLUSION: ChatGPT has provided satisfactory responses to inquiries related to 
      pediatric urology. Despite its limitations, it is foreseeable that this 
      continuously evolving platform will occupy a crucial position in the healthcare 
      industry.
CI  - Copyright © 2023 Journal of Pediatric Urology Company. Published by Elsevier Ltd. 
      All rights reserved.
FAU - Caglar, Ufuk
AU  - Caglar U
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey. 
      Electronic address: ufukcglr@gmail.com.
FAU - Yildiz, Oguzhan
AU  - Yildiz O
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
FAU - Meric, Arda
AU  - Meric A
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
FAU - Ayranci, Ali
AU  - Ayranci A
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
FAU - Gelmis, Mucahit
AU  - Gelmis M
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
FAU - Sarilar, Omer
AU  - Sarilar O
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
FAU - Ozgor, Faruk
AU  - Ozgor F
AD  - Department of Urology, Haseki Training and Research Hospital, Istanbul, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20230807
PL  - England
TA  - J Pediatr Urol
JT  - Journal of pediatric urology
JID - 101233150
SB  - IM
CIN - J Pediatr Urol. 2024 Feb;20(1):27. PMID: 37749006
CIN - J Pediatr Urol. 2024 Feb;20(1):28. PMID: 37749010
MH  - Child
MH  - Humans
MH  - Artificial Intelligence
MH  - *Urology
MH  - *Medicine
MH  - *Social Media
MH  - Urologists
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Health literacy
OT  - Patient knowledge
OT  - Pediatric urology
COIS- Conflict of interest None.
EDAT- 2023/08/19 11:41
MHDA- 2024/02/12 05:43
CRDT- 2023/08/18 22:03
PHST- 2023/05/31 00:00 [received]
PHST- 2023/07/25 00:00 [revised]
PHST- 2023/08/01 00:00 [accepted]
PHST- 2024/02/12 05:43 [medline]
PHST- 2023/08/19 11:41 [pubmed]
PHST- 2023/08/18 22:03 [entrez]
AID - S1477-5131(23)00318-2 [pii]
AID - 10.1016/j.jpurol.2023.08.003 [doi]
PST - ppublish
SO  - J Pediatr Urol. 2024 Feb;20(1):26.e1-26.e5. doi: 10.1016/j.jpurol.2023.08.003. 
      Epub 2023 Aug 7.

PMID- 37975899
OWN - NLM
STAT- MEDLINE
DCOM- 20240226
LR  - 20240226
IS  - 1432-0711 (Electronic)
IS  - 0932-0067 (Linking)
VI  - 309
IP  - 4
DP  - 2024 Apr
TI  - Will I soon be out of my job? Quality and guideline conformity of ChatGPT therapy 
      suggestions to patient inquiries with gynecologic symptoms in a palliative 
      setting.
PG  - 1543-1549
LID - 10.1007/s00404-023-07272-6 [doi]
AB  - PURPOSE: The market and application possibilities for artificial intelligence are 
      currently growing at high speed and are increasingly finding their way into 
      gynecology. While the medical side is highly represented in the current 
      literature, the patient's perspective is still lagging behind. Therefore, the aim 
      of this study was to evaluate the recommendations of ChatGPT regarding patient 
      inquiries about the possible therapy of gynecological leading symptoms in a 
      palliative situation by experts. METHODS: Case vignettes were constructed for 10 
      common concomitant symptoms in gynecologic oncology tumors in a palliative 
      setting, and patient queries regarding therapy of these symptoms were generated 
      as prompts for ChatGPT. Five experts in palliative care and gynecologic oncology 
      evaluated the responses with respect to guideline adherence and applicability and 
      identified advantages and disadvantages. RESULTS: The overall rating of ChatGPT 
      responses averaged 4.1 (5 = strongly agree; 1 = strongly disagree). The experts 
      saw an average guideline conformity of the therapy recommendations with a value 
      of 4.0. ChatGPT sometimes omits relevant therapies and does not provide an 
      individual assessment of the suggested therapies, but does indicate that a 
      physician consultation is additionally necessary. CONCLUSIONS: Language models, 
      such as ChatGPT, can provide valid and largely guideline-compliant therapy 
      recommendations in their freely available and thus in principle accessible 
      version for our patients. For a complete therapy recommendation, an evaluation of 
      the therapies, their individual adjustment as well as a filtering of possible 
      wrong recommendations, a medical expert's opinion remains indispensable.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Braun, Eva-Marie
AU  - Braun EM
AUID- ORCID: 0000-0002-7621-8482
AD  - Center for Integrative Oncology, Die Filderklinik, Im Haberschlai 7, 70794, 
      Filderstadt-Bonlanden, Germany. braun.em@web.de.
FAU - Juhasz-Böss, Ingolf
AU  - Juhasz-Böss I
AD  - Department of Gynecology, University Medical Center Freiburg, Hugstetter Straße 
      55, 79106, Freiburg, Germany.
FAU - Solomayer, Erich-Franz
AU  - Solomayer EF
AD  - Department of Gynecology, Obstetrics and Reproductive Medicine, Saarland 
      University Hospital, Kirrberger Straße, Building 9, 66421, Homburg, Germany.
FAU - Truhn, Daniel
AU  - Truhn D
AD  - Department of Diagnostic and Interventional Radiology, University Hospital 
      Aachen, Pauwelsstraße 30, 52074, Aachen, Germany.
FAU - Keller, Christiane
AU  - Keller C
AD  - Center for Palliative Medicine and Pediatric Pain Therapy, Saarland University 
      Hospital, Kirrberger Straße, Building 69, 66421, Homburg, Germany.
FAU - Heinrich, Vanessa
AU  - Heinrich V
AD  - Department of Radiation Oncology, University Hospital Tübingen, Crona Kliniken, 
      Hoppe-Seyler-Str. 3, 72076, Tübingen, Germany.
FAU - Braun, Benedikt Johannes
AU  - Braun BJ
AD  - Department of Trauma and Reconstructive Surgery at the Eberhard Karls University 
      Tübingen, BG Unfallklinik Tübingen, Schnarrenbergstrasse 95, 72076, Tübingen, 
      Germany.
LA  - eng
PT  - Journal Article
DEP - 20231117
PL  - Germany
TA  - Arch Gynecol Obstet
JT  - Archives of gynecology and obstetrics
JID - 8710213
SB  - IM
MH  - Humans
MH  - Female
MH  - Artificial Intelligence
MH  - *Genital Neoplasms, Female/drug therapy
MH  - Patient Compliance
MH  - Guideline Adherence
MH  - *Gynecology
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Gynecologic oncology
OT  - Language model
OT  - Palliative care
EDAT- 2023/11/17 15:29
MHDA- 2024/02/26 06:45
CRDT- 2023/11/17 11:04
PHST- 2023/08/13 00:00 [received]
PHST- 2023/10/15 00:00 [accepted]
PHST- 2024/02/26 06:45 [medline]
PHST- 2023/11/17 15:29 [pubmed]
PHST- 2023/11/17 11:04 [entrez]
AID - 10.1007/s00404-023-07272-6 [pii]
AID - 10.1007/s00404-023-07272-6 [doi]
PST - ppublish
SO  - Arch Gynecol Obstet. 2024 Apr;309(4):1543-1549. doi: 10.1007/s00404-023-07272-6. 
      Epub 2023 Nov 17.

PMID- 37792871
OWN - NLM
STAT- MEDLINE
DCOM- 20231006
LR  - 20231006
IS  - 1806-9282 (Electronic)
IS  - 0104-4230 (Print)
IS  - 0104-4230 (Linking)
VI  - 69
IP  - 10
DP  - 2023
TI  - Performance of ChatGPT-4 in answering questions from the Brazilian National 
      Examination for Medical Degree Revalidation.
PG  - e20230848
LID - S0104-42302023001000618 [pii]
LID - 10.1590/1806-9282.20230848 [doi]
LID - e20230848
AB  - OBJECTIVE: The aim of this study was to evaluate the performance of ChatGPT-4.0 
      in answering the 2022 Brazilian National Examination for Medical Degree 
      Revalidation (Revalida) and as a tool to provide feedback on the quality of the 
      examination. METHODS: A total of two independent physicians entered all 
      examination questions into ChatGPT-4.0. After comparing the outputs with the test 
      solutions, they classified the large language model answers as adequate, 
      inadequate, or indeterminate. In cases of disagreement, they adjudicated and 
      achieved a consensus decision on the ChatGPT accuracy. The performance across 
      medical themes and nullified questions was compared using chi-square statistical 
      analysis. RESULTS: In the Revalida examination, ChatGPT-4.0 answered 71 (87.7%) 
      questions correctly and 10 (12.3%) incorrectly. There was no statistically 
      significant difference in the proportions of correct answers among different 
      medical themes (p=0.4886). The artificial intelligence model had a lower accuracy 
      of 71.4% in nullified questions, with no statistical difference (p=0.241) between 
      non-nullified and nullified groups. CONCLUSION: ChatGPT-4.0 showed satisfactory 
      performance for the 2022 Brazilian National Examination for Medical Degree 
      Revalidation. The large language model exhibited worse performance on subjective 
      questions and public healthcare themes. The results of this study suggested that 
      the overall quality of the Revalida examination questions is satisfactory and 
      corroborates the nullified questions.
FAU - Gobira, Mauro
AU  - Gobira M
AUID- ORCID: 0000-0002-0483-8789
AD  - Instituto Paulista de Estudos e Pesquisas em Oftalmologia, Vision Institute - São 
      Paulo (SP), Brazil.
FAU - Nakayama, Luis Filipe
AU  - Nakayama LF
AUID- ORCID: 0000-0002-6847-6748
AD  - Instituto Paulista de Estudos e Pesquisas em Oftalmologia, Vision Institute - São 
      Paulo (SP), Brazil.
AD  - Massachusetts Institute of Technology, Institute for Medical Engineering and 
      Science - Cambridge (MA), USA.
FAU - Moreira, Rodrigo
AU  - Moreira R
AUID- ORCID: 0000-0002-9132-2819
AD  - Instituto Paulista de Estudos e Pesquisas em Oftalmologia, Vision Institute - São 
      Paulo (SP), Brazil.
FAU - Andrade, Eric
AU  - Andrade E
AUID- ORCID: 0000-0002-3331-786X
AD  - Universidade Federal de São Paulo, Department of Ophthalmology - São Paulo (SP), 
      Brazil.
FAU - Regatieri, Caio Vinicius Saito
AU  - Regatieri CVS
AUID- ORCID: 0000-0003-1511-8696
AD  - Universidade Federal de São Paulo, Department of Ophthalmology - São Paulo (SP), 
      Brazil.
FAU - Belfort, Rubens Jr
AU  - Belfort R Jr
AUID- ORCID: 0000-0002-8422-3898
AD  - Universidade Federal de São Paulo, Department of Ophthalmology - São Paulo (SP), 
      Brazil.
LA  - eng
PT  - Journal Article
DEP - 20230925
PL  - Brazil
TA  - Rev Assoc Med Bras (1992)
JT  - Revista da Associacao Medica Brasileira (1992)
JID - 9308586
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Brazil
MH  - *Health Personnel
MH  - Language
PMC - PMC10547492
EDAT- 2023/10/04 18:42
MHDA- 2023/10/06 06:43
PMCR- 2023/09/25
CRDT- 2023/10/04 13:47
PHST- 2023/07/06 00:00 [received]
PHST- 2023/07/17 00:00 [accepted]
PHST- 2023/10/06 06:43 [medline]
PHST- 2023/10/04 18:42 [pubmed]
PHST- 2023/10/04 13:47 [entrez]
PHST- 2023/09/25 00:00 [pmc-release]
AID - S0104-42302023001000618 [pii]
AID - 10.1590/1806-9282.20230848 [doi]
PST - epublish
SO  - Rev Assoc Med Bras (1992). 2023 Sep 25;69(10):e20230848. doi: 
      10.1590/1806-9282.20230848. eCollection 2023.

PMID- 37162073
OWN - NLM
STAT- MEDLINE
DCOM- 20230511
LR  - 20230513
IS  - 2317-6326 (Electronic)
IS  - 0102-6720 (Print)
IS  - 0102-6720 (Linking)
VI  - 36
DP  - 2023
TI  - FUTURE OF THE LANGUAGE MODELS IN HEALTHCARE: THE ROLE OF CHATGPT.
PG  - e1727
LID - S0102-67202023000100500 [pii]
LID - 10.1590/0102-672020230002e1727 [doi]
LID - e1727
AB  - The field of medicine has always been at the forefront of technological 
      innovation, constantly seeking new strategies to diagnose, treat, and prevent 
      diseases. Guidelines for clinical practice to orientate medical teams regarding 
      diagnosis, treatment, and prevention measures have increased over the years. The 
      purpose is to gather the most medical knowledge to construct an orientation for 
      practice. Evidence-based guidelines follow several main characteristics of a 
      systematic review, including systematic and unbiased search, selection, and 
      extraction of the source of evidence. In recent years, the rapid advancement of 
      artificial intelligence has provided clinicians and patients with access to 
      personalized, data-driven insights, support and new opportunities for healthcare 
      professionals to improve patient outcomes, increase efficiency, and reduce costs. 
      One of the most exciting developments in Artificial Intelligence has been the 
      emergence of chatbots. A chatbot is a computer program used to simulate 
      conversations with human users. Recently, OpenAI, a research organization focused 
      on machine learning, developed ChatGPT, a large language model that generates 
      human-like text. ChatGPT uses a type of AI known as a deep learning model. 
      ChatGPT can quickly search and select pieces of evidence through numerous 
      databases to provide answers to complex questions, reducing the time and effort 
      required to research a particular topic manually. Consequently, language models 
      can accelerate the creation of clinical practice guidelines. While there is no 
      doubt that ChatGPT has the potential to revolutionize the way healthcare is 
      delivered, it is essential to note that it should not be used as a substitute for 
      human healthcare professionals. Instead, ChatGPT should be considered a tool that 
      can be used to augment and support the work of healthcare professionals, helping 
      them to provide better care to their patients.
FAU - Tustumi, Francisco
AU  - Tustumi F
AUID- ORCID: 0000-0001-6695-0496
AD  - Universidade de São Paulo, Faculty of Medicine, Department of Gastroenterology - 
      São Paulo (SP), Brazil.
FAU - Andreollo, Nelson Adami
AU  - Andreollo NA
AUID- ORCID: 0000-0001-7452-1165
AD  - Universidade Estadual de Campinas, Faculty of Medical Sciences, Department of 
      Surgery, Digestive Disease Surgical Unit - Campinas (SP), Brazil.
FAU - Aguilar-Nascimento, José Eduardo de
AU  - Aguilar-Nascimento JE
AUID- ORCID: 0000-0002-3583-6612
AD  - Universidade Federal de Mato Grosso, Medical School, Department of Surgery - 
      Cuiabá (MT), Brazil.
LA  - eng
PT  - Journal Article
PT  - Systematic Review
DEP - 20230508
PL  - Brazil
TA  - Arq Bras Cir Dig
JT  - Arquivos brasileiros de cirurgia digestiva : ABCD = Brazilian archives of 
      digestive surgery
JID - 9100283
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Language
MH  - Software
MH  - Delivery of Health Care
PMC - PMC10168663
COIS- Conflicts of Interest: None
EDAT- 2023/05/10 12:42
MHDA- 2023/05/11 06:42
PMCR- 2023/05/08
CRDT- 2023/05/10 08:03
PHST- 2023/02/16 00:00 [received]
PHST- 2023/02/20 00:00 [accepted]
PHST- 2023/05/11 06:42 [medline]
PHST- 2023/05/10 12:42 [pubmed]
PHST- 2023/05/10 08:03 [entrez]
PHST- 2023/05/08 00:00 [pmc-release]
AID - S0102-67202023000100500 [pii]
AID - 10.1590/0102-672020230002e1727 [doi]
PST - epublish
SO  - Arq Bras Cir Dig. 2023 May 8;36:e1727. doi: 10.1590/0102-672020230002e1727. 
      eCollection 2023.

PMID- 37643186
OWN - NLM
STAT- MEDLINE
DCOM- 20230831
LR  - 20230904
IS  - 1932-6203 (Electronic)
IS  - 1932-6203 (Linking)
VI  - 18
IP  - 8
DP  - 2023
TI  - ChatGPT versus human in generating medical graduate exam multiple choice 
      questions-A multinational prospective study (Hong Kong S.A.R., Singapore, 
      Ireland, and the United Kingdom).
PG  - e0290691
LID - 10.1371/journal.pone.0290691 [doi]
LID - e0290691
AB  - INTRODUCTION: Large language models, in particular ChatGPT, have showcased 
      remarkable language processing capabilities. Given the substantial workload of 
      university medical staff, this study aims to assess the quality of 
      multiple-choice questions (MCQs) produced by ChatGPT for use in graduate medical 
      examinations, compared to questions written by university professoriate staffs 
      based on standard medical textbooks. METHODS: 50 MCQs were generated by ChatGPT 
      with reference to two standard undergraduate medical textbooks (Harrison's, and 
      Bailey &amp; Love's). Another 50 MCQs were drafted by two university professoriate 
      staff using the same medical textbooks. All 100 MCQ were individually numbered, 
      randomized and sent to five independent international assessors for MCQ quality 
      assessment using a standardized assessment score on five assessment domains, 
      namely, appropriateness of the question, clarity and specificity, relevance, 
      discriminative power of alternatives, and suitability for medical graduate 
      examination. RESULTS: The total time required for ChatGPT to create the 50 
      questions was 20 minutes 25 seconds, while it took two human examiners a total of 
      211 minutes 33 seconds to draft the 50 questions. When a comparison of the mean 
      score was made between the questions constructed by A.I. with those drafted by 
      humans, only in the relevance domain that the A.I. was inferior to humans (A.I.: 
      7.56 +/- 0.94 vs human: 7.88 +/- 0.52; p = 0.04). There was no significant 
      difference in question quality between questions drafted by A.I. versus humans, 
      in the total assessment score as well as in other domains. Questions generated by 
      A.I. yielded a wider range of scores, while those created by humans were 
      consistent and within a narrower range. CONCLUSION: ChatGPT has the potential to 
      generate comparable-quality MCQs for medical graduate examinations within a 
      significantly shorter time.
CI  - Copyright: © 2023 Cheung et al. This is an open access article distributed under 
      the terms of the Creative Commons Attribution License, which permits unrestricted 
      use, distribution, and reproduction in any medium, provided the original author 
      and source are credited.
FAU - Cheung, Billy Ho Hung
AU  - Cheung BHH
AUID- ORCID: 0000-0002-8843-7893
AD  - L.K.S. Faculty of Medicine, University of Hong Kong, Hong Kong, Hong Kong S.A.R.
FAU - Lau, Gary Kui Kai
AU  - Lau GKK
AD  - L.K.S. Faculty of Medicine, University of Hong Kong, Hong Kong, Hong Kong S.A.R.
FAU - Wong, Gordon Tin Chun
AU  - Wong GTC
AD  - L.K.S. Faculty of Medicine, University of Hong Kong, Hong Kong, Hong Kong S.A.R.
FAU - Lee, Elaine Yuen Phin
AU  - Lee EYP
AD  - L.K.S. Faculty of Medicine, University of Hong Kong, Hong Kong, Hong Kong S.A.R.
FAU - Kulkarni, Dhananjay
AU  - Kulkarni D
AD  - Department of Surgery, University of Edinburgh, Edinburgh, United Kingdom.
FAU - Seow, Choon Sheong
AU  - Seow CS
AD  - Department of Surgery, National University Cancer Institute Singapore, Singapore, 
      Singapore.
FAU - Wong, Ruby
AU  - Wong R
AUID- ORCID: 0000-0002-0847-8514
AD  - Department of Surgery, University of Galway, Galway, Ireland.
FAU - Co, Michael Tiong-Hong
AU  - Co MT
AUID- ORCID: 0000-0002-1705-051X
AD  - L.K.S. Faculty of Medicine, University of Hong Kong, Hong Kong, Hong Kong S.A.R.
LA  - eng
PT  - Journal Article
DEP - 20230829
PL  - United States
TA  - PLoS One
JT  - PloS one
JID - 101285081
SB  - IM
MH  - Humans
MH  - Hong Kong
MH  - Ireland
MH  - Prospective Studies
MH  - Singapore
MH  - United Kingdom
MH  - *Artificial Intelligence
MH  - *Education, Medical, Graduate
MH  - *Educational Measurement/methods
PMC - PMC10464959
COIS- No.
EDAT- 2023/08/29 18:42
MHDA- 2023/08/31 06:42
PMCR- 2023/08/29
CRDT- 2023/08/29 13:34
PHST- 2023/06/05 00:00 [received]
PHST- 2023/08/15 00:00 [accepted]
PHST- 2023/08/31 06:42 [medline]
PHST- 2023/08/29 18:42 [pubmed]
PHST- 2023/08/29 13:34 [entrez]
PHST- 2023/08/29 00:00 [pmc-release]
AID - PONE-D-23-14620 [pii]
AID - 10.1371/journal.pone.0290691 [doi]
PST - epublish
SO  - PLoS One. 2023 Aug 29;18(8):e0290691. doi: 10.1371/journal.pone.0290691. 
      eCollection 2023.

PMID- 38558009
OWN - NLM
STAT- Publisher
LR  - 20240401
IS  - 1097-0045 (Electronic)
IS  - 0270-4137 (Linking)
DP  - 2024 Apr 1
TI  - Performance of large language models on benign prostatic hyperplasia frequently 
      asked questions.
LID - 10.1002/pros.24699 [doi]
AB  - BACKGROUND: Benign prostatic hyperplasia (BPH) is a common condition, yet it is 
      challenging for the average BPH patient to find credible and accurate information 
      about BPH. Our goal is to evaluate and compare the accuracy and reproducibility 
      of large language models (LLMs), including ChatGPT-3.5, ChatGPT-4, and the New 
      Bing Chat in responding to a BPH frequently asked questions (FAQs) questionnaire. 
      METHODS: A total of 45 questions related to BPH were categorized into basic and 
      professional knowledge. Three LLM-ChatGPT-3.5, ChatGPT-4, and New Bing Chat-were 
      utilized to generate responses to these questions. Responses were graded as 
      comprehensive, correct but inadequate, mixed with incorrect/outdated data, or 
      completely incorrect. Reproducibility was assessed by generating two responses 
      for each question. All responses were reviewed and judged by experienced 
      urologists. RESULTS: All three LLMs exhibited high accuracy in generating 
      responses to questions, with accuracy rates ranging from 86.7% to 100%. However, 
      there was no statistically significant difference in response accuracy among the 
      three (p &gt; 0.017 for all comparisons). Additionally, the accuracy of the LLMs' 
      responses to the basic knowledge questions was roughly equivalent to that of the 
      specialized knowledge questions, showing a difference of less than 3.5% (GPT-3.5: 
      90% vs. 86.7%; GPT-4: 96.7% vs. 95.6%; New Bing: 96.7% vs. 93.3%). Furthermore, 
      all three LLMs demonstrated high reproducibility, with rates ranging from 93.3% 
      to 97.8%. CONCLUSIONS: ChatGPT-3.5, ChatGPT-4, and New Bing Chat offer accurate 
      and reproducible responses to BPH-related questions, establishing them as 
      valuable resources for enhancing health literacy and supporting BPH patients in 
      conjunction with healthcare professionals.
CI  - © 2024 Wiley Periodicals LLC.
FAU - Zhang, YuNing
AU  - Zhang Y
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Dong, Yijie
AU  - Dong Y
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Mei, Zihan
AU  - Mei Z
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Hou, Yiqing
AU  - Hou Y
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Wei, Minyan
AU  - Wei M
AUID- ORCID: 0009-0008-9140-3038
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Yeung, Yat Hin
AU  - Yeung YH
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Xu, Jiale
AU  - Xu J
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Hua, Qing
AU  - Hua Q
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Lai, LiMei
AU  - Lai L
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Li, Ning
AU  - Li N
AUID- ORCID: 0000-0002-4738-7340
AD  - Department of Ultrasound, Yunnan Kungang Hospital, The Seventh Affiliated 
      Hospital of Dali University, Anning, Yunnan, China.
FAU - Xia, ShuJun
AU  - Xia S
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Zhou, Chun
AU  - Zhou C
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
FAU - Zhou, JianQiao
AU  - Zhou J
AUID- ORCID: 0000-0002-0964-8407
AD  - Department of Ultrasound, Ruijin Hospital, Shanghai Jiaotong University School of 
      Medicine, Shanghai, China.
AD  - College of Health Science and Technology, Shanghai Jiao Tong University School of 
      Medicine, Shanghai, China.
LA  - eng
GR  - 202105AF150087/Academician Expert Workstation of Yunnan Province/
GR  - 82071928/National Natural Science Foundation of China/
PT  - Journal Article
DEP - 20240401
PL  - United States
TA  - Prostate
JT  - The Prostate
JID - 8101368
SB  - IM
OTO - NOTNLM
OT  - artificial intelligence
OT  - benign prostatic hyperplasia
OT  - large language model
EDAT- 2024/04/01 18:42
MHDA- 2024/04/01 18:42
CRDT- 2024/04/01 16:17
PHST- 2024/02/21 00:00 [revised]
PHST- 2024/01/15 00:00 [received]
PHST- 2024/03/20 00:00 [accepted]
PHST- 2024/04/01 18:42 [medline]
PHST- 2024/04/01 18:42 [pubmed]
PHST- 2024/04/01 16:17 [entrez]
AID - 10.1002/pros.24699 [doi]
PST - aheadofprint
SO  - Prostate. 2024 Apr 1. doi: 10.1002/pros.24699.

PMID- 37866949
OWN - NLM
STAT- MEDLINE
DCOM- 20231026
LR  - 20231112
IS  - 1672-173X (Print)
IS  - 1672-173X (Linking)
VI  - 54
IP  - 5
DP  - 2023 Sep
TI  - [Reflections on the Implications of the Developments in ChatGPT for Changes in 
      Medical Education Models].
PG  - 937-940
LID - 10.12182/20231360302 [doi]
AB  - Ever since its official launch, Chat Generative Pre-Trained Transformer, or 
      ChatGPT, a natural language processing tool driven by artificial intelligence 
      (AI) technology, has attracted much attention from the education community. 
      ChatGPT can play an important role in the field of medical education, with its 
      potential applications ranging from assisting teachers in designing 
      individualized teaching scenarios to enhancing students' practical ability for 
      solving clinical problems and improving teaching and research efficiency. With 
      the developments in technology, it is inevitable that ChatGPT, or other 
      generative AI models, will be thoroughly integrated in more and more medical 
      contexts, which will further enhance the efficiency and quality of medical 
      services and allow doctors to spend more time interacting with patients and 
      implement personalized health management. Herein, we suggested that proactive 
      reflections be made to figure out the best way to cultivate health professional 
      in the context of New Medical Education, to help more medical professionals 
      enhance their understanding of developments in artificial intelligence, and to 
      make preparations for the challenges that will emerge in the new round of 
      technological revolution. Medical educators should focus on guiding students to 
      make proper use of AI tools in the appropriate context, thereby prevening abuse 
      or overreliance caused by a lack of discrimating ability. Teachers should focus 
      on helping medical students make improvements in clinical reasoning skills, 
      self-directed learning, and clinical practical skills. Teachers should stress the 
      importance for medical students to understand the philosophical implications of 
      the mind-body unity concept, holistic medical thinking, and systematic medical 
      thinking. It is important to enhance medical students' humanistic qualities, 
      cultivate their empathy and communication skills, and continually enhance their 
      ability to meet the requirements of individualized precision diagnosis and 
      treatment so that they will better adapt to the future developments in medicine.
CI  - Copyright© by Editorial Board of Journal of Sichuan University (Medical 
      Sciences).
FAU - Qu, Xing
AU  - Qu X
AD  - Institute of Hospital Management, West China Hospital, Sichuan University, 
      Chengdu 610041, China.
FAU - Yang, Jinming
AU  - Yang J
AD  - School of Public Administration, Southwestern University of Finance and 
      Economics, Chengdu 611130, China.
FAU - Chen, Tao
AU  - Chen T
AD  - School of Finance, Southwestern University of Finance and Economics, Chengdu 
      611130, China.
FAU - Zhang, Wei
AU  - Zhang W
AD  - West China Biomedical Big Data Center, West China Hospital, Sichuan University, 
      Chengdu 610041, China.
LA  - chi
PT  - English Abstract
PT  - Journal Article
PT  - Review
PL  - China
TA  - Sichuan Da Xue Xue Bao Yi Xue Ban
JT  - Sichuan da xue xue bao. Yi xue ban = Journal of Sichuan University. Medical 
      science edition
JID - 101162609
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Educational Status
MH  - Students
MH  - *Education, Medical
MH  - Clinical Competence
PMC - PMC10579070
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Medical education
COIS- 利益冲突　所有作者均声明不存在利益冲突
EDAT- 2023/10/23 00:44
MHDA- 2023/10/26 06:42
PMCR- 2023/09/20
CRDT- 2023/10/22 21:52
PHST- 2023/10/26 06:42 [medline]
PHST- 2023/10/23 00:44 [pubmed]
PHST- 2023/10/22 21:52 [entrez]
PHST- 2023/09/20 00:00 [pmc-release]
AID - scdxxbyxb-54-5-937 [pii]
AID - 10.12182/20231360302 [doi]
PST - ppublish
SO  - Sichuan Da Xue Xue Bao Yi Xue Ban. 2023 Sep;54(5):937-940. doi: 
      10.12182/20231360302.

PMID- 37097229
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230427
LR  - 20230508
IS  - 0807-7096 (Electronic)
IS  - 0029-2001 (Linking)
VI  - 143
IP  - 6
DP  - 2023 Apr 25
TI  - From ELIZA to ChatGPT.
LID - 10.4045/tidsskr.23.0279 [doi]
FAU - Brean, Are
AU  - Brean A
AUID- ORCID: 0000-0001-5683-5099
LA  - eng
LA  - nor
PT  - Journal Article
TT  - Fra Eliza til ChatGPT.
DEP - 20230424
PL  - Norway
TA  - Tidsskr Nor Laegeforen
JT  - Tidsskrift for den Norske laegeforening : tidsskrift for praktisk medicin, ny 
      raekke
JID - 0413423
SB  - IM
EDAT- 2023/04/25 17:42
MHDA- 2023/04/25 17:43
CRDT- 2023/04/25 10:42
PHST- 2023/04/25 17:43 [medline]
PHST- 2023/04/25 17:42 [pubmed]
PHST- 2023/04/25 10:42 [entrez]
AID - 23-0279 [pii]
AID - 10.4045/tidsskr.23.0279 [doi]
PST - epublish
SO  - Tidsskr Nor Laegeforen. 2023 Apr 24;143(6). doi: 10.4045/tidsskr.23.0279. Print 
      2023 Apr 25.

PMID- 38271674
OWN - NLM
STAT- Publisher
LR  - 20240125
IS  - 1539-2864 (Electronic)
IS  - 0275-004X (Linking)
DP  - 2024 Jan 23
TI  - Performance Assessment of an Artificial Intelligence Chatbot in Clinical 
      Vitreoretinal Scenarios.
LID - 10.1097/IAE.0000000000004053 [doi]
AB  - PURPOSE: To determine how often ChatGPT is able to provide accurate and 
      comprehensive information regarding clinical vitreoretinal scenarios. To assess 
      the types of sources ChatGPT primarily utilizes and to determine if they are 
      hallucinated. METHODS: A retrospective cross-sectional study. We designed 40 
      open-ended clinical scenarios across 4 main topics in vitreoretinal disease. 
      Responses were graded on correctness and comprehensiveness by two blinded retina 
      specialists. The primary outcome was the number of clinical scenarios that 
      ChatGPT answered correctly and comprehensively. Secondary outcomes included: 
      theoretical harm to patients, the distribution of the type of references utilized 
      by the chatbot, and the frequency of hallucinated references. RESULTS: In June 
      2023, ChatGPT answered 83% (33/40) of clinical scenarios correctly but provided a 
      comprehensive answer in only 52.5% (21/40) of cases. Subgroup analysis 
      demonstrated an average correct score of 86.7% in nAMD, 100% in DR, 76.7% in 
      retinal vascular disease and 70% in the surgical domain. There were 6 incorrect 
      responses with 1 (16.7%) case of no harm, 3 (50%) cases of possible harm and 2 
      (33.3%) cases of definitive harm. CONCLUSION: ChatGPT correctly answered more 
      than 80% of complex open-ended vitreoretinal clinical scenarios, with a reduced 
      capability to provide a comprehensive response.
FAU - Maywood, Michael J
AU  - Maywood MJ
AUID- ORCID: 0009-0006-1917-7940
AD  - Department of Ophthalmology, Corewell Health William Beaumont University 
      Hospital, Royal Oak, MI, USA.
FAU - Parikh, Ravi
AU  - Parikh R
AD  - Manhattan Retina and Eye Consultants, New York, NY, USA.
AD  - Department of Ophthalmology, New York University School of Medicine, New York, 
      NY, USA.
FAU - Deobhakta, Avnish
AU  - Deobhakta A
AD  - Icahn School of Medicine of Mount Sinai, New York, NY, USA.
FAU - Begaj, Tedi
AU  - Begaj T
AUID- ORCID: 0000-0002-1880-3982
AD  - Department of Ophthalmology, Corewell Health William Beaumont University 
      Hospital, Royal Oak, MI, USA.
AD  - Associated Retinal Consultants, Royal Oak, MI, USA.
LA  - eng
PT  - Journal Article
DEP - 20240123
PL  - United States
TA  - Retina
JT  - Retina (Philadelphia, Pa.)
JID - 8309919
SB  - IM
COIS- Conflict of Interest: Tedi Begaj: Financial disclosures: Consultant fees for 
      Regenxbio, Eyepoint, Ravi Parikh: Financial disclosures: Consultant fees for 
      Anthem, Blue Cross Blue Shield, GLG Surveys, Avnish Deobhakta: Financial 
      disclosures: Consultant fees for Alimera Sciences, founder of Avant Sciences
EDAT- 2024/01/25 18:42
MHDA- 2024/01/25 18:42
CRDT- 2024/01/25 16:33
PHST- 2024/01/25 18:42 [medline]
PHST- 2024/01/25 18:42 [pubmed]
PHST- 2024/01/25 16:33 [entrez]
AID - 00006982-990000000-00586 [pii]
AID - 10.1097/IAE.0000000000004053 [doi]
PST - aheadofprint
SO  - Retina. 2024 Jan 23. doi: 10.1097/IAE.0000000000004053.

PMID- 38096831
OWN - NLM
STAT- MEDLINE
DCOM- 20231216
LR  - 20231216
IS  - 0016-3813 (Print)
IS  - 0016-3813 (Linking)
VI  - 159
IP  - 5
DP  - 2023
TI  - ChatGPT: opportunities and risks in the fields of medical care, teaching, and 
      research.
PG  - 372-379
LID - 10.24875/GMM.M23000811 [doi]
AB  - ChatGPT is a virtual assistant with artificial intelligence (AI) that uses 
      natural language to communicate, i.e., it holds conversations as those that would 
      take place with another human being. It can be applied at all educational levels, 
      including medical education, where it can impact medical training, research, the 
      writing of scientific articles, clinical care, and personalized medicine. It can 
      modify interactions between physicians and patients and thus improve the 
      standards of healthcare quality and safety, for example, by suggesting preventive 
      measures in a patient that sometimes are not considered by the physician for 
      multiple reasons. ChatGPT potential uses in medical education, as a tool to 
      support the writing of scientific articles, as a medical care assistant for 
      patients and doctors for a more personalized medical approach, are some of the 
      applications discussed in this article. Ethical aspects, originality, 
      inappropriate or incorrect content, incorrect citations, cybersecurity, 
      hallucinations, and plagiarism are some examples of situations to be considered 
      when using AI-based tools in medicine.
CI  - Copyright: © 2023 Permanyer.
FAU - Gutiérrez-Cirlos, Carlos
AU  - Gutiérrez-Cirlos C
AD  - Faculty of Medicine, Secretariat of Clinical Teaching, Medical Internship and 
      Social Service, Universidad Nacional Autónoma de México, Mexico City.
AD  - Department of Internal Medicine, Medical Directorate, Instituto Nacional de 
      Ciencias Médicas y Nutrición "Salvador Zubirán", Mexico City.
FAU - Carrillo-Pérez, Diego L
AU  - Carrillo-Pérez DL
AD  - Department of Internal Medicine, Medical Directorate, Instituto Nacional de 
      Ciencias Médicas y Nutrición "Salvador Zubirán", Mexico City.
AD  - School of Medicine and Health Sciences, Tecnológico de Monterrey, Mexico City.
FAU - Bermúdez-González, Jorge Luis
AU  - Bermúdez-González JL
AD  - Department of Internal Medicine, Medical Directorate, Instituto Nacional de 
      Ciencias Médicas y Nutrición "Salvador Zubirán", Mexico City.
FAU - Hidrogo-Montemayor, Irving
AU  - Hidrogo-Montemayor I
AD  - Division of Innovation with Emerging Technologies, Tecnológico de Monterrey, 
      Monterrey, Nuevo León.
FAU - Carrillo-Esper, Raúl
AU  - Carrillo-Esper R
AD  - Academia Nacional de Medicina de México, Mexico City.
FAU - Sánchez-Mendiola, Melchor
AU  - Sánchez-Mendiola M
AD  - Postgraduate Education Division, Faculty of Medicine, Universidad Nacional 
      Autónoma de México, Mexico City.
AD  - Open University Coordination, Educational Innovation and Distance Education, 
      Directorate of Educational Evaluation, Universidad Nacional Autónoma de México, 
      Mexico City. Mexico.
LA  - eng
PT  - Journal Article
TT  - ChatGPT: oportunidades y riesgos en la asistencia, docencia e investigación 
      médica.
PL  - Mexico
TA  - Gac Med Mex
JT  - Gaceta medica de Mexico
JID - 0010333
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Educational Status
MH  - *Allied Health Personnel
MH  - Communication
MH  - Precision Medicine
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Cuidado del paciente
OT  - Educación médica
OT  - Errores médicos
OT  - Inteligencia artificial
OT  - Medical education
OT  - Medical errors
OT  - Medicina de precisión
OT  - Patient care
OT  - Precision medicine
EDAT- 2023/12/15 00:44
MHDA- 2023/12/17 09:41
CRDT- 2023/12/14 18:43
PHST- 2023/12/17 09:41 [medline]
PHST- 2023/12/15 00:44 [pubmed]
PHST- 2023/12/14 18:43 [entrez]
AID - j159/5/372 [pii]
AID - 10.24875/GMM.M23000811 [doi]
PST - ppublish
SO  - Gac Med Mex. 2023;159(5):372-379. doi: 10.24875/GMM.M23000811.

PMID- 37915603
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231103
IS  - 2589-0042 (Electronic)
IS  - 2589-0042 (Linking)
VI  - 26
IP  - 11
DP  - 2023 Nov 17
TI  - Popular large language model chatbots' accuracy, comprehensiveness, and 
      self-awareness in answering ocular symptom queries.
PG  - 108163
LID - 10.1016/j.isci.2023.108163 [doi]
LID - 108163
AB  - In light of growing interest in using emerging large language models (LLMs) for 
      self-diagnosis, we systematically assessed the performance of ChatGPT-3.5, 
      ChatGPT-4.0, and Google Bard in delivering proficient responses to 37 common 
      inquiries regarding ocular symptoms. Responses were masked, randomly shuffled, 
      and then graded by three consultant-level ophthalmologists for accuracy (poor, 
      borderline, good) and comprehensiveness. Additionally, we evaluated the 
      self-awareness capabilities (ability to self-check and self-correct) of the 
      LLM-Chatbots. 89.2% of ChatGPT-4.0 responses were 'good'-rated, outperforming 
      ChatGPT-3.5 (59.5%) and Google Bard (40.5%) significantly (all p&nbsp;&lt;&nbsp;0.001). All 
      three LLM-Chatbots showed optimal mean comprehensiveness scores as well (ranging 
      from 4.6 to 4.7 out of 5). However, they exhibited subpar to moderate 
      self-awareness capabilities. Our study underscores the potential of ChatGPT-4.0 
      in delivering accurate and comprehensive responses to ocular symptom inquiries. 
      Future rigorous validation of their performance is crucial to ensure their 
      reliability and appropriateness for actual clinical use.
CI  - © 2023 The Authors.
FAU - Pushpanathan, Krithi
AU  - Pushpanathan K
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Centre for Innovation and Precision Eye Health &amp; Department of Ophthalmology, 
      Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
FAU - Lim, Zhi Wei
AU  - Lim ZW
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
FAU - Er Yew, Samantha Min
AU  - Er Yew SM
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Centre for Innovation and Precision Eye Health &amp; Department of Ophthalmology, 
      Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
FAU - Chen, David Ziyou
AU  - Chen DZ
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Centre for Innovation and Precision Eye Health &amp; Department of Ophthalmology, 
      Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Department of Ophthalmology, National University Hospital, Singapore, Singapore.
FAU - Hui'En Lin, Hazel Anne
AU  - Hui'En Lin HA
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Centre for Innovation and Precision Eye Health &amp; Department of Ophthalmology, 
      Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Department of Ophthalmology, National University Hospital, Singapore, Singapore.
FAU - Lin Goh, Jocelyn Hui
AU  - Lin Goh JH
AD  - Singapore Eye Research Institute, Singapore National Eye Centre, Singapore, 
      Singapore.
FAU - Wong, Wendy Meihua
AU  - Wong WM
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Centre for Innovation and Precision Eye Health &amp; Department of Ophthalmology, 
      Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Department of Ophthalmology, National University Hospital, Singapore, Singapore.
FAU - Wang, Xiaofei
AU  - Wang X
AD  - Key Laboratory for Biomechanics and Mechanobiology of Ministry of Education, 
      Beijing, China.
AD  - Advanced Innovation Centre for Biomedical Engineering, School of Biological 
      Science and Medical Engineering, Beihang University, Beijing, China.
FAU - Jin Tan, Marcus Chun
AU  - Jin Tan MC
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Centre for Innovation and Precision Eye Health &amp; Department of Ophthalmology, 
      Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Department of Ophthalmology, National University Hospital, Singapore, Singapore.
FAU - Chang Koh, Victor Teck
AU  - Chang Koh VT
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Centre for Innovation and Precision Eye Health &amp; Department of Ophthalmology, 
      Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Department of Ophthalmology, National University Hospital, Singapore, Singapore.
FAU - Tham, Yih-Chung
AU  - Tham YC
AD  - Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Centre for Innovation and Precision Eye Health &amp; Department of Ophthalmology, 
      Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
      Singapore.
AD  - Singapore Eye Research Institute, Singapore National Eye Centre, Singapore, 
      Singapore.
AD  - Ophthalmology and Visual Sciences Academic Clinical Programme (Eye ACP), Duke NUS 
      Medical School, Singapore, Singapore.
LA  - eng
PT  - Journal Article
DEP - 20231010
PL  - United States
TA  - iScience
JT  - iScience
JID - 101724038
PMC - PMC10616302
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Ophthalmology
COIS- All authors declare no competing interests.
EDAT- 2023/11/02 06:43
MHDA- 2023/11/02 06:44
PMCR- 2023/10/10
CRDT- 2023/11/02 04:03
PHST- 2023/08/07 00:00 [received]
PHST- 2023/09/19 00:00 [revised]
PHST- 2023/10/05 00:00 [accepted]
PHST- 2023/11/02 06:44 [medline]
PHST- 2023/11/02 06:43 [pubmed]
PHST- 2023/11/02 04:03 [entrez]
PHST- 2023/10/10 00:00 [pmc-release]
AID - S2589-0042(23)02240-X [pii]
AID - 108163 [pii]
AID - 10.1016/j.isci.2023.108163 [doi]
PST - epublish
SO  - iScience. 2023 Oct 10;26(11):108163. doi: 10.1016/j.isci.2023.108163. eCollection 
      2023 Nov 17.

PMID- 37776670
OWN - NLM
STAT- Publisher
LR  - 20231020
IS  - 1872-8243 (Electronic)
IS  - 1386-5056 (Linking)
VI  - 179
DP  - 2023 Nov
TI  - A comprehensive evaluation of ChatGPT consultation quality for augmentation 
      mammoplasty: A comparative analysis between plastic surgeons and laypersons.
PG  - 105219
LID - S1386-5056(23)00237-X [pii]
LID - 10.1016/j.ijmedinf.2023.105219 [doi]
AB  - OBJECTIVES: ChatGPT has gained significant popularity as a source of healthcare 
      information among the general population. Evaluating the quality of chatbot 
      responses is crucial, requiring comprehensive and qualitative analysis. This 
      study aims to assess the answers provided by ChatGPT during hypothetical breast 
      augmentation consultations across various categories and depths. The evaluation 
      involves the utilization of validated tools and a comparison of scores between 
      plastic surgeons and laypersons. METHODS: A panel consisting of five plastic 
      surgeons and five laypersons evaluated ChatGPT's responses to 25 questions 
      spanning consultation, procedure, recovery, and sentiment categories. The DISCERN 
      and PEMAT tools were employed to assess the responses, while emotional context 
      was examined through ten specific questions. Additionally, readability was 
      measured using the Flesch Reading Ease score. Qualitative analysis was performed 
      to identify the overall strengths and weaknesses. RESULTS: Plastic surgeons 
      generally scored lower than laypersons across most domains. Scores for each 
      evaluation domain varied by category, with the consultation category 
      demonstrating lower scores in terms of DISCERN reliability, information quality, 
      and DISCERN score. Plastic surgeons assigned significantly lower overall quality 
      ratings to the procedure category compared to other question categories. They 
      also gave lower emotion scores in the procedure category compared to laypersons. 
      The depth of the questions did not impact the scoring. CONCLUSIONS: Existing 
      health information evaluation tools may not be entirely suitable for 
      comprehensively evaluating the quality of individual responses generated by 
      ChatGPT. Consequently, the development and implementation of appropriate 
      evaluation tools to assess the appropriateness and quality of AI consultations 
      are necessary.
CI  - Copyright © 2023 The Authors. Published by Elsevier B.V. All rights reserved.
FAU - Yun, Ji Young
AU  - Yun JY
AD  - Department of Plastic and Reconstructive Surgery, Busan Paik Hospital, Inje 
      University School of Medicine, Busan, Republic of Korea.
FAU - Kim, Dong Jin
AU  - Kim DJ
AD  - Department of Plastic Surgery, Asan Medical Center, University of Ulsan College 
      of Medicine, Seoul, Republic of Korea.
FAU - Lee, Nara
AU  - Lee N
AD  - Department of Plastic Surgery, Asan Medical Center, University of Ulsan College 
      of Medicine, Seoul, Republic of Korea.
FAU - Kim, Eun Key
AU  - Kim EK
AD  - Department of Plastic Surgery, Asan Medical Center, University of Ulsan College 
      of Medicine, Seoul, Republic of Korea. Electronic address: nicekek@korea.com.
LA  - eng
PT  - Journal Article
DEP - 20230920
PL  - Ireland
TA  - Int J Med Inform
JT  - International journal of medical informatics
JID - 9711057
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Consumer health information
OT  - Health literacy
OT  - Internet
COIS- Declaration of Competing Interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2023/10/01 04:44
MHDA- 2023/10/01 04:44
CRDT- 2023/09/30 18:05
PHST- 2023/08/03 00:00 [received]
PHST- 2023/09/05 00:00 [revised]
PHST- 2023/09/16 00:00 [accepted]
PHST- 2023/10/01 04:44 [pubmed]
PHST- 2023/10/01 04:44 [medline]
PHST- 2023/09/30 18:05 [entrez]
AID - S1386-5056(23)00237-X [pii]
AID - 10.1016/j.ijmedinf.2023.105219 [doi]
PST - ppublish
SO  - Int J Med Inform. 2023 Nov;179:105219. doi: 10.1016/j.ijmedinf.2023.105219. Epub 
      2023 Sep 20.

PMID- 37223340
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230525
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 5
DP  - 2023 May
TI  - ChatGPT-4 and the Global Burden of Disease Study: Advancing Personalized 
      Healthcare Through Artificial Intelligence in Clinical and Translational 
      Medicine.
PG  - e39384
LID - 10.7759/cureus.39384 [doi]
LID - e39384
AB  - The fusion of insights from the comprehensive global burden of disease (GBD) 
      study and the advanced artificial intelligence of open artificial intelligence 
      (AI) chat generative pre-trained transformer version 4 (ChatGPT-4) brings the 
      potential to transform personalized healthcare planning. By integrating the 
      data-driven findings of the GBD study with the powerful conversational 
      capabilities of ChatGPT-4, healthcare professionals can devise customized 
      healthcare plans that are adapted to patients' lifestyles and preferences. We 
      propose that this innovative partnership can lead to the creation of a novel 
      AI-assisted personalized disease burden (AI-PDB) assessment and planning tool. 
      For the successful implementation of this unconventional technology, it is 
      crucial to ensure continuous and accurate updates, expert supervision, and 
      address potential biases and limitations. Healthcare professionals and 
      stakeholders should have a balanced and dynamic approach, emphasizing 
      interdisciplinary collaborations, data accuracy, transparency, ethical 
      compliance, and ongoing training. By investing in the unique strengths of both 
      ChatGPT-4, especially its newly introduced features such as live internet 
      browsing or plugins, and the GBD study, we may enhance personalized healthcare 
      planning. This innovative approach has the potential to&nbsp;improve patient outcomes 
      and optimize resource utilization, as well as&nbsp;pave the way for the worldwide 
      implementation of precision medicine, thereby revolutionizing the existing 
      healthcare landscape. However, to fully harness these benefits at both the global 
      and individual levels, further research and development are warranted. This will 
      ensure that we effectively tap into the&nbsp;potential of this synergy, bringing 
      societies closer to a future where personalized healthcare is the norm rather 
      than the exception.
CI  - Copyright © 2023, Temsah et al.
FAU - Temsah, Mohamad-Hani
AU  - Temsah MH
AD  - Pediatric Intensive Care Unit, Department of Pediatrics, King Saud University 
      Medical City, College of Medicine, King Saud University, Riyadh, SAU.
FAU - Jamal, Amr
AU  - Jamal A
AD  - Department of Family and Community Medicine, College of Medicine, King Saud 
      University, Riyadh, SAU.
AD  - Evidence-Based Health Care &amp; Knowledge Translation Research Chair, Department of 
      Family and Community Medicine, College of Medicine, King Saud University, Riyadh, 
      SAU.
FAU - Aljamaan, Fadi
AU  - Aljamaan F
AD  - Department of Critical Care, College of Medicine, King Saud University, Riyadh, 
      SAU.
FAU - Al-Tawfiq, Jaffar A
AU  - Al-Tawfiq JA
AD  - Department of Specialty Internal Medicine and Quality, Johns Hopkins Aramco 
      Healthcare, Dhahran, SAU.
AD  - Infectious Disease Division, Department of Medicine, Indiana University School of 
      Medicine, Indianapolis, USA.
AD  - Infectious Disease Division, Department of Medicine, Johns Hopkins University 
      School of Medicine, Baltimore, USA.
FAU - Al-Eyadhy, Ayman
AU  - Al-Eyadhy A
AD  - Pediatric Intensive Care Unit, Department of Pediatrics, College of Medicine, 
      King Saud University, Riyadh, SAU.
AD  - Pediatric Intensive Care Unit, King Saud University Medical City, Riyadh, SAU.
LA  - eng
PT  - Editorial
DEP - 20230523
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10204616
OTO - NOTNLM
OT  - ai-assisted personalized disease burden
OT  - chatbots
OT  - chatgpt-4
OT  - global burden of disease (gbd)
OT  - personalized healthcare plan
OT  - precision medicine
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/05/24 13:08
MHDA- 2023/05/24 13:09
PMCR- 2023/05/23
CRDT- 2023/05/24 11:46
PHST- 2023/05/23 00:00 [accepted]
PHST- 2023/05/24 13:09 [medline]
PHST- 2023/05/24 13:08 [pubmed]
PHST- 2023/05/24 11:46 [entrez]
PHST- 2023/05/23 00:00 [pmc-release]
AID - 10.7759/cureus.39384 [doi]
PST - epublish
SO  - Cureus. 2023 May 23;15(5):e39384. doi: 10.7759/cureus.39384. eCollection 2023 
      May.

PMID- 37536678
OWN - NLM
STAT- MEDLINE
DCOM- 20230925
LR  - 20240115
IS  - 1555-0273 (Electronic)
IS  - 1555-0265 (Linking)
VI  - 18
IP  - 10
DP  - 2023 Oct 1
TI  - ChatGPT for Sample-Size Calculation in Sports Medicine and Exercise Sciences: A 
      Cautionary Note.
PG  - 1219-1223
LID - 10.1123/ijspp.2023-0109 [doi]
AB  - PURPOSE: To investigate the accuracy of ChatGPT (Chat generative pretrained 
      transformer), a large language model, in calculating sample size for 
      sport-sciences and sports-medicine research studies. METHODS: We conducted an 
      analysis on 4 published papers (ie,&nbsp;examples 1-4) encompassing various study 
      designs and approaches for calculating sample size in 3 sport-science and 
      -medicine journals, including 3 randomized controlled trials and 1 survey paper. 
      We provided ChatGPT with all necessary data such as mean, percentage SD, normal 
      deviates (Zα/2 and Z1-β), and study design. Prompting from 1 example has 
      subsequently been reused to gain insights into the reproducibility of the ChatGPT 
      response. RESULTS: ChatGPT correctly calculated the sample size for 1 randomized 
      controlled trial but failed in the remaining 3 examples, including the incorrect 
      identification of the formula in one example of a survey paper. After interaction 
      with ChatGPT, the correct sample size was obtained for the survey paper. 
      Intriguingly, when the prompt from Example 3 was reused, ChatGPT provided a 
      completely different sample size than its initial response. CONCLUSIONS: While 
      the use of artificial-intelligence tools holds great promise, it should be noted 
      that it might lead to errors and inconsistencies in sample-size calculations even 
      when the tool is fed with the necessary correct information. As 
      artificial-intelligence technology continues to advance and learn from human 
      feedback, there is hope for improvement in sample-size calculation and other 
      research tasks. However, it is important for scientists to exercise caution in 
      utilizing these tools. Future studies should assess more advanced/powerful 
      versions of this tool (ie,&nbsp;ChatGPT4).
FAU - Methnani, Jabeur
AU  - Methnani J
AUID- ORCID: 0000-0002-1422-3059
AD  - LR19ES09, Laboratoire de Physiologie de l'Exercice et Physiopathologie: de 
      l'Intégré au Moléculaire "Biologie, Médecine et Santé," Faculty of Medicine of 
      Sousse, University of&nbsp; Sousse, Sousse,Tunisia.
AD  - High Institute of Sport and Physical Education, Ksar said University of Manouba, 
      Ksar said,Tunisia.
FAU - Latiri, Imed
AU  - Latiri I
AUID- ORCID: 0000-0002-6074-4736
AD  - Research Laboratory LR12SP09 "Heart Failure" Farhat HACHED Hospital, University 
      of Sousse, Sousse,Tunisia.
FAU - Dergaa, Ismail
AU  - Dergaa I
AUID- ORCID: 0000-0001-8091-1856
AD  - Primary Health Care Corporation (PHCC), Doha,Qatar.
AD  - Aspetar, Orthopedic and Sports Medicine Hospital, FIFA Medical Center of 
      Excellence, Doha,Qatar.
AD  - Research Unit Physical Activity, Sport, and Health, UR18JS01, National 
      Observatory of Sport, Tunis,Tunisia.
FAU - Chamari, Karim
AU  - Chamari K
AUID- ORCID: 0000-0001-9178-7678
AD  - High Institute of Sport and Physical Education, University of Sfax, Sfax,Tunisia.
FAU - Ben Saad, Helmi
AU  - Ben Saad H
AUID- ORCID: 0000-0002-7477-2965
AD  - High Institute of Sport and Physical Education, Ksar said University of Manouba, 
      Ksar said,Tunisia.
AD  - Service of Physiology and Functional Explorations, Farhat HACHED Hospital, 
      University of Sousse, Sousse,Tunisia.
LA  - eng
PT  - Journal Article
DEP - 20230803
PL  - United States
TA  - Int J Sports Physiol Perform
JT  - International journal of sports physiology and performance
JID - 101276430
SB  - IM
MH  - Humans
MH  - Reproducibility of Results
MH  - Exercise
MH  - *Sports Medicine
MH  - *Sports
MH  - Research Design
MH  - Randomized Controlled Trials as Topic
OTO - NOTNLM
OT  - artificial intelligence
OT  - effect size
OT  - higher education
OT  - large language model
OT  - methodology
OT  - natural language processing
OT  - peer review
OT  - power
OT  - precision
OT  - research
OT  - sport science
OT  - statistics
EDAT- 2023/08/04 01:07
MHDA- 2023/09/25 06:42
CRDT- 2023/08/03 20:02
PHST- 2023/04/05 00:00 [received]
PHST- 2023/06/04 00:00 [revised]
PHST- 2023/06/05 00:00 [accepted]
PHST- 2023/09/25 06:42 [medline]
PHST- 2023/08/04 01:07 [pubmed]
PHST- 2023/08/03 20:02 [entrez]
AID - 10.1123/ijspp.2023-0109 [doi]
PST - epublish
SO  - Int J Sports Physiol Perform. 2023 Aug 3;18(10):1219-1223. doi: 
      10.1123/ijspp.2023-0109. Print 2023 Oct 1.

PMID- 37794649
OWN - NLM
STAT- Publisher
LR  - 20231005
IS  - 1601-0825 (Electronic)
IS  - 1354-523X (Linking)
DP  - 2023 Oct 4
TI  - How ChatGPT performs in Oral Medicine: The case of oral potentially malignant 
      disorders.
LID - 10.1111/odi.14750 [doi]
FAU - Diniz-Freitas, M
AU  - Diniz-Freitas M
AUID- ORCID: 0000-0001-5349-1091
AD  - Medical-Surgical Dentistry Research Group (OMEQUI), Health Research Institute of 
      Santiago de Compostela (IDIS), University of Santiago de Compostela (USC), A 
      Coruña, Spain.
FAU - Rivas-Mundiña, B
AU  - Rivas-Mundiña B
AUID- ORCID: 0000-0003-4976-395X
AD  - Medical-Surgical Dentistry Research Group (OMEQUI), Health Research Institute of 
      Santiago de Compostela (IDIS), University of Santiago de Compostela (USC), A 
      Coruña, Spain.
FAU - García-Iglesias, J R
AU  - García-Iglesias JR
AD  - Medical-Surgical Dentistry Research Group (OMEQUI), Health Research Institute of 
      Santiago de Compostela (IDIS), University of Santiago de Compostela (USC), A 
      Coruña, Spain.
FAU - García-Mato, E
AU  - García-Mato E
AD  - Medical-Surgical Dentistry Research Group (OMEQUI), Health Research Institute of 
      Santiago de Compostela (IDIS), University of Santiago de Compostela (USC), A 
      Coruña, Spain.
FAU - Diz-Dios, P
AU  - Diz-Dios P
AUID- ORCID: 0000-0002-1483-401X
AD  - Medical-Surgical Dentistry Research Group (OMEQUI), Health Research Institute of 
      Santiago de Compostela (IDIS), University of Santiago de Compostela (USC), A 
      Coruña, Spain.
LA  - eng
PT  - Journal Article
DEP - 20231004
PL  - Denmark
TA  - Oral Dis
JT  - Oral diseases
JID - 9508565
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - oral leukoplakia
OT  - oral potentially malignant disorders
EDAT- 2023/10/05 06:44
MHDA- 2023/10/05 06:44
CRDT- 2023/10/05 01:13
PHST- 2023/08/15 00:00 [revised]
PHST- 2023/04/17 00:00 [received]
PHST- 2023/09/13 00:00 [accepted]
PHST- 2023/10/05 06:44 [medline]
PHST- 2023/10/05 06:44 [pubmed]
PHST- 2023/10/05 01:13 [entrez]
AID - 10.1111/odi.14750 [doi]
PST - aheadofprint
SO  - Oral Dis. 2023 Oct 4. doi: 10.1111/odi.14750.

PMID- 38054196
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231207
IS  - 2666-3287 (Electronic)
IS  - 2666-3287 (Linking)
VI  - 14
DP  - 2024 Mar
TI  - Best practices for implementing ChatGPT, large language models, and artificial 
      intelligence in qualitative and survey-based research.
PG  - 22-23
LID - 10.1016/j.jdin.2023.10.001 [doi]
FAU - Kantor, Jonathan
AU  - Kantor J
AD  - Department of Dermatology, Center for Global Health, and Center for Clinical 
      Epidemiology and Biostatistics, Perelman School of Medicine at the University of 
      Pennsylvania, Philadelphia, Pennsylvania.
AD  - Florida Center for Dermatology, St Augustine, Florida.
LA  - eng
PT  - Editorial
DEP - 20231008
PL  - United States
TA  - JAAD Int
JT  - JAAD international
JID - 101774762
PMC - PMC10694559
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - GPT-4
OT  - Google Bard
OT  - LLM
OT  - Microsoft Bing
OT  - NLP
OT  - QDA
OT  - annotation
OT  - artificial intelligence
OT  - factor analysis
OT  - focus group
OT  - large language models
OT  - medical education
OT  - natural language processing
OT  - practice management
OT  - prompt engineering
OT  - qualitative data analysis software
OT  - qualitative research
COIS- None disclosed.
EDAT- 2023/12/06 06:42
MHDA- 2023/12/06 06:43
PMCR- 2023/10/08
CRDT- 2023/12/06 04:20
PHST- 2023/12/06 06:43 [medline]
PHST- 2023/12/06 06:42 [pubmed]
PHST- 2023/12/06 04:20 [entrez]
PHST- 2023/10/08 00:00 [pmc-release]
AID - S2666-3287(23)00157-8 [pii]
AID - 10.1016/j.jdin.2023.10.001 [doi]
PST - epublish
SO  - JAAD Int. 2023 Oct 8;14:22-23. doi: 10.1016/j.jdin.2023.10.001. eCollection 2024 
      Mar.

PMID- 37815084
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20240216
LR  - 20240314
IS  - 1098-2299 (Electronic)
IS  - 0272-4391 (Linking)
VI  - 85
IP  - 2
DP  - 2024 Apr
TI  - Artificial intelligence utility for drug development: ChatGPT and beyond.
PG  - e22121
LID - 10.1002/ddr.22121 [doi]
FAU - Gurwitz, David
AU  - Gurwitz D
AUID- ORCID: 0000-0002-9363-1869
AD  - Department of Human Molecular Genetics and Biochemistry, Faculty of Medicine, Tel 
      Aviv University, Tel Aviv, Israel.
AD  - Sagol School of Neuroscience, Tel Aviv, Israel.
FAU - Shomron, Noam
AU  - Shomron N
AUID- ORCID: 0000-0001-9913-6124
AD  - Sagol School of Neuroscience, Tel Aviv, Israel.
AD  - Department of Cell and Developmental Biology, Faculty of Medicine, Tel Aviv 
      University, Tel Aviv, Israel.
AD  - Edmond J Safra Center for Bioinformatics, Tel Aviv University, Tel Aviv, Israel.
AD  - Tel Aviv University Innovation Labs (TILabs), Tel Aviv, Israel.
AD  - Djerassi Institute of Oncology, Tel Aviv University, Tel Aviv, Israel.
LA  - eng
GR  - Ministry of Health, State of Israel/
PT  - Journal Article
DEP - 20231010
PL  - United States
TA  - Drug Dev Res
JT  - Drug development research
JID - 8204468
SB  - IM
OTO - NOTNLM
OT  - ChatGPT
OT  - NCBI
OT  - PubMed
OT  - drug development
EDAT- 2023/10/10 12:42
MHDA- 2024/02/16 06:42
CRDT- 2023/10/10 07:13
PHST- 2023/09/20 00:00 [revised]
PHST- 2023/07/19 00:00 [received]
PHST- 2023/10/02 00:00 [accepted]
PHST- 2024/02/16 06:42 [medline]
PHST- 2023/10/10 12:42 [pubmed]
PHST- 2023/10/10 07:13 [entrez]
AID - 10.1002/ddr.22121 [doi]
PST - ppublish
SO  - Drug Dev Res. 2024 Apr;85(2):e22121. doi: 10.1002/ddr.22121. Epub 2023 Oct 10.

PMID- 37465170
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230721
LR  - 20230721
IS  - 2296-2565 (Electronic)
IS  - 2296-2565 (Linking)
VI  - 11
DP  - 2023
TI  - Applying ChatGPT in public health: a SWOT and PESTLE analysis.
PG  - 1225861
LID - 10.3389/fpubh.2023.1225861 [doi]
LID - 1225861
FAU - Morita, Plinio P
AU  - Morita PP
AD  - School of Public Health Sciences, University of Waterloo, Waterloo, ON, Canada.
AD  - Department of Systems Design Engineering, University of Waterloo, Waterloo, ON, 
      Canada.
AD  - Research Institute for Aging, University of Waterloo, Waterloo, ON, Canada.
AD  - Centre for Digital Therapeutics, Techna Institute, University Health Network, 
      Toronto, ON, Canada.
AD  - Dalla Lana School of Public Health, Institute of Health Policy, Management, and 
      Evaluation, University of Toronto, Toronto, ON, Canada.
FAU - Abhari, Shahabeddin
AU  - Abhari S
AD  - School of Public Health Sciences, University of Waterloo, Waterloo, ON, Canada.
FAU - Kaur, Jasleen
AU  - Kaur J
AD  - School of Public Health Sciences, University of Waterloo, Waterloo, ON, Canada.
FAU - Lotto, Matheus
AU  - Lotto M
AD  - School of Public Health Sciences, University of Waterloo, Waterloo, ON, Canada.
AD  - Department of Pediatric Dentistry, Orthodontics, and Public Health, Bauru School 
      of Dentistry, University of São Paulo, Bauru, Brazil.
FAU - Miranda, Pedro Augusto Da Silva E Souza
AU  - Miranda PADSES
AD  - School of Public Health Sciences, University of Waterloo, Waterloo, ON, Canada.
FAU - Oetomo, Arlene
AU  - Oetomo A
AD  - School of Public Health Sciences, University of Waterloo, Waterloo, ON, Canada.
LA  - eng
PT  - Journal Article
DEP - 20230703
PL  - Switzerland
TA  - Front Public Health
JT  - Frontiers in public health
JID - 101616579
SB  - IM
PMC - PMC10350520
OTO - NOTNLM
OT  - ChatGPT
OT  - SWOT analysis
OT  - artificial intelligence
OT  - healthcare
OT  - public health
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2023/07/19 06:42
MHDA- 2023/07/21 06:44
PMCR- 2023/07/03
CRDT- 2023/07/19 03:56
PHST- 2023/05/20 00:00 [received]
PHST- 2023/06/16 00:00 [accepted]
PHST- 2023/07/21 06:44 [medline]
PHST- 2023/07/19 06:42 [pubmed]
PHST- 2023/07/19 03:56 [entrez]
PHST- 2023/07/03 00:00 [pmc-release]
AID - 10.3389/fpubh.2023.1225861 [doi]
PST - epublish
SO  - Front Public Health. 2023 Jul 3;11:1225861. doi: 10.3389/fpubh.2023.1225861. 
      eCollection 2023.

PMID- 37179029
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230822
LR  - 20230830
IS  - 1097-6787 (Electronic)
IS  - 0190-9622 (Linking)
VI  - 89
IP  - 3
DP  - 2023 Sep
TI  - Experimenting with ChatGPT: Concerns for academic medicine.
PG  - e127-e129
LID - S0190-9622(23)00747-8 [pii]
LID - 10.1016/j.jaad.2023.04.045 [doi]
FAU - Hirani, Rahim
AU  - Hirani R
AD  - New York Medical College, Valhalla, New York.
FAU - Farabi, Banu
AU  - Farabi B
AD  - New York Medical College, Valhalla, New York; Department of Dermatology, NYC 
      Health + Hospitals/Metropolitan Medical Center, New York, New York; Department of 
      Medicine, NYC Health + Hospitals/South Brooklyn Health, Ruth Bader Ginsburg 
      Hospital, Brooklyn, New York.
FAU - Marmon, Shoshana
AU  - Marmon S
AD  - New York Medical College, Valhalla, New York; Department of Dermatology, NYC 
      Health + Hospitals/Metropolitan Medical Center, New York, New York; Department of 
      Medicine, NYC Health + Hospitals/South Brooklyn Health, Ruth Bader Ginsburg 
      Hospital, Brooklyn, New York. Electronic address: Shoshana.Marmon@nychhc.org.
LA  - eng
PT  - Comment
PT  - Letter
DEP - 20230511
PL  - United States
TA  - J Am Acad Dermatol
JT  - Journal of the American Academy of Dermatology
JID - 7907132
SB  - IM
CON - NPJ Digit Med. 2023 Apr 26;6(1):75. PMID: 37100871
OTO - NOTNLM
OT  - ChatGPT
OT  - academic publishing
OT  - artificial intelligence (AI)
OT  - dermatology journals
OT  - gender bias
OT  - large language model
OT  - machine learning
OT  - racial bias
OT  - scientific integrity
COIS- Conflicts of interest None disclosed.
EDAT- 2023/05/14 01:07
MHDA- 2023/05/14 01:08
CRDT- 2023/05/13 19:29
PHST- 2023/02/21 00:00 [received]
PHST- 2023/04/04 00:00 [revised]
PHST- 2023/04/10 00:00 [accepted]
PHST- 2023/05/14 01:08 [medline]
PHST- 2023/05/14 01:07 [pubmed]
PHST- 2023/05/13 19:29 [entrez]
AID - S0190-9622(23)00747-8 [pii]
AID - 10.1016/j.jaad.2023.04.045 [doi]
PST - ppublish
SO  - J Am Acad Dermatol. 2023 Sep;89(3):e127-e129. doi: 10.1016/j.jaad.2023.04.045. 
      Epub 2023 May 11.

PMID- 38188345
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240109
IS  - 2296-858X (Print)
IS  - 2296-858X (Electronic)
IS  - 2296-858X (Linking)
VI  - 10
DP  - 2023
TI  - The opportunities and challenges of adopting ChatGPT in medical research.
PG  - 1259640
LID - 10.3389/fmed.2023.1259640 [doi]
LID - 1259640
AB  - PURPOSE: This study aims to investigate the opportunities and challenges of 
      adopting ChatGPT in medical research. METHODS: A qualitative approach with focus 
      groups is adopted in this study. A total of 62 participants including academic 
      researchers from different streams in medicine and eHealth, participated in this 
      study. RESULTS: A total of five themes with 16 sub-themes related to the 
      opportunities; and a total of five themes with 12 sub-themes related to the 
      challenges were identified. The major opportunities include improved data 
      collection and analysis, improved communication and accessibility, and support 
      for researchers in multiple streams of medical research. The major challenges 
      identified were limitations of training data leading to bias, ethical issues, 
      technical limitations, and limitations in data collection and analysis. 
      CONCLUSION: Although ChatGPT can be used as a potential tool in medical research, 
      there is a need for further evidence to generalize its impact on the different 
      research activities.
CI  - Copyright © 2023 Alsadhan, Al-Anezi, Almohanna, Alnaim, Alzahrani, Shinawi, 
      AboAlsamh, Bakhshwain, Alenazy, Arif, Alyousef, Alhamidi, Alghamdi, AlShrayfi, 
      Rubaian, Alanzi, AlSahli, Alturki and Herzallah.
FAU - Alsadhan, Abeer
AU  - Alsadhan A
AD  - Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia.
FAU - Al-Anezi, Fahad
AU  - Al-Anezi F
AD  - Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia.
FAU - Almohanna, Asmaa
AU  - Almohanna A
AD  - Princess Nourah Bint Abdulrahman University, Riyadh, Saudi Arabia.
FAU - Alnaim, Norah
AU  - Alnaim N
AD  - Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia.
FAU - Alzahrani, Hayat
AU  - Alzahrani H
AD  - Northern Border University, Arar, Saudi Arabia.
FAU - Shinawi, Reem
AU  - Shinawi R
AD  - Eastern Health Cluster, Dammam, Saudi Arabia.
FAU - AboAlsamh, Hoda
AU  - AboAlsamh H
AD  - Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia.
FAU - Bakhshwain, Amal
AU  - Bakhshwain A
AD  - Ministry of Health, Riyadh, Riyadh, Saudi Arabia.
FAU - Alenazy, Maha
AU  - Alenazy M
AD  - King Saud University, Riyadh, Riyadh, Saudi Arabia.
FAU - Arif, Wejdan
AU  - Arif W
AD  - King Saud University, Riyadh, Riyadh, Saudi Arabia.
FAU - Alyousef, Seham
AU  - Alyousef S
AD  - King Saud University, Riyadh, Riyadh, Saudi Arabia.
FAU - Alhamidi, Sami
AU  - Alhamidi S
AD  - King Saud University, Riyadh, Riyadh, Saudi Arabia.
FAU - Alghamdi, Alya
AU  - Alghamdi A
AD  - King Saud University, Riyadh, Riyadh, Saudi Arabia.
FAU - AlShrayfi, Nour
AU  - AlShrayfi N
AD  - Public Authority for Applied Education and Training, Kuwait City, Kuwait.
FAU - Rubaian, Nouf Bin
AU  - Rubaian NB
AD  - Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia.
FAU - Alanzi, Turki
AU  - Alanzi T
AD  - Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia.
FAU - AlSahli, Alaa
AU  - AlSahli A
AD  - King Saud bin Abdulaziz University for Health Sciences, Riyadh, Saudi Arabia.
FAU - Alturki, Rasha
AU  - Alturki R
AD  - Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia.
FAU - Herzallah, Nawal
AU  - Herzallah N
AD  - University College London, London, United Kingdom.
LA  - eng
PT  - Journal Article
DEP - 20231222
PL  - Switzerland
TA  - Front Med (Lausanne)
JT  - Frontiers in medicine
JID - 101648047
PMC - PMC10766839
OTO - NOTNLM
OT  - ChatGPT
OT  - challenges and successes
OT  - healthcare
OT  - medical research
OT  - opportunities and application
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2024/01/08 06:42
MHDA- 2024/01/08 06:43
PMCR- 2023/12/22
CRDT- 2024/01/08 05:10
PHST- 2023/07/16 00:00 [received]
PHST- 2023/12/07 00:00 [accepted]
PHST- 2024/01/08 06:43 [medline]
PHST- 2024/01/08 06:42 [pubmed]
PHST- 2024/01/08 05:10 [entrez]
PHST- 2023/12/22 00:00 [pmc-release]
AID - 10.3389/fmed.2023.1259640 [doi]
PST - epublish
SO  - Front Med (Lausanne). 2023 Dec 22;10:1259640. doi: 10.3389/fmed.2023.1259640. 
      eCollection 2023.

PMID- 37880412
OWN - NLM
STAT- MEDLINE
DCOM- 20231216
LR  - 20231216
IS  - 1573-2630 (Electronic)
IS  - 0165-5701 (Linking)
VI  - 43
IP  - 12
DP  - 2023 Dec
TI  - A comparative study on the knowledge levels of artificial intelligence programs 
      in diagnosing ophthalmic pathologies and intraocular tumors evaluated their 
      superiority and potential utility.
PG  - 4905-4909
LID - 10.1007/s10792-023-02893-x [doi]
AB  - PURPOSE: This study aimed to test the knowledge levels of ChatGPT, Bing, and Bard 
      artificial intelligence chatbots, which have been released by three different 
      manufacturers, about ophthalmic pathologies and intraocular tumors, to test their 
      usability and to investigate the presence of superiority to each other. METHODS: 
      Thirty-six questions were obtained from the American Academy and Ophthalmology 
      2022-2023 Basic and Clinical Science Course Ophthalmic Pathology and Intraocular 
      Tumor study questions section. Each question was asked separately for the 
      ChatGPT, Bing, and Bard artificial intelligence programs. Answers to the 
      questions were categorized as correct or incorrect. The statistical relationship 
      between the correct and incorrect response rates of the artificial intelligence 
      programs was determined. RESULTS: From the artificial intelligence chatbots, 
      ChatGPT gave the correct answer to 58.6% of the questions asked, Bing gave the 
      correct answer to 63.9%, and Bard gave the correct answer to 69.4%. No 
      statistical significance was found between the rates of correct answers to the 
      questions in all 3 artificial intelligence programs (p = 0.705, Pearson 
      Chi-square test). CONCLUSION: Artificial intelligence chatbots can be used to 
      access information related to ophthalmic pathologies and intraocular tumors. 
      However, in the evaluation of the data, it should be noted that not all questions 
      can be answered correctly. Care should be taken when examining the answers.
CI  - © 2023. The Author(s), under exclusive licence to Springer Nature B.V.
FAU - Sensoy, Eyupcan
AU  - Sensoy E
AUID- ORCID: 0000-0002-4401-8435
AD  - Ankara Etlik City Hospital, Ankara, Turkey. dreyupcansensoy@yahoo.com.
FAU - Citirik, Mehmet
AU  - Citirik M
AUID- ORCID: 0000-0002-0558-5576
AD  - Ankara Etlik City Hospital, Ankara, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20231026
PL  - Netherlands
TA  - Int Ophthalmol
JT  - International ophthalmology
JID - 7904294
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Neoplasms
MH  - *Ophthalmology
OTO - NOTNLM
OT  - Bard
OT  - Bing
OT  - ChatGPT
OT  - Intraocular tumors
OT  - Ophthalmic pathology
EDAT- 2023/10/26 00:42
MHDA- 2023/12/17 09:42
CRDT- 2023/10/25 23:40
PHST- 2023/08/19 00:00 [received]
PHST- 2023/09/27 00:00 [accepted]
PHST- 2023/12/17 09:42 [medline]
PHST- 2023/10/26 00:42 [pubmed]
PHST- 2023/10/25 23:40 [entrez]
AID - 10.1007/s10792-023-02893-x [pii]
AID - 10.1007/s10792-023-02893-x [doi]
PST - ppublish
SO  - Int Ophthalmol. 2023 Dec;43(12):4905-4909. doi: 10.1007/s10792-023-02893-x. Epub 
      2023 Oct 26.

PMID- 37181995
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230516
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 4
DP  - 2023 Apr
TI  - Bilateral Vocal Fold Paralysis in a Patient With Neurosarcoidosis: A 
      ChatGPT-Driven Case Report Describing an Unusual Presentation.
PG  - e37368
LID - 10.7759/cureus.37368 [doi]
LID - e37368
AB  - This ChatGPT-driven case report describes a unique presentation of 
      neurosarcoidosis. The patient, a 58-year-old female, initially presented with 
      hoarseness and was found to have bilateral jugular foramen tumors and thoracic 
      lymphadenopathy. Imaging revealed significant enlargement and thickening of the 
      vagus nerve and a separate mass of the cervical sympathetic trunk. The patient 
      was referred for an ultrasound-guided biopsy of the abnormal neck masses to 
      establish a pathologic diagnosis. The patient subsequently underwent neck 
      dissection for exposure of the vagus nerve and isolation of the great vessels in 
      preparation for a transmastoid approach to the skull base. The presence of 
      multifocal tumors prompted the need for a biopsy, which ultimately revealed 
      sarcoid granulomas in the nervous system. The patient was diagnosed with 
      neurosarcoidosis. This case highlights the potential for sarcoidosis to affect 
      the nervous system, with multiple cranial nerve involvement, seizures, and 
      cognitive impairment. It also emphasizes the need for a combination of clinical, 
      radiological, and pathological findings for an accurate diagnosis of 
      neurosarcoidosis. Additionally, this case highlights the utility of natural 
      language processing (NLP), as the entire case report was written using ChatGPT. 
      This report serves as a comparison of the quality of case reports generated by 
      humans versus NLP algorithms. The original case report can be found in the 
      references.
CI  - Copyright © 2023, Guirguis et al.
FAU - Guirguis, Christopher A
AU  - Guirguis CA
AD  - Otolaryngology - Head and Neck Surgery, MedStar Georgetown University Hospital, 
      Washington, D.C., USA.
FAU - Crossley, Jason R
AU  - Crossley JR
AD  - Otolaryngology - Head and Neck Surgery, MedStar Georgetown University Hospital, 
      Washington, D.C., USA.
FAU - Malekzadeh, Sonya
AU  - Malekzadeh S
AD  - Otolaryngology - Head and Neck Surgery, MedStar Georgetown University Hospital, 
      Washington, D.C., USA.
LA  - eng
PT  - Case Reports
DEP - 20230410
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10171033
OTO - NOTNLM
OT  - bilateral vocal fold paralysis
OT  - case report
OT  - chatgpt
OT  - granulomas
OT  - inflammatory disease
OT  - neurosarcoidosis
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/05/14 19:13
MHDA- 2023/05/14 19:14
PMCR- 2023/04/10
CRDT- 2023/05/14 12:26
PHST- 2023/04/10 00:00 [accepted]
PHST- 2023/05/14 19:14 [medline]
PHST- 2023/05/14 19:13 [pubmed]
PHST- 2023/05/14 12:26 [entrez]
PHST- 2023/04/10 00:00 [pmc-release]
AID - 10.7759/cureus.37368 [doi]
PST - epublish
SO  - Cureus. 2023 Apr 10;15(4):e37368. doi: 10.7759/cureus.37368. eCollection 2023 
      Apr.

PMID- 37025739
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230411
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - Toxic Epidermal Necrolysis in a Critically Ill African American Woman: A Case 
      Report Written With ChatGPT Assistance.
PG  - e35742
LID - 10.7759/cureus.35742 [doi]
LID - e35742
AB  - Stevens-Johnson syndrome (SJS) and toxic epidermal necrolysis (TEN) are 
      life-threatening spectrum diseases in which a medication triggers a mucocutaneous 
      reaction associated with severe necrosis and loss of epidermal integrity. The 
      disease has a high mortality rate that can be assessed by dermatology scoring 
      scales based on an affected total body surface area (TBSA). Sloughing of &lt;10% 
      TBSA is considered SJS, with a mortality of 10%. Sloughing of &gt;30% TBSA is termed 
      TEN, with an increased mortality rate of 25% to 35%. We present a case and 
      management of TEN that involved &gt;30% TBSA in a critically ill African American 
      woman. Identification of the offending agent was difficult due to complicated 
      medication exposure throughout her multi-facility care management. This case 
      conveys the importance of close monitoring of a critically ill patient during a 
      clinical course involving SJS-/TEN-inducing drugs. We also discuss the potential 
      increased risks for SJS/TEN in the African American population due to genetic or 
      epigenetic predispositions to skin conditions. This case report also contributes 
      to increasing skin of color representation in the current literature. 
      Additionally, we discuss the use of&nbsp;Chat Generative Pre-trained Transformer 
      (ChatGPT, OpenAI LP, OpenAI Inc., San Francisco, CA, USA) and list its benefits 
      and errors.
CI  - Copyright © 2023, Lantz et al.
FAU - Lantz, Rebekah
AU  - Lantz R
AD  - Medicine, Miami Valley Hospital, Dayton, USA.
LA  - eng
PT  - Case Reports
DEP - 20230303
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10072179
OTO - NOTNLM
OT  - chatgpt
OT  - dermatologic drug reaction
OT  - drug rash
OT  - skin of color
OT  - stevens-johnson syndrome (sjs)
OT  - toxic epidermal necrolysis (ten)
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/04/08 06:00
MHDA- 2023/04/08 06:01
PMCR- 2023/03/03
CRDT- 2023/04/07 02:34
PHST- 2023/03/03 00:00 [accepted]
PHST- 2023/04/08 06:01 [medline]
PHST- 2023/04/07 02:34 [entrez]
PHST- 2023/04/08 06:00 [pubmed]
PHST- 2023/03/03 00:00 [pmc-release]
AID - 10.7759/cureus.35742 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 3;15(3):e35742. doi: 10.7759/cureus.35742. eCollection 2023 Mar.

PMID- 37190006
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230519
IS  - 2227-9067 (Print)
IS  - 2227-9067 (Electronic)
IS  - 2227-9067 (Linking)
VI  - 10
IP  - 4
DP  - 2023 Apr 21
TI  - May Artificial Intelligence Influence Future Pediatric Research?-The Case of 
      ChatGPT.
LID - 10.3390/children10040757 [doi]
LID - 757
AB  - BACKGROUND: In recent months, there has been growing interest in the potential of 
      artificial intelligence (AI) to revolutionize various aspects of medicine, 
      including research, education, and clinical practice. ChatGPT represents a 
      leading AI language model, with possible unpredictable effects on the quality of 
      future medical research, including clinical decision-making, medical education, 
      drug development, and better research outcomes. AIM AND METHODS: In this 
      interview with ChatGPT, we explore the potential impact of AI on future pediatric 
      research. Our discussion covers a range of topics, including the potential 
      positive effects of AI, such as improved clinical decision-making, enhanced 
      medical education, faster drug development, and better research outcomes. We also 
      examine potential negative effects, such as bias and fairness concerns, safety 
      and security issues, overreliance on technology, and ethical considerations. 
      CONCLUSIONS: While AI continues to advance, it is crucial to remain vigilant 
      about the possible risks and limitations of these technologies and to consider 
      the implications of these technologies and their use in the medical field. The 
      development of AI language models represents a significant advancement in the 
      field of artificial intelligence and has the potential to revolutionize daily 
      clinical practice in every branch of medicine, both surgical and clinical. 
      Ethical and social implications must also be considered to ensure that these 
      technologies are used in a responsible and beneficial manner.
FAU - Corsello, Antonio
AU  - Corsello A
AUID- ORCID: 0000-0003-4578-0066
AD  - Department of Clinical Sciences and Community Health, University of Milan, 20122 
      Milan, Italy.
FAU - Santangelo, Andrea
AU  - Santangelo A
AUID- ORCID: 0000-0003-2668-6373
AD  - Department of Pediatrics, Santa Chiara Hospital, University of Pisa, 56126 Pisa, 
      Italy.
LA  - eng
PT  - Journal Article
DEP - 20230421
PL  - Switzerland
TA  - Children (Basel)
JT  - Children (Basel, Switzerland)
JID - 101648936
PMC - PMC10136583
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - clinical decision-making
OT  - pediatric research
OT  - predictive modeling
COIS- The authors declare that they have no competing interest.
EDAT- 2023/05/16 06:42
MHDA- 2023/05/16 06:43
PMCR- 2023/04/21
CRDT- 2023/05/16 01:12
PHST- 2023/03/13 00:00 [received]
PHST- 2023/04/17 00:00 [revised]
PHST- 2023/04/19 00:00 [accepted]
PHST- 2023/05/16 06:43 [medline]
PHST- 2023/05/16 06:42 [pubmed]
PHST- 2023/05/16 01:12 [entrez]
PHST- 2023/04/21 00:00 [pmc-release]
AID - children10040757 [pii]
AID - children-10-00757 [pii]
AID - 10.3390/children10040757 [doi]
PST - epublish
SO  - Children (Basel). 2023 Apr 21;10(4):757. doi: 10.3390/children10040757.

PMID- 37851495
OWN - NLM
STAT- MEDLINE
DCOM- 20231023
LR  - 20231104
IS  - 1438-8871 (Electronic)
IS  - 1439-4456 (Print)
IS  - 1438-8871 (Linking)
VI  - 25
DP  - 2023 Oct 18
TI  - Health Care Trainees' and Professionals' Perceptions of ChatGPT in Improving 
      Medical Knowledge Training: Rapid Survey Study.
PG  - e49385
LID - 10.2196/49385 [doi]
LID - e49385
AB  - BACKGROUND: ChatGPT is a powerful pretrained large language model. It has both 
      demonstrated potential and raised concerns related to knowledge translation and 
      knowledge transfer. To apply and improve knowledge transfer in the real world, it 
      is essential to assess the perceptions and acceptance of the users of 
      ChatGPT-assisted training. OBJECTIVE: We aimed to investigate the perceptions of 
      health care trainees and professionals on ChatGPT-assisted training, using 
      biomedical informatics as an example. METHODS: We used purposeful sampling to 
      include all health care undergraduate trainees and graduate professionals (n=195) 
      from January to May 2023 in the School of Public Health at the National Defense 
      Medical Center in Taiwan. Subjects were asked to watch a 2-minute video 
      introducing 5 scenarios about ChatGPT-assisted training in biomedical informatics 
      and then answer a self-designed online (web- and mobile-based) questionnaire 
      according to the Kirkpatrick model. The survey responses were used to develop 4 
      constructs: "perceived knowledge acquisition," "perceived training motivation," 
      "perceived training satisfaction," and "perceived training effectiveness." The 
      study used structural equation modeling (SEM) to evaluate and test the structural 
      model and hypotheses. RESULTS: The online questionnaire response rate was 152 of 
      195 (78%); 88 of 152 participants (58%) were undergraduate trainees and 90 of 152 
      participants (59%) were women. The ages ranged from 18 to 53 years (mean 23.3, SD 
      6.0 years). There was no statistical difference in perceptions of training 
      evaluation between men and women. Most participants were enthusiastic about the 
      ChatGPT-assisted training, while the graduate professionals were more 
      enthusiastic than undergraduate trainees. Nevertheless, some concerns were raised 
      about potential cheating on training assessment. The average scores for knowledge 
      acquisition, training motivation, training satisfaction, and training 
      effectiveness were 3.84 (SD 0.80), 3.76 (SD 0.93), 3.75 (SD 0.87), and 3.72 (SD 
      0.91), respectively (Likert scale 1-5: strongly disagree to strongly agree). 
      Knowledge acquisition had the highest score and training effectiveness the 
      lowest. In the SEM results, training effectiveness was influenced predominantly 
      by knowledge acquisition and partially met the hypotheses in the research 
      framework. Knowledge acquisition had a direct effect on training effectiveness, 
      training satisfaction, and training motivation, with β coefficients of .80, .87, 
      and .97, respectively (all P&lt;.001). CONCLUSIONS: Most health care trainees and 
      professionals perceived ChatGPT-assisted training as an aid in knowledge 
      transfer. However, to improve training effectiveness, it should be combined with 
      empirical experts for proper guidance and dual interaction. In a future study, we 
      recommend using a larger sample size for evaluation of internet-connected large 
      language models in medical knowledge transfer.
CI  - ©Je-Ming Hu, Feng-Cheng Liu, Chi-Ming Chu, Yu-Tien Chang. Originally published in 
      the Journal of Medical Internet Research (https://www.jmir.org), 18.10.2023.
FAU - Hu, Je-Ming
AU  - Hu JM
AUID- ORCID: 0000-0002-7377-0984
AD  - Division of Colorectal Surgery, Department of Surgery, Tri-service General 
      Hospital, National Defense Medical Center, Taipei, Taiwan.
AD  - Graduate Institute of Medical Sciences, National Defense Medical Center, Taipei, 
      Taiwan.
AD  - School of Medicine, National Defense Medical Center, Taipei, Taiwan.
FAU - Liu, Feng-Cheng
AU  - Liu FC
AUID- ORCID: 0000-0003-1291-8946
AD  - Division of Rheumatology/Immunology and Allergy, Department of Medicine, 
      Tri-Service General Hospital, National Defense Medical Center, Taipei, Taiwan.
FAU - Chu, Chi-Ming
AU  - Chu CM
AUID- ORCID: 0000-0001-8563-1378
AD  - Graduate Institute of Medical Sciences, National Defense Medical Center, Taipei, 
      Taiwan.
AD  - School of Public Health, National Defense Medical Center, Taipei, Taiwan.
AD  - Graduate Institute of Life Sciences, National Defense Medical Center, Taipei, 
      Taiwan.
AD  - Big Data Research Center, College of Medicine, Fu-Jen Catholic University, New 
      Taipei City, Taiwan.
AD  - Department of Public Health, Kaohsiung Medical University, Kaohsiung, Taiwan.
AD  - Department of Public Health, China Medical University, Taichung, Taiwan.
FAU - Chang, Yu-Tien
AU  - Chang YT
AUID- ORCID: 0000-0002-1801-7041
AD  - School of Public Health, National Defense Medical Center, Taipei, Taiwan.
LA  - eng
PT  - Journal Article
DEP - 20231018
PL  - Canada
TA  - J Med Internet Res
JT  - Journal of medical Internet research
JID - 100959882
SB  - IM
MH  - Adolescent
MH  - Adult
MH  - Female
MH  - Humans
MH  - Male
MH  - Middle Aged
MH  - Young Adult
MH  - *Attitude of Health Personnel
MH  - Medicine
MH  - Perception
MH  - *Students
MH  - Surveys and Questionnaires
MH  - Taiwan
MH  - *Artificial Intelligence
PMC - PMC10620632
OTO - NOTNLM
OT  - ChatGPT
OT  - SEM
OT  - internet survey
OT  - large language model
OT  - medicine
OT  - perception evaluation
OT  - structural equation modeling
COIS- Conflicts of Interest: None declared.
EDAT- 2023/10/18 12:42
MHDA- 2023/10/23 01:18
PMCR- 2023/10/18
CRDT- 2023/10/18 11:53
PHST- 2023/05/29 00:00 [received]
PHST- 2023/09/29 00:00 [accepted]
PHST- 2023/07/13 00:00 [revised]
PHST- 2023/10/23 01:18 [medline]
PHST- 2023/10/18 12:42 [pubmed]
PHST- 2023/10/18 11:53 [entrez]
PHST- 2023/10/18 00:00 [pmc-release]
AID - v25i1e49385 [pii]
AID - 10.2196/49385 [doi]
PST - epublish
SO  - J Med Internet Res. 2023 Oct 18;25:e49385. doi: 10.2196/49385.

PMID- 37731897
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231102
IS  - 2666-6065 (Electronic)
IS  - 2666-6065 (Linking)
VI  - 41
DP  - 2023 Dec
TI  - ChatGPT: promise and challenges for deployment in low- and middle-income 
      countries.
PG  - 100905
LID - 10.1016/j.lanwpc.2023.100905 [doi]
LID - 100905
AB  - In low- and middle-income countries (LMICs), the fields of medicine and public 
      health grapple with numerous challenges that continue to hinder patients' access 
      to healthcare services. ChatGPT, a publicly accessible chatbot, has emerged as a 
      potential tool in aiding public health efforts in LMICs. This viewpoint details 
      the potential benefits of employing ChatGPT in LMICs to improve medicine and 
      public health encompassing a broad spectrum of domains ranging from health 
      literacy, screening, triaging, remote healthcare support, mental health support, 
      multilingual capabilities, healthcare communication and documentation, medical 
      training and education, and support for healthcare professionals. Additionally, 
      we also share potential concerns and limitations associated with the use of 
      ChatGPT and provide a balanced discussion on the opportunities and challenges of 
      using ChatGPT in LMICs.
CI  - © 2023 The Author(s).
FAU - Wang, Xiaofei
AU  - Wang X
AD  - Key Laboratory for Biomechanics and Mechanobiology of Ministry of Education, 
      Beijing Advanced Innovation Center for Biomedical Engineering, School of 
      Biological Science and Medical Engineering, Beihang University, Beijing, China.
FAU - Sanders, Hayley M
AU  - Sanders HM
AD  - Section of Plastic Surgery, Department of Surgery, University of Michigan Medical 
      School, Ann Arbor, MI, USA.
FAU - Liu, Yuchen
AU  - Liu Y
AD  - Key Laboratory for Biomechanics and Mechanobiology of Ministry of Education, 
      Beijing Advanced Innovation Center for Biomedical Engineering, School of 
      Biological Science and Medical Engineering, Beihang University, Beijing, China.
FAU - Seang, Kennarey
AU  - Seang K
AD  - Grant Management Office, University of Health Sciences, Phnom Penh, Cambodia.
FAU - Tran, Bach Xuan
AU  - Tran BX
AD  - Department of Health Economics, Institute for Preventive Medicine and Public 
      Health, Hanoi Medical University, Hanoi, Vietnam.
AD  - Institute of Health Economics and Technology, Hanoi, Vietnam.
FAU - Atanasov, Atanas G
AU  - Atanasov AG
AD  - Ludwig Boltzmann Institute Digital Health and Patient Safety, Medical University 
      of Vienna, Spitalgasse 23, 1090, Vienna, Austria.
AD  - Institute of Genetics and Animal Biotechnology of the Polish Academy of Sciences, 
      Jastrzebiec, 05-552, Magdalenka, Poland.
FAU - Qiu, Yue
AU  - Qiu Y
AD  - Institute for Hospital Management, Tsinghua University, Beijing, China.
FAU - Tang, Shenglan
AU  - Tang S
AD  - Duke Global Health Institute, Duke University, Durham, NC, USA.
FAU - Car, Josip
AU  - Car J
AD  - Centre for Population Health Sciences, Lee Kong Chian School of Medicine, Nanyang 
      Technological University Singapore, Singapore.
AD  - Department of Primary Care and Public Health, School of Public Health, Imperial 
      College London, London, United Kingdom.
FAU - Wang, Ya Xing
AU  - Wang YX
AD  - Beijing Institute of Ophthalmology, Beijing Ophthalmology and Visual Science Key 
      Lab, Beijing Tongren Eye Center, Beijing Tongren Hospital, Capital Medical 
      University, Beijing, China.
FAU - Wong, Tien Yin
AU  - Wong TY
AD  - Singapore Eye Research Institute, Singapore National Eye Centre, Singapore.
AD  - Tsinghua Medicine, Tsinghua University, Beijing, China.
AD  - School of Clinical Medicine, Beijing Tsinghua Changgung Hospital, Beijing, China.
FAU - Tham, Yih-Chung
AU  - Tham YC
AD  - Singapore Eye Research Institute, Singapore National Eye Centre, Singapore.
AD  - Centre for Innovation and Precision Eye Health, Department of Ophthalmology, Yong 
      Loo Lin School of Medicine, National University of Singapore, Singapore.
AD  - Ophthalmology and Visual Science Academic Clinical Program, Duke-NUS Medical 
      School, Singapore.
FAU - Chung, Kevin C
AU  - Chung KC
AD  - Section of Plastic Surgery, Department of Surgery, University of Michigan Medical 
      School, Ann Arbor, MI, USA.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20230915
PL  - England
TA  - Lancet Reg Health West Pac
JT  - The Lancet regional health. Western Pacific
JID - 101774968
PMC - PMC10507635
OTO - NOTNLM
OT  - ChatGPT
OT  - Equity
OT  - Global health
OT  - Large language model
OT  - Low to middle income countries
OT  - Public health
COIS- YQ received payment from Asia Pacific Medical Technology Association for 
      presentation. TYW received editorial support and medical writing from ApotheCom, 
      study funding and article processing charges from Bayer AG, Leverkusen, Germany; 
      funding of editorial support and medical writing from Bayer Consumer Care AG, 
      Basel, Switzerland; study funding from Regeneron Pharmaceuticals, Inc; consulting 
      fees from Aldropika Therapeutics, Bayer, Boehringer Ingelheim, Genetech, Iveric 
      Bio, Novartis, Oxurion, Plano, Roche, Sanofi and Shanghai Henlius. He is also an 
      inventor, patent-holder and a cofounder of start-up companies EyRiS and Visre. KC 
      received funding from the National Institutes of Health, book royalties from 
      Wolters Kluwer and Elsevier, and a research grant from Sonex to study carpal 
      tunnel outcomes.
EDAT- 2023/09/21 06:42
MHDA- 2023/09/21 06:43
PMCR- 2023/09/15
CRDT- 2023/09/21 04:06
PHST- 2023/05/09 00:00 [received]
PHST- 2023/08/14 00:00 [revised]
PHST- 2023/09/03 00:00 [accepted]
PHST- 2023/09/21 06:43 [medline]
PHST- 2023/09/21 06:42 [pubmed]
PHST- 2023/09/21 04:06 [entrez]
PHST- 2023/09/15 00:00 [pmc-release]
AID - S2666-6065(23)00223-7 [pii]
AID - 100905 [pii]
AID - 10.1016/j.lanwpc.2023.100905 [doi]
PST - epublish
SO  - Lancet Reg Health West Pac. 2023 Sep 15;41:100905. doi: 
      10.1016/j.lanwpc.2023.100905. eCollection 2023 Dec.

PMID- 37940756
OWN - NLM
STAT- MEDLINE
DCOM- 20240328
LR  - 20240330
IS  - 1525-1497 (Electronic)
IS  - 0884-8734 (Print)
IS  - 0884-8734 (Linking)
VI  - 39
IP  - 4
DP  - 2024 Mar
TI  - New Frontiers in Health Literacy: Using ChatGPT to Simplify Health Information 
      for People in the Community.
PG  - 573-577
LID - 10.1007/s11606-023-08469-w [doi]
AB  - BACKGROUND: Most health information does not meet the health literacy needs of 
      our communities. Writing health information in plain language is time-consuming 
      but the release of tools like ChatGPT may make it easier to produce reliable 
      plain language health information. OBJECTIVE: To investigate the capacity for 
      ChatGPT to produce plain language versions of health texts. DESIGN: Observational 
      study of 26 health texts from reputable websites. METHODS: ChatGPT was prompted 
      to 'rewrite the text for people with low literacy'. Researchers captured three 
      revised versions of each original text. MAIN MEASURES: Objective health literacy 
      assessment, including Simple Measure of Gobbledygook (SMOG), proportion of the 
      text that contains complex language (%), number of instances of passive voice and 
      subjective ratings of key messages retained (%). KEY RESULTS: On average, 
      original texts were written at grade 12.8 (SD = 2.2) and revised to grade 11.0 
      (SD = 1.2), p &lt; 0.001. Original texts were on average 22.8% complex (SD = 7.5%) 
      compared to 14.4% (SD = 5.6%) in revised texts, p &lt; 0.001. Original texts had on 
      average 4.7 instances (SD = 3.2) of passive text compared to 1.7 (SD = 1.2) in 
      revised texts, p &lt; 0.001. On average 80% of key messages were retained 
      (SD = 15.0). The more complex original texts showed more improvements than less 
      complex original texts. For example, when original texts were ≥ grade 13, revised 
      versions improved by an average 3.3 grades (SD = 2.2), p &lt; 0.001. Simpler 
      original texts (&lt; grade 11) improved by an average 0.5 grades (SD = 1.4), 
      p &lt; 0.001. CONCLUSIONS: This study used multiple objective assessments of health 
      literacy to demonstrate that ChatGPT can simplify health information while 
      retaining most key messages. However, the revised texts typically did not meet 
      health literacy targets for grade reading score, and improvements were marginal 
      for texts that were already relatively simple.
CI  - © 2023. The Author(s).
FAU - Ayre, Julie
AU  - Ayre J
AUID- ORCID: 0000-0002-5279-5189
AD  - Sydney Health Literacy Lab, Sydney School of Public Health, Faculty of Medicine 
      and Health, The University of Sydney, Rm 128C Edward Ford Building, Sydney, NSW, 
      Australia. Julie.ayre@sydney.edu.au.
FAU - Mac, Olivia
AU  - Mac O
AD  - Sydney Health Literacy Lab, Sydney School of Public Health, Faculty of Medicine 
      and Health, The University of Sydney, Rm 128C Edward Ford Building, Sydney, NSW, 
      Australia.
FAU - McCaffery, Kirsten
AU  - McCaffery K
AD  - Sydney Health Literacy Lab, Sydney School of Public Health, Faculty of Medicine 
      and Health, The University of Sydney, Rm 128C Edward Ford Building, Sydney, NSW, 
      Australia.
FAU - McKay, Brad R
AU  - McKay BR
AD  - Sydney Health Literacy Lab, Sydney School of Public Health, Faculty of Medicine 
      and Health, The University of Sydney, Rm 128C Edward Ford Building, Sydney, NSW, 
      Australia.
FAU - Liu, Mingyi
AU  - Liu M
AD  - Sydney Health Literacy Lab, Sydney School of Public Health, Faculty of Medicine 
      and Health, The University of Sydney, Rm 128C Edward Ford Building, Sydney, NSW, 
      Australia.
FAU - Shi, Yi
AU  - Shi Y
AD  - Sydney Health Literacy Lab, Sydney School of Public Health, Faculty of Medicine 
      and Health, The University of Sydney, Rm 128C Edward Ford Building, Sydney, NSW, 
      Australia.
FAU - Rezwan, Atria
AU  - Rezwan A
AD  - Sydney Health Literacy Lab, Sydney School of Public Health, Faculty of Medicine 
      and Health, The University of Sydney, Rm 128C Edward Ford Building, Sydney, NSW, 
      Australia.
FAU - Dunn, Adam G
AU  - Dunn AG
AD  - Discipline of Biomedical Informatics and Digital Health, School of Medical 
      Sciences, Faculty of Medicine and Health, The University of Sydney, Sydney, NSW, 
      Australia.
LA  - eng
PT  - Journal Article
PT  - Observational Study
DEP - 20231108
PL  - United States
TA  - J Gen Intern Med
JT  - Journal of general internal medicine
JID - 8605834
SB  - IM
MH  - Humans
MH  - *Health Literacy
MH  - Comprehension
MH  - Language
MH  - Reading
PMC - PMC10973278
OTO - NOTNLM
OT  - ChatGPT
OT  - health communication
OT  - health literacy
OT  - patient education
COIS- Members of the research team (JA, KM) are directors of a health literacy 
      consultancy (Health Literacy Solutions Ltd., Pty). No other declared conflicts of 
      interest.
EDAT- 2023/11/09 00:42
MHDA- 2024/03/28 06:46
PMCR- 2023/11/08
CRDT- 2023/11/08 23:26
PHST- 2023/07/10 00:00 [received]
PHST- 2023/10/06 00:00 [accepted]
PHST- 2024/03/28 06:46 [medline]
PHST- 2023/11/09 00:42 [pubmed]
PHST- 2023/11/08 23:26 [entrez]
PHST- 2023/11/08 00:00 [pmc-release]
AID - 10.1007/s11606-023-08469-w [pii]
AID - 8469 [pii]
AID - 10.1007/s11606-023-08469-w [doi]
PST - ppublish
SO  - J Gen Intern Med. 2024 Mar;39(4):573-577. doi: 10.1007/s11606-023-08469-w. Epub 
      2023 Nov 8.

PMID- 38502607
OWN - NLM
STAT- Publisher
LR  - 20240319
IS  - 1538-9855 (Electronic)
IS  - 0363-3624 (Linking)
DP  - 2024 Mar 18
TI  - Work With ChatGPT, Not Against: 3 Teaching Strategies That Harness the Power of 
      Artificial Intelligence.
LID - 10.1097/NNE.0000000000001634 [doi]
AB  - BACKGROUND: Technological advances have expanded nursing education to include 
      generative artificial intelligence (AI) tools such as ChatGPT. PROBLEM: 
      Generative AI tools challenge academic integrity, pose a challenge to validating 
      information accuracy, and require strategies to ensure the credibility of 
      AI-generated information. APPROACH: This article presents a dual-purpose approach 
      integrating AI tools into prelicensure nursing education to enhance learning 
      while promoting critical evaluation skills. Constructivist theories and 
      Vygotsky's Zone of Proximal Development framework support this integration, with 
      AI as a scaffold for developing critical thinking. OUTCOMES: The approach 
      involves practical activities for students to engage with AI-generated content 
      critically, thereby reinforcing clinical judgment and preparing them for 
      AI-prevalent health care environments. CONCLUSIONS: Incorporating AI tools such 
      as ChatGPT into nursing curricula represents a strategic educational advancement, 
      equipping students with essential skills to navigate modern health care.
CI  - Copyright © 2024 Wolters Kluwer Health, Inc. All rights reserved.
FAU - Simms, Rachel Cox
AU  - Simms RC
AD  - Author Affiliation: Assistant Professor, School of Nursing, MGH Institute of 
      Health Professions, Boston, Massachusetts.
LA  - eng
PT  - Journal Article
DEP - 20240318
PL  - United States
TA  - Nurse Educ
JT  - Nurse educator
JID - 7701902
COIS- The author declares no conflicts of interest.
EDAT- 2024/03/19 18:42
MHDA- 2024/03/19 18:42
CRDT- 2024/03/19 13:23
PHST- 2024/03/19 18:42 [medline]
PHST- 2024/03/19 18:42 [pubmed]
PHST- 2024/03/19 13:23 [entrez]
AID - 00006223-990000000-00430 [pii]
AID - 10.1097/NNE.0000000000001634 [doi]
PST - aheadofprint
SO  - Nurse Educ. 2024 Mar 18. doi: 10.1097/NNE.0000000000001634.

PMID- 37041067
OWN - NLM
STAT- MEDLINE
DCOM- 20230717
LR  - 20231116
IS  - 1468-2060 (Electronic)
IS  - 0003-4967 (Print)
IS  - 0003-4967 (Linking)
VI  - 82
IP  - 8
DP  - 2023 Aug
TI  - ChatGPT: when artificial intelligence replaces the rheumatologist in medical 
      writing.
PG  - 1015-1017
LID - 10.1136/ard-2023-223936 [doi]
AB  - In this editorial we discuss the place of artificial intelligence (AI) in the 
      writing of scientific articles and especially editorials. We asked chatGPT « to 
      write an editorial for Annals of Rheumatic Diseases about how AI may replace the 
      rheumatologist in editorial writing ». chatGPT's response is diplomatic and 
      describes AI as a tool to help the rheumatologist but not replace him. AI is 
      already used in medicine, especially in image analysis, but the domains are 
      infinite and it is possible that AI could quickly help or replace rheumatologists 
      in the writing of scientific articles. We discuss the ethical aspects and the 
      future role of rheumatologists.
CI  - © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No 
      commercial re-use. See rights and permissions. Published by BMJ.
FAU - Verhoeven, Frank
AU  - Verhoeven F
AUID- ORCID: 0000-0003-2708-2918
AD  - Rheumatology, CHU Besancon, Besancon, France.
AD  - EA 4267 PEPITE, Université de Franche-Comté, Besancon, France.
FAU - Wendling, Daniel
AU  - Wendling D
AUID- ORCID: 0000-0002-4687-5780
AD  - Rheumatology, CHU Besancon, Besancon, France.
AD  - EA4266 EPILAB, Université de Franche-Comté, Besancon, France.
FAU - Prati, Clément
AU  - Prati C
AD  - Rheumatology, CHU Besancon, Besancon, France cprati@chu-besancon.fr.
AD  - EA 4267 PEPITE, Université de Franche-Comté, Besancon, France.
LA  - eng
PT  - Journal Article
DEP - 20230411
PL  - England
TA  - Ann Rheum Dis
JT  - Annals of the rheumatic diseases
JID - 0372355
SB  - IM
MH  - Humans
MH  - Male
MH  - Rheumatologists
MH  - Artificial Intelligence
MH  - *Medical Writing
MH  - *Rheumatic Diseases
PMC - PMC10359572
OTO - NOTNLM
OT  - Health services research
OT  - Patient Care Team
OT  - Qualitative research
OT  - Social work
COIS- Competing interests: None declared.
EDAT- 2023/04/12 06:00
MHDA- 2023/07/17 06:42
PMCR- 2023/07/21
CRDT- 2023/04/11 21:12
PHST- 2023/01/25 00:00 [received]
PHST- 2023/03/06 00:00 [accepted]
PHST- 2023/07/17 06:42 [medline]
PHST- 2023/04/12 06:00 [pubmed]
PHST- 2023/04/11 21:12 [entrez]
PHST- 2023/07/21 00:00 [pmc-release]
AID - ard-2023-223936 [pii]
AID - 10.1136/ard-2023-223936 [doi]
PST - ppublish
SO  - Ann Rheum Dis. 2023 Aug;82(8):1015-1017. doi: 10.1136/ard-2023-223936. Epub 2023 
      Apr 11.

PMID- 37651677
OWN - NLM
STAT- MEDLINE
DCOM- 20231228
LR  - 20240123
IS  - 1938-808X (Electronic)
IS  - 1040-2446 (Linking)
VI  - 99
IP  - 1
DP  - 2024 Jan 1
TI  - ChatGPT and Generative Artificial Intelligence for Medical Education: Potential 
      Impact and Opportunity.
PG  - 22-27
LID - 10.1097/ACM.0000000000005439 [doi]
AB  - ChatGPT has ushered in a new era of artificial intelligence (AI) that already has 
      significant consequences for many industries, including health care and 
      education. Generative AI tools, such as ChatGPT, refer to AI that is designed to 
      create or generate new content, such as text, images, or music, from their 
      trained parameters. With free access online and an easy-to-use conversational 
      interface, ChatGPT quickly accumulated more than 100 million users within the 
      first few months of its launch. Recent headlines in the popular press have 
      ignited concerns relevant to medical education over the possible implications of 
      cheating and plagiarism in assessments as well as excitement over new 
      opportunities for learning, assessment, and research. In this Scholarly 
      Perspective, the authors offer insights and recommendations about generative AI 
      for medical educators based on literature review, including the AI literacy 
      framework. The authors provide a definition of generative AI, introduce an AI 
      literacy framework and competencies, and offer considerations for potential 
      impacts and opportunities to optimize integration of generative AI for 
      admissions, learning, assessment, and medical education research to help medical 
      educators navigate and start planning for this new environment. As generative AI 
      tools continue to expand, educators need to increase their AI literacy through 
      education and vigilance around new advances in the technology and serve as 
      stewards of AI literacy to foster social responsibility and ethical awareness 
      around the use of AI.
CI  - Copyright © 2023 the Association of American Medical Colleges.
FAU - Boscardin, Christy K
AU  - Boscardin CK
FAU - Gin, Brian
AU  - Gin B
FAU - Golde, Polo Black
AU  - Golde PB
FAU - Hauer, Karen E
AU  - Hauer KE
LA  - eng
PT  - Journal Article
DEP - 20230831
PL  - United States
TA  - Acad Med
JT  - Academic medicine : journal of the Association of American Medical Colleges
JID - 8904605
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Educational Status
MH  - Learning
MH  - Literacy
MH  - *Education, Medical
EDAT- 2023/08/31 18:41
MHDA- 2023/12/28 06:42
CRDT- 2023/08/31 16:04
PHST- 2023/12/28 06:42 [medline]
PHST- 2023/08/31 18:41 [pubmed]
PHST- 2023/08/31 16:04 [entrez]
AID - 00001888-202401000-00011 [pii]
AID - 10.1097/ACM.0000000000005439 [doi]
PST - ppublish
SO  - Acad Med. 2024 Jan 1;99(1):22-27. doi: 10.1097/ACM.0000000000005439. Epub 2023 
      Aug 31.

PMID- 38384621
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240224
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 16
IP  - 1
DP  - 2024 Jan
TI  - Can DALL-E 3 Reliably Generate 12-Lead ECGs and Teaching Illustrations?
PG  - e52748
LID - 10.7759/cureus.52748 [doi]
LID - e52748
AB  - The recent integration of the latest image generation model DALL-E 3 into ChatGPT 
      allows text prompts to easily generate&nbsp;the&nbsp;corresponding images, enabling 
      multimodal output from ChatGPT. We explored the feasibility of DALL-E 3 for 
      drawing a 12-lead ECG and found that it can draw rudimentary 12-lead 
      electrocardiograms (ECG) displaying some of the parameters, although the details 
      are not completely accurate. We also explored DALL-E 3's capacity to create vivid 
      illustrations for teaching resuscitation-related medical knowledge. DALL-E 3 
      produced accurate CPR illustrations emphasizing proper hand placement and 
      technique. For ECG principles, it produced creative heart-shaped waveforms tying 
      ECGs to the heart. With further training, DALL-E 3 shows promise to expand 
      easy-to-understand visual medical teaching materials and ECG simulations for 
      different disease states. In conclusion, DALL-E 3 has the potential to generate 
      realistic 12-lead ECGs and teaching schematics, but expert validation is still 
      needed.
CI  - Copyright © 2024, Zhu et al.
FAU - Zhu, Lingxuan
AU  - Zhu L
AD  - Department of Oncology, Zhujiang Hospital of Southern Medical University, 
      Guangzhou, CHN.
FAU - Mou, Weiming
AU  - Mou W
AD  - Department of Urology, Shanghai General Hospital, Shanghai Jiao Tong University 
      School of Medicine, Shanghai, CHN.
FAU - Wu, Keren
AU  - Wu K
AD  - Department of Oncology, Zhujiang Hospital of Southern Medical University, 
      Guangzhou, CHN.
FAU - Zhang, Jian
AU  - Zhang J
AD  - Department of Oncology, Zhujiang Hospital of Southern Medical University, 
      Guangzhou, CHN.
FAU - Luo, Peng
AU  - Luo P
AD  - Department of Oncology, Zhujiang Hospital of Southern Medical University, 
      Guangzhou, CHN.
LA  - eng
PT  - Journal Article
DEP - 20240122
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10879738
OTO - NOTNLM
OT  - 12-lead ecg
OT  - artificial intelligence in medicine
OT  - chatgpt
OT  - dall-e 3
OT  - medical education
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/02/22 06:43
MHDA- 2024/02/22 06:44
PMCR- 2024/01/22
CRDT- 2024/02/22 03:52
PHST- 2024/01/22 00:00 [accepted]
PHST- 2024/02/22 06:44 [medline]
PHST- 2024/02/22 06:43 [pubmed]
PHST- 2024/02/22 03:52 [entrez]
PHST- 2024/01/22 00:00 [pmc-release]
AID - 10.7759/cureus.52748 [doi]
PST - epublish
SO  - Cureus. 2024 Jan 22;16(1):e52748. doi: 10.7759/cureus.52748. eCollection 2024 
      Jan.

PMID- 37492313
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231031
IS  - 1682-024X (Print)
IS  - 1681-715X (Electronic)
IS  - 1681-715X (Linking)
VI  - 39
IP  - 4
DP  - 2023 Jul-Aug
TI  - Chat-GPT: Opportunities and Challenges in Child Mental Healthcare.
PG  - 1191-1193
LID - 10.12669/pjms.39.4.8118 [doi]
AB  - Mental health in children and young people is a global public health concern. 
      With the increasing prevalence of mental illnesses and a significant treatment 
      gap, Mental Health in children and adolescents is now a global public health 
      concern. The development and extensive research in the field of Artificial 
      Intelligence(AI) for healthcare has been quite promising. The emergence of AI 
      based alternatives could be a viable solution for reducing mental health 
      treatment gap for people belonging to low and middle income countries. 
      Development of Chatbots like ChatGPT which is trained, using large amount of 
      textual data from the internet, can revolutionize child and adolescent mental 
      healthcare by acting as an effective assisting tool but a lot of caution is 
      required for its safe and responsible use in times to come.e.
CI  - Copyright: © Pakistan Journal of Medical Sciences.
FAU - Imran, Nazish
AU  - Imran N
AD  - Dr. Nazish Imran, MBBS; FRCPsych(London); MRCPsych (London); MHPE;PhD. Professor, 
      Department of Child &amp; Family Psychiatry, King Edward Medical University/Mayo 
      Hospital, Lahore, Pakistan.
FAU - Hashmi, Aateqa
AU  - Hashmi A
AD  - Dr. Aateqa Hashmi, MBBS. House Officer, Department of Child &amp; Family Psychiatry, 
      King Edward Medical University/Mayo Hospital, Lahore, Pakistan.
FAU - Imran, Ahad
AU  - Imran A
AD  - Ahad Imran, Third year Computer Sciences Student, Syed Babar Ali School of 
      Science &amp; Engineering (SBASSE), Lahore University of Management Sciences (LUMS), 
      Lahore, Pakistan.
LA  - eng
PT  - Journal Article
PL  - Pakistan
TA  - Pak J Med Sci
JT  - Pakistan journal of medical sciences
JID - 100913117
PMC - PMC10364280
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - ChatGPT
OT  - Child Mental Health
OT  - Mental Illness
OT  - Open AI
EDAT- 2023/07/26 06:43
MHDA- 2023/07/26 06:44
PMCR- 2023/07/01
CRDT- 2023/07/26 03:49
PHST- 2023/05/05 00:00 [received]
PHST- 2023/05/22 00:00 [revised]
PHST- 2023/05/31 00:00 [accepted]
PHST- 2023/07/26 06:44 [medline]
PHST- 2023/07/26 06:43 [pubmed]
PHST- 2023/07/26 03:49 [entrez]
PHST- 2023/07/01 00:00 [pmc-release]
AID - PJMS-39-1191 [pii]
AID - 10.12669/pjms.39.4.8118 [doi]
PST - ppublish
SO  - Pak J Med Sci. 2023 Jul-Aug;39(4):1191-1193. doi: 10.12669/pjms.39.4.8118.

PMID- 37378043
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230701
IS  - 2095-4689 (Print)
IS  - 2095-4697 (Electronic)
IS  - 2095-4689 (Linking)
VI  - 11
IP  - 2
DP  - 2023 Jun
TI  - Empowering beginners in bioinformatics with ChatGPT.
PG  - 105-108
LID - 10.15302/j-qb-023-0327 [doi]
AB  - The impressive conversational and programming abilities of ChatGPT make it an 
      attractive tool for facilitating the education of bioinformatics data analysis 
      for beginners. In this study, we proposed an iterative model to fine-tune 
      instructions for guiding a chatbot in generating code for bioinformatics data 
      analysis tasks. We demonstrated the feasibility of the model by applying it to 
      various bioinformatics topics. Additionally, we discussed practical 
      considerations and limitations regarding the use of the model in chatbot-aided 
      bioinformatics education.
FAU - Shue, Evelyn
AU  - Shue E
AD  - Department of Microbiology, Immunology &amp; Cell Biology, West Virginia University, 
      Morgantown, WV 26506, USA.
FAU - Liu, Li
AU  - Liu L
AD  - College of Health Solutions, Arizona State University, Phoenix, AZ 85004, USA.
AD  - Biodesign Institute, Arizona State University, Tempe, AZ 85281, USA.
FAU - Li, Bingxin
AU  - Li B
AD  - Finance Department, John Chambers College of Business and Economics, West 
      Virginia University, Morgantown, WV 26506, USA.
FAU - Feng, Zifeng
AU  - Feng Z
AD  - Department of Economics and Finance, The University of Texas at El Paso, El Paso, 
      TX 79902, USA.
FAU - Li, Xin
AU  - Li X
AD  - Lane Department of Computer Science and Electrical Engineering, West Virginia 
      University, Morgantown, WV 26506, USA.
FAU - Hu, Gangqing
AU  - Hu G
AD  - Department of Microbiology, Immunology &amp; Cell Biology, West Virginia University, 
      Morgantown, WV 26506, USA.
LA  - eng
GR  - P20 GM103434/GM/NIGMS NIH HHS/United States
GR  - P20 GM121322/GM/NIGMS NIH HHS/United States
GR  - R01 LM013438/LM/NLM NIH HHS/United States
GR  - U54 GM104942/GM/NIGMS NIH HHS/United States
PT  - Journal Article
DEP - 20230331
PL  - China
TA  - Quant Biol
JT  - Quantitative biology (Beijing, China)
JID - 101639642
PMC - PMC10299548
MID - NIHMS1890079
OTO - NOTNLM
OT  - ChatGPT
OT  - bioinformatics
OT  - education
OT  - scientific data analysis
COIS- COMPLIANCE WITH ETHICS GUIDELINES Evelyn Shue, Li Liu, Bingxin Li, Zifeng Feng, 
      Xin Li and Gangqing Hu declare that they have no conflict of interest. This 
      article is a perspective article and does not contain any studies with human or 
      animal subjects performed by any of the authors.
EDAT- 2023/06/28 13:08
MHDA- 2023/06/28 13:09
PMCR- 2023/06/27
CRDT- 2023/06/28 09:27
PHST- 2023/06/28 13:09 [medline]
PHST- 2023/06/28 13:08 [pubmed]
PHST- 2023/06/28 09:27 [entrez]
PHST- 2023/06/27 00:00 [pmc-release]
AID - 10.15302/j-qb-023-0327 [doi]
PST - ppublish
SO  - Quant Biol. 2023 Jun;11(2):105-108. doi: 10.15302/j-qb-023-0327. Epub 2023 Mar 
      31.

PMID- 36763148
OWN - NLM
STAT- MEDLINE
DCOM- 20230313
LR  - 20230526
IS  - 2731-703X (Electronic)
IS  - 2731-7021 (Linking)
VI  - 126
IP  - 3
DP  - 2023 Mar
TI  - [ChatGPT : Milestone text AI with game changing potential].
PG  - 252-254
LID - 10.1007/s00113-023-01296-y [doi]
FAU - Krettek, C
AU  - Krettek C
AD  - Unfallchirurgische Klinik, Medizinische Hochschule Hannover, Carl-Neuberg-Str.&nbsp;1, 
      30625, Hannover, Deutschland. krettek.christian@mh-hannover.de.
LA  - ger
PT  - Journal Article
TT  - ChatGPT : Milestone-Text-KI mit Game-Changer-Potenzial.
DEP - 20230210
PL  - Germany
TA  - Unfallchirurgie (Heidelb)
JT  - Unfallchirurgie (Heidelberg, Germany)
JID - 9918384886306676
SB  - IM
MH  - *Artificial Intelligence
EDAT- 2023/02/11 06:00
MHDA- 2023/03/14 06:00
CRDT- 2023/02/10 11:14
PHST- 2023/01/16 00:00 [accepted]
PHST- 2023/02/11 06:00 [pubmed]
PHST- 2023/03/14 06:00 [medline]
PHST- 2023/02/10 11:14 [entrez]
AID - 10.1007/s00113-023-01296-y [pii]
AID - 10.1007/s00113-023-01296-y [doi]
PST - ppublish
SO  - Unfallchirurgie (Heidelb). 2023 Mar;126(3):252-254. doi: 
      10.1007/s00113-023-01296-y. Epub 2023 Feb 10.

PMID- 38024074
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231201
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 11
DP  - 2023 Nov
TI  - Pilot Testing of a Tool to Standardize the Assessment of the Quality of Health 
      Information Generated by Artificial Intelligence-Based Models.
PG  - e49373
LID - 10.7759/cureus.49373 [doi]
LID - e49373
AB  - Background Artificial intelligence (AI)-based conversational models, such as Chat 
      Generative Pre-trained Transformer (ChatGPT), Microsoft Bing, and Google Bard, 
      have emerged as valuable sources of health information for lay individuals. 
      However, the accuracy of the information provided by these AI models remains a 
      significant concern.&nbsp;This pilot study aimed to test a new tool with key themes 
      for inclusion as follows:&nbsp;Completeness of content,&nbsp;Lack of false information in 
      the content,&nbsp;Evidence supporting the content,&nbsp;Appropriateness of the content, 
      and&nbsp;Relevance, referred to as "CLEAR", designed to assess the quality of health 
      information delivered by AI-based models. Methods Tool development involved a 
      literature review on health information quality, followed by the initial 
      establishment of the CLEAR tool, which comprised five items that aimed to assess 
      the following: completeness, lack of false information, evidence support, 
      appropriateness, and relevance. Each item was scored on a five-point Likert scale 
      from excellent to poor. Content validity was checked by expert review. Pilot 
      testing involved 32 healthcare professionals using the CLEAR tool to assess 
      content on eight different health topics deliberately designed with varying 
      qualities. The internal consistency was checked with Cronbach's alpha (α). 
      Feedback from the pilot test resulted in language modifications to improve the 
      clarity of the items. The final CLEAR tool was used to assess the quality of 
      health information generated by four distinct AI models on five health topics. 
      The AI models were ChatGPT 3.5, ChatGPT 4, Microsoft Bing, and Google Bard, and 
      the content generated was scored by two independent raters with Cohen's kappa (κ) 
      for inter-rater agreement. Results The final five CLEAR items were: (1) Is the 
      content sufficient?; (2) Is the content accurate?; (3) Is the content 
      evidence-based?; (4) Is the content clear, concise, and easy to understand?; and 
      (5) Is the content free from irrelevant information? Pilot testing on the eight 
      health topics revealed acceptable internal consistency with a Cronbach's α range 
      of 0.669-0.981. The use of the final CLEAR tool yielded the following average 
      scores: Microsoft Bing (mean=24.4±0.42), ChatGPT-4 (mean=23.6±0.96), Google Bard 
      (mean=21.2±1.79), and ChatGPT-3.5 (mean=20.6±5.20). The inter-rater agreement 
      revealed the following Cohen κ values: for ChatGPT-3.5 (κ=0.875, P&lt;.001), 
      ChatGPT-4 (κ=0.780, P&lt;.001), Microsoft Bing (κ=0.348, P=.037), and Google Bard 
      (κ=.749, P&lt;.001). Conclusions The CLEAR tool is a brief yet helpful tool that can 
      aid in standardizing testing of the quality of health information generated by 
      AI-based models. Future studies are recommended to validate the utility of the 
      CLEAR tool in the quality assessment of AI-generated health-related content using 
      a larger sample across various complex health topics.
CI  - Copyright © 2023, Sallam et al.
FAU - Sallam, Malik
AU  - Sallam M
AD  - Department of Pathology, Microbiology, and Forensic Medicine, School of Medicine, 
      University of Jordan, Amman, JOR.
AD  - Department of Clinical Laboratories and Forensic Medicine, Jordan University 
      Hospital, Amman, JOR.
FAU - Barakat, Muna
AU  - Barakat M
AD  - Department of Clinical Pharmacy and Therapeutics, School of Pharmacy, Applied 
      Science Private University, Amman, JOR.
AD  - Department of Research, Middle East University, Amman, JOR.
FAU - Sallam, Mohammed
AU  - Sallam M
AD  - Department of Pharmacy, Mediclinic Parkview Hospital, Mediclinic Middle East, 
      Dubai, ARE.
LA  - eng
PT  - Journal Article
DEP - 20231124
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10674084
OTO - NOTNLM
OT  - ai in healthcare
OT  - ai-generated health information
OT  - assessment tool feasibility
OT  - health information reliability
OT  - quality of health information
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/11/29 18:42
MHDA- 2023/11/29 18:43
PMCR- 2023/11/24
CRDT- 2023/11/29 16:02
PHST- 2023/11/24 00:00 [accepted]
PHST- 2023/11/29 18:43 [medline]
PHST- 2023/11/29 18:42 [pubmed]
PHST- 2023/11/29 16:02 [entrez]
PHST- 2023/11/24 00:00 [pmc-release]
AID - 10.7759/cureus.49373 [doi]
PST - epublish
SO  - Cureus. 2023 Nov 24;15(11):e49373. doi: 10.7759/cureus.49373. eCollection 2023 
      Nov.

PMID- 38145486
OWN - NLM
STAT- MEDLINE
DCOM- 20231226
LR  - 20240110
IS  - 1438-8871 (Electronic)
IS  - 1439-4456 (Print)
IS  - 1438-8871 (Linking)
VI  - 25
DP  - 2023 Dec 25
TI  - Comparisons of Quality, Correctness, and Similarity Between ChatGPT-Generated and 
      Human-Written Abstracts for Basic Research: Cross-Sectional Study.
PG  - e51229
LID - 10.2196/51229 [doi]
LID - e51229
AB  - BACKGROUND: ChatGPT may act as a research assistant to help organize the 
      direction of thinking and summarize research findings. However, few studies have 
      examined the quality, similarity (abstracts being similar to the original one), 
      and accuracy of the abstracts generated by ChatGPT when researchers provide 
      full-text basic research papers. OBJECTIVE: We aimed to assess the applicability 
      of an artificial intelligence (AI) model in generating abstracts for basic 
      preclinical research. METHODS: We selected 30 basic research papers from Nature, 
      Genome Biology, and Biological Psychiatry. Excluding abstracts, we inputted the 
      full text into ChatPDF, an application of a language model based on ChatGPT, and 
      we prompted it to generate abstracts with the same style as used in the original 
      papers. A total of 8 experts were invited to evaluate the quality of these 
      abstracts (based on a Likert scale of 0-10) and identify which abstracts were 
      generated by ChatPDF, using a blind approach. These abstracts were also evaluated 
      for their similarity to the original abstracts and the accuracy of the AI 
      content. RESULTS: The quality of ChatGPT-generated abstracts was lower than that 
      of the actual abstracts (10-point Likert scale: mean 4.72, SD 2.09 vs mean 8.09, 
      SD 1.03; P&lt;.001). The difference in quality was significant in the unstructured 
      format (mean difference -4.33; 95% CI -4.79 to -3.86; P&lt;.001) but minimal in the 
      4-subheading structured format (mean difference -2.33; 95% CI -2.79 to -1.86). 
      Among the 30 ChatGPT-generated abstracts, 3 showed wrong conclusions, and 10 were 
      identified as AI content. The mean percentage of similarity between the original 
      and the generated abstracts was not high (2.10%-4.40%). The blinded reviewers 
      achieved a 93% (224/240) accuracy rate in guessing which abstracts were written 
      using ChatGPT. CONCLUSIONS: Using ChatGPT to generate a scientific abstract may 
      not lead to issues of similarity when using real full texts written by humans. 
      However, the quality of the ChatGPT-generated abstracts was suboptimal, and their 
      accuracy was not 100%.
CI  - ©Shu-Li Cheng, Shih-Jen Tsai, Ya-Mei Bai, Chih-Hung Ko, Chih-Wei Hsu, Fu-Chi 
      Yang, Chia-Kuang Tsai, Yu-Kang Tu, Szu-Nian Yang, Ping-Tao Tseng, Tien-Wei Hsu, 
      Chih-Sung Liang, Kuan-Pin Su. Originally published in the Journal of Medical 
      Internet Research (https://www.jmir.org), 25.12.2023.
FAU - Cheng, Shu-Li
AU  - Cheng SL
AUID- ORCID: 0000-0002-1523-8519
AD  - Department of Nursing, Mackay Medical College, Taipei, Taiwan.
FAU - Tsai, Shih-Jen
AU  - Tsai SJ
AUID- ORCID: 0000-0002-9987-022X
AD  - Department of Psychiatry, Taipei Veterans General Hospital, Taipei, Taiwan.
AD  - Division of Psychiatry, School of Medicine, National Yang-Ming University, 
      Taipei, Taiwan.
FAU - Bai, Ya-Mei
AU  - Bai YM
AUID- ORCID: 0000-0003-3779-9074
AD  - Department of Psychiatry, Taipei Veterans General Hospital, Taipei, Taiwan.
AD  - Division of Psychiatry, School of Medicine, National Yang-Ming University, 
      Taipei, Taiwan.
FAU - Ko, Chih-Hung
AU  - Ko CH
AUID- ORCID: 0000-0001-8034-0221
AD  - Department of Psychiatry, Kaohsiung Medical University Hospital, Kaohsiung, 
      Taiwan.
AD  - Department of Psychiatry, College of Medicine, Kaohsiung Medical University, 
      Kaohsiung, Taiwan.
AD  - Department of Psychiatry, Kaohsiung Municipal Siaogang Hospital, Kaohsiung 
      Medical University, Kaohsiung, Taiwan.
FAU - Hsu, Chih-Wei
AU  - Hsu CW
AUID- ORCID: 0000-0002-8650-4060
AD  - Department of Psychiatry, Kaohsiung Chang Gung Memorial Hospital, Kaohsiung, 
      Taiwan.
FAU - Yang, Fu-Chi
AU  - Yang FC
AUID- ORCID: 0000-0001-6831-3634
AD  - Department of Neurology, Tri-Service General Hospital, National Defense Medical 
      Center, Taipei, Taiwan.
FAU - Tsai, Chia-Kuang
AU  - Tsai CK
AUID- ORCID: 0000-0001-7693-1408
AD  - Department of Neurology, Tri-Service General Hospital, National Defense Medical 
      Center, Taipei, Taiwan.
FAU - Tu, Yu-Kang
AU  - Tu YK
AUID- ORCID: 0000-0002-2461-474X
AD  - Institute of Epidemiology and Preventive Medicine, College of Public Health, 
      National Taiwan University, Taipei, Taiwan.
AD  - Department of Dentistry, National Taiwan University Hospital, Taipei, Taiwan.
FAU - Yang, Szu-Nian
AU  - Yang SN
AUID- ORCID: 0000-0002-6091-0263
AD  - Department of Psychiatry, Tri-service Hospital, Beitou branch, Taipei, Taiwan.
AD  - Department of Psychiatry, Armed Forces Taoyuan General Hospital, Taoyuan, Taiwan.
AD  - Graduate Institute of Health and Welfare Policy, National Yang Ming Chiao Tung 
      University, Taipei, Taiwan.
FAU - Tseng, Ping-Tao
AU  - Tseng PT
AUID- ORCID: 0000-0001-5761-7800
AD  - Institute of Biomedical Sciences, Institute of Precision Medicine, National Sun 
      Yat-sen University, Kaohsiung, Taiwan.
AD  - Department of Psychology, College of Medical and Health Science, Asia University, 
      Taichung, Taiwan.
AD  - Prospect Clinic for Otorhinolaryngology and Neurology, Kaohsiung, Taiwan.
FAU - Hsu, Tien-Wei
AU  - Hsu TW
AUID- ORCID: 0000-0003-4136-1251
AD  - Department of Psychiatry, E-Da Dachang Hospital, I-Shou University, Kaohsiung, 
      Taiwan.
AD  - Department of Psychiatry, E-Da Hospital, I-Shou University, Kaohsiung, Taiwan.
FAU - Liang, Chih-Sung
AU  - Liang CS
AUID- ORCID: 0000-0003-1138-5586
AD  - Department of Psychiatry, Tri-service Hospital, Beitou branch, Taipei, Taiwan.
AD  - Department of Psychiatry, National Defense Medical Center, Taipei, Taiwan.
FAU - Su, Kuan-Pin
AU  - Su KP
AUID- ORCID: 0000-0002-4501-2502
AD  - College of Medicine, China Medical University, Taichung, Taiwan.
AD  - Mind-Body Interface Laboratory, China Medical University and Hospital, Taichung, 
      Taiwan.
AD  - An-Nan Hospital, China Medical University, Tainan, Taiwan.
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20231225
PL  - Canada
TA  - J Med Internet Res
JT  - Journal of medical Internet research
JID - 100959882
SB  - IM
MH  - Humans
MH  - Cross-Sectional Studies
MH  - *Artificial Intelligence
MH  - *Research
MH  - Research Personnel
MH  - Language
PMC - PMC10760418
OTO - NOTNLM
OT  - AI-generated scientific content
OT  - ChatGPT
OT  - LLM
OT  - NLP
OT  - abstract
OT  - abstracts
OT  - academic research
OT  - artificial intelligence
OT  - extract
OT  - extraction
OT  - generation
OT  - generative
OT  - language model
OT  - language models
OT  - natural language processing
OT  - plagiarism
OT  - publication
OT  - publications
OT  - scientific research
OT  - text
OT  - textual
COIS- Conflicts of Interest: None declared.
EDAT- 2023/12/25 12:42
MHDA- 2023/12/26 06:41
PMCR- 2023/12/25
CRDT- 2023/12/25 11:54
PHST- 2023/07/25 00:00 [received]
PHST- 2023/11/20 00:00 [accepted]
PHST- 2023/10/17 00:00 [revised]
PHST- 2023/12/26 06:41 [medline]
PHST- 2023/12/25 12:42 [pubmed]
PHST- 2023/12/25 11:54 [entrez]
PHST- 2023/12/25 00:00 [pmc-release]
AID - v25i1e51229 [pii]
AID - 10.2196/51229 [doi]
PST - epublish
SO  - J Med Internet Res. 2023 Dec 25;25:e51229. doi: 10.2196/51229.

PMID- 38163049
OWN - NLM
STAT- MEDLINE
DCOM- 20240103
LR  - 20240106
IS  - 2212-277X (Electronic)
IS  - 2212-2761 (Print)
IS  - 2212-2761 (Linking)
VI  - 12
IP  - 1
DP  - 2023
TI  - Will ChatGPT's Free Language Editing Service Level the Playing Field in Science 
      Communication?: Insights from a Collaborative Project with Non-native English 
      Scholars.
PG  - 565-574
LID - 10.5334/pme.1246 [doi]
AB  - ChatGPT has been widely heralded as a way to level the playing field in 
      scientific communication through its free language editing service. However, such 
      claims lack systematic evidence. A writing scholar (LL) and six non-native 
      English scholars researching health professions education collaborated on this 
      Writer's Craft to fill this gap. Our overarching aim was to provide experiential 
      evidence about ChatGPT's performance as a language editor and writing coach. We 
      implemented three cycles of a systematic procedure, describing how we developed 
      our prompts, selected text for editing, incrementally prompted to refine 
      ChatGPT's responses, and analyzed the quality of its language edits and 
      explanations. From this experience, we offer five insights, and we conclude that 
      the optimism about ChatGPT's capacity to level the playing field for non-native 
      English writers should be tempered. In the writer's craft section we offer simple 
      tips to improve your writing in one of three areas: Energy, Clarity and 
      Persuasiveness. Each entry focuses on a key writing feature or strategy, 
      illustrates how it commonly goes wrong, teaches the grammatical underpinnings 
      necessary to understand it and offers suggestions to wield it effectively. We 
      encourage readers to share comments on or suggestions for this section on 
      Twitter, using the hashtag: #how'syourwriting?
CI  - Copyright: © 2023 The Author(s).
FAU - Lingard, Lorelei
AU  - Lingard L
AUID- ORCID: 0000-0002-4150-3355
AD  - Centre for Education Research &amp; Innovation, Schulich School of Medicine &amp; 
      Dentistry, Western University, London, Ontario, Canada.
FAU - Chandritilake, Madawa
AU  - Chandritilake M
AUID- ORCID: 0000-0001-5644-264X
AD  - Faculty of Medicine, University of Kelaniya, Sri Lanka.
FAU - de Heer, Merel
AU  - de Heer M
AD  - Amsterdam UMC, Vrije Universiteit Amsterdam, Research in Medical Education, 
      Amsterdam, Netherlands.
FAU - Klasen, Jennifer
AU  - Klasen J
AUID- ORCID: 0000-0001-8355-1606
AD  - University Digestive Health Care Center, Department of Visceral Surgery, St. 
      Clara Hospital and University Hospital Basel, Switzerland.
FAU - Maulina, Fury
AU  - Maulina F
AUID- ORCID: 0000-0001-8748-0569
AD  - Faculty of Health, Medicine and Life Sciences, Maastricht University, Maastricht, 
      the Netherlands.
AD  - Department of Public Health, Faculty of Medicine, Universitas Malikussaleh, 
      Lhokseumawe, Aceh, Indonesia.
FAU - Olmos-Vega, Francisco
AU  - Olmos-Vega F
AUID- ORCID: 0000-0003-1629-4309
AD  - Department of Medicine and Health Profession Education Center, Universitéde 
      Sherbrooke, Sherbrooke, Canada.
FAU - St-Onge, Christina
AU  - St-Onge C
AUID- ORCID: 0000-0001-5313-0456
AD  - Department of Medicine and Health Profession Education Center, Universitéde 
      Sherbrooke, Sherbrooke, Canada.
LA  - eng
PT  - Journal Article
DEP - 20231219
PL  - Netherlands
TA  - Perspect Med Educ
JT  - Perspectives on medical education
JID - 101590643
SB  - IM
MH  - Humans
MH  - *Communication
MH  - *Language
MH  - Persuasive Communication
MH  - Writing
PMC - PMC10756157
COIS- The authors have no competing interests to declare.
EDAT- 2024/01/02 11:46
MHDA- 2024/01/03 09:43
PMCR- 2023/12/19
CRDT- 2024/01/01 04:34
PHST- 2023/11/28 00:00 [received]
PHST- 2023/11/28 00:00 [accepted]
PHST- 2024/01/03 09:43 [medline]
PHST- 2024/01/02 11:46 [pubmed]
PHST- 2024/01/01 04:34 [entrez]
PHST- 2023/12/19 00:00 [pmc-release]
AID - 10.5334/pme.1246 [doi]
PST - epublish
SO  - Perspect Med Educ. 2023 Dec 19;12(1):565-574. doi: 10.5334/pme.1246. eCollection 
      2023.

PMID- 37392002
OWN - NLM
STAT- Publisher
LR  - 20230701
IS  - 1741-2854 (Electronic)
IS  - 0020-7640 (Linking)
DP  - 2023 Jun 30
TI  - ChatGPT and social psychiatry: A commentary on the article 'Old dog, new tricks? 
      Exploring the potential functionalities of ChatGPT in supporting educational 
      methods in social psychiatry'.
PG  - 207640231178488
LID - 10.1177/00207640231178488 [doi]
FAU - Torales, Julio
AU  - Torales J
AUID- ORCID: 0000-0003-3277-7036
AD  - Department of Medical Psychology, School of Medical Sciences, National University 
      of Asunción, San Lorenzo, Paraguay.
AD  - Department of Psychiatry, School of Medical Sciences, National University of 
      Asunción, San Lorenzo, Paraguay.
FAU - O'Higgins, Marcelo
AU  - O'Higgins M
AD  - Department of Psychiatry, School of Medical Sciences, National University of 
      Asunción, San Lorenzo, Paraguay.
LA  - eng
PT  - Editorial
DEP - 20230630
PL  - England
TA  - Int J Soc Psychiatry
JT  - The International journal of social psychiatry
JID - 0374726
SB  - IM
EDAT- 2023/07/01 11:42
MHDA- 2023/07/01 11:42
CRDT- 2023/07/01 02:33
PHST- 2023/07/01 11:42 [medline]
PHST- 2023/07/01 11:42 [pubmed]
PHST- 2023/07/01 02:33 [entrez]
AID - 10.1177/00207640231178488 [doi]
PST - aheadofprint
SO  - Int J Soc Psychiatry. 2023 Jun 30:207640231178488. doi: 
      10.1177/00207640231178488.

PMID- 37827724
OWN - NLM
STAT- MEDLINE
DCOM- 20231023
LR  - 20231023
IS  - 2632-1009 (Electronic)
IS  - 2632-1009 (Linking)
VI  - 30
IP  - 1
DP  - 2023 Oct
TI  - Comparative study of ChatGPT and human evaluators on the assessment of medical 
      literature according to recognised reporting standards.
LID - 10.1136/bmjhci-2023-100830 [doi]
LID - e100830
AB  - INTRODUCTION: Amid clinicians' challenges in staying updated with medical 
      research, artificial intelligence (AI) tools like the large language model (LLM) 
      ChatGPT could automate appraisal of research quality, saving time and reducing 
      bias. This study compares the proficiency of ChatGPT3 against human evaluation in 
      scoring abstracts to determine its potential as a tool for evidence synthesis. 
      METHODS: We compared ChatGPT's scoring of implant dentistry abstracts with human 
      evaluators using the Consolidated Standards of Reporting Trials for Abstracts 
      reporting standards checklist, yielding an overall compliance score (OCS). 
      Bland-Altman analysis assessed agreement between human and AI-generated OCS 
      percentages. Additional error analysis included mean difference of OCS subscores, 
      Welch's t-test and Pearson's correlation coefficient. RESULTS: Bland-Altman 
      analysis showed a mean difference of 4.92% (95% CI 0.62%, 0.37%) in OCS between 
      human evaluation and ChatGPT. Error analysis displayed small mean differences in 
      most domains, with the highest in 'conclusion' (0.764 (95% CI 0.186, 0.280)) and 
      the lowest in 'blinding' (0.034 (95% CI 0.818, 0.895)). The strongest 
      correlations between were in 'harms' (r=0.32, p&lt;0.001) and 'trial registration' 
      (r=0.34, p=0.002), whereas the weakest were in 'intervention' (r=0.02, p&lt;0.001) 
      and 'objective' (r=0.06, p&lt;0.001). CONCLUSION: LLMs like ChatGPT can help 
      automate appraisal of medical literature, aiding in the identification of 
      accurately reported research. Possible applications of ChatGPT include 
      integration within medical databases for abstract evaluation. Current limitations 
      include the token limit, restricting its usage to abstracts. As AI technology 
      advances, future versions like GPT4 could offer more reliable, comprehensive 
      evaluations, enhancing the identification of high-quality research and 
      potentially improving patient outcomes.
CI  - © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY. Published 
      by BMJ.
FAU - Roberts, Richard Hr
AU  - Roberts RH
AUID- ORCID: 0000-0002-9600-5943
AD  - Reconstructive Surgery and Regenerative Medicine Research Centre, Swansea 
      University, Swansea, UK 838272@swansea.ac.uk.
AD  - Swansea University Medical School, Swansea University, Swansea, UK.
AD  - Welsh Centre for Burns and Plastic Surgery, Morriston Hospital, Swansea, UK.
FAU - Ali, Stephen R
AU  - Ali SR
AD  - Reconstructive Surgery and Regenerative Medicine Research Centre, Swansea 
      University, Swansea, UK.
AD  - Welsh Centre for Burns and Plastic Surgery, Morriston Hospital, Swansea, UK.
FAU - Hutchings, Hayley A
AU  - Hutchings HA
AD  - Swansea University Medical School, Swansea University, Swansea, UK.
FAU - Dobbs, Thomas D
AU  - Dobbs TD
AD  - Reconstructive Surgery and Regenerative Medicine Research Centre, Swansea 
      University, Swansea, UK.
AD  - Welsh Centre for Burns and Plastic Surgery, Morriston Hospital, Swansea, UK.
FAU - Whitaker, Iain S
AU  - Whitaker IS
AD  - Reconstructive Surgery and Regenerative Medicine Research Centre, Swansea 
      University, Swansea, UK.
AD  - Welsh Centre for Burns and Plastic Surgery, Morriston Hospital, Swansea, UK.
LA  - eng
PT  - Journal Article
PL  - England
TA  - BMJ Health Care Inform
JT  - BMJ health &amp; care informatics
JID - 101745500
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Biomedical Research
MH  - Checklist
MH  - Databases, Factual
MH  - Patient Compliance
PMC - PMC10583079
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Medical Informatics
COIS- Competing interests: None declared.
EDAT- 2023/10/13 00:43
MHDA- 2023/10/23 01:18
PMCR- 2023/10/12
CRDT- 2023/10/12 21:12
PHST- 2023/06/14 00:00 [received]
PHST- 2023/09/05 00:00 [accepted]
PHST- 2023/10/23 01:18 [medline]
PHST- 2023/10/13 00:43 [pubmed]
PHST- 2023/10/12 21:12 [entrez]
PHST- 2023/10/12 00:00 [pmc-release]
AID - bmjhci-2023-100830 [pii]
AID - 10.1136/bmjhci-2023-100830 [doi]
PST - ppublish
SO  - BMJ Health Care Inform. 2023 Oct;30(1):e100830. doi: 10.1136/bmjhci-2023-100830.

PMID- 38315183
OWN - NLM
STAT- MEDLINE
DCOM- 20240308
LR  - 20240321
IS  - 2731-6866 (Electronic)
IS  - 2731-6858 (Linking)
VI  - 73
IP  - 3
DP  - 2024 Mar
TI  - [ChatGPT: aid to medical ethics decision making?].
PG  - 186-192
LID - 10.1007/s00101-024-01385-6 [doi]
AB  - BACKGROUND: Physicians have to make countless decisions every day. The medical, 
      ethical and legal aspects are often intertwined and subject to change over time. 
      Involving an ethics committee or arranging an ethical consultation are examples 
      of potential aids to decision making. Whether and how artificial intelligence 
      (AI) and the large language model (LLM) of the company OpenAI (San Francisco, CA, 
      USA), known under the name ChatGPT, can also help and support ethical decision 
      making is increasingly becoming a&nbsp;matter of controversial debate. MATERIAL AND 
      METHODS: Based on a&nbsp;case example, in which a&nbsp;female physician is confronted with 
      ethical and legal issues and presents these to ChatGPT to come up with answers, 
      the first indications of the strengths and weaknesses are ascertained. 
      CONCLUSION: Due to the rapid technical development and access to ever increasing 
      quantities of data, the utilization should be closely observed and evaluated.
CI  - © 2024. The Author(s), under exclusive licence to Springer Medizin Verlag GmbH, 
      ein Teil von Springer Nature.
FAU - Schmidt, Kurt W
AU  - Schmidt KW
AD  - Zentrum für Ethik in der Medizin, Agaplesion Markus Krankenhaus, 
      Wilhelm-Epstein-Str.&nbsp;4, 60431, Frankfurt a. M., Deutschland. 
      kurt.schmidt@ekhn.de.
FAU - Lechner, Fabian
AU  - Lechner F
AD  - Institut für Künstliche Intelligenz, Universitätsklinikum Gießen und Marburg, 
      Marburg, Deutschland.
LA  - ger
PT  - English Abstract
PT  - Journal Article
PT  - Review
TT  - ChatGPT: Hilfe bei der medizinethischen Entscheidungsfindung?
PL  - Germany
TA  - Anaesthesiologie
JT  - Die Anaesthesiologie
JID - 9918384886806676
SB  - IM
MH  - Female
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Ethics Committees
MH  - Clinical Decision-Making
MH  - Decision Making
MH  - Ethics, Medical
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Empathy
OT  - Ethical analysis
OT  - Ethics committees
OT  - Legal aspects
EDAT- 2024/02/05 14:44
MHDA- 2024/03/08 06:42
CRDT- 2024/02/05 11:04
PHST- 2024/03/08 06:42 [medline]
PHST- 2024/02/05 14:44 [pubmed]
PHST- 2024/02/05 11:04 [entrez]
AID - 10.1007/s00101-024-01385-6 [pii]
AID - 10.1007/s00101-024-01385-6 [doi]
PST - ppublish
SO  - Anaesthesiologie. 2024 Mar;73(3):186-192. doi: 10.1007/s00101-024-01385-6.

PMID- 38165624
OWN - NLM
STAT- MEDLINE
DCOM- 20240104
LR  - 20240104
IS  - 1940-6029 (Electronic)
IS  - 1064-3745 (Linking)
VI  - 2742
DP  - 2024
TI  - Applying BERT and ChatGPT for Sentiment Analysis of Lyme Disease in Scientific 
      Literature.
PG  - 173-183
LID - 10.1007/978-1-0716-3561-2_14 [doi]
AB  - This chapter presents a practical guide for conducting sentiment analysis using 
      Natural Language Processing (NLP) techniques in the domain of tick-borne disease 
      text. The aim is to demonstrate the process of how the presence of bias in the 
      discourse surrounding chronic manifestations of the disease can be evaluated. The 
      goal is to use a dataset of 5643 abstracts collected from scientific journals on 
      the topic of chronic Lyme disease to demonstrate using Python, the steps for 
      conducting sentiment analysis using pretrained language models and the process of 
      validating the preliminary results using both interpretable machine learning 
      tools, as well as a novel methodology of leveraging emerging state-of-the-art 
      large language models like ChatGPT. This serves as a useful resource for 
      researchers and practitioners interested in using NLP techniques for sentiment 
      analysis in the medical domain.
CI  - © 2024. The Author(s), under exclusive license to Springer Science+Business 
      Media, LLC, part of Springer Nature.
FAU - Susnjak, Teo
AU  - Susnjak T
AD  - School of Mathematical and Computational Sciences, Massey University, Auckland, 
      New Zealand. t.susnjak@massey.ac.nz.
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Methods Mol Biol
JT  - Methods in molecular biology (Clifton, N.J.)
JID - 9214969
SB  - IM
MH  - Humans
MH  - *Sentiment Analysis
MH  - *Lyme Disease
MH  - Publications
MH  - Language
MH  - Machine Learning
OTO - NOTNLM
OT  - BERT
OT  - ChatGPT
OT  - Explainable AI
OT  - Language models
OT  - Lyme disease text analysis
OT  - NLP
OT  - SHAP
OT  - Sentiment analysis
EDAT- 2024/01/02 12:41
MHDA- 2024/01/04 11:43
CRDT- 2024/01/02 11:13
PHST- 2024/01/04 11:43 [medline]
PHST- 2024/01/02 12:41 [pubmed]
PHST- 2024/01/02 11:13 [entrez]
AID - 10.1007/978-1-0716-3561-2_14 [doi]
PST - ppublish
SO  - Methods Mol Biol. 2024;2742:173-183. doi: 10.1007/978-1-0716-3561-2_14.

PMID- 37821756
OWN - NLM
STAT- MEDLINE
DCOM- 20231027
LR  - 20231027
IS  - 2731-7099 (Electronic)
IS  - 2731-7080 (Linking)
VI  - 64
IP  - 11
DP  - 2023 Nov
TI  - [ChatGPT: aid to medical ethics decision making?].
PG  - 1065-1071
LID - 10.1007/s00108-023-01601-2 [doi]
AB  - BACKGROUND: Physicians have to make countless decisions every day. The medical, 
      ethical and legal aspects are often intertwined and subject to change over time. 
      Involving an ethics committee or arranging an ethical consultation are examples 
      of potential aids to decision making. Whether and how artificial intelligence 
      (AI) and the large language model (LLM) of the company OpenAI (San Francisco, CA, 
      USA), known under the name ChatGPT, can also help and support ethical decision 
      making is increasingly becoming a&nbsp;matter of controversial debate. MATERIAL AND 
      METHODS: Based on a&nbsp;case example, in which a&nbsp;female physician is confronted with 
      ethical and legal issues and presents these to ChatGPT to come up with answers, 
      the first indications of the strengths and weaknesses are ascertained. 
      CONCLUSION: Due to the rapid technical development and access to ever increasing 
      quantities of data, the utilization should be closely observed and evaluated.
CI  - © 2023. The Author(s), under exclusive licence to Springer Medizin Verlag GmbH, 
      ein Teil von Springer Nature.
FAU - Schmidt, Kurt W
AU  - Schmidt KW
AD  - Zentrum für Ethik in der Medizin, Agaplesion Markus Krankenhaus, 
      Wilhelm-Epstein-Str.&nbsp;4, 60431, Frankfurt a. M., Deutschland. 
      kurt.schmidt@ekhn.de.
FAU - Lechner, Fabian
AU  - Lechner F
AD  - Institut für Künstliche Intelligenz, Universitätsklinikum Gießen und Marburg, 
      Marburg, Deutschland.
LA  - ger
PT  - English Abstract
PT  - Journal Article
PT  - Review
TT  - ChatGPT: Hilfe bei der medizinethischen Entscheidungsfindung?
DEP - 20231011
PL  - Germany
TA  - Inn Med (Heidelb)
JT  - Innere Medizin (Heidelberg, Germany)
JID - 9918384885306676
SB  - IM
MH  - Female
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Ethics Committees
MH  - Clinical Decision-Making
MH  - Decision Making
MH  - Ethics, Medical
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Empathy
OT  - Ethical analysis
OT  - Ethics committees
OT  - Legal aspects
EDAT- 2023/10/12 00:43
MHDA- 2023/10/27 06:42
CRDT- 2023/10/11 23:40
PHST- 2023/09/14 00:00 [accepted]
PHST- 2023/10/27 06:42 [medline]
PHST- 2023/10/12 00:43 [pubmed]
PHST- 2023/10/11 23:40 [entrez]
AID - 10.1007/s00108-023-01601-2 [pii]
AID - 10.1007/s00108-023-01601-2 [doi]
PST - ppublish
SO  - Inn Med (Heidelb). 2023 Nov;64(11):1065-1071. doi: 10.1007/s00108-023-01601-2. 
      Epub 2023 Oct 11.

PMID- 37999958
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231211
IS  - 1929-0748 (Print)
IS  - 1929-0748 (Electronic)
IS  - 1929-0748 (Linking)
VI  - 12
DP  - 2023 Nov 24
TI  - Usability and Efficacy of Artificial Intelligence Chatbots (ChatGPT) for Health 
      Sciences Students: Protocol for a Crossover Randomized Controlled Trial.
PG  - e51873
LID - 10.2196/51873 [doi]
LID - e51873
AB  - BACKGROUND: The integration of artificial intelligence (AI) into health sciences 
      students' education holds significant importance. The rapid advancement of AI has 
      opened new horizons in scientific writing and has the potential to reshape 
      human-technology interactions. AI in education may impact critical thinking, 
      leading to unintended consequences that need to be addressed. Understanding the 
      implications of AI adoption in education is essential for ensuring its 
      responsible and effective use, empowering health sciences students to navigate 
      AI-driven technologies' evolving field with essential knowledge and skills. 
      OBJECTIVE: This study aims to provide details on the study protocol and the 
      methods used to investigate the usability and efficacy of ChatGPT, a large 
      language model. The primary focus is on assessing its role as a supplementary 
      learning tool for improving learning processes and outcomes among undergraduate 
      health sciences students, with a specific emphasis on chronic diseases. METHODS: 
      This single-blinded, crossover, randomized, controlled trial is part of a broader 
      mixed methods study, and the primary emphasis of this paper is on the 
      quantitative component of the overall research. A total of 50 students will be 
      recruited for this study. The alternative hypothesis posits that there will be a 
      significant difference in learning outcomes and technology usability between 
      students using ChatGPT (group A) and those using standard web-based tools (group 
      B) to access resources and complete assignments. Participants will be allocated 
      to sequence AB or BA in a 1:1 ratio using computer-generated randomization. Both 
      arms include students' participation in a writing assignment intervention, with a 
      washout period of 21 days between interventions. The primary outcome is the 
      measure of the technology usability and effectiveness of ChatGPT, whereas the 
      secondary outcome is the measure of students' perceptions and experiences with 
      ChatGPT as a learning tool. Outcome data will be collected up to 24 hours after 
      the interventions. RESULTS: This study aims to understand the potential benefits 
      and challenges of incorporating AI as an educational tool, particularly in the 
      context of student learning. The findings are expected to identify critical areas 
      that need attention and help educators develop a deeper understanding of AI's 
      impact on the educational field. By exploring the differences in the usability 
      and efficacy between ChatGPT and conventional web-based tools, this study seeks 
      to inform educators and students on the responsible integration of AI into 
      academic settings, with a specific focus on health sciences education. 
      CONCLUSIONS: By exploring the usability and efficacy of ChatGPT compared with 
      conventional web-based tools, this study seeks to inform educators and students 
      about the responsible integration of AI into academic settings. TRIAL 
      REGISTRATION: ClinicalTrails.gov NCT05963802; 
      https://clinicaltrials.gov/study/NCT05963802. INTERNATIONAL REGISTERED REPORT 
      IDENTIFIER (IRRID): PRR1-10.2196/51873.
CI  - ©Mirella Veras, Joseph-Omer Dyer, Morgan Rooney, Paulo Goberlânio Barros Silva, 
      Derek Rutherford, Dahlia Kairy. Originally published in JMIR Research Protocols 
      (https://www.researchprotocols.org), 24.11.2023.
FAU - Veras, Mirella
AU  - Veras M
AUID- ORCID: 0000-0002-4124-1543
AD  - Health Sciences, Carleton University, Ottawa, ON, Canada.
AD  - Centre for Interdisciplinary Research in Rehabilitation of Greater Montreal, 
      Montréal, QC, Canada.
FAU - Dyer, Joseph-Omer
AU  - Dyer JO
AUID- ORCID: 0000-0002-7570-9941
AD  - École de Réadaptation, Faculté de Médecine, Université de Montréal, Montréal, QC, 
      Canada.
AD  - Groupe Interdisciplinaire de Recherche sur la Cognition et le Raisonnement 
      Professionnel, Faculty of Medicine, Université de Montréal, Montréal, QC, Canada.
FAU - Rooney, Morgan
AU  - Rooney M
AUID- ORCID: 0000-0001-5261-5929
AD  - Teaching and Learning Services, Carleton University, Ottawa, ON, Canada.
FAU - Barros Silva, Paulo Goberlânio
AU  - Barros Silva PG
AUID- ORCID: 0000-0002-1513-9027
AD  - Centro Universitário Christus, Fortaleza, Ceara, Brazil.
FAU - Rutherford, Derek
AU  - Rutherford D
AUID- ORCID: 0000-0003-2688-6201
AD  - School of Physiotherapy, Dalhousie University, Halifax, NS, Canada.
FAU - Kairy, Dahlia
AU  - Kairy D
AUID- ORCID: 0000-0001-6872-6607
AD  - Centre for Interdisciplinary Research in Rehabilitation of Greater Montreal, 
      Montréal, QC, Canada.
AD  - École de Réadaptation, Faculté de Médecine, Université de Montréal, Montréal, QC, 
      Canada.
AD  - Institut Universitaire sur la Réadaptation en Déficience Physique de Montréal, 
      Centre Intégré Universitaire de Santé et Services Sociaux du 
      Centre-Sud-de-l'Île-de-Montréal, Montréal, QC, Canada.
LA  - eng
SI  - ClinicalTrials.gov/NCT05963802
PT  - Journal Article
DEP - 20231124
PL  - Canada
TA  - JMIR Res Protoc
JT  - JMIR research protocols
JID - 101599504
PMC - PMC10709780
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - OpenAI
OT  - RCT
OT  - artificial intelligence
OT  - crossover RCT
OT  - education
OT  - health sciences
OT  - learning outcomes
OT  - perceptions
OT  - randomized controlled trial
OT  - usability
COIS- Conflicts of Interest: None declared.
EDAT- 2023/11/24 12:43
MHDA- 2023/11/24 12:44
PMCR- 2023/11/24
CRDT- 2023/11/24 11:54
PHST- 2023/08/15 00:00 [received]
PHST- 2023/10/20 00:00 [accepted]
PHST- 2023/10/18 00:00 [revised]
PHST- 2023/11/24 12:44 [medline]
PHST- 2023/11/24 12:43 [pubmed]
PHST- 2023/11/24 11:54 [entrez]
PHST- 2023/11/24 00:00 [pmc-release]
AID - v12i1e51873 [pii]
AID - 10.2196/51873 [doi]
PST - epublish
SO  - JMIR Res Protoc. 2023 Nov 24;12:e51873. doi: 10.2196/51873.

PMID- 38483451
OWN - NLM
STAT- MEDLINE
DCOM- 20240315
LR  - 20240331
IS  - 1438-8871 (Electronic)
IS  - 1439-4456 (Print)
IS  - 1438-8871 (Linking)
VI  - 26
DP  - 2024 Mar 14
TI  - Quality and Dependability of ChatGPT and DingXiangYuan Forums for Remote 
      Orthopedic Consultations: Comparative Analysis.
PG  - e50882
LID - 10.2196/50882 [doi]
LID - e50882
AB  - BACKGROUND: The widespread use of artificial intelligence, such as ChatGPT 
      (OpenAI), is transforming sectors, including health care, while separate 
      advancements of the internet have enabled platforms such as China's DingXiangYuan 
      to offer remote medical services. OBJECTIVE: This study evaluates ChatGPT-4's 
      responses against those of professional health care providers in telemedicine, 
      assessing artificial intelligence's capability to support the surge in remote 
      medical consultations and its impact on health care delivery. METHODS: We sourced 
      remote orthopedic consultations from "Doctor DingXiang," with responses from its 
      certified physicians as the control and ChatGPT's responses as the experimental 
      group. In all, 3 blindfolded, experienced orthopedic surgeons assessed responses 
      against 7 criteria: "logical reasoning," "internal information," "external 
      information," "guiding function," "therapeutic effect," "medical knowledge 
      popularization education," and "overall satisfaction." We used Fleiss κ to 
      measure agreement among multiple raters. RESULTS: Initially, consultation records 
      for a cumulative count of 8 maladies (equivalent to 800 cases) were gathered. We 
      ultimately included 73 consultation records by May 2023, following primary and 
      rescreening, in which no communication records containing private information, 
      images, or voice messages were transmitted. After statistical scoring, we 
      discovered that ChatGPT's "internal information" score (mean 4.61, SD 0.52 points 
      vs mean 4.66, SD 0.49 points; P=.43) and "therapeutic effect" score (mean 4.43, 
      SD 0.75 points vs mean 4.55, SD 0.62 points; P=.32) were lower than those of the 
      control group, but the differences were not statistically significant. ChatGPT 
      showed better performance with a higher "logical reasoning" score (mean 4.81, SD 
      0.36 points vs mean 4.75, SD 0.39 points; P=.38), "external information" score 
      (mean 4.06, SD 0.72 points vs mean 3.92, SD 0.77 points; P=.25), and "guiding 
      function" score (mean 4.73, SD 0.51 points vs mean 4.72, SD 0.54 points; P=.96), 
      although the differences were not statistically significant. Meanwhile, the 
      "medical knowledge popularization education" score of ChatGPT was better than 
      that of the control group (mean 4.49, SD 0.67 points vs mean 3.87, SD 1.01 
      points; P&lt;.001), and the difference was statistically significant. In terms of 
      "overall satisfaction," the difference was not statistically significant between 
      the groups (mean 8.35, SD 1.38 points vs mean 8.37, SD 1.24 points; P=.92). 
      According to how Fleiss κ values were interpreted, 6 of the control group's score 
      points were classified as displaying "fair agreement" (P&lt;.001), and 1 was 
      classified as showing "substantial agreement" (P&lt;.001). In the experimental 
      group, 3 points were classified as indicating "fair agreement," while 4 suggested 
      "moderate agreement" (P&lt;.001). CONCLUSIONS: ChatGPT-4 matches the expertise found 
      in DingXiangYuan forums' paid consultations, excelling particularly in scientific 
      education. It presents a promising alternative for remote health advice. For 
      health care professionals, it could act as an aid in patient education, while 
      patients may use it as a convenient tool for health inquiries.
CI  - ©Zhaowen Xue, Yiming Zhang, Wenyi Gan, Huajun Wang, Guorong She, Xiaofei Zheng. 
      Originally published in the Journal of Medical Internet Research 
      (https://www.jmir.org), 14.03.2024.
FAU - Xue, Zhaowen
AU  - Xue Z
AUID- ORCID: 0009-0001-5807-9810
AD  - Department of Bone and Joint Surgery and Sports Medicine Center, The First 
      Affiliated Hospital, The First Affiliated Hospital of Jinan University, 
      Guangzhou, China.
FAU - Zhang, Yiming
AU  - Zhang Y
AUID- ORCID: 0000-0003-3366-9790
AD  - Department of Bone and Joint Surgery and Sports Medicine Center, The First 
      Affiliated Hospital, The First Affiliated Hospital of Jinan University, 
      Guangzhou, China.
FAU - Gan, Wenyi
AU  - Gan W
AUID- ORCID: 0000-0003-1886-8062
AD  - Department of Bone and Joint Surgery and Sports Medicine Center, The First 
      Affiliated Hospital, The First Affiliated Hospital of Jinan University, 
      Guangzhou, China.
FAU - Wang, Huajun
AU  - Wang H
AUID- ORCID: 0000-0001-6235-8997
AD  - Department of Bone and Joint Surgery and Sports Medicine Center, The First 
      Affiliated Hospital, The First Affiliated Hospital of Jinan University, 
      Guangzhou, China.
FAU - She, Guorong
AU  - She G
AUID- ORCID: 0000-0001-7392-2055
AD  - Department of Bone and Joint Surgery and Sports Medicine Center, The First 
      Affiliated Hospital, The First Affiliated Hospital of Jinan University, 
      Guangzhou, China.
FAU - Zheng, Xiaofei
AU  - Zheng X
AUID- ORCID: 0000-0001-7502-6131
AD  - Department of Bone and Joint Surgery and Sports Medicine Center, The First 
      Affiliated Hospital, The First Affiliated Hospital of Jinan University, 
      Guangzhou, China.
LA  - eng
PT  - Journal Article
DEP - 20240314
PL  - Canada
TA  - J Med Internet Res
JT  - Journal of medical Internet research
JID - 100959882
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - *Remote Consultation
MH  - *Telemedicine
MH  - *Education, Medical
MH  - Educational Status
PMC - PMC10979330
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - consultation
OT  - musculoskeletal
OT  - natural language processing
OT  - orthopaedic
OT  - orthopaedics
OT  - remote medical consultation
COIS- Conflicts of Interest: None declared.
EDAT- 2024/03/14 12:47
MHDA- 2024/03/15 06:44
PMCR- 2024/03/14
CRDT- 2024/03/14 11:53
PHST- 2023/07/15 00:00 [received]
PHST- 2024/01/30 00:00 [accepted]
PHST- 2023/11/04 00:00 [revised]
PHST- 2024/03/15 06:44 [medline]
PHST- 2024/03/14 12:47 [pubmed]
PHST- 2024/03/14 11:53 [entrez]
PHST- 2024/03/14 00:00 [pmc-release]
AID - v26i1e50882 [pii]
AID - 10.2196/50882 [doi]
PST - epublish
SO  - J Med Internet Res. 2024 Mar 14;26:e50882. doi: 10.2196/50882.

PMID- 38361810
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240217
IS  - 2398-8835 (Electronic)
IS  - 2398-8835 (Linking)
VI  - 7
IP  - 2
DP  - 2024 Feb
TI  - What are the applications of ChatGPT in healthcare: Gain or loss?
PG  - e1878
LID - 10.1002/hsr2.1878 [doi]
LID - e1878
FAU - Montazeri, Mahdieh
AU  - Montazeri M
AUID- ORCID: 0000-0001-6987-6199
AD  - Department of Health Information Sciences, Faculty of Management and Medical 
      Information Sciences Kerman University of Medical Sciences Kerman Iran.
FAU - Galavi, Zahra
AU  - Galavi Z
AUID- ORCID: 0000-0003-1179-0055
AD  - Department of Health Information Sciences, Faculty of Management and Medical 
      Information Sciences Kerman University of Medical Sciences Kerman Iran.
FAU - Ahmadian, Leila
AU  - Ahmadian L
AUID- ORCID: 0000-0002-6487-2209
AD  - Department of Health Information Sciences, Faculty of Management and Medical 
      Information Sciences Kerman University of Medical Sciences Kerman Iran.
LA  - eng
PT  - Journal Article
DEP - 20240214
PL  - United States
TA  - Health Sci Rep
JT  - Health science reports
JID - 101728855
PMC - PMC10867364
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - language model
OT  - recommendations
COIS- The authors declare no conflicts of interest.
EDAT- 2024/02/16 06:43
MHDA- 2024/02/16 06:44
PMCR- 2024/02/14
CRDT- 2024/02/16 03:48
PHST- 2023/08/13 00:00 [received]
PHST- 2023/12/23 00:00 [revised]
PHST- 2024/01/21 00:00 [accepted]
PHST- 2024/02/16 06:44 [medline]
PHST- 2024/02/16 06:43 [pubmed]
PHST- 2024/02/16 03:48 [entrez]
PHST- 2024/02/14 00:00 [pmc-release]
AID - HSR21878 [pii]
AID - 10.1002/hsr2.1878 [doi]
PST - epublish
SO  - Health Sci Rep. 2024 Feb 14;7(2):e1878. doi: 10.1002/hsr2.1878. eCollection 2024 
      Feb.

PMID- 37675304
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230908
IS  - 2305-5839 (Print)
IS  - 2305-5847 (Electronic)
IS  - 2305-5839 (Linking)
VI  - 11
IP  - 10
DP  - 2023 Aug 30
TI  - New era after ChatGPT in ophthalmology: advances from data-based decision support 
      to patient-centered generative artificial intelligence.
PG  - 337
LID - 10.21037/atm-23-1598 [doi]
LID - 337
FAU - Choi, Joon Yul
AU  - Choi JY
AD  - Department of Biomedical Engineering, Yonsei University, Wonju, South Korea.
FAU - Yoo, Tae Keun
AU  - Yoo TK
AD  - B&amp;VIIT Eye Center, Seoul, South Korea.
AD  - VISUWORKS, Seoul, South Korea.
LA  - eng
PT  - Comment
PT  - Editorial
DEP - 20230630
PL  - China
TA  - Ann Transl Med
JT  - Annals of translational medicine
JID - 101617978
CON - Ann Transl Med. 2023 Mar 15;11(5):219. PMID: 37007552
PMC - PMC10477620
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - generative artificial intelligence (generative AI)
OT  - ophthalmology
COIS- Conflicts of Interest: Both authors have completed the ICMJE uniform disclosure 
      form (available at 
      https://atm.amegroups.com/article/view/10.21037/atm-23-1598/coif). TKY reported 
      that he served as a speaker for an academic lecture in VUNO and Hangil Eye 
      Hospital, and served as a lecturer for a commercial conference held by the Korea 
      Association of Intelligence Wellcare Industries (KIWI). TKY is an employee of 
      B&amp;VIIT Eye Center and VISUWORKS. He received a salary as part of the standard 
      compensation package. He also received research grants for refractive surgery 
      from Carl Zeiss Meditec AG. The research grants did not affect this manuscript. 
      The other author has no conflicts of interest to declare.
EDAT- 2023/09/07 06:42
MHDA- 2023/09/07 06:43
PMCR- 2023/08/30
CRDT- 2023/09/07 04:19
PHST- 2023/05/07 00:00 [received]
PHST- 2023/06/28 00:00 [accepted]
PHST- 2023/09/07 06:43 [medline]
PHST- 2023/09/07 06:42 [pubmed]
PHST- 2023/09/07 04:19 [entrez]
PHST- 2023/08/30 00:00 [pmc-release]
AID - atm-11-10-337 [pii]
AID - 10.21037/atm-23-1598 [doi]
PST - ppublish
SO  - Ann Transl Med. 2023 Aug 30;11(10):337. doi: 10.21037/atm-23-1598. Epub 2023 Jun 
      30.

PMID- 37950763
OWN - NLM
STAT- MEDLINE
DCOM- 20240130
LR  - 20240302
IS  - 1434-3916 (Electronic)
IS  - 0936-8051 (Linking)
VI  - 144
IP  - 2
DP  - 2024 Feb
TI  - Simplifying radiologic reports with natural language processing: a novel approach 
      using ChatGPT in enhancing patient understanding of MRI results.
PG  - 611-618
LID - 10.1007/s00402-023-05113-4 [doi]
AB  - PURPOSE: The aim of this prospective cohort study was to assess the factual 
      accuracy, completeness of medical information, and potential harmfulness of 
      incorrect conclusions by medical professionals in automatically generated texts 
      of varying complexity (1) using ChatGPT, Furthermore, patients without a medical 
      background were asked to evaluate comprehensibility, information density, and 
      conclusion possibilities (2). METHODS: In the study, five different simplified 
      versions of MRI findings of the knee of different complexity (A: simple, B: 
      moderate, C: complex) were each created using ChatGPT. Subsequently, a group of 
      four medical professionals (two orthopedic surgeons and two radiologists) and a 
      group of 20 consecutive patients evaluated the created reports. For this purpose, 
      all participants received a group of simplified reports (simple, moderate, and 
      severe) at intervals of 1&nbsp;week each for their respective evaluation using a 
      specific questionnaire. Each questionnaire consisted of the original report, the 
      simplified report, and a series of statements to assess the quality of the 
      simplified reports. Participants were asked to rate their level of agreement with 
      a five-point Likert scale. RESULTS: The evaluation of the medical specialists 
      showed that the findings produced were consistent in quality depending on their 
      complexity. Factual correctness, reproduction of relevant information and 
      comprehensibility for patients were rated on average as "Agree". The question 
      about possible harm resulted in an average of "Disagree". The evaluation of 
      patients also revealed consistent quality of reports, depending on complexity. 
      Simplicity of word choice and sentence structure was rated "Agree" on average, 
      with significant differences between simple and complex findings (p = 0.0039) as 
      well as between moderate and complex findings (p = 0.0222). Participants reported 
      being significantly better at knowing what the text was about (p = 0.001) and 
      drawing the correct conclusions the more simplified the report of findings was 
      (p = 0.013829). The question of whether the text informed them as well as a 
      healthcare professional was answered as "Neutral" across all findings. 
      CONCLUSION: By using ChatGPT, MRI reports can be simplified automatically with 
      consistent quality so that the relevant information is understandable to 
      patients. However, a report generated in this way does not replace a thorough 
      discussion between specialist and patient.
CI  - © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
      part of Springer Nature.
FAU - Schmidt, Sebastian
AU  - Schmidt S
AD  - Department of Orthopaedic and Trauma Surgery, Orthopädische Klinik Paulinenhilfe, 
      Diakonieklinikum, Rosenbergstrasse 38, 70192, Stuttgart, Germany. 
      schmidt.sebastian@fn.de.
FAU - Zimmerer, Alexander
AU  - Zimmerer A
AD  - Department of Orthopaedic and Trauma Surgery, Orthopädische Klinik Paulinenhilfe, 
      Diakonieklinikum, Rosenbergstrasse 38, 70192, Stuttgart, Germany.
AD  - Department of Orthopaedics and Orthopaedic Surgery, University Medicine 
      Greifswald, Ferdinand-Sauerbruch-Straße, 17475, Greifswald, Germany.
FAU - Cucos, Tudor
AU  - Cucos T
AD  - Department of Radiology, ViDia Christliche Kliniken Karlsruhe, Steinhäuser Straße 
      18, 76135, Karlsruhe, Germany.
FAU - Feucht, Matthias
AU  - Feucht M
AD  - Department of Orthopaedic and Trauma Surgery, Orthopädische Klinik Paulinenhilfe, 
      Diakonieklinikum, Rosenbergstrasse 38, 70192, Stuttgart, Germany.
FAU - Navas, Luis
AU  - Navas L
AD  - Department of Orthopaedic and Trauma Surgery, Orthopädische Klinik Paulinenhilfe, 
      Diakonieklinikum, Rosenbergstrasse 38, 70192, Stuttgart, Germany.
LA  - eng
PT  - Journal Article
DEP - 20231111
PL  - Germany
TA  - Arch Orthop Trauma Surg
JT  - Archives of orthopaedic and trauma surgery
JID - 9011043
SB  - IM
MH  - Humans
MH  - *Natural Language Processing
MH  - Prospective Studies
MH  - *Health Personnel
MH  - Knee Joint
MH  - Magnetic Resonance Imaging
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - Knee
OT  - MRI
EDAT- 2023/11/11 20:49
MHDA- 2024/01/30 12:43
CRDT- 2023/11/11 11:03
PHST- 2023/06/26 00:00 [received]
PHST- 2023/10/15 00:00 [accepted]
PHST- 2024/01/30 12:43 [medline]
PHST- 2023/11/11 20:49 [pubmed]
PHST- 2023/11/11 11:03 [entrez]
AID - 10.1007/s00402-023-05113-4 [pii]
AID - 10.1007/s00402-023-05113-4 [doi]
PST - ppublish
SO  - Arch Orthop Trauma Surg. 2024 Feb;144(2):611-618. doi: 
      10.1007/s00402-023-05113-4. Epub 2023 Nov 11.

PMID- 37056538
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230415
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - Assessing the Capability of ChatGPT in Answering First- and Second-Order 
      Knowledge Questions on Microbiology as per Competency-Based Medical Education 
      Curriculum.
PG  - e36034
LID - 10.7759/cureus.36034 [doi]
LID - e36034
AB  - Background and objective ChatGPT is an&nbsp;artificial intelligence (AI) language 
      model that has been trained to process and respond to questions across a wide 
      range of topics. It is also capable of solving problems in medical educational 
      topics. However, the capability of ChatGPT to accurately answer first- and 
      second-order knowledge questions in the field of microbiology has not been 
      explored so far. Hence, in this study, we aimed to analyze the capability of 
      ChatGPT in answering first- and second-order questions on the subject of 
      microbiology. Materials and methods Based on the competency-based medical 
      education (CBME) curriculum of the subject of microbiology, we prepared a set of 
      first-order and second-order questions. For the total of eight modules in the 
      CBME curriculum for microbiology, we prepared six first-order and six 
      second-order knowledge questions according to the National Medical 
      Commission-recommended CBME curriculum, amounting to a total of (8 x 12) 96 
      questions. The questions were checked for content validity by three expert 
      microbiologists. These questions were used to converse with ChatGPT by a single 
      user and responses were recorded for further analysis. The answers were scored by 
      three microbiologists on a rating scale of 0-5. The average of three scores was 
      taken as the final score for analysis. As the data were not normally distributed, 
      we used a non-parametric statistical test. The overall scores were tested by a 
      one-sample median test with&nbsp;hypothetical values of 4 and 5. The scores of answers 
      to first-order and second-order questions were compared by the Mann-Whitney U 
      test. Module-wise responses were tested by the Kruskall-Wallis test followed by 
      the post hoc test for pairwise comparisons. Results The overall score of 96 
      answers was 4.04 ±0.37 (median: 4.17, Q1-Q3: 3.88-4.33) with the mean score 
      of&nbsp;answers to first-order knowledge questions being 4.07 ±0.32 (median: 4.17, 
      Q1-Q3: 4-4.33) and that of answers to second-order knowledge questions being 3.99 
      ±0.43 (median: 4, Q1-Q3: 3.67-4.33) (Mann-Whitney p=0.4). The score was 
      significantly below the score of 5 (one-sample median test p&lt;0.0001) and similar 
      to 4 (one-sample median test p=0.09). Overall, there was a variation in median 
      scores obtained in eight categories of topics in microbiology, indicating 
      inconsistent performance in different topics. Conclusion The results of the study 
      indicate that ChatGPT is capable of answering both first- and second-order 
      knowledge questions related to the subject of microbiology. The model achieved an 
      accuracy of approximately 80% and there was no difference between the model's 
      capability of answering first-order questions and second-order knowledge 
      questions. The findings of this study suggest that ChatGPT has the potential to 
      be an effective tool for automated question-answering in the field of 
      microbiology. However, continued improvement in the training and development of 
      language models is necessary to enhance their performance and make them suitable 
      for academic use.
CI  - Copyright © 2023, Das et al.
FAU - Das, Dipmala
AU  - Das D
AD  - Microbiology, All India Institute of Medical Sciences, Deoghar, Deoghar, IND.
FAU - Kumar, Nikhil
AU  - Kumar N
AD  - Pathology, All India Institute of Medical Sciences, Deoghar, Deoghar, IND.
FAU - Longjam, Langamba Angom
AU  - Longjam LA
AD  - Microbiology, IQ City Medical College Hospital, Durgapur, IND.
FAU - Sinha, Ranwir
AU  - Sinha R
AD  - Pathology, All India Institute of Medical Sciences, Deoghar, Deoghar, IND.
FAU - Deb Roy, Asitava
AU  - Deb Roy A
AD  - Pathology, All India Institute of Medical Sciences, Deoghar, Deoghar, IND.
FAU - Mondal, Himel
AU  - Mondal H
AD  - Physiology, All India Institute of Medical Sciences, Deoghar, Deoghar, IND.
FAU - Gupta, Pratima
AU  - Gupta P
AD  - Microbiology, All India Institute of Medical Sciences, Deoghar, Deoghar, IND.
LA  - eng
PT  - Journal Article
DEP - 20230312
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10086829
OTO - NOTNLM
OT  - artificial intelligence
OT  - automated question-answering
OT  - chatgpt
OT  - competency-based medical education
OT  - first-order questions
OT  - language model
OT  - medical education
OT  - microbiology
OT  - question
OT  - second-order questions
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/04/15 06:00
MHDA- 2023/04/15 06:01
PMCR- 2023/03/12
CRDT- 2023/04/14 02:29
PHST- 2023/03/11 00:00 [accepted]
PHST- 2023/04/15 06:01 [medline]
PHST- 2023/04/14 02:29 [entrez]
PHST- 2023/04/15 06:00 [pubmed]
PHST- 2023/03/12 00:00 [pmc-release]
AID - 10.7759/cureus.36034 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 12;15(3):e36034. doi: 10.7759/cureus.36034. eCollection 2023 
      Mar.

PMID- 38078148
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231211
IS  - 2666-3864 (Electronic)
IS  - 2666-3864 (Linking)
VI  - 4
IP  - 11
DP  - 2023 Nov
TI  - Accurately detecting AI text when ChatGPT is told to write like a chemist.
LID - 101672 [pii]
LID - 10.1016/j.xcrp.2023.101672 [doi]
AB  - Large language models like ChatGPT can generate authentic-seeming text at 
      lightning speed, but many journal publishers reject language models as authors on 
      manuscripts. Thus, a means to accurately distinguish human-generated from 
      artificial intelligence (AI)-generated text is immediately needed. We recently 
      developed an accurate AI text detector for scientific journals and, herein, test 
      its ability in a variety of challenging situations, including on human text from 
      a wide variety of chemistry journals, on AI text from the most advanced publicly 
      available language model (GPT-4), and, most important, on AI text generated using 
      prompts designed to obfuscate AI use. In all cases, AI and human text was 
      assigned with high accuracy. ChatGPT-generated text can be readily detected in 
      chemistry journals; this advance is a fundamental prerequisite for understanding 
      how automated text generation will impact scientific publishing from now into the 
      future.
FAU - Desaire, Heather
AU  - Desaire H
AD  - Department of Chemistry, University of Kansas, Lawrence, KS 66045, USA.
AD  - Lead contact.
FAU - Chua, Aleesa E
AU  - Chua AE
AD  - Department of Chemistry, University of Kansas, Lawrence, KS 66045, USA.
FAU - Kim, Min-Gyu
AU  - Kim MG
AD  - Department of Chemistry, University of Kansas, Lawrence, KS 66045, USA.
FAU - Hua, David
AU  - Hua D
AD  - Department of Chemistry, University of Kansas, Lawrence, KS 66045, USA.
LA  - eng
GR  - R35 GM130354/GM/NIGMS NIH HHS/United States
PT  - Journal Article
DEP - 20231106
PL  - United States
TA  - Cell Rep Phys Sci
JT  - Cell reports. Physical science
JID - 101769239
PMC - PMC10704924
MID - NIHMS1945523
COIS- DECLARATION OF INTERESTS The authors declare no competing interests.
EDAT- 2023/12/11 12:42
MHDA- 2023/12/11 12:43
PMCR- 2023/12/08
CRDT- 2023/12/11 06:40
PHST- 2023/12/11 12:43 [medline]
PHST- 2023/12/11 12:42 [pubmed]
PHST- 2023/12/11 06:40 [entrez]
PHST- 2023/12/08 00:00 [pmc-release]
AID - 101672 [pii]
AID - 10.1016/j.xcrp.2023.101672 [doi]
PST - ppublish
SO  - Cell Rep Phys Sci. 2023 Nov;4(11):101672. doi: 10.1016/j.xcrp.2023.101672. Epub 
      2023 Nov 6.

PMID- 37598727
OWN - NLM
STAT- MEDLINE
DCOM- 20231106
LR  - 20231106
IS  - 1879-8519 (Electronic)
IS  - 1879-8500 (Linking)
VI  - 13
IP  - 6
DP  - 2023 Nov-Dec
TI  - Unlocking the Power of ChatGPT, Artificial Intelligence, and Large Language 
      Models: Practical Suggestions for Radiation Oncologists.
PG  - e484-e490
LID - S1879-8500(23)00213-8 [pii]
LID - 10.1016/j.prro.2023.06.011 [doi]
AB  - Recent advances in artificial intelligence (AI), such as generative AI and large 
      language models (LLMs), have generated significant excitement about the potential 
      of AI to revolutionize our lives, work, and interaction with technology. This 
      article explores the practical applications of LLMs, particularly ChatGPT, in the 
      field of radiation oncology. We offer a guide on how radiation oncologists can 
      interact with LLMs like ChatGPT in their routine clinical and administrative 
      tasks, highlighting potential use cases of the present and future. We also 
      highlight limitations and ethical considerations, including the current state of 
      LLMs in decision making, protection of sensitive data, and the important role of 
      human review of AI-generated content.
CI  - Copyright © 2023 American Society for Radiation Oncology. Published by Elsevier 
      Inc. All rights reserved.
FAU - Waters, Michael R
AU  - Waters MR
AD  - Department of Radiation Oncology, Washington University School of Medicine, St. 
      Louis, Missouri.
FAU - Aneja, Sanjay
AU  - Aneja S
AD  - Department of Radiation Oncology, Yale School of Medicine, New Haven, 
      Connecticut.
FAU - Hong, Julian C
AU  - Hong JC
AD  - Department of Radiation Oncology and Bakar Computational Health Sciences 
      Institute, University of California, San Francisco, San Francisco, California. 
      Electronic address: Julian.Hong@ucsf.edu.
LA  - eng
PT  - Journal Article
DEP - 20230819
PL  - United States
TA  - Pract Radiat Oncol
JT  - Practical radiation oncology
JID - 101558279
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Radiation Oncologists
MH  - Language
MH  - *Radiation Oncology
EDAT- 2023/08/21 00:41
MHDA- 2023/11/06 06:42
CRDT- 2023/08/20 19:23
PHST- 2023/06/21 00:00 [received]
PHST- 2023/06/28 00:00 [revised]
PHST- 2023/06/29 00:00 [accepted]
PHST- 2023/11/06 06:42 [medline]
PHST- 2023/08/21 00:41 [pubmed]
PHST- 2023/08/20 19:23 [entrez]
AID - S1879-8500(23)00213-8 [pii]
AID - 10.1016/j.prro.2023.06.011 [doi]
PST - ppublish
SO  - Pract Radiat Oncol. 2023 Nov-Dec;13(6):e484-e490. doi: 
      10.1016/j.prro.2023.06.011. Epub 2023 Aug 19.

PMID- 37426542
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230718
IS  - 2666-3864 (Electronic)
IS  - 2666-3864 (Linking)
VI  - 4
IP  - 6
DP  - 2023 Jun 21
TI  - Distinguishing academic science writing from humans or ChatGPT with over 99% 
      accuracy using off-the-shelf machine learning tools.
LID - 101426 [pii]
LID - 10.1016/j.xcrp.2023.101426 [doi]
AB  - ChatGPT has enabled access to artificial intelligence (AI)-generated writing for 
      the masses, initiating a culture shift in the way people work, learn, and write. 
      The need to discriminate human writing from AI is now both critical and urgent. 
      Addressing this need, we report a method for discriminating text generated by 
      ChatGPT from (human) academic scientists, relying on prevalent and accessible 
      supervised classification methods. The approach uses new features for 
      discriminating (these) humans from AI; as examples, scientists write long 
      paragraphs and have a penchant for equivocal language, frequently using words 
      like "but," "however," and "although." With a set of 20 features, we built a 
      model that assigns the author, as human or AI, at over 99% accuracy. This 
      strategy could be further adapted and developed by others with basic skills in 
      supervised classification, enabling access to many highly accurate and targeted 
      models for detecting AI usage in academic writing and beyond.
FAU - Desaire, Heather
AU  - Desaire H
AD  - Department of Chemistry, University of Kansas, Lawrence, KS 66045, USA.
AD  - Lead contact.
FAU - Chua, Aleesa E
AU  - Chua AE
AD  - Department of Chemistry, University of Kansas, Lawrence, KS 66045, USA.
FAU - Isom, Madeline
AU  - Isom M
AD  - Department of Chemistry, University of Kansas, Lawrence, KS 66045, USA.
FAU - Jarosova, Romana
AU  - Jarosova R
AD  - Department of Chemistry, University of Kansas, Lawrence, KS 66045, USA.
FAU - Hua, David
AU  - Hua D
AD  - Department of Chemistry, University of Kansas, Lawrence, KS 66045, USA.
LA  - eng
GR  - R35 GM130354/GM/NIGMS NIH HHS/United States
PT  - Journal Article
DEP - 20230607
PL  - United States
TA  - Cell Rep Phys Sci
JT  - Cell reports. Physical science
JID - 101769239
PMC - PMC10328544
MID - NIHMS1911044
COIS- DECLARATION OF INTERESTS The authors declare no competing interests.
EDAT- 2023/07/10 06:42
MHDA- 2023/07/10 06:43
PMCR- 2023/07/07
CRDT- 2023/07/10 05:11
PHST- 2023/07/10 06:43 [medline]
PHST- 2023/07/10 06:42 [pubmed]
PHST- 2023/07/10 05:11 [entrez]
PHST- 2023/07/07 00:00 [pmc-release]
AID - 101426 [pii]
AID - 10.1016/j.xcrp.2023.101426 [doi]
PST - ppublish
SO  - Cell Rep Phys Sci. 2023 Jun 21;4(6):101426. doi: 10.1016/j.xcrp.2023.101426. Epub 
      2023 Jun 7.

PMID- 37972444
OWN - NLM
STAT- MEDLINE
DCOM- 20240102
LR  - 20240216
IS  - 1878-0539 (Electronic)
IS  - 1748-6815 (Linking)
VI  - 88
DP  - 2024 Jan
TI  - Modern Machiavelli? The illusion of ChatGPT-generated patient reviews in plastic 
      and aesthetic surgery based on 9000 review classifications.
PG  - 99-108
LID - S1748-6815(23)00689-7 [pii]
LID - 10.1016/j.bjps.2023.10.119 [doi]
AB  - BACKGROUND: Online patient reviews are crucial in guiding individuals who seek 
      plastic surgery, but artificial chatbots pose a threat of disseminating fake 
      reviews. This study aimed to compare real patient feedback with ChatGPT-generated 
      reviews for the top five US plastic surgery procedures. METHODS: Thirty real 
      patient reviews on rhinoplasty, blepharoplasty, facelift, liposuction, and breast 
      augmentation were collected from RealSelf and used as templates for ChatGPT to 
      generate matching patient reviews. Prolific users (n&nbsp;=&nbsp;30) assessed 150 pairs of 
      reviews to identify human-written and artificial intelligence (AI)-generated 
      reviews. Patient reviews were further assessed using AI content detector software 
      (Copyleaks AI). RESULTS: Among the 9000 classification tasks, 64.3% and 35.7% of 
      reviews were classified as authentic and fake, respectively. On an average, the 
      author (human versus machine) was correctly identified in 59.6% of cases, and 
      this poor classification performance was consistent across all procedures. 
      Patients with prior aesthetic treatment showed poorer classification performance 
      than those without (p&nbsp;&lt;&nbsp;0.05). The mean character count in human-written reviews 
      was significantly higher (p&nbsp;&lt;&nbsp;0.001) that that in AI-generated reviews, with a 
      significant correlation between character count and participants' accuracy rate 
      (p&nbsp;&lt;&nbsp;0.001). Emotional timbre of reviews differed significantly with "happiness" 
      being more prevalent in human-written reviews (p&nbsp;&lt;&nbsp;0.001), and "disappointment" 
      being more prevalent in AI reviews (p&nbsp;=&nbsp;0.005). Copyleaks AI correctly classified 
      96.7% and 69.3% of human-written and ChatGPT-generated reviews, respectively. 
      CONCLUSION: ChatGPT convincingly replicates authentic patient reviews, even 
      deceiving commercial AI detection software. Analyzing emotional tone and review 
      length can help differentiate real from fake reviews, underscoring the need to 
      educate both patients and physicians to prevent misinformation and mistrust.
CI  - Copyright © 2023 British Association of Plastic, Reconstructive and Aesthetic 
      Surgeons. Published by Elsevier Ltd. All rights reserved.
FAU - Knoedler, Samuel
AU  - Knoedler S
AD  - Department of Plastic Surgery and Hand Surgery, Klinikum Rechts der Isar, 
      Technical University of Munich, Munich, Germany; Division of Plastic Surgery, 
      Department of Surgery, Brigham and Women's Hospital, Harvard Medical School, 
      Boston, MA, USA; Instituto Ivo Pitanguy, Hospital Santa Casa de Misericórdia, 
      Pontifícia Universidade Católica do Rio de Janeiro, Rio de Janeiro, Brazil. 
      Electronic address: samuel.knoedler@tum.de.
FAU - Sofo, Giuseppe
AU  - Sofo G
AD  - Instituto Ivo Pitanguy, Hospital Santa Casa de Misericórdia, Pontifícia 
      Universidade Católica do Rio de Janeiro, Rio de Janeiro, Brazil.
FAU - Kern, Barbara
AU  - Kern B
AD  - Department of Plastic Surgery, Charité - Universitätsmedizin Berlin, Corporate 
      Member of Freie Universität Berlin, Humboldt-Universität zu Berlin and Berlin 
      Institute of Health, Berlin, Germany.
FAU - Frank, Konstantin
AU  - Frank K
AD  - Ocean Clinic, Marbella, Spain.
FAU - Cotofana, Sebastian
AU  - Cotofana S
AD  - Centre for Cutaneous Research, Blizard Institute, Queen Mary University of 
      London, London, UK; Department of Dermatology, Erasmus Hospital, Rotterdam, the 
      Netherlands.
FAU - von Isenburg, Sarah
AU  - von Isenburg S
AD  - Private Practice, Plastische Chirurgie München Dres. Neuhann-Lorenz &amp; v. 
      Isenburg, Munich, Germany.
FAU - Könneker, Sören
AU  - Könneker S
AD  - Department of Plastic Surgery and Hand Surgery, University Hospital Zürich, 
      Zurich, Switzerland.
FAU - Mazzarone, Francesco
AU  - Mazzarone F
AD  - Instituto Ivo Pitanguy, Hospital Santa Casa de Misericórdia, Pontifícia 
      Universidade Católica do Rio de Janeiro, Rio de Janeiro, Brazil.
FAU - Dorafshar, Amir H
AU  - Dorafshar AH
AD  - Division of Plastic and Reconstructive Surgery, Rush University Medical Center, 
      Chicago, IL, USA.
FAU - Knoedler, Leonard
AU  - Knoedler L
AD  - Division of Plastic and Reconstructive Surgery, Massachusetts General Hospital, 
      Harvard Medical School, Boston, MA, USA.
FAU - Alfertshofer, Michael
AU  - Alfertshofer M
AD  - Division of Hand, Plastic and Aesthetic Surgery, Ludwig-Maximilians-University 
      Munich, Munich, Germany.
LA  - eng
PT  - Journal Article
DEP - 20231029
PL  - Netherlands
TA  - J Plast Reconstr Aesthet Surg
JT  - Journal of plastic, reconstructive &amp; aesthetic surgery : JPRAS
JID - 101264239
SB  - IM
MH  - Humans
MH  - *Illusions
MH  - Artificial Intelligence
MH  - *Surgery, Plastic
MH  - *Plastic Surgery Procedures
MH  - Esthetics
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - GPT-4
OT  - Large language model
OT  - Patient review
OT  - Plastic Surgery
COIS- Declaration of Competing Interest None declared.
EDAT- 2023/11/17 15:24
MHDA- 2024/01/02 11:46
CRDT- 2023/11/16 18:01
PHST- 2023/09/27 00:00 [received]
PHST- 2023/10/18 00:00 [revised]
PHST- 2023/10/23 00:00 [accepted]
PHST- 2024/01/02 11:46 [medline]
PHST- 2023/11/17 15:24 [pubmed]
PHST- 2023/11/16 18:01 [entrez]
AID - S1748-6815(23)00689-7 [pii]
AID - 10.1016/j.bjps.2023.10.119 [doi]
PST - ppublish
SO  - J Plast Reconstr Aesthet Surg. 2024 Jan;88:99-108. doi: 
      10.1016/j.bjps.2023.10.119. Epub 2023 Oct 29.

PMID- 37083166
OWN - NLM
STAT- MEDLINE
DCOM- 20230530
LR  - 20240124
IS  - 1437-4331 (Electronic)
IS  - 1434-6621 (Linking)
VI  - 61
IP  - 7
DP  - 2023 Jun 27
TI  - Potentials and pitfalls of ChatGPT and natural-language artificial intelligence 
      models for the understanding of laboratory medicine test results. An assessment 
      by the European Federation of Clinical Chemistry and Laboratory Medicine (EFLM) 
      Working Group on Artificial Intelligence (WG-AI).
PG  - 1158-1166
LID - 10.1515/cclm-2023-0355 [doi]
AB  - OBJECTIVES: ChatGPT, a tool based on natural language processing (NLP), is on 
      everyone's mind, and several potential applications in healthcare have been 
      already proposed. However, since the ability of this tool to interpret laboratory 
      test results has not yet been tested, the EFLM Working group on Artificial 
      Intelligence (WG-AI) has set itself the task of closing this gap with a 
      systematic approach. METHODS: WG-AI members generated 10 simulated laboratory 
      reports of common parameters, which were then passed to ChatGPT for 
      interpretation, according to reference intervals (RI) and units, using an 
      optimized prompt. The results were subsequently evaluated independently by all 
      WG-AI members with respect to relevance, correctness, helpfulness and safety. 
      RESULTS: ChatGPT recognized all laboratory tests, it could detect if they 
      deviated from the RI and gave a test-by-test as well as an overall 
      interpretation. The interpretations were rather superficial, not always correct, 
      and, only in some cases, judged coherently. The magnitude of the deviation from 
      the RI seldom plays a role in the interpretation of laboratory tests, and 
      artificial intelligence (AI) did not make any meaningful suggestion regarding 
      follow-up diagnostics or further procedures in general. CONCLUSIONS: ChatGPT in 
      its current form, being not specifically trained on medical data or laboratory 
      data in particular, may only be considered a tool capable of interpreting a 
      laboratory report on a test-by-test basis at best, but not on the interpretation 
      of an overall diagnostic picture. Future generations of similar AIs with medical 
      ground truth training data might surely revolutionize current processes in 
      healthcare, despite this implementation is not ready yet.
CI  - © 2023 Walter de Gruyter GmbH, Berlin/Boston.
FAU - Cadamuro, Janne
AU  - Cadamuro J
AUID- ORCID: 0000-0002-6200-9831
AD  - Department of Laboratory Medicine, Paracelsus Medical University Salzburg, 
      Salzburg, Austria.
FAU - Cabitza, Federico
AU  - Cabitza F
AD  - DISCo, Università degli Studi di Milano-Bicocca, Milano, Italy.
AD  - IRCCS Istituto Ortopedico Galeazzi, Milan, Italy.
FAU - Debeljak, Zeljko
AU  - Debeljak Z
AD  - Faculty of Medicine, Josip Juraj Strossmayer University of Osijek, Osijek, 
      Croatia.
AD  - Clinical Institute of Laboratory Diagnostics, University Hospital Center Osijek, 
      Osijek, Croatia.
FAU - De Bruyne, Sander
AU  - De Bruyne S
AD  - Department of Laboratory Medicine, Ghent University Hospital, Ghent, Belgium.
FAU - Frans, Glynis
AU  - Frans G
AUID- ORCID: 0000-0001-8528-5719
AD  - Department of Laboratory Medicine, University Hospitals Leuven, KU Leuven, 
      Leuven, Belgium.
FAU - Perez, Salomon Martin
AU  - Perez SM
AD  - Unidad de Bioquímica Clínica, Hospital Universitario Virgen Macarena, Sevilla, 
      Spain.
FAU - Ozdemir, Habib
AU  - Ozdemir H
AUID- ORCID: 0000-0002-1267-7233
AD  - Department of Medical Biochemistry, Faculty of Medicine, Manisa Celal Bayar 
      University, Manisa, Türkiye.
FAU - Tolios, Alexander
AU  - Tolios A
AD  - Department of Transfusion Medicine and Cell Therapy, Medical University of 
      Vienna, Vienna, Austria.
FAU - Carobene, Anna
AU  - Carobene A
AD  - IRCCS San Raffaele Scientific Institute, Milan, Italy.
FAU - Padoan, Andrea
AU  - Padoan A
AUID- ORCID: 0000-0003-1284-7885
AD  - Department of Medicine (DIMED), University of Padova, Padova, Italy.
LA  - eng
PT  - Journal Article
DEP - 20230424
PL  - Germany
TA  - Clin Chem Lab Med
JT  - Clinical chemistry and laboratory medicine
JID - 9806306
SB  - IM
CIN - Clin Chem Lab Med. 2023 Apr 25;61(7):1131-1132. PMID: 37092365
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Chemistry, Clinical
MH  - Laboratories
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - chatbot
OT  - laboratory tests
OT  - natural language processing
EDAT- 2023/04/21 12:42
MHDA- 2023/05/30 06:42
CRDT- 2023/04/21 07:13
PHST- 2023/04/07 00:00 [received]
PHST- 2023/04/12 00:00 [accepted]
PHST- 2023/05/30 06:42 [medline]
PHST- 2023/04/21 12:42 [pubmed]
PHST- 2023/04/21 07:13 [entrez]
AID - cclm-2023-0355 [pii]
AID - 10.1515/cclm-2023-0355 [doi]
PST - epublish
SO  - Clin Chem Lab Med. 2023 Apr 24;61(7):1158-1166. doi: 10.1515/cclm-2023-0355. 
      Print 2023 Jun 27.

PMID- 37261590
OWN - NLM
STAT- Publisher
LR  - 20231024
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Print)
IS  - 0090-6964 (Linking)
VI  - 51
IP  - 11
DP  - 2023 Nov
TI  - Realization of Green 5G Cellular Network Role in Medical Applications: Use of 
      ChatGPT-AI.
PG  - 2337-2339
LID - 10.1007/s10439-023-03257-3 [doi]
AB  - Wireless communication in medical applications improves patient monitoring, care 
      coordination, early disease detection, and patient empowerment. It improves 
      healthcare and patient outcomes. The design and configuration of a solar-powered 
      emergency battery backup system for 5G telecommunication base stations, including 
      medical applications, may vary depending on local climate, power requirements, 
      and resources. In this connection, uninterrupted power supply to the base 
      stations become crucial. The author utilizes the ChatGPT-AI features and prepared 
      this comprehensive letter for realizing the role of sustainable practices towards 
      climatic changes.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Janamala, Varaprasad
AU  - Janamala V
AUID- ORCID: 0000-0001-9716-5458
AD  - Department of Electrical and Electronics Engineering, School of Engineering and 
      Technology, CHRIST (Deemed to Be University), Bangalore, Karnataka, 560074, 
      India. varaprasad.janamala@christuniversity.in.
FAU - Ram, Inkollu Sai
AU  - Ram IS
AUID- ORCID: 0000-0003-2597-818X
AD  - Department of Electrical and Electronics Engineering, Dhanekula Institute of 
      Engineering &amp; Technology, Vijayawada, Andhra Pradesh, 521139, India.
FAU - Daram, Suresh Babu
AU  - Daram SB
AUID- ORCID: 0000-0001-5304-8620
AD  - Department of Electrical and Electronics Engineering, Mohan Babu University (Erst 
      while Sree Vidyanikethan Engineering College), Tirupati, Andhra Pradesh, 517102, 
      India.
LA  - eng
PT  - Letter
DEP - 20230601
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
PMC - PMC10234226
OTO - NOTNLM
OT  - 5G cellular networks
OT  - ChatGPT
OT  - Green technologies
OT  - Medical research
EDAT- 2023/06/01 13:10
MHDA- 2023/06/01 13:10
PMCR- 2023/06/01
CRDT- 2023/06/01 11:12
PHST- 2023/05/24 00:00 [received]
PHST- 2023/05/25 00:00 [accepted]
PHST- 2023/06/01 13:10 [pubmed]
PHST- 2023/06/01 13:10 [medline]
PHST- 2023/06/01 11:12 [entrez]
PHST- 2023/06/01 00:00 [pmc-release]
AID - 10.1007/s10439-023-03257-3 [pii]
AID - 3257 [pii]
AID - 10.1007/s10439-023-03257-3 [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2023 Nov;51(11):2337-2339. doi: 10.1007/s10439-023-03257-3. Epub 
      2023 Jun 1.

PMID- 37099761
OWN - NLM
STAT- MEDLINE
DCOM- 20230901
LR  - 20240307
IS  - 1741-3850 (Electronic)
IS  - 1741-3842 (Linking)
VI  - 45
IP  - 3
DP  - 2023 Aug 28
TI  - No worries with ChatGPT: building bridges between artificial intelligence and 
      education with critical thinking soft skills.
PG  - e602-e603
LID - 10.1093/pubmed/fdad049 [doi]
AB  - This correspondence discusses the role of artificial intelligence (AI) like 
      ChatGPT in education and research, focusing on developing critical thinking 
      skills and maintaining academic integrity. AI can complement learning and 
      research processes when used ethically and responsibly. Integrating specific 
      teaching methods in education and research can help develop better critical 
      thinking skills and a deeper understanding of the contexts in which AI is used. 
      The article emphasizes the importance of developing critical thinking skills 
      among students and researchers to effectively use AI and distinguish accurate 
      information from hoaxes and misinformation. In conclusion, the collaboration 
      between AI and humans in learning and research will yield significant benefits 
      for individuals and society as long as critical thinking skills and academic 
      integrity remain top priorities.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of Faculty 
      of Public Health. All rights reserved. For permissions, please e-mail: 
      journals.permissions@oup.com.
FAU - Rusandi, M Arli
AU  - Rusandi MA
AUID- ORCID: 0000-0001-7385-104X
AD  - Department of Guidance and Counseling, Universitas Riau, Pekanbaru, Indonesia.
FAU - Ahman
AU  - Ahman
AD  - Faculty of Educational Science, Universitas Pendidikan Indonesia, Bandung, 
      Indonesia.
FAU - Saripah, Ipah
AU  - Saripah I
AD  - Faculty of Educational Science, Universitas Pendidikan Indonesia, Bandung, 
      Indonesia.
FAU - Khairun, Deasy Yunika
AU  - Khairun DY
AD  - Department of Guidance and Counseling, Universitas Sultan Ageng Tirtayasa, 
      Serang, Indonesia.
FAU - Mutmainnah
AU  - Mutmainnah
AD  - Department of Guidance and Counseling, Universitas Muhammadiyah Enrekang, 
      Enrekang, Indonesia.
LA  - eng
PT  - Letter
PT  - Research Support, Non-U.S. Gov't
PL  - England
TA  - J Public Health (Oxf)
JT  - Journal of public health (Oxford, England)
JID - 101188638
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Communication
MH  - Learning
MH  - *Thinking
MH  - *Education
OTO - NOTNLM
OT  - ChatGPT
OT  - academic integrity
OT  - artificial intelligence
OT  - collaboration
OT  - critical thinking skills
OT  - digital literacy
OT  - education
OT  - ethical use
OT  - misinformation
OT  - research
EDAT- 2023/04/26 18:42
MHDA- 2023/09/01 06:43
CRDT- 2023/04/26 16:53
PHST- 2023/03/29 00:00 [received]
PHST- 2022/04/04 00:00 [revised]
PHST- 2023/09/01 06:43 [medline]
PHST- 2023/04/26 18:42 [pubmed]
PHST- 2023/04/26 16:53 [entrez]
AID - 7136742 [pii]
AID - 10.1093/pubmed/fdad049 [doi]
PST - ppublish
SO  - J Public Health (Oxf). 2023 Aug 28;45(3):e602-e603. doi: 10.1093/pubmed/fdad049.

PMID- 37094207
OWN - NLM
STAT- MEDLINE
DCOM- 20230426
LR  - 20230620
IS  - 1669-9106 (Electronic)
IS  - 0025-7680 (Linking)
VI  - 83
IP  - 2
DP  - 2023
TI  - [Artificial intelligence and chatGPT. Would you read an artificial author?].
PG  - 329-332
FAU - De Vito, Eduardo L
AU  - De Vito EL
AD  - Instituto de Investigaciones Médicas Alfredo Lanari, Buenos Aires, Argentina. 
      E-mail: eldevito@gmail.com.
LA  - spa
PT  - Editorial
TT  - Inteligencia artificial y chatGPT. ¿Usted leería a un autor artificial?
PL  - Argentina
TA  - Medicina (B Aires)
JT  - Medicina
JID - 0204271
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
EDAT- 2023/04/24 18:42
MHDA- 2023/04/26 06:41
CRDT- 2023/04/24 03:43
PHST- 2023/04/26 06:41 [medline]
PHST- 2023/04/24 18:42 [pubmed]
PHST- 2023/04/24 03:43 [entrez]
PST - ppublish
SO  - Medicina (B Aires). 2023;83(2):329-332.

PMID- 38520470
OWN - NLM
STAT- Publisher
LR  - 20240323
IS  - 1532-8473 (Electronic)
IS  - 1089-9472 (Linking)
DP  - 2024 Mar 21
TI  - Comparison of Conventional Anesthesia Nurse Education and an Artificial 
      Intelligence Chatbot (ChatGPT) Intervention on Preoperative Anxiety: A Randomized 
      Controlled Trial.
LID - S1089-9472(23)01073-0 [pii]
LID - 10.1016/j.jopan.2023.12.005 [doi]
AB  - PURPOSE: This study aimed to evaluate the effects of an artificial intelligence 
      (AI) chatbot (ChatGPT-3.5, OpenAI) on preoperative anxiety reduction and patient 
      satisfaction in adult patients undergoing surgery under general anesthesia. 
      DESIGN: The study used a single-blind, randomized controlled trial design. 
      METHODS: In this study, 100 adult patients were enrolled and divided into two 
      groups: 50 in the control group, in which patients received standard preoperative 
      information from anesthesia nurses, and 50 in the intervention group, in which 
      patients interacted with ChatGPT. The primary outcome, preoperative anxiety 
      reduction, was measured using the Japanese State-Trait Anxiety Inventory (STAI) 
      self-report questionnaire. The secondary endpoints included participant 
      satisfaction (Q1), comprehension of the treatment process (Q2), and the 
      perception of the AI chatbot's responses as more relevant than those of the 
      nurses (Q3). FINDINGS: Of the 85 participants who completed the study, the STAI 
      scores in the control group remained stable, whereas those in the intervention 
      group decreased. The mixed-effects model showed significant effects of time and 
      group-time interaction on the STAI scores; however, no main group effect was 
      observed. The secondary endpoints revealed mixed results; some patients found 
      that the chatbot's responses were more relevant, whereas others were dissatisfied 
      or experienced difficulties. CONCLUSIONS: The ChatGPT intervention significantly 
      reduced preoperative anxiety compared with the control group; however, no overall 
      difference in the STAI scores was observed. The mixed secondary endpoint results 
      highlight the need for refining chatbot algorithms and knowledge bases to improve 
      performance and satisfaction. AI chatbots should complement, rather than replace, 
      human health care providers. Seamless integration and effective communication 
      among AI chatbots, patients, and health care providers are essential for 
      optimizing patient outcomes.
CI  - Copyright © 2024. Published by Elsevier Inc.
FAU - Yahagi, Musashi
AU  - Yahagi M
AD  - Department of Anesthesiology, Hitachi General Hospital, San Francisco, CA. 
      Electronic address: musasum0710@yahoo.co.jp.
FAU - Hiruta, Rie
AU  - Hiruta R
AD  - Department of Surgery, Hitachi General Hospital, San Francisco, CA.
FAU - Miyauchi, Chisato
AU  - Miyauchi C
AD  - Department of Surgery, Hitachi General Hospital, San Francisco, CA.
FAU - Tanaka, Shoko
AU  - Tanaka S
AD  - Department of Surgery, Hitachi General Hospital, San Francisco, CA.
FAU - Taguchi, Aya
AU  - Taguchi A
AD  - Department of Surgery, Hitachi General Hospital, San Francisco, CA.
FAU - Yaguchi, Yuichi
AU  - Yaguchi Y
AD  - Department of Anesthesiology, Hitachi General Hospital, San Francisco, CA.
LA  - eng
PT  - Journal Article
DEP - 20240321
PL  - United States
TA  - J Perianesth Nurs
JT  - Journal of perianesthesia nursing : official journal of the American Society of 
      PeriAnesthesia Nurses
JID - 9610507
OTO - NOTNLM
OT  - AI Chatbot
OT  - anesthesia
OT  - perioperative anxiety
OT  - preoperative
COIS- Declaration of Competing Interest None to report.
EDAT- 2024/03/23 20:49
MHDA- 2024/03/23 20:49
CRDT- 2024/03/23 11:40
PHST- 2023/06/06 00:00 [received]
PHST- 2023/12/03 00:00 [revised]
PHST- 2023/12/04 00:00 [accepted]
PHST- 2024/03/23 20:49 [medline]
PHST- 2024/03/23 20:49 [pubmed]
PHST- 2024/03/23 11:40 [entrez]
AID - S1089-9472(23)01073-0 [pii]
AID - 10.1016/j.jopan.2023.12.005 [doi]
PST - aheadofprint
SO  - J Perianesth Nurs. 2024 Mar 21:S1089-9472(23)01073-0. doi: 
      10.1016/j.jopan.2023.12.005.

PMID- 38312244
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240206
IS  - 2690-442X (Electronic)
IS  - 2690-442X (Linking)
VI  - 4
IP  - 1
DP  - 2024 Feb
TI  - Comparison of large language models in management advice for melanoma: Google's 
      AI BARD, BingAI and ChatGPT.
PG  - e313
LID - 10.1002/ski2.313 [doi]
LID - e313
AB  - Large language models (LLMs) are emerging artificial intelligence (AI) technology 
      refining research and healthcare. Their use in medicine has seen numerous recent 
      applications. One area where LLMs have shown particular promise is in the 
      provision of medical information and guidance to practitioners. This study aims 
      to assess three prominent LLMs-Google's AI BARD, BingAI and ChatGPT-4 in 
      providing management advice for melanoma by comparing their responses to current 
      clinical guidelines and existing literature. Five questions on melanoma pathology 
      were prompted to three LLMs. A panel of three experienced Board-certified plastic 
      surgeons evaluated the responses for reliability using reliability matrix (Flesch 
      Reading Ease Score, the Flesch-Kincaid Grade Level and the Coleman-Liau Index), 
      suitability (modified DISCERN score) and comparing them to existing guidelines. 
      t-Test was performed to calculate differences in mean readability and reliability 
      scores between LLMs and p value&nbsp;&lt;0.05 was considered statistically significant. 
      The mean readability scores across three LLMs were same. ChatGPT exhibited 
      superiority with a Flesch Reading Ease Score of 35.42 (±21.02), Flesch-Kincaid 
      Grade Level of 11.98 (±4.49) and Coleman-Liau Index of 12.00 (±5.10), however all 
      of these were insignificant (p&nbsp;&gt;&nbsp;0.05). Suitability-wise using DISCERN score, 
      ChatGPT 58 (±6.44) significantly (p&nbsp;=&nbsp;0.04) outperformed BARD 36.2 (±34.06) and 
      was insignificant to BingAI's 49.8 (±22.28). This study demonstrates that ChatGPT 
      marginally outperforms BARD and BingAI in providing reliable, evidence-based 
      clinical advice, but they still face limitations in depth and specificity. Future 
      research should improve LLM performance by integrating specialized databases and 
      expert knowledge to support patient-centred care.
CI  - © 2023 The Authors. Skin Health and Disease published by John Wiley &amp; Sons Ltd on 
      behalf of British Association of Dermatologists.
FAU - Mu, Xin
AU  - Mu X
AD  - Department of Plastic Surgery Peninsula Health Melbourne Victoria Australia.
FAU - Lim, Bryan
AU  - Lim B
AD  - Department of Plastic Surgery Peninsula Health Melbourne Victoria Australia.
AD  - Central Clinical School Monash University Melbourne Victoria Australia.
FAU - Seth, Ishith
AU  - Seth I
AUID- ORCID: 0000-0001-5444-8925
AD  - Department of Plastic Surgery Peninsula Health Melbourne Victoria Australia.
AD  - Central Clinical School Monash University Melbourne Victoria Australia.
FAU - Xie, Yi
AU  - Xie Y
AD  - Department of Plastic Surgery Peninsula Health Melbourne Victoria Australia.
FAU - Cevik, Jevan
AU  - Cevik J
AD  - Department of Plastic Surgery Peninsula Health Melbourne Victoria Australia.
FAU - Sofiadellis, Foti
AU  - Sofiadellis F
AD  - Department of Plastic Surgery Peninsula Health Melbourne Victoria Australia.
FAU - Hunter-Smith, David J
AU  - Hunter-Smith DJ
AD  - Department of Plastic Surgery Peninsula Health Melbourne Victoria Australia.
AD  - Central Clinical School Monash University Melbourne Victoria Australia.
FAU - Rozen, Warren M
AU  - Rozen WM
AD  - Department of Plastic Surgery Peninsula Health Melbourne Victoria Australia.
AD  - Central Clinical School Monash University Melbourne Victoria Australia.
LA  - eng
PT  - Journal Article
DEP - 20231128
PL  - England
TA  - Skin Health Dis
JT  - Skin health and disease
JID - 9918227353706676
PMC - PMC10831541
COIS- The authors declare no conflicts of interest.
EDAT- 2024/02/05 06:43
MHDA- 2024/02/05 06:44
PMCR- 2023/11/28
CRDT- 2024/02/05 04:16
PHST- 2023/09/18 00:00 [received]
PHST- 2023/10/08 00:00 [revised]
PHST- 2023/11/03 00:00 [accepted]
PHST- 2024/02/05 06:44 [medline]
PHST- 2024/02/05 06:43 [pubmed]
PHST- 2024/02/05 04:16 [entrez]
PHST- 2023/11/28 00:00 [pmc-release]
AID - SKI2313 [pii]
AID - 10.1002/ski2.313 [doi]
PST - epublish
SO  - Skin Health Dis. 2023 Nov 28;4(1):e313. doi: 10.1002/ski2.313. eCollection 2024 
      Feb.

PMID- 38300168
OWN - NLM
STAT- Publisher
LR  - 20240216
IS  - 1097-6752 (Electronic)
IS  - 0889-5406 (Linking)
DP  - 2024 Feb 1
TI  - Examination of the reliability and readability of Chatbot Generative Pretrained 
      Transformer's (ChatGPT) responses to questions about orthodontics and the 
      evolution of these responses in an updated version.
LID - S0889-5406(24)00007-6 [pii]
LID - 10.1016/j.ajodo.2023.11.012 [doi]
AB  - INTRODUCTION: This study aimed to assess the reliability and readability of 
      Chatbot Generative Pretrained Transformer (ChatGPT) responses to questions about 
      orthodontics and the evolution of these responses in an updated version. METHODS: 
      Frequently asked questions about orthodontics by laypeople on Web sites were 
      determined using the Google Search Tool. These questions were asked to both 
      ChatGPT's March 23 version and May 24 version on April 20, 2023, and July 12, 
      2023, respectively. Responses were assessed for readability and reliability 
      using&nbsp;the Flesch-Kincaid and DISCERN tests. RESULTS: The mean DISCERN value for 
      general questions was 2.96 ± 0.05, 3.04 ± 0.06, 2.38 ± 0.27, and 2.82 ± 0.31 for 
      treatment-related questions; the&nbsp;mean Flesch-Kincaid Reading Ease score for 
      general questions was 29.28 ± 8.22, 25.12 ± 7.39, 47.67&nbsp;± 10.77, and 41.60 ± 9.54 
      for treatment-related questions; mean Flesch-Kincaid Grade Level for general 
      questions was 14.52 ± 1.48 and 14.04 ± 1.25 and 11.90 ± 2.08 and 11.41 ± 1.88 for 
      treatment-related questions; in first and second evaluations respectively (P&nbsp;= 
      0.001). CONCLUSIONS: In the second evaluation, the reliability of the answers 
      given to general questions and treatment-related questions increased. However, in 
      both evaluations, the reliability of the answers was found to be moderate 
      according to the DISCERN tool. On the second evaluation, Flesch Reading Ease 
      Scores for both general questions and treatment-related questions decreased, 
      meaning that the readability of the new response texts became more difficult. 
      Flesch-Kincaid Grade Level results were found at the college graduate level in 
      the first and second evaluations for general questions and at the high school 
      level in the first and second evaluations for treatment-related questions.
CI  - Copyright © 2024 American Association of Orthodontists. Published by Elsevier 
      Inc. All rights reserved.
FAU - Kılınç, Delal Dara
AU  - Kılınç DD
AD  - Department of Orthodontics, School of Dental Medicine, Bahçeşehir University, 
      Istanbul, Turkey. Electronic address: delaldara.kilinc@bau.edu.tr.
FAU - Mansız, Duygu
AU  - Mansız D
AD  - Department of Orthodontics, Faculty of Dentistry, Istanbul Aydin University, 
      Istanbul, Turkey.
LA  - eng
PT  - Journal Article
DEP - 20240201
PL  - United States
TA  - Am J Orthod Dentofacial Orthop
JT  - American journal of orthodontics and dentofacial orthopedics : official 
      publication of the American Association of Orthodontists, its constituent 
      societies, and the American Board of Orthodontics
JID - 8610224
SB  - IM
EDAT- 2024/02/01 12:43
MHDA- 2024/02/01 12:43
CRDT- 2024/02/01 10:32
PHST- 2023/05/01 00:00 [received]
PHST- 2023/11/01 00:00 [revised]
PHST- 2023/11/01 00:00 [accepted]
PHST- 2024/02/01 12:43 [pubmed]
PHST- 2024/02/01 12:43 [medline]
PHST- 2024/02/01 10:32 [entrez]
AID - S0889-5406(24)00007-6 [pii]
AID - 10.1016/j.ajodo.2023.11.012 [doi]
PST - aheadofprint
SO  - Am J Orthod Dentofacial Orthop. 2024 Feb 1:S0889-5406(24)00007-6. doi: 
      10.1016/j.ajodo.2023.11.012.

PMID- 38277081
OWN - NLM
STAT- MEDLINE
DCOM- 20240129
LR  - 20240206
IS  - 0942-0940 (Electronic)
IS  - 0001-6268 (Print)
IS  - 0001-6268 (Linking)
VI  - 166
IP  - 1
DP  - 2024 Jan 26
TI  - Leveraging artificial intelligence in neurosurgery-unveiling ChatGPT for 
      neurosurgical discharge summaries and operative reports.
PG  - 38
LID - 10.1007/s00701-024-05908-3 [doi]
LID - 38
AB  - PURPOSE: Chat generative pre-trained transformer (GPT) is a novel large 
      pre-trained natural language processing software that can enable scientific 
      writing amongst a litany of other features. Given this, there is a growing 
      interest in exploring the use of ChatGPT models as a modality to 
      facilitate/assist in the provision of clinical care. METHODS: We investigated the 
      time taken for the composition of neurosurgical discharge summaries and operative 
      reports at a major University hospital. In so doing, we compared currently 
      employed speech recognition software (i.e., SpeaKING) vs novel ChatGPT for three 
      distinct neurosurgical diseases: chronic subdural hematoma, spinal decompression, 
      and craniotomy. Furthermore, factual correctness was analyzed for the 
      abovementioned diseases. RESULTS: The composition of neurosurgical discharge 
      summaries and operative reports with the assistance of ChatGPT leads to a 
      statistically significant time reduction across all three diseases/report types: 
      p &lt; 0.001 for chronic subdural hematoma, p &lt; 0.001 for decompression of spinal 
      stenosis, and p &lt; 0.001 for craniotomy and tumor resection. However, despite a 
      high degree of factual correctness, the preparation of a surgical report for 
      craniotomy proved to be significantly lower (p = 0.002). CONCLUSION: ChatGPT 
      assisted in the writing of discharge summaries and operative reports as evidenced 
      by an impressive reduction in time spent as compared to standard speech 
      recognition software. While promising, the optimal use cases and ethics of 
      AI-generated medical writing remain to be fully elucidated and must be further 
      explored in future studies.
CI  - © 2024. The Author(s).
FAU - Dubinski, Daniel
AU  - Dubinski D
AUID- ORCID: 0000-0001-5568-8429
AD  - Department of Neurosurgery, University Medicine Rostock, Rostock, Germany. 
      danieldubinski@gmail.com.
FAU - Won, Sae-Yeon
AU  - Won SY
AD  - Department of Neurosurgery, University Medicine Rostock, Rostock, Germany.
FAU - Trnovec, Svorad
AU  - Trnovec S
AD  - Department of Neurosurgery, University Medicine Rostock, Rostock, Germany.
FAU - Behmanesh, Bedjan
AU  - Behmanesh B
AD  - Department of Neurosurgery, University Medicine Rostock, Rostock, Germany.
FAU - Baumgarten, Peter
AU  - Baumgarten P
AD  - Department of Neurosurgery, University Hospital, Schiller University Jena, Jena, 
      Germany.
FAU - Dinc, Nazife
AU  - Dinc N
AD  - Department of Neurosurgery, University Hospital, Schiller University Jena, Jena, 
      Germany.
FAU - Konczalla, Juergen
AU  - Konczalla J
AD  - Department of Neurosurgery, Goethe-University Hospital, Frankfurt am Main, 
      Germany.
FAU - Chan, Alvin
AU  - Chan A
AD  - David H. Koch Institute for Integrated Cancer Research, MIT, Cambridge, MA, USA.
FAU - Bernstock, Joshua D
AU  - Bernstock JD
AD  - Department of Neurosurgery, Brigham and Women's Hospital, Harvard Medical School, 
      Boston, MA, USA.
FAU - Freiman, Thomas M
AU  - Freiman TM
AD  - Department of Neurosurgery, University Medicine Rostock, Rostock, Germany.
FAU - Gessler, Florian
AU  - Gessler F
AD  - Department of Neurosurgery, University Medicine Rostock, Rostock, Germany.
LA  - eng
PT  - Journal Article
DEP - 20240126
PL  - Austria
TA  - Acta Neurochir (Wien)
JT  - Acta neurochirurgica
JID - 0151000
SB  - IM
MH  - Humans
MH  - *Neurosurgery
MH  - Artificial Intelligence
MH  - *Hematoma, Subdural, Chronic
MH  - Patient Discharge
MH  - Neurosurgical Procedures
PMC - PMC10817836
OTO - NOTNLM
OT  - AI-generated output
OT  - Artificial intelligence
OT  - Computer science
OT  - Medical documentation
COIS- The authors declare no competing interests.
EDAT- 2024/01/26 12:43
MHDA- 2024/01/29 06:43
PMCR- 2024/01/26
CRDT- 2024/01/26 11:08
PHST- 2023/06/18 00:00 [received]
PHST- 2023/11/06 00:00 [accepted]
PHST- 2024/01/29 06:43 [medline]
PHST- 2024/01/26 12:43 [pubmed]
PHST- 2024/01/26 11:08 [entrez]
PHST- 2024/01/26 00:00 [pmc-release]
AID - 10.1007/s00701-024-05908-3 [pii]
AID - 5908 [pii]
AID - 10.1007/s00701-024-05908-3 [doi]
PST - epublish
SO  - Acta Neurochir (Wien). 2024 Jan 26;166(1):38. doi: 10.1007/s00701-024-05908-3.

PMID- 37985815
OWN - NLM
STAT- Publisher
LR  - 20240328
IS  - 1476-5489 (Electronic)
IS  - 0955-9930 (Linking)
DP  - 2023 Nov 20
TI  - Assessing ChatGPT's ability to answer questions pertaining to erectile 
      dysfunction: can our patients trust it?
LID - 10.1038/s41443-023-00797-z [doi]
AB  - Erectile dysfunction (ED) is a disorder that can cause distress and shame for men 
      suffering from it. Men with ED will often turn to online support and chat groups 
      to ask intimate questions about their health. ChatGPT is an artificial 
      intelligence (AI)-based software that has been trained to engage in conversation 
      with human input. We sought to assess the accuracy, readability, and 
      reproducibility of ChatGPT's responses to frequently asked questions regarding 
      the diagnosis, management, and care of patients with ED. Questions pertaining to 
      ED were derived from clinic encounters with patients as well as online chat 
      forums. These were entered into the free ChatGPT version 3.5 during the month of 
      August 2023. Questions were asked on two separate days from unique accounts and 
      computers to prevent the software from memorizing responses linked to a specific 
      user. A total of 35 questions were asked. Outcomes measured were accuracy using 
      grading from board certified urologists, readability with the Gunning Fog Index, 
      and reproducibility by comparing responses between days. For epidemiology of 
      disease, the percentage of responses that were graded as "comprehensive" or 
      "correct but inadequate" was 100% across both days. There was fair 
      reproducibility and median readability of 15.9 (IQR 2.5). For treatment and 
      prevention, the percentage of responses that were graded as "comprehensive" or 
      "correct but inadequate" was 78.9%. There was poor reproducibility of responses 
      with a median readability of 14.5 (IQR 4.0). Risks of treatment and counseling 
      both had 100% of questions graded as "comprehensive" or "correct but inadequate." 
      The readability score for risks of treatment was median 13.9 (IQR 1.1) and for 
      counseling median 13.8 (IQR 0.5), with good reproducibility for both question 
      domains. ChatGPT provides accurate answers to common patient questions pertaining 
      to ED, although its understanding of treatment options is incomplete and 
      responses are at a reading level too advanced for the average patient.
CI  - © 2023. The Author(s), under exclusive licence to Springer Nature Limited.
FAU - Razdan, Shirin
AU  - Razdan S
AUID- ORCID: 0000-0002-9098-3447
AD  - Department of Urology, Icahn School of Medicine at Mount Sinai Hospital, New 
      York, NY, 10029, USA. shirin.razdan@mountsinai.org.
FAU - Siegal, Alexandra R
AU  - Siegal AR
AD  - Department of Urology, Icahn School of Medicine at Mount Sinai Hospital, New 
      York, NY, 10029, USA.
FAU - Brewer, Yukiko
AU  - Brewer Y
AD  - Department of Internal Medicine, HCA Florida Sarasota Doctors Hospital, Sarasota, 
      FL, 34233, USA.
FAU - Sljivich, Michaela
AU  - Sljivich M
AD  - Department of Urology, Icahn School of Medicine at Mount Sinai Hospital, New 
      York, NY, 10029, USA.
FAU - Valenzuela, Robert J
AU  - Valenzuela RJ
AUID- ORCID: 0009-0006-5831-5947
AD  - Department of Urology, Icahn School of Medicine at Mount Sinai Hospital, New 
      York, NY, 10029, USA.
LA  - eng
PT  - Comment
PT  - Journal Article
DEP - 20231120
PL  - England
TA  - Int J Impot Res
JT  - International journal of impotence research
JID - 9007383
SB  - IM
CON - Int J Impot Res. 2024 Mar 11;:. PMID: 38467775
EDAT- 2023/11/21 06:42
MHDA- 2023/11/21 06:42
CRDT- 2023/11/21 01:28
PHST- 2023/10/01 00:00 [received]
PHST- 2023/11/06 00:00 [accepted]
PHST- 2023/10/28 00:00 [revised]
PHST- 2023/11/21 06:42 [pubmed]
PHST- 2023/11/21 06:42 [medline]
PHST- 2023/11/21 01:28 [entrez]
AID - 10.1038/s41443-023-00797-z [pii]
AID - 10.1038/s41443-023-00797-z [doi]
PST - aheadofprint
SO  - Int J Impot Res. 2023 Nov 20. doi: 10.1038/s41443-023-00797-z.

PMID- 38318684
OWN - NLM
STAT- Publisher
LR  - 20240206
IS  - 1527-330X (Electronic)
IS  - 1090-820X (Linking)
DP  - 2024 Feb 6
TI  - Utility and Comparative Performance of Current Artificial Intelligence Large 
      Language Models as Postoperative Medical Support Chatbots in Aesthetic Surgery.
LID - sjae025 [pii]
LID - 10.1093/asj/sjae025 [doi]
AB  - BACKGROUND: Large Language Models (LLMs) have revolutionized the way plastic 
      surgeons and their patients may access and leverage artificial Intelligence (AI). 
      OBJECTIVES: The present study aims to comparatively assess the performance of two 
      current publically-available and patient-accessible LLMs in the potential 
      application of AI as postoperative medical support chatbots in an aesthetic 
      surgeon's practice. METHODS: Twenty-two simulated postoperative patient 
      presentations following aesthetic breast plastic surgery were devised and 
      expert-validated. Complications varied in their latency within the postoperative 
      period, as well as urgency of required medical attention. In response to each 
      patient-reported presentation, Open AI's ChatGPT and Google's Bard, in their 
      unmodified and freely available versions, were objectively assessed for their 
      comparative accuracy in generating an appropriate differential diagnosis, most 
      likely diagnosis, suggested medical disposition, treatments or interventions to 
      begin from home, and/or red flag signs/symptoms indicating deterioration. 
      RESULTS: ChatGPT cumulatively and significantly outperformed Bard across all 
      objective assessement metrics examined (66% vs. 55%, respectively; p &lt; 0.05). 
      Accuracy in generating an appropriate differential diagnosis were 61% for 
      ChatGPT, and 57% for Bard (p = 0.45). ChatGPT asked an average of 9.2 questions 
      on history, relative to 6.8 questions by Bard (p &lt; 0.001), following which, 
      accuracies of 91% vs. 68% at arriving at the most-likely diagnosis were noted, 
      respectively (p &lt; 0.01). Appropriate medical dispositions were suggested with an 
      accuracy of 50% by ChatGPT, and 41% by Bard (p = 0.40); relevant home 
      interventions/treatments with an accuracy of 59% and 55% (p = 0.94), and red flag 
      signs/symptoms with accuracies of 79% and 54% (p &lt; 0.01), respectively. Detailed 
      and comparative performance breakdowns according to complication latency and 
      urgency are presented herein. CONCLUSIONS: ChatGPT represents the superior LLM 
      for the potential application of AI technology in postoperative medical support 
      chatbots. Imperfect performance and limitations identified herein may guide the 
      necessary refinement to facilitate adoption.
CI  - © The Author(s) 2024. Published by Oxford University Press on behalf of The 
      Aesthetic Society. All rights reserved. For permissions, please e-mail: 
      journals.permissions@oup.com.
FAU - Abi-Rafeh, Jad
AU  - Abi-Rafeh J
AUID- ORCID: 0000-0002-7483-1515
AD  - Division of Plastic, Reconstructive, and Aesthetic Surgery, McGill University 
      Health Centre, Montreal, Quebec, Canada.
FAU - Henry, Nader
AU  - Henry N
AD  - Division of Plastic, Reconstructive, and Aesthetic Surgery, McGill University 
      Health Centre, Montreal, Quebec, Canada.
FAU - Xu, Hong Hao
AU  - Xu HH
AUID- ORCID: 0000-0002-0742-3618
AD  - Department of Medicine, Laval University, Quebec City, Quebec, Canada.
FAU - Bassiri-Tehrani, Brian
AU  - Bassiri-Tehrani B
AUID- ORCID: 0000-0003-4891-4920
AD  - Plastic surgeon in private practice, New York, New York, USA.
FAU - Arezki, Adel
AU  - Arezki A
AD  - Division of Urology, McGill University Health Centre, Montreal, Quebec, Canada.
FAU - Kazan, Roy
AU  - Kazan R
AD  - Division of Plastic, Reconstructive, and Aesthetic Surgery, McGill University 
      Health Centre, Montreal, Quebec, Canada.
FAU - Gilardino, Mirko S
AU  - Gilardino MS
AD  - Division of Plastic, Reconstructive, and Aesthetic Surgery, McGill University 
      Health Centre, Montreal, Quebec, Canada.
FAU - Nahai, Foad
AU  - Nahai F
AD  - Division of Plastic and Reconstructive Surgery, Emory University School of 
      Medicine, Atlanta, GA, USA.
LA  - eng
PT  - Journal Article
DEP - 20240206
PL  - England
TA  - Aesthet Surg J
JT  - Aesthetic surgery journal
JID - 9707469
SB  - IM
EDAT- 2024/02/06 06:42
MHDA- 2024/02/06 06:42
CRDT- 2024/02/06 04:33
PHST- 2023/12/17 00:00 [received]
PHST- 2024/01/24 00:00 [revised]
PHST- 2024/01/26 00:00 [accepted]
PHST- 2024/02/06 06:42 [medline]
PHST- 2024/02/06 06:42 [pubmed]
PHST- 2024/02/06 04:33 [entrez]
AID - 7601408 [pii]
AID - 10.1093/asj/sjae025 [doi]
PST - aheadofprint
SO  - Aesthet Surg J. 2024 Feb 6:sjae025. doi: 10.1093/asj/sjae025.

PMID- 37972974
OWN - NLM
STAT- MEDLINE
DCOM- 20240304
LR  - 20240304
IS  - 1537-2677 (Electronic)
IS  - 0740-9303 (Linking)
VI  - 40
IP  - 2
DP  - 2024 Mar-Apr 01
TI  - Optimizing Ophthalmology Patient Education via ChatBot-Generated Materials: 
      Readability Analysis of AI-Generated Patient Education Materials and The American 
      Society of Ophthalmic Plastic and Reconstructive Surgery Patient Brochures.
PG  - 212-216
LID - 10.1097/IOP.0000000000002549 [doi]
AB  - PURPOSE: This study aims to compare the readability of patient education 
      materials (PEM) of the American Society of Ophthalmic Plastic and Reconstructive 
      Surgery to that of PEMs generated by the AI-chat bots ChatGPT and Google Bard. 
      METHODS: PEMs on 16 common American Society of Ophthalmic Plastic and 
      Reconstructive Surgery topics were generated by 2 AI models, ChatGPT 4.0 and 
      Google Bard, with and without a 6th-grade reading level prompt modifier. The PEMs 
      were analyzed using 7 readability metrics: Flesch Reading Ease Score, Gunning Fog 
      Index, Flesch-Kincaid Grade Level, Coleman-Liau Index, Simple Measure of 
      Gobbledygook Index Score, Automated Readability Index, and Linsear Write 
      Readability Score. Each AI-generated PEM was compared with the equivalent 
      American Society of Ophthalmic Plastic and Reconstructive Surgery PEM. RESULTS: 
      Across all readability indices, PEM generated by ChatGPT 4.0 consistently had the 
      highest readability scores, indicating that the material generated by this AI 
      chatbot may be most difficult to read in its unprompted form (Flesch Reading Ease 
      Score: 36.5; Simple Measure of Gobbledygook: 14.7). Google's Bard was able to 
      generate content that was easier to read than both the American Society of 
      Ophthalmic Plastic and Reconstructive Surgery and ChatGPT 4.0 (Flesch Reading 
      Ease Score: 52.3; Simple Measure of Gobbledygook: 12.7). When prompted to produce 
      PEM at a 6th-grade reading level, both ChatGPT 4.0 and Bard were able to 
      significantly improve in their readability scores, with prompted ChatGPT 4.0 
      being able to consistently generate content that was easier to read (Flesch 
      Reading Ease Score: 67.9, Simple Measure of Gobbledygook: 10.2). CONCLUSION: This 
      study suggests that AI tools, when guided by appropriate prompts, can generate 
      accessible and comprehensible PEMs in the field of ophthalmic plastic and 
      reconstructive surgeries, balancing readability with the complexity of the 
      necessary information.
CI  - Copyright © 2023 The American Society of Ophthalmic Plastic and Reconstructive 
      Surgery, Inc.
FAU - Eid, Kevin
AU  - Eid K
AD  - Department of Ophthalmology, Moran Eye Center, University of Utah, Salt Lake 
      City, Utah, U.S.A.
FAU - Eid, Alen
AU  - Eid A
AD  - Department of Ophthalmology and Visual Sciences, West Virginia University, 
      Morgantown, West Virginia, U.S.A.
FAU - Wang, Diane
AU  - Wang D
AD  - Department of Ophthalmology and Visual Sciences, West Virginia University, 
      Morgantown, West Virginia, U.S.A.
FAU - Raiker, Rahul S
AU  - Raiker RS
AD  - Department of Medical Education, West Virginia University, Morgantown, West 
      Virginia, U.S.A.
FAU - Chen, Stephen
AU  - Chen S
AD  - Department of Medical Education, West Virginia University, Morgantown, West 
      Virginia, U.S.A.
FAU - Nguyen, John
AU  - Nguyen J
AD  - Department of Ophthalmology and Visual Sciences, West Virginia University, 
      Morgantown, West Virginia, U.S.A.
AD  - Department of Otolaryngology and Head and Neck Surgery, West Virginia University, 
      Morgantown, West Virginia, U.S.A.
LA  - eng
PT  - Journal Article
DEP - 20231116
PL  - United States
TA  - Ophthalmic Plast Reconstr Surg
JT  - Ophthalmic plastic and reconstructive surgery
JID - 8508431
SB  - IM
MH  - Humans
MH  - Comprehension
MH  - *Ophthalmology
MH  - Pamphlets
MH  - *Surgery, Plastic
MH  - Patient Education as Topic
COIS- The authors have no financial or conflicts of interest to disclose.
EDAT- 2023/11/17 15:30
MHDA- 2024/03/04 06:46
CRDT- 2023/11/16 19:44
PHST- 2024/03/04 06:46 [medline]
PHST- 2023/11/17 15:30 [pubmed]
PHST- 2023/11/16 19:44 [entrez]
AID - 00002341-990000000-00288 [pii]
AID - 10.1097/IOP.0000000000002549 [doi]
PST - ppublish
SO  - Ophthalmic Plast Reconstr Surg. 2024 Mar-Apr 01;40(2):212-216. doi: 
      10.1097/IOP.0000000000002549. Epub 2023 Nov 16.

PMID- 38300581
OWN - NLM
STAT- Publisher
LR  - 20240201
IS  - 1931-1559 (Electronic)
IS  - 0894-4105 (Linking)
DP  - 2024 Feb 1
TI  - Potential cognitive risks of generative transformer-based AI chatbots on higher 
      order executive functions.
LID - 10.1037/neu0000948 [doi]
AB  - BACKGROUND: Chat generative retrained transformer (ChatGPT) represents a 
      groundbreaking advancement in Artificial Intelligence (AI-chatbot) technology, 
      utilizing transformer algorithms to enhance natural language processing and 
      facilitating their use for addressing specific tasks. These AI chatbots can 
      respond to questions by generating verbal instructions similar to those a person 
      would provide during the problem-solving process. AIM: ChatGPT has become the 
      fastest growing software in terms of user adoption in history, leading to an 
      anticipated widespread use of this technology in the general population. Current 
      literature is predominantly focused on the functional aspects of these 
      technologies, but the field has not yet explored hypotheses on how these AI 
      chatbots could impact the evolutionary aspects of human cognitive development. 
      Thesis: The "neuronal recycling hypothesis" posits that the brain undergoes 
      structural transformation by incorporating new cultural tools into "neural 
      niches," consequently altering individual cognition. In the case of technological 
      tools, it has been established that they reduce the cognitive demand needed to 
      solve tasks through a process called "cognitive offloading." In this theoretical 
      article, three hypotheses were proposed via forward inference about how 
      algorithms such as ChatGPT and similar models may influence the cognitive 
      processes and structures of upcoming generations. CONCLUSIONS: By forecasting the 
      neurocognitive effects of these technologies, educational and political 
      communities can anticipate future scenarios and formulate strategic plans to 
      either mitigate or enhance the cognitive influence that these factors may have on 
      the general population. (PsycInfo Database Record (c) 2024 APA, all rights 
      reserved).
FAU - León-Domínguez, Umberto
AU  - León-Domínguez U
AUID- ORCID: 0000-0001-5170-2553
AD  - School of Psychology, University of Monterrey.
LA  - eng
PT  - Journal Article
DEP - 20240201
PL  - United States
TA  - Neuropsychology
JT  - Neuropsychology
JID - 8904467
SB  - IM
EDAT- 2024/02/01 12:43
MHDA- 2024/02/01 12:43
CRDT- 2024/02/01 11:34
PHST- 2024/02/01 12:43 [medline]
PHST- 2024/02/01 12:43 [pubmed]
PHST- 2024/02/01 11:34 [entrez]
AID - 2024-50096-001 [pii]
AID - 10.1037/neu0000948 [doi]
PST - aheadofprint
SO  - Neuropsychology. 2024 Feb 1. doi: 10.1037/neu0000948.

PMID- 38170691
OWN - NLM
STAT- MEDLINE
DCOM- 20240111
LR  - 20240318
IS  - 1932-6203 (Electronic)
IS  - 1932-6203 (Linking)
VI  - 19
IP  - 1
DP  - 2024
TI  - Applying ChatGPT to tackle the side effects of personal learning environments 
      from learner and learning perspective: An interview of experts in higher 
      education.
PG  - e0295646
LID - 10.1371/journal.pone.0295646 [doi]
LID - e0295646
AB  - This paper investigates the capacity of ChatGPT, an advanced language model 
      created by OpenAI, to mitigate the side effects encountered by learners in 
      Personal Learning Environments (PLEs) within higher education. A series of 
      semi-structured interviews were conducted with six professors and three 
      Information and Communication Technology (ICT) experts. Employing thematic 
      analysis, the interview data were assessed, revealing that the side effects 
      stemming from the learner and learning perspectives could be primarily 
      categorized into cognitive, non-cognitive, and metacognitive challenges. The 
      findings of the thematic analysis indicate that, from a cognitive standpoint, 
      ChatGPT can generate relevant and trustworthy information, furnish personalized 
      learning resources, and facilitate interdisciplinary learning to fully actualize 
      learners' potential. Moreover, ChatGPT can aid learners in cultivating 
      non-cognitive skills, including motivation, perseverance, self-regulation, and 
      self-efficacy, as well as metacognitive abilities such as self-determination, 
      self-efficacy, and self-regulation, by providing tailored feedback, fostering 
      creativity, and stimulating critical thinking activities. This study offers 
      valuable insights for integrating artificial intelligence technologies to unleash 
      the full potential of PLEs in higher education.
CI  - Copyright: © 2024 Xu et al. This is an open access article distributed under the 
      terms of the Creative Commons Attribution License, which permits unrestricted 
      use, distribution, and reproduction in any medium, provided the original author 
      and source are credited.
FAU - Xu, XiaoShu
AU  - Xu X
AUID- ORCID: 0000-0002-0667-4511
AD  - School of Foreign Studies, Wenzhou University, Wenzhou, Zhejiang, China.
FAU - Wang, XiBing
AU  - Wang X
AD  - School of Information Engineering, Yunnan Vocational College of Mechanical and 
      Electrical Technology, Kunming, Yunnan, China.
AD  - Centre for Portuguese Studies, Macao Polytechnique University, Sé Freguesias, 
      Macao, China.
FAU - Zhang, YunFeng
AU  - Zhang Y
AD  - School of Education, Baoshan University, Baoshan, Yunnan, China.
FAU - Zheng, Rong
AU  - Zheng R
AUID- ORCID: 0000-0003-2480-492X
AD  - Centre for Portuguese Studies, Macao Polytechnique University, Sé Freguesias, 
      Macao, China.
AD  - Graduate School, Stamford International University, Suan Luang, Bangkok, 
      Thailand.
LA  - eng
PT  - Interview
DEP - 20240103
PL  - United States
TA  - PLoS One
JT  - PloS one
JID - 101285081
SB  - IM
MH  - *Artificial Intelligence
MH  - Creativity
MH  - Learning
MH  - *Metacognition
MH  - Thinking
PMC - PMC10763943
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/01/04 01:18
MHDA- 2024/01/05 06:42
PMCR- 2024/01/03
CRDT- 2024/01/03 13:32
PHST- 2023/04/26 00:00 [received]
PHST- 2023/11/23 00:00 [accepted]
PHST- 2024/01/05 06:42 [medline]
PHST- 2024/01/04 01:18 [pubmed]
PHST- 2024/01/03 13:32 [entrez]
PHST- 2024/01/03 00:00 [pmc-release]
AID - PONE-D-23-08682 [pii]
AID - 10.1371/journal.pone.0295646 [doi]
PST - epublish
SO  - PLoS One. 2024 Jan 3;19(1):e0295646. doi: 10.1371/journal.pone.0295646. 
      eCollection 2024.

PMID- 37474421
OWN - NLM
STAT- MEDLINE
DCOM- 20230825
LR  - 20230828
IS  - 1471-6771 (Electronic)
IS  - 0007-0912 (Linking)
VI  - 131
IP  - 3
DP  - 2023 Sep
TI  - Large language models in anaesthesiology: use of ChatGPT for American Society of 
      Anesthesiologists physical status classification.
PG  - e73-e75
LID - S0007-0912(23)00358-6 [pii]
LID - 10.1016/j.bja.2023.06.052 [doi]
FAU - Lim, Daniel Y Z
AU  - Lim DYZ
AD  - Department of Gastroenterology and Hepatology, Singapore General Hospital, 
      Singapore; Duke-NUS Medical School, Singapore.
FAU - Ke, Yu He
AU  - Ke YH
AD  - Department of Anaesthesiology and Perioperative Medicine, Singapore General 
      Hospital, Singapore.
FAU - Sng, Gerald G R
AU  - Sng GGR
AD  - Department of Endocrinology, Singapore General Hospital, Singapore.
FAU - Tung, Joshua Y M
AU  - Tung JYM
AD  - Department of Urology, Singapore General Hospital, Singapore.
FAU - Chai, Jia X
AU  - Chai JX
AD  - Department of Anaesthesiology and Perioperative Medicine, Singapore General 
      Hospital, Singapore.
FAU - Abdullah, Hairil R
AU  - Abdullah HR
AD  - Duke-NUS Medical School, Singapore; Department of Anaesthesiology and 
      Perioperative Medicine, Singapore General Hospital, Singapore. Electronic 
      address: hairil.rizal.abdullah@singhealth.com.sg.
LA  - eng
PT  - Letter
DEP - 20230718
PL  - England
TA  - Br J Anaesth
JT  - British journal of anaesthesia
JID - 0372541
SB  - IM
MH  - Humans
MH  - United States
MH  - *Anesthesiology
MH  - Anesthesiologists
OTO - NOTNLM
OT  - American Society of Anesthesiologists physical status
OT  - ChatGPT
OT  - GPT
OT  - artificial intelligence
OT  - large language models
OT  - machine learning
EDAT- 2023/07/21 01:12
MHDA- 2023/08/25 06:42
CRDT- 2023/07/20 21:59
PHST- 2023/05/16 00:00 [received]
PHST- 2023/06/02 00:00 [revised]
PHST- 2023/06/09 00:00 [accepted]
PHST- 2023/08/25 06:42 [medline]
PHST- 2023/07/21 01:12 [pubmed]
PHST- 2023/07/20 21:59 [entrez]
AID - S0007-0912(23)00358-6 [pii]
AID - 10.1016/j.bja.2023.06.052 [doi]
PST - ppublish
SO  - Br J Anaesth. 2023 Sep;131(3):e73-e75. doi: 10.1016/j.bja.2023.06.052. Epub 2023 
      Jul 18.

PMID- 38046723
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231205
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 11
DP  - 2023 Nov
TI  - "Bot or Not": Turing Problem in Otolaryngology.
PG  - e48170
LID - 10.7759/cureus.48170 [doi]
LID - e48170
AB  - The aim of this article is to shed light on the evolving landscape of artificial 
      intelligence (AI) integration in otolaryngology and its implications, 
      particularly focusing on the ethical considerations surrounding AI applications, 
      and to highlight the potential benefits of ChatGPT in patient management and 
      scientific research within otolaryngology while emphasizing the necessity for 
      ethical guidelines and validation processes. Ultimately, the article seeks to 
      encourage a responsible and informed approach to AI adoption in otolaryngology, 
      promoting collaboration between AI and healthcare professionals for the 
      betterment of science and human well-being.
CI  - Copyright © 2023, Aliyeva et al.
FAU - Aliyeva, Aynur
AU  - Aliyeva A
AD  - Otolaryngology - Head and Neck Surgery, Cincinnati Children's Hospital Medical 
      Center, Ohio, USA.
LA  - eng
PT  - Editorial
DEP - 20231102
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10693309
OTO - NOTNLM
OT  - artificial intelligence
OT  - bot or not
OT  - chatgpt
OT  - otolaryngology
OT  - turing problem
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/12/04 06:42
MHDA- 2023/12/04 06:43
PMCR- 2023/11/02
CRDT- 2023/12/04 05:08
PHST- 2023/11/02 00:00 [accepted]
PHST- 2023/12/04 06:43 [medline]
PHST- 2023/12/04 06:42 [pubmed]
PHST- 2023/12/04 05:08 [entrez]
PHST- 2023/11/02 00:00 [pmc-release]
AID - 10.7759/cureus.48170 [doi]
PST - epublish
SO  - Cureus. 2023 Nov 2;15(11):e48170. doi: 10.7759/cureus.48170. eCollection 2023 
      Nov.

PMID- 37489920
OWN - NLM
STAT- MEDLINE
DCOM- 20231120
LR  - 20231120
IS  - 1615-7109 (Electronic)
IS  - 1203-4754 (Linking)
VI  - 27
IP  - 4
DP  - 2023 Jul-Aug
TI  - Performance of ChatGPT on a Practice Dermatology Board Certification Examination.
PG  - 407-409
LID - 10.1177/12034754231188437 [doi]
FAU - Joly-Chevrier, Maxine
AU  - Joly-Chevrier M
AUID- ORCID: 0000-0001-6706-8139
AD  - Faculty of Medicine, Université de Montréal, Montreal, Canada.
FAU - Nguyen, Anne Xuan-Lan
AU  - Nguyen AX
AD  - Faculty of Medicine, McGill University, Montreal, Canada.
FAU - Lesko-Krleza, Michael
AU  - Lesko-Krleza M
AD  - Division of Computer Engineering, Department of Electrical and Computer 
      Engineering, Concordia University, Montreal, Canada.
FAU - Lefrançois, Philippe
AU  - Lefrançois P
AD  - Division of Dermatology, Department of Medicine, McGill University, Montreal, 
      Canada.
AD  - Division of Dermatology, Department of Medicine, Jewish General Hospital, 
      Montreal, Canada.
AD  - Lady Davis Institute for Medical Research, Montreal, Canada.
LA  - eng
PT  - Letter
PT  - Research Support, Non-U.S. Gov't
DEP - 20230725
PL  - United States
TA  - J Cutan Med Surg
JT  - Journal of cutaneous medicine and surgery
JID - 9614685
SB  - IM
MH  - Humans
MH  - *Dermatology
MH  - *Artificial Intelligence
MH  - *Certification
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - chatbot
OT  - dermatology
OT  - medical education
EDAT- 2023/07/25 13:09
MHDA- 2023/07/25 13:10
CRDT- 2023/07/25 09:13
PHST- 2023/07/25 13:10 [medline]
PHST- 2023/07/25 13:09 [pubmed]
PHST- 2023/07/25 09:13 [entrez]
AID - 10.1177/12034754231188437 [doi]
PST - ppublish
SO  - J Cutan Med Surg. 2023 Jul-Aug;27(4):407-409. doi: 10.1177/12034754231188437. 
      Epub 2023 Jul 25.

PMID- 37129631
OWN - NLM
STAT- MEDLINE
DCOM- 20231023
LR  - 20231101
IS  - 1435-702X (Electronic)
IS  - 0721-832X (Linking)
VI  - 261
IP  - 10
DP  - 2023 Oct
TI  - Artificial intelligence-based ChatGPT chatbot responses for patient and parent 
      questions on vernal keratoconjunctivitis.
PG  - 3041-3043
LID - 10.1007/s00417-023-06078-1 [doi]
FAU - Rasmussen, Marie Louise Roed
AU  - Rasmussen MLR
AUID- ORCID: 0000-0002-9392-8697
AD  - Department of Ophthalmology, Rigshospitalet, Valdemar Hansens Vej 3, DK-2600, 
      Glostrup, Denmark. marie.louise.roed.rasmussen@regionh.dk.
AD  - Department of Clinical Medicine, University of Copenhagen, Copenhagen, Denmark. 
      marie.louise.roed.rasmussen@regionh.dk.
FAU - Larsen, Ann-Cathrine
AU  - Larsen AC
AD  - Department of Ophthalmology, Rigshospitalet, Valdemar Hansens Vej 3, DK-2600, 
      Glostrup, Denmark.
FAU - Subhi, Yousif
AU  - Subhi Y
AD  - Department of Ophthalmology, Rigshospitalet, Valdemar Hansens Vej 3, DK-2600, 
      Glostrup, Denmark.
AD  - Department of Clinical Research, University of Southern Denmark, Odense, Denmark.
FAU - Potapenko, Ivan
AU  - Potapenko I
AD  - Department of Ophthalmology, Rigshospitalet, Valdemar Hansens Vej 3, DK-2600, 
      Glostrup, Denmark.
LA  - eng
PT  - Journal Article
DEP - 20230502
PL  - Germany
TA  - Graefes Arch Clin Exp Ophthalmol
JT  - Graefe's archive for clinical and experimental ophthalmology = Albrecht von 
      Graefes Archiv fur klinische und experimentelle Ophthalmologie
JID - 8205248
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Conjunctivitis, Allergic/diagnosis
OTO - NOTNLM
OT  - ChatGPT
OT  - Information accuracy
OT  - Patient information
OT  - Vernal keratoconjunctivitis
EDAT- 2023/05/02 12:44
MHDA- 2023/10/23 12:41
CRDT- 2023/05/02 11:05
PHST- 2023/03/06 00:00 [received]
PHST- 2023/04/15 00:00 [accepted]
PHST- 2023/04/11 00:00 [revised]
PHST- 2023/10/23 12:41 [medline]
PHST- 2023/05/02 12:44 [pubmed]
PHST- 2023/05/02 11:05 [entrez]
AID - 10.1007/s00417-023-06078-1 [pii]
AID - 10.1007/s00417-023-06078-1 [doi]
PST - ppublish
SO  - Graefes Arch Clin Exp Ophthalmol. 2023 Oct;261(10):3041-3043. doi: 
      10.1007/s00417-023-06078-1. Epub 2023 May 2.

PMID- 38321995
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240210
IS  - 2666-1683 (Electronic)
IS  - 2666-1691 (Print)
IS  - 2666-1683 (Linking)
VI  - 60
DP  - 2024 Feb
TI  - Artificial Intelligence: Ready To Pass the European Board Examinations in 
      Urology?
PG  - 44-46
LID - 10.1016/j.euros.2024.01.002 [doi]
AB  - The role of artificial intelligence (AI) in the medical domain is increasing on 
      an annual basis. AI allows instant access to the latest scientific data in 
      urological surgery, facilitating a level of theoretical knowledge that previously 
      required several years of practice and training. To evaluate the capability of AI 
      to provide robust data in a specialized domain, we submitted the in-service 
      assessment of the European Board of Urology to three different AI tools: ChatGPT 
      3.5, ChatGPT 4.0, and Bard. The assessment consists of 100 single-answer 
      questions with four multiple-choice options. We compared the responses of 736 
      participants to the AI responses. The average score for the 736 participants was 
      67.20. ChatGPT 3.5 scored 59 points, ranking in 570th place. ChatGPT 4.0 scored 
      80 points, ranking 80th, just on the border of the top 10%. Google Bard scored 68 
      points, ranking 340th. Our study demonstrates that AI systems have the capability 
      to participate in a urological examination and achieve satisfactory results. 
      However, a critical perspective must be maintained, as current AI systems are not 
      infallible. Finally, the role of AI in the acquisition of knowledge and the 
      dissemination of information remains to be delineated. PATIENT SUMMARY: We 
      submitted questions from the European Diploma in Urological Surgery to three 
      artificial intelligence (AI) systems. Our findings reveal that AI tools show 
      remarkable performance in assessments of urological surgical knowledge. However, 
      certain limitations were also observed.
CI  - © 2024 The Authors. Published by Elsevier B.V. on behalf of European Association 
      of Urology.
FAU - Mesnard, Benoît
AU  - Mesnard B
AD  - Urology Department, Hôpital Foch, Suresnes, France.
FAU - Schirmann, Aurélie
AU  - Schirmann A
AD  - Urology Department, Hôpital Foch, Suresnes, France.
FAU - Branchereau, Julien
AU  - Branchereau J
AD  - Urology Department, Nantes University Hospital, Nantes, France.
FAU - Perrot, Ophélie
AU  - Perrot O
AD  - Urology Department, Hôpital Foch, Suresnes, France.
FAU - Bogaert, Guy
AU  - Bogaert G
AD  - Urology Department, University of Leuven, Leuven, Belgium.
FAU - Neuzillet, Yann
AU  - Neuzillet Y
AD  - Urology Department, Hôpital Foch, Suresnes, France.
FAU - Lebret, Thierry
AU  - Lebret T
AD  - Urology Department, Hôpital Foch, Suresnes, France.
FAU - Madec, François-Xavier
AU  - Madec FX
AD  - Urology Department, Hôpital Foch, Suresnes, France.
LA  - eng
PT  - Journal Article
DEP - 20240130
PL  - Netherlands
TA  - Eur Urol Open Sci
JT  - European urology open science
JID - 101771568
PMC - PMC10845241
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Clinical reasoning
OT  - Urology
OT  - Urology degree
EDAT- 2024/02/07 06:42
MHDA- 2024/02/07 06:43
PMCR- 2024/01/30
CRDT- 2024/02/07 03:54
PHST- 2024/01/12 00:00 [accepted]
PHST- 2024/02/07 06:43 [medline]
PHST- 2024/02/07 06:42 [pubmed]
PHST- 2024/02/07 03:54 [entrez]
PHST- 2024/01/30 00:00 [pmc-release]
AID - S2666-1683(24)00211-8 [pii]
AID - 10.1016/j.euros.2024.01.002 [doi]
PST - epublish
SO  - Eur Urol Open Sci. 2024 Jan 30;60:44-46. doi: 10.1016/j.euros.2024.01.002. 
      eCollection 2024 Feb.

PMID- 38078915
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240112
IS  - 1432-5241 (Electronic)
IS  - 0364-216X (Linking)
VI  - 47
IP  - 6
DP  - 2023 Dec
TI  - Correction: Complications Following Facelift and Neck Lift: Implementation and 
      Assessment of Large Language Model and Artificial Intelligence (ChatGPT) 
      Performance Across 16 Simulated Patient Presentations.
PG  - 2911
LID - 10.1007/s00266-023-03790-5 [doi]
FAU - Abi-Rafeh, Jad
AU  - Abi-Rafeh J
AUID- ORCID: 0000-0002-7483-1515
AD  - Division of Plastic, Reconstructive, and Aesthetic Surgery, McGill University 
      Health Centre, Montreal, QC, Canada.
FAU - Hanna, Steven
AU  - Hanna S
AD  - Manhattan Eye, Ear and Throat Hospital, New York, NY, USA.
FAU - Bassiri-Tehrani, Brian
AU  - Bassiri-Tehrani B
AD  - Private Practice, Atlanta, GA, USA.
FAU - Kazan, Roy
AU  - Kazan R
AD  - Division of Plastic and Reconstructive Surgery, University of Pittsburgh, 
      Pittsburgh, PA, USA.
FAU - Nahai, Foad
AU  - Nahai F
AD  - Department of Surgery, Emory University, Atlanta, GA, USA. nahaimd@aol.com.
LA  - eng
PT  - Published Erratum
PL  - United States
TA  - Aesthetic Plast Surg
JT  - Aesthetic plastic surgery
JID - 7701756
SB  - IM
EFR - Aesthetic Plast Surg. 2023 Dec;47(6):2407-2414. PMID: 37589944
EDAT- 2023/12/11 12:42
MHDA- 2023/12/11 12:43
CRDT- 2023/12/11 11:04
PHST- 2023/12/11 12:43 [medline]
PHST- 2023/12/11 12:42 [pubmed]
PHST- 2023/12/11 11:04 [entrez]
AID - 10.1007/s00266-023-03790-5 [pii]
AID - 10.1007/s00266-023-03790-5 [doi]
PST - ppublish
SO  - Aesthetic Plast Surg. 2023 Dec;47(6):2911. doi: 10.1007/s00266-023-03790-5.

PMID- 37103921
OWN - NLM
STAT- MEDLINE
DCOM- 20230612
LR  - 20240320
IS  - 2168-619X (Electronic)
IS  - 2168-6181 (Print)
IS  - 2168-6181 (Linking)
VI  - 149
IP  - 6
DP  - 2023 Jun 1
TI  - Comparison Between ChatGPT and Google Search as Sources of Postoperative Patient 
      Instructions.
PG  - 556-558
LID - 10.1001/jamaoto.2023.0704 [doi]
FAU - Ayoub, Noel F
AU  - Ayoub NF
AD  - Department of Otolaryngology-Head and Neck Surgery, Stanford University School of 
      Medicine, Stanford, California.
FAU - Lee, Yu-Jin
AU  - Lee YJ
AD  - Department of Otolaryngology-Head and Neck Surgery, Stanford University School of 
      Medicine, Stanford, California.
FAU - Grimm, David
AU  - Grimm D
AD  - Department of Otolaryngology-Head and Neck Surgery, Stanford University School of 
      Medicine, Stanford, California.
FAU - Balakrishnan, Karthik
AU  - Balakrishnan K
AD  - Department of Otolaryngology-Head and Neck Surgery, Stanford University School of 
      Medicine, Stanford, California.
LA  - eng
PT  - Journal Article
PL  - United States
TA  - JAMA Otolaryngol Head Neck Surg
JT  - JAMA otolaryngology-- head &amp; neck surgery
JID - 101589542
SB  - IM
MH  - Humans
MH  - *Search Engine
MH  - *Health Literacy
PMC - PMC10141286
OAB - This qualitative study rates the level of understandability, actionability, and 
      procedure-specific content in postoperative instructions generated from ChatGPT, 
      Google Search, and Stanford University.
OABL- eng
COIS- Conflict of Interest Disclosures: Dr Balakrishnan reported receiving royalties 
      from Springer Inc outside the submitted work. No other disclosures were reported.
EDAT- 2023/04/27 12:41
MHDA- 2023/06/12 06:42
PMCR- 2024/04/27
CRDT- 2023/04/27 11:33
PHST- 2024/04/27 00:00 [pmc-release]
PHST- 2023/06/12 06:42 [medline]
PHST- 2023/04/27 12:41 [pubmed]
PHST- 2023/04/27 11:33 [entrez]
AID - 2804300 [pii]
AID - old230004 [pii]
AID - 10.1001/jamaoto.2023.0704 [doi]
PST - ppublish
SO  - JAMA Otolaryngol Head Neck Surg. 2023 Jun 1;149(6):556-558. doi: 
      10.1001/jamaoto.2023.0704.

PMID- 38250148
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240123
IS  - 2055-2076 (Print)
IS  - 2055-2076 (Electronic)
IS  - 2055-2076 (Linking)
VI  - 10
DP  - 2024 Jan-Dec
TI  - Reliability of ChatGPT for performing triage task in the emergency department 
      using the Korean Triage and Acuity Scale.
PG  - 20552076241227132
LID - 10.1177/20552076241227132 [doi]
LID - 20552076241227132
AB  - BACKGROUND: Artificial intelligence (AI) technology can enable more efficient 
      decision-making in healthcare settings. There is a growing interest in improving 
      the speed and accuracy of AI systems in providing responses for given tasks in 
      healthcare settings. OBJECTIVE: This study aimed to assess the reliability of 
      ChatGPT in determining emergency department (ED) triage accuracy using the Korean 
      Triage and Acuity Scale (KTAS). METHODS: Two hundred and two virtual patient 
      cases were built. The gold standard triage classification for each case was 
      established by an experienced ED physician. Three other human raters (ED 
      paramedics) were involved and rated the virtual cases individually. The virtual 
      cases were also rated by two different versions of the chat generative 
      pre-trained transformer (ChatGPT, 3.5 and 4.0). Inter-rater reliability was 
      examined using Fleiss' kappa and intra-class correlation coefficient (ICC). 
      RESULTS: The kappa values for the agreement between the four human raters and 
      ChatGPTs were .523 (version 4.0) and .320 (version 3.5). Of the five levels, the 
      performance was poor when rating patients at levels 1 and 5, as well as case 
      scenarios with additional text descriptions. There were differences in the 
      accuracy of the different versions of GPTs. The ICC between version 3.5 and the 
      gold standard was .520, and that between version 4.0 and the gold standard was 
      .802. CONCLUSIONS: A substantial level of inter-rater reliability was revealed 
      when GPTs were used as KTAS raters. The current study showed the potential of 
      using GPT in emergency healthcare settings. Considering the shortage of 
      experienced manpower, this AI method may help improve triaging accuracy.
CI  - © The Author(s) 2024.
FAU - Kim, Jae Hyuk
AU  - Kim JH
AD  - Department of Emergency Medicine, Mokpo Hankook Hospital, Jeonnam, South Korea.
FAU - Kim, Sun Kyung
AU  - Kim SK
AUID- ORCID: 0000-0001-8839-5577
AD  - Department of Nursing, Mokpo National University, Jeonnam, South Korea. RINGGOLD: 
      34991
AD  - Department of Biomedicine, Health &amp; Life Convergence Sciences, Biomedical and 
      Healthcare Research Institute, Jeonnam, South Korea.
FAU - Choi, Jongmyung
AU  - Choi J
AD  - Department of Computer Engineering, Mokpo National University, Jeonnam, South 
      Korea. RINGGOLD: 34991
FAU - Lee, Youngho
AU  - Lee Y
AD  - Department of Computer Engineering, Mokpo National University, Jeonnam, South 
      Korea. RINGGOLD: 34991
LA  - eng
PT  - Journal Article
DEP - 20240117
PL  - United States
TA  - Digit Health
JT  - Digital health
JID - 101690863
PMC - PMC10798071
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - KTAS
OT  - classification
OT  - emergency department
OT  - inter-rater reliability
COIS- The author(s) declared no potential conflicts of interest with respect to the 
      research, authorship, and/or publication of this article.
EDAT- 2024/01/22 06:42
MHDA- 2024/01/22 06:43
PMCR- 2024/01/17
CRDT- 2024/01/22 05:14
PHST- 2023/08/14 00:00 [received]
PHST- 2023/12/28 00:00 [accepted]
PHST- 2024/01/22 06:43 [medline]
PHST- 2024/01/22 06:42 [pubmed]
PHST- 2024/01/22 05:14 [entrez]
PHST- 2024/01/17 00:00 [pmc-release]
AID - 10.1177_20552076241227132 [pii]
AID - 10.1177/20552076241227132 [doi]
PST - epublish
SO  - Digit Health. 2024 Jan 17;10:20552076241227132. doi: 10.1177/20552076241227132. 
      eCollection 2024 Jan-Dec.

PMID- 38076046
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231211
IS  - 2405-8440 (Print)
IS  - 2405-8440 (Electronic)
IS  - 2405-8440 (Linking)
VI  - 9
IP  - 12
DP  - 2023 Dec
TI  - ChatGPT-assisted deep learning for diagnosing bone metastasis in bone scans: 
      Bridging the AI Gap for Clinicians.
PG  - e22409
LID - 10.1016/j.heliyon.2023.e22409 [doi]
LID - e22409
AB  - BACKGROUND: Bone scans are often used to identify bone metastases, but their low 
      specificity may necessitate further studies. Deep learning models may improve 
      diagnostic accuracy but require both medical and programming expertise. 
      Therefore, we investigated the feasibility of constructing a deep learning model 
      employing ChatGPT for the diagnosis of bone metastasis in bone scans and to 
      evaluate its diagnostic performance. METHOD: We examined 4626 consecutive cancer 
      patients (age, 65.1&nbsp;±&nbsp;11.3 years; 2334 female) who had bone scans for metastasis 
      assessment. A nuclear medicine physician developed a deep learning model using 
      ChatGPT 3.5 (OpenAI). We employed ResNet50 as the backbone network and compared 
      the diagnostic performance of four strategies (original training set, original 
      training set with 1:10 class weight, 10-fold data augmentation for positive 
      images only, and 10-fold data augmentation for all images) to address the class 
      imbalance. We used a class activation map algorithm for visualization. RESULTS: 
      Among the four strategies, the deep learning model with 10-fold data augmentation 
      for positive cases only, using a batch size of 16 and an epoch size of 150, 
      achieved the area under curve of 0.8156, the sensitivity of 56.0&nbsp;%, and 
      specificity of 88.7&nbsp;%. The class activation map indicated that the model focused 
      on disseminated bone metastases within the spine but might confuse them with 
      benign spinal lesions or intense urinary activity. CONCLUSIONS: Our study 
      illustrates that a clinical physician with rudimentary programming skills can 
      develop a deep learning model for medical image analysis, such as diagnosing bone 
      metastasis in bone scans using ChatGPT. Model visualization may offer guidance in 
      enhancing deep learning model development, including preprocessing, and 
      potentially support clinical decision-making processes.
CI  - © 2023 The Authors.
FAU - Son, Hye Joo
AU  - Son HJ
AD  - Department of Nuclear Medicine, Dankook University Medical Center, Cheonan, 
      Chungnam, Republic of Korea.
FAU - Kim, Soo-Jong
AU  - Kim SJ
AD  - Department of Neurology, Samsung Medical Center, Sungkyunkwan University School 
      of Medicine, Seoul, Republic of Korea.
AD  - Department of Health Sciences and Technology, SAIHST, Sungkyunkwan University, 
      Seoul, Republic of Korea.
AD  - Department of Intelligent Precision Healthcare Convergence, Sungkyunkwan 
      University, Suwon, Republic of Korea.
FAU - Pak, Sehyun
AU  - Pak S
AD  - Department of Medicine, Hallym University College of Medicine, Chuncheon, 
      Gangwon, Republic of Korea.
FAU - Lee, Suk Hyun
AU  - Lee SH
AD  - Department of Radiology, Hallym University Kangnam Sacred Heart Hospital, Hallym 
      University College of Medicine, Seoul, Republic of Korea.
LA  - eng
PT  - Journal Article
DEP - 20231120
PL  - England
TA  - Heliyon
JT  - Heliyon
JID - 101672560
PMC - PMC10709387
OTO - NOTNLM
OT  - Bone metastasis
OT  - Bone scan
OT  - ChatGPT
OT  - Convolutional neural network
OT  - Deep learning
COIS- The authors declare that they have no known competing financial interests or 
      personal relationships that could have appeared to influence the work reported in 
      this paper.
EDAT- 2023/12/11 12:42
MHDA- 2023/12/11 12:43
PMCR- 2023/11/20
CRDT- 2023/12/11 06:05
PHST- 2023/10/10 00:00 [received]
PHST- 2023/11/09 00:00 [revised]
PHST- 2023/11/10 00:00 [accepted]
PHST- 2023/12/11 12:43 [medline]
PHST- 2023/12/11 12:42 [pubmed]
PHST- 2023/12/11 06:05 [entrez]
PHST- 2023/11/20 00:00 [pmc-release]
AID - S2405-8440(23)09617-2 [pii]
AID - e22409 [pii]
AID - 10.1016/j.heliyon.2023.e22409 [doi]
PST - epublish
SO  - Heliyon. 2023 Nov 20;9(12):e22409. doi: 10.1016/j.heliyon.2023.e22409. 
      eCollection 2023 Dec.

PMID- 38486198
OWN - NLM
STAT- MEDLINE
DCOM- 20240318
LR  - 20240318
IS  - 1472-6947 (Electronic)
IS  - 1472-6947 (Linking)
VI  - 24
IP  - 1
DP  - 2024 Mar 14
TI  - Exploring the potential of ChatGPT in medical dialogue summarization: a study on 
      consistency with human preferences.
PG  - 75
LID - 10.1186/s12911-024-02481-8 [doi]
LID - 75
AB  - BACKGROUND: Telemedicine has experienced rapid growth in recent years, aiming to 
      enhance medical efficiency and reduce the workload of healthcare professionals. 
      During the COVID-19 pandemic in 2019, it became especially crucial, enabling 
      remote screenings and access to healthcare services while maintaining social 
      distancing. Online consultation platforms have emerged, but the demand has 
      strained the availability of medical professionals, directly leading to research 
      and development in automated medical consultation. Specifically, there is a need 
      for efficient and accurate medical dialogue summarization algorithms to condense 
      lengthy conversations into shorter versions focused on relevant medical facts. 
      The success of large language models like generative pre-trained transformer 
      (GPT)-3 has recently prompted a paradigm shift in natural language processing 
      (NLP) research. In this paper, we will explore its impact on medical dialogue 
      summarization. METHODS: We present the performance and evaluation results of two 
      approaches on a medical dialogue dataset. The first approach is based on 
      fine-tuned pre-trained language models, such as bert-based summarization 
      (BERTSUM) and bidirectional auto-regressive Transformers (BART). The second 
      approach utilizes a large language models (LLMs) GPT-3.5 with inter-context 
      learning (ICL). Evaluation is conducted using automated metrics such as ROUGE and 
      BERTScore. RESULTS: In comparison to the BART and ChatGPT models, the summaries 
      generated by the BERTSUM model not only exhibit significantly lower ROUGE and 
      BERTScore values but also fail to pass the testing for any of the metrics in 
      manual evaluation. On the other hand, the BART model achieved the highest ROUGE 
      and BERTScore values among all evaluated models, surpassing ChatGPT. Its ROUGE-1, 
      ROUGE-2, ROUGE-L, and BERTScore values were 14.94%, 53.48%, 32.84%, and 6.73% 
      higher respectively than ChatGPT's best results. However, in the manual 
      evaluation by medical experts, the summaries generated by the BART model exhibit 
      satisfactory performance only in the "Readability" metric, with less than 30% 
      passing the manual evaluation in other metrics. When compared to the BERTSUM and 
      BART models, the ChatGPT model was evidently more favored by human medical 
      experts. CONCLUSION: On one hand, the GPT-3.5 model can manipulate the style and 
      outcomes of medical dialogue summaries through various prompts. The generated 
      content is not only better received than results from certain human experts but 
      also more comprehensible, making it a promising avenue for automated medical 
      dialogue summarization. On the other hand, automated evaluation mechanisms like 
      ROUGE and BERTScore fall short in fully assessing the outputs of large language 
      models like GPT-3.5. Therefore, it is necessary to research more appropriate 
      evaluation criteria.
CI  - © 2024. The Author(s).
FAU - Liu, Yong
AU  - Liu Y
AD  - Department of Computer Science, Sichuan University, No. 24, South Section 1, 1st 
      Ring Road, Chendu, 610065, Sichuan, China.
FAU - Ju, Shenggen
AU  - Ju S
AD  - Department of Computer Science, Sichuan University, No. 24, South Section 1, 1st 
      Ring Road, Chendu, 610065, Sichuan, China. jsg@scu.edu.cn.
FAU - Wang, Junfeng
AU  - Wang J
AD  - Department of Computer Science, Sichuan University, No. 24, South Section 1, 1st 
      Ring Road, Chendu, 610065, Sichuan, China.
LA  - eng
GR  - 62137001/key project of the National Natural Science Foundation/
GR  - 62137001/key project of the National Natural Science Foundation/
GR  - 62137001/key project of the National Natural Science Foundation/
PT  - Journal Article
DEP - 20240314
PL  - England
TA  - BMC Med Inform Decis Mak
JT  - BMC medical informatics and decision making
JID - 101088682
SB  - IM
MH  - Humans
MH  - *Pandemics
MH  - Algorithms
MH  - Benchmarking
MH  - *COVID-19
MH  - Communication
PMC - PMC10938713
OTO - NOTNLM
OT  - Automated medical consultation
OT  - ChatGPT
OT  - Internet Healthcare
OT  - Large language models
OT  - Medical dialogue summarization
COIS- The authors declare no competing interests.
EDAT- 2024/03/15 06:43
MHDA- 2024/03/18 06:44
PMCR- 2024/03/14
CRDT- 2024/03/15 00:46
PHST- 2023/10/02 00:00 [received]
PHST- 2024/03/11 00:00 [accepted]
PHST- 2024/03/18 06:44 [medline]
PHST- 2024/03/15 06:43 [pubmed]
PHST- 2024/03/15 00:46 [entrez]
PHST- 2024/03/14 00:00 [pmc-release]
AID - 10.1186/s12911-024-02481-8 [pii]
AID - 2481 [pii]
AID - 10.1186/s12911-024-02481-8 [doi]
PST - epublish
SO  - BMC Med Inform Decis Mak. 2024 Mar 14;24(1):75. doi: 10.1186/s12911-024-02481-8.

PMID- 36968864
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230328
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 2
DP  - 2023 Feb
TI  - Applicability of ChatGPT in Assisting to Solve Higher Order Problems in 
      Pathology.
PG  - e35237
LID - 10.7759/cureus.35237 [doi]
LID - e35237
AB  - Background Artificial intelligence (AI) is evolving for healthcare services. 
      Higher cognitive thinking in AI refers to the ability of the system to perform 
      advanced cognitive processes, such as problem-solving, decision-making, 
      reasoning, and perception. This type of thinking goes beyond simple data 
      processing and involves the ability to understand and manipulate abstract 
      concepts, interpret, and use information in a contextually relevant way, and 
      generate new insights based on past experiences and accumulated knowledge. 
      Natural language processing models like ChatGPT is a conversational program that 
      can interact with humans to provide answers to queries. Objective We aimed to 
      ascertain the capability of ChatGPT in solving higher-order reasoning in the 
      subject of pathology. Methods This cross-sectional study was conducted on the 
      internet using an AI-based chat program that provides free service for research 
      purposes. The current version of ChatGPT (January 30 version) was used to 
      converse with a total of 100 higher-order reasoning queries. These questions were 
      randomly selected from the question bank of the institution and categorized 
      according to different systems. The responses to each question were collected and 
      stored for further analysis. The responses were evaluated by three expert 
      pathologists on a zero to five scale and categorized into the structure of the 
      observed learning outcome (SOLO) taxonomy categories. The score was compared by a 
      one-sample median test with hypothetical values to find its accuracy. Result A 
      total of 100 higher-order reasoning questions were solved by the program in an 
      average of 45.31±7.14 seconds for an answer. The overall median score was 4.08 
      (Q1-Q3: 4-4.33) which was below the hypothetical maximum value of five (one-test 
      median test p &lt;0.0001) and similar to four (one-test median test p = 0.14). The 
      majority (86%) of the responses were in the "relational"&nbsp;category in the SOLO 
      taxonomy. There was no difference in the scores of the responses for questions 
      asked from various organ systems in the subject of Pathology (Kruskal Wallis p = 
      0.55). The scores rated by three pathologists had an excellent level of 
      inter-rater reliability (ICC = 0.975 [95% CI: 0.965-0.983]; F = 40.26; p &lt; 
      0.0001). Conclusion The capability of ChatGPT to solve higher-order reasoning 
      questions in pathology had a relational level of accuracy. Hence, the text output 
      had connections among its parts to provide a meaningful response. The answers 
      from the program can score approximately 80%. Hence, academicians or students can 
      get help from the program for solving reasoning-type questions also. As the 
      program is evolving, further studies are needed to find its accuracy level in any 
      further versions.
CI  - Copyright © 2023, Sinha et al.
FAU - Sinha, Ranwir K
AU  - Sinha RK
AD  - Pathology, All India Institute of Medical Sciences, Deoghar, Jharkhand, IND.
FAU - Deb Roy, Asitava
AU  - Deb Roy A
AD  - Pathology, All India Institute of Medical Sciences, Deoghar, Jharkhand, IND.
FAU - Kumar, Nikhil
AU  - Kumar N
AD  - Pathology, All India Institute of Medical Sciences, Deoghar, Jharkhand, IND.
FAU - Mondal, Himel
AU  - Mondal H
AD  - Physiology, All India Institute of Medical Sciences, Deoghar, Jharkhand, IND.
LA  - eng
PT  - Journal Article
DEP - 20230220
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10033699
OTO - NOTNLM
OT  - artificial intelligence
OT  - chatgpt
OT  - cognition
OT  - critical reasoning
OT  - decision making
OT  - intelligence
OT  - microcomputers
OT  - pathologists
OT  - problem-solving
OT  - students
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/03/28 06:00
MHDA- 2023/03/28 06:01
PMCR- 2023/02/20
CRDT- 2023/03/27 03:31
PHST- 2023/02/20 00:00 [accepted]
PHST- 2023/03/27 03:31 [entrez]
PHST- 2023/03/28 06:00 [pubmed]
PHST- 2023/03/28 06:01 [medline]
PHST- 2023/02/20 00:00 [pmc-release]
AID - 10.7759/cureus.35237 [doi]
PST - epublish
SO  - Cureus. 2023 Feb 20;15(2):e35237. doi: 10.7759/cureus.35237. eCollection 2023 
      Feb.

PMID- 36909565
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231020
DP  - 2023 Feb 28
TI  - Assessing the Accuracy and Reliability of AI-Generated Medical Responses: An 
      Evaluation of the Chat-GPT Model.
LID - rs.3.rs-2566942 [pii]
LID - 10.21203/rs.3.rs-2566942/v1 [doi]
AB  - BACKGROUND: Natural language processing models such as ChatGPT can generate 
      text-based content and are poised to become a major information source in 
      medicine and beyond. The accuracy and completeness of ChatGPT for medical queries 
      is not known. METHODS: Thirty-three physicians across 17 specialties generated 
      284 medical questions that they subjectively classified as easy, medium, or hard 
      with either binary (yes/no) or descriptive answers. The physicians then graded 
      ChatGPT-generated answers to these questions for accuracy (6-point Likert scale; 
      range 1 - completely incorrect to 6 - completely correct) and completeness 
      (3-point Likert scale; range 1 - incomplete to 3 - complete plus additional 
      context). Scores were summarized with descriptive statistics and compared using 
      Mann-Whitney U or Kruskal-Wallis testing. RESULTS: Across all questions (n=284), 
      median accuracy score was 5.5 (between almost completely and completely correct) 
      with mean score of 4.8 (between mostly and almost completely correct). Median 
      completeness score was 3 (complete and comprehensive) with mean score of 2.5. For 
      questions rated easy, medium, and hard, median accuracy scores were 6, 5.5, and 5 
      (mean 5.0, 4.7, and 4.6; p=0.05). Accuracy scores for binary and descriptive 
      questions were similar (median 6 vs. 5; mean 4.9 vs. 4.7; p=0.07). Of 36 
      questions with scores of 1-2, 34 were re-queried/re-graded 8-17 days later with 
      substantial improvement (median 2 vs. 4; p&lt;0.01). CONCLUSIONS: ChatGPT generated 
      largely accurate information to diverse medical queries as judged by academic 
      physician specialists although with important limitations. Further research and 
      model development are needed to correct inaccuracies and for validation.
FAU - Johnson, Douglas
AU  - Johnson D
AUID- ORCID: 0000-0002-6390-773X
AD  - Vanderbilt University Medical Center.
FAU - Goodman, Rachel
AU  - Goodman R
AUID- ORCID: 0000-0001-7992-8108
AD  - Vanderbilt University School of Medicine.
FAU - Patrinely, J
AU  - Patrinely J
AD  - Vanderbilt University Medical Center.
FAU - Stone, Cosby
AU  - Stone C
AD  - Vanderbilt University Medical Center, Nashville, Tennessee.
FAU - Zimmerman, Eli
AU  - Zimmerman E
AD  - Vanderbilt University Medical Center.
FAU - Donald, Rebecca
AU  - Donald R
AUID- ORCID: 0000-0003-1341-6796
AD  - Vanderbilt University Medical Center.
FAU - Chang, Sam
AU  - Chang S
AD  - Vanderbilt University Medical Center.
FAU - Berkowitz, Sean
AU  - Berkowitz S
AD  - Vanderbilt University Medical Center.
FAU - Finn, Avni
AU  - Finn A
AD  - Vanderbilt University Medical Center.
FAU - Jahangir, Eiman
AU  - Jahangir E
AD  - Vanderbilt University Medical Center.
FAU - Scoville, Elizabeth
AU  - Scoville E
AD  - Vanderbilt University Medical Center.
FAU - Reese, Tyler
AU  - Reese T
AD  - Vanderbilt University Medical Center.
FAU - Friedman, Debra
AU  - Friedman D
AD  - Vanderbilt University Medical Center.
FAU - Bastarache, Julie
AU  - Bastarache J
AD  - Vanderbilt University Medical Center.
FAU - van der Heijden, Yuri
AU  - van der Heijden Y
AD  - Vanderbilt University Medical Center.
FAU - Wright, Jordan
AU  - Wright J
AD  - Vanderbilt University Medical Center.
FAU - Carter, Nicholas
AU  - Carter N
AD  - Vanderbilt University Medical Center.
FAU - Alexander, Matthew
AU  - Alexander M
AD  - Vanderbilt University Medical Center.
FAU - Choe, Jennifer
AU  - Choe J
AD  - Vanderbilt University Medical Center.
FAU - Chastain, Cody
AU  - Chastain C
AD  - Vanderbilt University Medical Center.
FAU - Zic, John
AU  - Zic J
AD  - Vanderbilt University Medical Center.
FAU - Horst, Sara
AU  - Horst S
AD  - Vanderbilt University Medical Center.
FAU - Turker, Isik
AU  - Turker I
AUID- ORCID: 0000-0003-3216-3499
AD  - Vanderbilt University Medical Center.
FAU - Agarwal, Rajiv
AU  - Agarwal R
AD  - Vanderbilt University Medical Center.
FAU - Osmundson, Evan
AU  - Osmundson E
AD  - Vanderbilt University Medical Center.
FAU - Idrees, Kamran
AU  - Idrees K
AD  - Vanderbilt University Medical Center.
FAU - Kieman, Colleen
AU  - Kieman C
AD  - Vanderbilt University Medical Center.
FAU - Padmanabhan, Chandrasekhar
AU  - Padmanabhan C
AD  - Vanderbilt University Medical Center.
FAU - Bailey, Christina
AU  - Bailey C
AD  - Vanderbilt University Medical Center.
FAU - Schlegel, Cameron
AU  - Schlegel C
AD  - Vanderbilt University Medical Center.
FAU - Chambless, Lola
AU  - Chambless L
AD  - Vanderbilt University.
FAU - Gibson, Mike
AU  - Gibson M
AD  - Vanderbilt University Medical Center.
FAU - Osterman, Travis
AU  - Osterman T
AD  - Vanderbilt University Medical Center.
FAU - Wheless, Lee
AU  - Wheless L
AD  - Vanderbilt University Medical Center.
LA  - eng
PT  - Preprint
DEP - 20230228
PL  - United States
TA  - Res Sq
JT  - Research square
JID - 101768035
PMC - PMC10002821
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - clinical decision making
OT  - deep learning
OT  - knowledge dissemination
OT  - large language model
OT  - medical education
OT  - natural language processing
EDAT- 2023/03/14 06:00
MHDA- 2023/03/14 06:01
PMCR- 2023/03/10
CRDT- 2023/03/13 04:00
PHST- 2023/03/14 06:00 [pubmed]
PHST- 2023/03/14 06:01 [medline]
PHST- 2023/03/13 04:00 [entrez]
PHST- 2023/03/10 00:00 [pmc-release]
AID - rs.3.rs-2566942 [pii]
AID - 10.21203/rs.3.rs-2566942/v1 [doi]
PST - epublish
SO  - Res Sq [Preprint]. 2023 Feb 28:rs.3.rs-2566942. doi: 10.21203/rs.3.rs-2566942/v1.

PMID- 36960444
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230501
IS  - 2666-3899 (Electronic)
IS  - 2666-3899 (Linking)
VI  - 4
IP  - 3
DP  - 2023 Mar 10
TI  - Is using ChatGPT cheating, plagiarism, both, neither, or forward thinking?
PG  - 100694
LID - 10.1016/j.patter.2023.100694 [doi]
LID - 100694
AB  - The recent emergence of ChatGPT has led to multiple considerations and 
      discussions regarding the ethics and usage of AI. In particular, the potential 
      exploitation in the educational realm must be considered, future-proofing 
      curriculum for the inevitable wave of AI-assisted assignments. Here, Brent Anders 
      discusses some of the key issues and concerns.
CI  - © 2023 The Author(s).
FAU - Anders, Brent A
AU  - Anders BA
AD  - Office of Institutional Research and Assessment and the Center for Teaching and 
      Learning, American University of Armenia, Yerevan, Armenia.
LA  - eng
PT  - News
DEP - 20230228
PL  - United States
TA  - Patterns (N Y)
JT  - Patterns (New York, N.Y.)
JID - 101767765
CIN - Patterns (N Y). 2023 Apr 14;4(4):100731. PMID: 37123441
PMC - PMC10028419
COIS- The author declares no competing interests.
EDAT- 2023/03/25 06:00
MHDA- 2023/03/25 06:01
PMCR- 2023/02/28
CRDT- 2023/03/24 02:31
PHST- 2023/03/24 02:31 [entrez]
PHST- 2023/03/25 06:00 [pubmed]
PHST- 2023/03/25 06:01 [medline]
PHST- 2023/02/28 00:00 [pmc-release]
AID - S2666-3899(23)00025-9 [pii]
AID - 100694 [pii]
AID - 10.1016/j.patter.2023.100694 [doi]
PST - epublish
SO  - Patterns (N Y). 2023 Feb 28;4(3):100694. doi: 10.1016/j.patter.2023.100694. 
      eCollection 2023 Mar 10.

PMID- 38294466
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20240205
LR  - 20240205
IS  - 2101-017X (Electronic)
IS  - 0035-2640 (Linking)
VI  - 73
IP  - 10
DP  - 2023 Dec
TI  - [Is ChatGPT reliable? Put to the test by medical students and teachers].
PG  - 1063-1064
FAU - Ménard, Joël
AU  - Ménard J
AD  - Professeur émérite, faculté de médecine Paris-Descartes, Paris, France.
LA  - fre
PT  - Journal Article
TT  - ChatGPT est-il fiable ? Mise à l’épreuve par des étudiants et enseignants en 
      médecine.
PL  - France
TA  - Rev Prat
JT  - La Revue du praticien
JID - 0404334
SB  - IM
OTO - NOTNLM
OT  - Artificial Intelligence
COIS- L’auteur déclare n’avoir aucun lien d’intérêts.
EDAT- 2024/01/31 12:42
MHDA- 2024/02/01 06:43
CRDT- 2024/01/31 10:49
PHST- 2024/02/01 06:43 [medline]
PHST- 2024/01/31 12:42 [pubmed]
PHST- 2024/01/31 10:49 [entrez]
PST - ppublish
SO  - Rev Prat. 2023 Dec;73(10):1063-1064.

PMID- 38162975
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240103
IS  - 1664-1078 (Print)
IS  - 1664-1078 (Electronic)
IS  - 1664-1078 (Linking)
VI  - 14
DP  - 2023
TI  - Enhancing academic writing skills and motivation: assessing the efficacy of 
      ChatGPT in AI-assisted language learning for EFL students.
PG  - 1260843
LID - 10.3389/fpsyg.2023.1260843 [doi]
LID - 1260843
AB  - INTRODUCTION: This mixed-methods study evaluates the impact of AI-assisted 
      language learning on Chinese English as a Foreign Language (EFL) students' 
      writing skills and writing motivation. As artificial intelligence (AI) becomes 
      more prevalent in educational settings, understanding its effects on language 
      learning outcomes is crucial. METHODS: The study employs a comprehensive 
      approach, combining quantitative and qualitative methods. The quantitative phase 
      utilizes a pre-test and post-test design to assess writing skills. Fifty EFL 
      students, matched for proficiency, are randomly assigned to experimental 
      (AI-assisted instruction via ChatGPT) or control (traditional instruction) 
      groups. Writing samples are evaluated using established scoring rubrics. 
      Concurrently, semi-structured interviews are conducted with a subset of 
      participants to explore writing motivation and experiences with AI-assisted 
      learning. RESULTS: Quantitative analysis reveals significant improvements in both 
      writing skills and motivation among students who received AI-assisted instruction 
      compared to the control group. The experimental group demonstrates enhanced 
      proficiency in various aspects of writing, including organization, coherence, 
      grammar, and vocabulary. Qualitative findings showcase diverse perspectives, 
      ranging from recognition of AI's innovative instructional role and its positive 
      influence on writing skills and motivation to concerns about contextual accuracy 
      and over-reliance. Participants also reflect on the long-term impact and 
      sustainability of AI-assisted instruction, emphasizing the need for ongoing 
      development and adaptation of AI tools. DISCUSSION: The nuanced findings offer a 
      comprehensive understanding of AI's transformative potential in education. These 
      insights have practical implications for practitioners and researchers, 
      emphasizing the benefits, challenges, and the evolving nature of AI's role in 
      language instruction.
CI  - Copyright © 2023 Song and Song.
FAU - Song, Cuiping
AU  - Song C
AD  - School of Foreign Studies, North Minzu University, Yinchuan, Ningxia, China.
FAU - Song, Yanping
AU  - Song Y
AD  - School of Public Administration, Central South University, Changsha, Hunan, 
      China.
LA  - eng
PT  - Journal Article
DEP - 20231215
PL  - Switzerland
TA  - Front Psychol
JT  - Frontiers in psychology
JID - 101550902
PMC - PMC10754989
OTO - NOTNLM
OT  - AI-assisted language learning
OT  - ChatGPT
OT  - EFL students
OT  - mixed methods study
OT  - writing motivation
OT  - writing skills
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2024/01/02 11:46
MHDA- 2024/01/02 11:47
PMCR- 2023/12/15
CRDT- 2024/01/01 04:32
PHST- 2023/07/18 00:00 [received]
PHST- 2023/11/28 00:00 [accepted]
PHST- 2024/01/02 11:47 [medline]
PHST- 2024/01/02 11:46 [pubmed]
PHST- 2024/01/01 04:32 [entrez]
PHST- 2023/12/15 00:00 [pmc-release]
AID - 10.3389/fpsyg.2023.1260843 [doi]
PST - epublish
SO  - Front Psychol. 2023 Dec 15;14:1260843. doi: 10.3389/fpsyg.2023.1260843. 
      eCollection 2023.

PMID- 37958050
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231122
IS  - 2227-9032 (Print)
IS  - 2227-9032 (Electronic)
IS  - 2227-9032 (Linking)
VI  - 11
IP  - 21
DP  - 2023 Nov 6
TI  - The Role of ChatGPT in the Advancement of Diagnosis, Management, and Prognosis of 
      Cardiovascular and Cerebrovascular Disease.
LID - 10.3390/healthcare11212906 [doi]
LID - 2906
AB  - Cardiovascular and cerebrovascular disease incidence has risen mainly due to poor 
      control of preventable risk factors and still constitutes a significant financial 
      and health burden worldwide. ChatGPT is an artificial intelligence language-based 
      model developed by OpenAI. Due to the model's unique cognitive capabilities 
      beyond data processing and the production of high-quality text, there has been a 
      surge of research interest concerning its role in the scientific community and 
      contemporary clinical practice. To fully exploit ChatGPT's potential benefits and 
      reduce its possible misuse, extreme caution must be taken to ensure its 
      implications ethically and equitably. In this narrative review, we explore the 
      language model's possible applications and limitations while emphasizing its 
      potential value for diagnosing, managing, and prognosis of cardiovascular and 
      cerebrovascular disease.
FAU - Chlorogiannis, David-Dimitris
AU  - Chlorogiannis DD
AD  - Department of Radiology, Brigham and Women's Hospital, Boston, MA 02115, USA.
FAU - Apostolos, Anastasios
AU  - Apostolos A
AUID- ORCID: 0000-0003-2616-7952
AD  - First Department of Cardiology, School of Medicine, National Kapodistrian 
      University of Athens, Hippokrateion General Hospital of Athens, 115 27 Athens, 
      Greece.
FAU - Chlorogiannis, Anargyros
AU  - Chlorogiannis A
AD  - Department of Health Economics, Policy and Management, Karolinska Institutet, 171 
      77 Stockholm, Sweden.
FAU - Palaiodimos, Leonidas
AU  - Palaiodimos L
AUID- ORCID: 0000-0003-4682-5991
AD  - Division of Hospital Medicine, Jacobi Medical Center, NYC H+H, Albert Einstein 
      College of Medicine, New York, NY 10461, USA.
FAU - Giannakoulas, George
AU  - Giannakoulas G
AUID- ORCID: 0000-0001-7491-6319
AD  - Department of Cardiology, AHEPA University Hospital, Aristotle University of 
      Thessaloniki, 541 24 Thessaloniki, Greece.
FAU - Pargaonkar, Sumant
AU  - Pargaonkar S
AUID- ORCID: 0000-0002-2568-2664
AD  - Division of Hospital Medicine, Jacobi Medical Center, NYC H+H, Albert Einstein 
      College of Medicine, New York, NY 10461, USA.
FAU - Xesfingi, Sofia
AU  - Xesfingi S
AD  - Department of Economics, University of Piraeus, 185 34 Piraeus, Greece.
FAU - Kokkinidis, Damianos G
AU  - Kokkinidis DG
AD  - Section of Cardiovascular Medicine, Yale University School of Medicine, New 
      Haven, CT 06510, USA.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20231106
PL  - Switzerland
TA  - Healthcare (Basel)
JT  - Healthcare (Basel, Switzerland)
JID - 101666525
PMC - PMC10648908
OTO - NOTNLM
OT  - cardiovascular disease
OT  - cerebrovascular disease
OT  - chatGPT
OT  - diagnosis
OT  - language-based models
OT  - myocardial infarction
OT  - stroke
OT  - treatment
COIS- The authors declare no conflict of interest.
EDAT- 2023/11/14 06:43
MHDA- 2023/11/14 06:44
PMCR- 2023/11/06
CRDT- 2023/11/14 02:05
PHST- 2023/10/03 00:00 [received]
PHST- 2023/10/24 00:00 [revised]
PHST- 2023/11/04 00:00 [accepted]
PHST- 2023/11/14 06:44 [medline]
PHST- 2023/11/14 06:43 [pubmed]
PHST- 2023/11/14 02:05 [entrez]
PHST- 2023/11/06 00:00 [pmc-release]
AID - healthcare11212906 [pii]
AID - healthcare-11-02906 [pii]
AID - 10.3390/healthcare11212906 [doi]
PST - epublish
SO  - Healthcare (Basel). 2023 Nov 6;11(21):2906. doi: 10.3390/healthcare11212906.

PMID- 38371109
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240220
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 16
IP  - 1
DP  - 2024 Jan
TI  - A Comparative Analysis of AI Models in Complex Medical Decision-Making Scenarios: 
      Evaluating ChatGPT, Claude AI, Bard, and Perplexity.
PG  - e52485
LID - 10.7759/cureus.52485 [doi]
LID - e52485
AB  - This study rigorously evaluates the performance of four artificial intelligence 
      (AI) language models - ChatGPT, Claude AI, Google Bard, and Perplexity AI - 
      across four key metrics: accuracy, relevance, clarity, and completeness.&nbsp;We used 
      a strong mix of research methods, getting opinions from 14 scenarios. This helped 
      us make sure our findings were accurate and dependable.&nbsp;The study showed that 
      Claude AI performs better than others because it gives complete responses. Its 
      average score was 3.64 for relevance and 3.43 for completeness compared to other 
      AI tools. ChatGPT always did well, and Google Bard had unclear responses, which 
      varied greatly, making it difficult to understand it, so there was no consistency 
      in Google Bard.&nbsp;These results give important information about what AI language 
      models are doing well or not for medical suggestions. They help us use them 
      better, telling us how to improve future tech changes that use AI.&nbsp;The study 
      shows that AI abilities match complex medical scenarios.
CI  - Copyright © 2024, Uppalapati et al.
FAU - Uppalapati, Vamsi Krishna
AU  - Uppalapati VK
AD  - Department of Anesthesiology, Tata Main Hospital, Jamshedpur, IND.
FAU - Nag, Deb Sanjay
AU  - Nag DS
AD  - Department of Anesthesiology, Tata Main Hospital, Jamshedpur, IND.
LA  - eng
PT  - Journal Article
DEP - 20240118
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10874112
OTO - NOTNLM
OT  - ai comparison
OT  - ai efficacy
OT  - future medicine
OT  - healthcare ai
OT  - medical decision-making
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/02/19 06:42
MHDA- 2024/02/19 06:43
PMCR- 2024/01/18
CRDT- 2024/02/19 04:20
PHST- 2024/01/18 00:00 [accepted]
PHST- 2024/02/19 06:43 [medline]
PHST- 2024/02/19 06:42 [pubmed]
PHST- 2024/02/19 04:20 [entrez]
PHST- 2024/01/18 00:00 [pmc-release]
AID - 10.7759/cureus.52485 [doi]
PST - epublish
SO  - Cureus. 2024 Jan 18;16(1):e52485. doi: 10.7759/cureus.52485. eCollection 2024 
      Jan.

PMID- 36945641
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231030
DP  - 2023 Mar 8
TI  - Empowering Beginners in Bioinformatics with ChatGPT.
LID - 2023.03.07.531414 [pii]
LID - 10.1101/2023.03.07.531414 [doi]
AB  - The impressive conversational and programming abilities of ChatGPT make it an 
      attractive tool for facilitating the education of bioinformatics data analysis 
      for beginners. In this study, we proposed an iterative model to fine-tune 
      instructions for guiding a ChatGPT in generating code for bioinformatics data 
      analysis tasks. We demonstrated the feasibility of the model by applying it to 
      various bioinformatics topics. Additionally, we discussed practical 
      considerations and limitations regarding the use of the model in chatbot-aided 
      bioinformatics education.
FAU - Shue, Evelyn
AU  - Shue E
AD  - Department of Microbiology, Immunology &amp; Cell Biology, West Virginia University, 
      Morgantown, WV, USA.
FAU - Liu, Li
AU  - Liu L
AD  - College of Health Solutions, Arizona State University, Phoenix, AZ, USA.
AD  - Biodesign Institute, Arizona State University, Tempe, AZ, USA.
FAU - Li, Bingxin
AU  - Li B
AD  - Finance Department, John Chambers College of Business and Economics, West 
      Virginia University, Morgantown, WV, USA.
FAU - Feng, Zifeng
AU  - Feng Z
AD  - Department of Economics and Finance, The University of Texas at El Paso, El Paso, 
      TX, USA.
FAU - Li, Xin
AU  - Li X
AD  - Lane Department of Computer Science and Electrical Engineering, West Virginia 
      University, Morgantown, WV, USA.
FAU - Hu, Gangqing
AU  - Hu G
AD  - Department of Microbiology, Immunology &amp; Cell Biology, West Virginia University, 
      Morgantown, WV, USA.
LA  - eng
GR  - P20 GM103434/GM/NIGMS NIH HHS/United States
GR  - P20 GM121322/GM/NIGMS NIH HHS/United States
GR  - R01 LM013438/LM/NLM NIH HHS/United States
GR  - U54 GM104942/GM/NIGMS NIH HHS/United States
PT  - Preprint
DEP - 20230308
PL  - United States
TA  - bioRxiv
JT  - bioRxiv : the preprint server for biology
JID - 101680187
UIN - This article has been published with doi: 10.15302/J-QB-023-0327
CIN - Quant Biol. 2023 Jun;11(2):204-206. PMID: 37900935
PMC - PMC10028953
EDAT- 2023/03/23 06:00
MHDA- 2023/03/23 06:01
PMCR- 2023/03/21
CRDT- 2023/03/22 01:58
PHST- 2023/03/23 06:00 [pubmed]
PHST- 2023/03/23 06:01 [medline]
PHST- 2023/03/22 01:58 [entrez]
PHST- 2023/03/21 00:00 [pmc-release]
AID - 2023.03.07.531414 [pii]
AID - 10.1101/2023.03.07.531414 [doi]
PST - epublish
SO  - bioRxiv [Preprint]. 2023 Mar 8:2023.03.07.531414. doi: 10.1101/2023.03.07.531414.

PMID- 37431972
OWN - NLM
STAT- Publisher
LR  - 20231024
IS  - 1784-973X (Electronic)
IS  - 0001-5385 (Linking)
VI  - 78
IP  - 9
DP  - 2023 Nov
TI  - Does the long-term administration of proton pump inhibitors increase the risk of 
      adverse cardiovascular outcomes? A ChatGPT powered umbrella review.
PG  - 980-988
LID - 10.1080/00015385.2023.2231299 [doi]
AB  - BACKGROUND: Proton pump inhibitors (PPIs) are commonly prescribed for the 
      treatment of acid-related disorders. In the context of coronary artery disease 
      (CAD), PPIs are commonly prescribed along with antiplatelet medications. In fact, 
      the potential interaction between these two classes of medications has been 
      subject to much debate. This review aimed to summarise the findings from 
      systematic reviews and meta-analyses on the casual relationship between PPI use 
      (alone) and major adverse cardiovascular events (MACE). Furthermore, the recent 
      release of ChatGPT has provided reviewers with a powerful natural language 
      processing tool. We therefore aimed to assess the utility of ChatGPT in the 
      systematic review process. METHODS: A comprehensive search of PubMed was 
      conducted to identify relevant systematic reviews and meta-analyses published up 
      to March 2023. Two independent reviewers assessed the eligibility of the studies, 
      extracted the data, and assessed the methodological quality using AMSTAR 2.0. The 
      population of interest was adults that received the medications of interest 
      (PPIs) for a minimum of three months, regardless of indication. Control groups 
      were defined as placebo or active comparators. The outcomes of interest were 
      described under the general term MACE, which include cardiovascular death, 
      non-fatal myocardial infarction, and non-fatal stroke. There were no restrictions 
      with regards to time, but we only included reports in English. A different group 
      of independent reviewers simultaneously ran the same process using ChatGPT. The 
      results were then compared with the human generated results. RESULTS: Seven 
      systematic reviews and meta-analyses were included, involving a total of 46 
      randomised controlled trials and 33 observational studies. The studies examined 
      the association between PPI use and MACE, including stroke, myocardial 
      infarction, and all-cause mortality. The results of the individual studies were 
      conflicting, with some showing a positive association between PPI use and MACE, 
      some showing no association, and others showing mixed results. However, the 
      majority of the studies that included observational data reported a positive 
      association between PPI use and MACE. Sensitivity analyses conducted in some 
      studies did not significantly alter the primary results, suggesting that the 
      findings were robust. Furthermore, ChatGPT was successfully prompted to execute 
      most tasks involved in this review. We therefore present text that was generated 
      by ChatGPT, including the abstract, introduction, results, and discussion 
      sections. CONCLUSION: The findings of this umbrella review suggest that a causal 
      relationship between PPI use and an increased risk of MACE cannot be ruled out. 
      Further research is needed to better understand this relationship, particularly 
      the underlying mechanisms and potential confounding factors. Healthcare 
      professionals should consider the long-term use of PPIs and carefully weigh the 
      risks and benefits for each patient. Finally, ChatGPT was successfully prompted 
      to execute most of the tasks involved in this review. We therefore feel that this 
      tool will be of great assistance in the field of evidence synthesis in the near 
      future.
FAU - Teperikidis, Eleftherios
AU  - Teperikidis E
AD  - Third Department of Cardiology, Ippokratio General Hospital, Aristotle University 
      of Thessa-loniki, Thessaloniki, Greece.
AD  - Clinical Research Unit, Special Unit for Biomedical Research and Education 
      (SUBRE), School of Medicine, Aristotle University of Thessaloniki, Thessaloniki, 
      Greece.
FAU - Boulmpou, Aristi
AU  - Boulmpou A
AD  - Third Department of Cardiology, Ippokratio General Hospital, Aristotle University 
      of Thessa-loniki, Thessaloniki, Greece.
FAU - Potoupni, Victoria
AU  - Potoupni V
AD  - Third Department of Cardiology, Ippokratio General Hospital, Aristotle University 
      of Thessa-loniki, Thessaloniki, Greece.
FAU - Kundu, Satyabrata
AU  - Kundu S
AD  - Department of Pharmacology, ISF College of Pharmacy, Moga, India.
FAU - Singh, Balpreet
AU  - Singh B
AD  - Department of Biochemistry, Punjab Agricultural University, Ludhiana, India.
FAU - Papadopoulos, Christodoulos
AU  - Papadopoulos C
AD  - Third Department of Cardiology, Ippokratio General Hospital, Aristotle University 
      of Thessa-loniki, Thessaloniki, Greece.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20230711
PL  - England
TA  - Acta Cardiol
JT  - Acta cardiologica
JID - 0370570
SB  - IM
OTO - NOTNLM
OT  - AI
OT  - MACE
OT  - PPI
OT  - Proton pump inhibitors
OT  - artificial intelligence
OT  - major adverse cardiovascular outcomes
EDAT- 2023/07/11 13:10
MHDA- 2023/07/11 13:10
CRDT- 2023/07/11 08:33
PHST- 2023/07/11 13:10 [pubmed]
PHST- 2023/07/11 13:10 [medline]
PHST- 2023/07/11 08:33 [entrez]
AID - 10.1080/00015385.2023.2231299 [doi]
PST - ppublish
SO  - Acta Cardiol. 2023 Nov;78(9):980-988. doi: 10.1080/00015385.2023.2231299. Epub 
      2023 Jul 11.

PMID- 38545679
OWN - NLM
STAT- Publisher
LR  - 20240328
IS  - 1531-4995 (Electronic)
IS  - 0023-852X (Linking)
DP  - 2024 Mar 28
TI  - Does ChatGPT Answer Otolaryngology Questions Accurately?
LID - 10.1002/lary.31410 [doi]
AB  - OBJECTIVE: Investigate the accuracy of ChatGPT in the manner of medical questions 
      related to otolaryngology. METHODS: A ChatGPT session was opened within which 93 
      questions were asked related to otolaryngology topics. Questions were drawn from 
      all major domains within otolaryngology and based upon key action statements 
      (KAS) from clinical practice guidelines (CPGs). Twenty-one "patient-level" 
      questions were also asked of the program. Answers were graded as either 
      "correct," "partially correct," "incorrect," or "non-answer." RESULTS: Correct 
      answers were given at a rate of 45.5% (71.4% correct in patient-level, 37.3% 
      CPG); partially correct answers at 31.8% (28.6% patient-level, 32.8% CPG); 
      incorrect at 21.6% (0% patient-level, 28.4% CPG); and 1.1% non-answers (% 
      patient-level, 1.5% CPG). There was no difference in the rate of correct answers 
      between CPGs published before or after the period of data collection cited by 
      ChatGPT. CPG-based questions were less likely to be correct than patient-level 
      questions (p = 0.003). CONCLUSION: Publicly available artificial intelligence 
      software has become increasingly popular with consumers for everything from 
      story-telling to data collection. In this study, we examined the accuracy of 
      ChatGPT responses to questions related to otolaryngology over 7 domains and 21 
      published CPGs. Physicians and patients should understand the limitations of this 
      software as it applies to otolaryngology, and programmers in future iterations 
      should consider giving greater weight to information published by 
      well-established journals and written by national content experts. LEVEL OF 
      EVIDENCE: N/A Laryngoscope, 2024.
CI  - © 2024 The Authors. The Laryngoscope published by Wiley Periodicals LLC on behalf 
      of The American Laryngological, Rhinological and Otological Society, Inc.
FAU - Maksimoski, Matthew
AU  - Maksimoski M
AUID- ORCID: 0000-0002-7587-8403
AD  - Division of Pediatric Otolaryngology, Cincinnati Children's Hospital Medical 
      Center, Cincinnati, Ohio, U.S.A.
AD  - Department of Otolaryngology - Head and Neck Surgery, University of Cincinnati, 
      231 Albert Sabin Way, Cincinnati, USA.
FAU - Noble, Anisha Rhea
AU  - Noble AR
AUID- ORCID: 0000-0002-1442-3819
AD  - Division of Pediatric Otolaryngology, Cincinnati Children's Hospital Medical 
      Center, Cincinnati, Ohio, U.S.A.
AD  - Department of Otolaryngology - Head and Neck Surgery, University of Cincinnati, 
      231 Albert Sabin Way, Cincinnati, USA.
FAU - Smith, David F
AU  - Smith DF
AUID- ORCID: 0000-0002-0048-4012
AD  - Division of Pediatric Otolaryngology, Cincinnati Children's Hospital Medical 
      Center, Cincinnati, Ohio, U.S.A.
AD  - Department of Otolaryngology - Head and Neck Surgery, University of Cincinnati, 
      231 Albert Sabin Way, Cincinnati, USA.
AD  - Division of Sleep and Circadian Medicine, Cincinnati Children's Hospital Medical 
      Center, Cincinnati, Ohio, U.S.A.
LA  - eng
PT  - Journal Article
DEP - 20240328
PL  - United States
TA  - Laryngoscope
JT  - The Laryngoscope
JID - 8607378
SB  - IM
OTO - NOTNLM
OT  - computers
OT  - innovation
OT  - machine learning
OT  - patient advocacy
EDAT- 2024/03/28 06:46
MHDA- 2024/03/28 06:46
CRDT- 2024/03/28 04:17
PHST- 2024/02/28 00:00 [revised]
PHST- 2023/12/19 00:00 [received]
PHST- 2024/03/12 00:00 [accepted]
PHST- 2024/03/28 06:46 [medline]
PHST- 2024/03/28 06:46 [pubmed]
PHST- 2024/03/28 04:17 [entrez]
AID - 10.1002/lary.31410 [doi]
PST - aheadofprint
SO  - Laryngoscope. 2024 Mar 28. doi: 10.1002/lary.31410.

PMID- 38319619
OWN - NLM
STAT- Publisher
LR  - 20240206
IS  - 1539-2031 (Electronic)
IS  - 0192-0790 (Linking)
DP  - 2024 Feb 7
TI  - Colorectal Cancer Prevention: Is Chat Generative Pretrained Transformer (Chat 
      GPT) ready to Assist Physicians in Determining Appropriate Screening and 
      Surveillance Recommendations?
LID - 10.1097/MCG.0000000000001979 [doi]
AB  - OBJECTIVE: To determine whether a publicly available advanced language model 
      could help determine appropriate colorectal cancer (CRC) screening and 
      surveillance recommendations. BACKGROUND: Poor physician knowledge or inability 
      to accurately recall recommendations might affect adherence to CRC screening 
      guidelines. Adoption of newer technologies can help improve the delivery of such 
      preventive care services. METHODS: An assessment with 10 multiple choice 
      questions, including 5 CRC screening and 5 CRC surveillance clinical vignettes, 
      was inputted into chat generative pretrained transformer (ChatGPT) 3.5 in 4 
      separate sessions. Responses were recorded and screened for accuracy to determine 
      the reliability of this tool. The mean number of correct answers was then 
      compared against a control group of gastroenterologists and colorectal surgeons 
      answering the same questions with and without the help of a previously validated 
      CRC screening mobile app. RESULTS: The average overall performance of ChatGPT was 
      45%. The mean number of correct answers was 2.75 (95% CI: 2.26-3.24), 1.75 (95% 
      CI: 1.26-2.24), and 4.5 (95% CI: 3.93-5.07) for screening, surveillance, and 
      total questions, respectively. ChatGPT showed inconsistency and gave a different 
      answer in 4 questions among the different sessions. A total of 238 physicians 
      also responded to the assessment; 123 (51.7%) without and 115 (48.3%) with the 
      mobile app. The mean number of total correct answers of ChatGPT was significantly 
      lower than those of physicians without [5.62 (95% CI: 5.32-5.92)] and with the 
      mobile app [7.71 (95% CI: 7.39-8.03); P &lt; 0.001]. CONCLUSIONS: Large language 
      models developed with artificial intelligence require further refinements to 
      serve as reliable assistants in clinical practice.
CI  - Copyright © 2024 Wolters Kluwer Health, Inc. All rights reserved.
FAU - Pereyra, Lisandro
AU  - Pereyra L
AD  - Department of Gastroenterology.
AD  - Endoscopy Unit, Department of Surgery.
FAU - Schlottmann, Francisco
AU  - Schlottmann F
AD  - Endoscopy Unit, Department of Surgery.
AD  - Department of Surgery, Hospital Alemán of Buenos Aires.
FAU - Steinberg, Leandro
AU  - Steinberg L
AD  - Department of Gastroenterology, Fundacion Favaloro, Buenos Aires, Argentina.
FAU - Lasa, Juan
AU  - Lasa J
AD  - Department of Gastroenterology, CEMIC, Buenos Aires, Argentina.
LA  - eng
PT  - Journal Article
DEP - 20240207
PL  - United States
TA  - J Clin Gastroenterol
JT  - Journal of clinical gastroenterology
JID - 7910017
SB  - IM
EDAT- 2024/02/06 13:44
MHDA- 2024/02/06 13:44
CRDT- 2024/02/06 11:22
PHST- 2023/09/11 00:00 [received]
PHST- 2024/01/12 00:00 [accepted]
PHST- 2024/02/06 13:44 [medline]
PHST- 2024/02/06 13:44 [pubmed]
PHST- 2024/02/06 11:22 [entrez]
AID - 00004836-990000000-00265 [pii]
AID - 10.1097/MCG.0000000000001979 [doi]
PST - aheadofprint
SO  - J Clin Gastroenterol. 2024 Feb 7. doi: 10.1097/MCG.0000000000001979.

PMID- 38157785
OWN - NLM
STAT- MEDLINE
DCOM- 20240206
LR  - 20240206
IS  - 1872-8243 (Electronic)
IS  - 1386-5056 (Linking)
VI  - 183
DP  - 2024 Mar
TI  - Zero-shot information extraction from radiological reports using ChatGPT.
PG  - 105321
LID - S1386-5056(23)00339-8 [pii]
LID - 10.1016/j.ijmedinf.2023.105321 [doi]
AB  - INTRODUCTION: Electronic health records contain an enormous amount of valuable 
      information recorded in free text. Information extraction is the strategy to 
      transform free text into structured data, but some of its components require 
      annotated data to tune, which has become a bottleneck. Large language models 
      achieve good performances on various downstream NLP tasks without parameter 
      tuning, becoming a possible way to extract information in a zero-shot manner. 
      METHODS: In this study, we aim to explore whether the most popular large language 
      model, ChatGPT, can extract information from the radiological reports. We first 
      design the prompt template for the interested information in the CT reports. 
      Then, we generate the prompts by combining the prompt template with the CT 
      reports as the inputs of ChatGPT to obtain the responses. A post-processing 
      module is developed to transform the responses into structured extraction 
      results. Besides, we add prior medical knowledge to the prompt template to reduce 
      wrong extraction results. We also explore the consistency of the extraction 
      results. RESULTS: We conducted the experiments with 847 real CT reports. The 
      experimental results indicate that ChatGPT can achieve competitive performances 
      for some extraction tasks like tumor location, tumor long and short diameters 
      compared with the baseline information extraction system. By adding some prior 
      medical knowledge to the prompt template, extraction tasks about tumor 
      spiculations and lobulations obtain significant improvements but tasks about 
      tumor density and lymph node status do not achieve better performances. 
      CONCLUSION: ChatGPT can achieve competitive information extraction for 
      radiological reports in a zero-shot manner. Adding prior medical knowledge as 
      instructions can further improve performances for some extraction tasks but may 
      lead to worse performances for some complex extraction tasks.
CI  - Copyright © 2023 Elsevier B.V. All rights reserved.
FAU - Hu, Danqing
AU  - Hu D
AD  - Zhejiang Lab, Hangzhou, 311121, Zhejiang, China. Electronic address: 
      hudq@zhejianglab.com.
FAU - Liu, Bing
AU  - Liu B
AD  - Department of Thoracic Surgery II, Peking University Cancer Hospital and 
      Institute, Beijing, 100142, China.
FAU - Zhu, Xiaofeng
AU  - Zhu X
AD  - Zhejiang Lab, Hangzhou, 311121, Zhejiang, China. Electronic address: 
      andy.zhu@zhejianglab.com.
FAU - Lu, Xudong
AU  - Lu X
AD  - College of Biomedical Engineering and Instrumental Science, Zhejiang University, 
      Hangzhou, 310027, Zhejiang, China.
FAU - Wu, Nan
AU  - Wu N
AD  - Department of Thoracic Surgery II, Peking University Cancer Hospital and 
      Institute, Beijing, 100142, China. Electronic address: nanwu@bjmu.edu.cn.
LA  - eng
PT  - Journal Article
DEP - 20231221
PL  - Ireland
TA  - Int J Med Inform
JT  - International journal of medical informatics
JID - 9711057
SB  - IM
MH  - Humans
MH  - *Electronic Health Records
MH  - Information Storage and Retrieval
MH  - Knowledge
MH  - Language
MH  - *Neoplasms
OTO - NOTNLM
OT  - Information extraction
OT  - Large language model
OT  - Lung cancer
OT  - Question answering
OT  - Radiological report
COIS- Declaration of Competing Interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2024/01/02 11:46
MHDA- 2024/02/06 06:42
CRDT- 2023/12/29 18:10
PHST- 2023/09/06 00:00 [received]
PHST- 2023/12/04 00:00 [revised]
PHST- 2023/12/16 00:00 [accepted]
PHST- 2024/02/06 06:42 [medline]
PHST- 2024/01/02 11:46 [pubmed]
PHST- 2023/12/29 18:10 [entrez]
AID - S1386-5056(23)00339-8 [pii]
AID - 10.1016/j.ijmedinf.2023.105321 [doi]
PST - ppublish
SO  - Int J Med Inform. 2024 Mar;183:105321. doi: 10.1016/j.ijmedinf.2023.105321. Epub 
      2023 Dec 21.

PMID- 36925365
OWN - NLM
STAT- MEDLINE
DCOM- 20230529
LR  - 20230720
IS  - 2211-5684 (Electronic)
IS  - 2211-5684 (Linking)
VI  - 104
IP  - 6
DP  - 2023 Jun
TI  - Beyond chatting: The opportunities and challenges of ChatGPT in medicine and 
      radiology.
PG  - 263-264
LID - S2211-5684(23)00030-X [pii]
LID - 10.1016/j.diii.2023.02.006 [doi]
FAU - Ferres, Juan M Lavista
AU  - Ferres JML
AD  - Microsoft, Inc., AI for Good Research Lab, Redmond, WA 98052, USA.
FAU - Weeks, William B
AU  - Weeks WB
AD  - Microsoft, Inc., AI for Good Research Lab, Redmond, WA 98052, USA.
FAU - Chu, Linda C
AU  - Chu LC
AD  - The Russell H. Morgan Department of Radiology and Radiological Science, Johns 
      Hopkins University School of Medicine, Baltimore, MD 2128, USA.
FAU - Rowe, Steven P
AU  - Rowe SP
AD  - The Russell H. Morgan Department of Radiology and Radiological Science, Johns 
      Hopkins University School of Medicine, Baltimore, MD 2128, USA. Electronic 
      address: srowe8@jhmi.edu.
FAU - Fishman, Elliot K
AU  - Fishman EK
AD  - The Russell H. Morgan Department of Radiology and Radiological Science, Johns 
      Hopkins University School of Medicine, Baltimore, MD 2128, USA.
LA  - eng
PT  - Comment
PT  - Editorial
DEP - 20230314
PL  - France
TA  - Diagn Interv Imaging
JT  - Diagnostic and interventional imaging
JID - 101568499
SB  - IM
CON - Diagn Interv Imaging. 2023 Jun;104(6):269-274. PMID: 36858933
MH  - Humans
MH  - *Radiology
MH  - Radiography
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Generative pre-trained transformer
OT  - Radiology
COIS- Disclosure of interest SPR is a co-founder of, stockholder in, and consultant for 
      PlenaryAI, Inc. He has received research funding from, and serves as a consultant 
      for, Lantheus Pharmaceuticals, Inc, the producer of Pylarify AI. Juan M. Lavista 
      Ferres and William B. Weeks are employees of Microsoft. The other authors report 
      no relevant conflicts of interest.
EDAT- 2023/03/17 06:00
MHDA- 2023/05/29 06:42
CRDT- 2023/03/16 23:10
PHST- 2023/02/20 00:00 [received]
PHST- 2023/02/21 00:00 [accepted]
PHST- 2023/05/29 06:42 [medline]
PHST- 2023/03/17 06:00 [pubmed]
PHST- 2023/03/16 23:10 [entrez]
AID - S2211-5684(23)00030-X [pii]
AID - 10.1016/j.diii.2023.02.006 [doi]
PST - ppublish
SO  - Diagn Interv Imaging. 2023 Jun;104(6):263-264. doi: 10.1016/j.diii.2023.02.006. 
      Epub 2023 Mar 14.

PMID- 38427906
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20240304
LR  - 20240304
IS  - 1557-7740 (Electronic)
IS  - 1557-7740 (Linking)
VI  - 27
IP  - 3
DP  - 2024 Mar
TI  - Letter to the Editor: To ChatGPT or Not to ChatGPT: Code of Conduct for User.
PG  - 291
LID - 10.1089/jpm.2023.0651 [doi]
FAU - Daungsupawong, Hinpetch
AU  - Daungsupawong H
AUID- ORCID: 0009-0002-5881-2709
AD  - Private Academic Consultant, Phonhong, Lao People's Democratic Republic.
FAU - Wiwanitkit, Viroj
AU  - Wiwanitkit V
AUID- ORCID: 0000-0003-1039-3728
AD  - Department of Research Analytics, Saveetha Dental College and Hospitals, Saveetha 
      Institute of Medical and Technical Sciences, Saveetha University, Chennai, India.
LA  - eng
PT  - Journal Article
PL  - United States
TA  - J Palliat Med
JT  - Journal of palliative medicine
JID - 9808462
SB  - IM
EDAT- 2024/03/01 18:44
MHDA- 2024/03/01 18:45
CRDT- 2024/03/01 15:53
PHST- 2024/03/01 18:45 [medline]
PHST- 2024/03/01 18:44 [pubmed]
PHST- 2024/03/01 15:53 [entrez]
AID - 10.1089/jpm.2023.0651 [doi]
PST - ppublish
SO  - J Palliat Med. 2024 Mar;27(3):291. doi: 10.1089/jpm.2023.0651.

PMID- 37606629
OWN - NLM
STAT- Publisher
LR  - 20230822
IS  - 1549-960X (Electronic)
IS  - 1549-9596 (Linking)
DP  - 2023 Aug 22
TI  - ChatGPT Generated Content and Similarity Index in Chemistry†.
LID - 10.1021/acs.jcim.3c01110 [doi]
AB  - This study aims to verify similarity index of ChatGPT generated content in 
      chemistry. Twenty subsubjects of chemistry based on controlled vocabulary tools, 
      such as Dewey Decimal Classification, Sears List, and LCSH have been considered. 
      The similarity index has been checked using iThenticate, Urkund, and Turnitin. 
      Surprisingly, the matching percentage is relatively low.
FAU - Kirtania, Deep Kumar
AU  - Kirtania DK
AUID- ORCID: 0000-0002-5319-4705
AD  - Bankura Sammilani College, Bankura, West Bengal 722102, India.
LA  - eng
PT  - Journal Article
DEP - 20230822
PL  - United States
TA  - J Chem Inf Model
JT  - Journal of chemical information and modeling
JID - 101230060
SB  - IM
EDAT- 2023/08/22 13:42
MHDA- 2023/08/22 13:42
CRDT- 2023/08/22 10:55
PHST- 2023/08/22 13:42 [medline]
PHST- 2023/08/22 13:42 [pubmed]
PHST- 2023/08/22 10:55 [entrez]
AID - 10.1021/acs.jcim.3c01110 [doi]
PST - aheadofprint
SO  - J Chem Inf Model. 2023 Aug 22. doi: 10.1021/acs.jcim.3c01110.

PMID- 36895547
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230828
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 2
DP  - 2023 Feb
TI  - Use of ChatGPT in Academic Publishing: A Rare Case of Seronegative Systemic Lupus 
      Erythematosus in a Patient With HIV Infection.
PG  - e34616
LID - 10.7759/cureus.34616 [doi]
LID - e34616
AB  - Diagnosing systemic lupus erythematosus (SLE) may be difficult in cases of 
      negative results for antinuclear antibodies (ANAs) and anti-double stranded DNA 
      (dsDNA) antibodies, which is known as seronegative SLE. Additionally, in patients 
      with HIV infection, the diagnosis of SLE is made complicated by the overlap of 
      symptoms and the possibility of false negative results on antibody tests. Herein, 
      we report the case of a 24-year-old female with HIV infection on anti-retroviral 
      therapy who presented with vesicles and plaques over the malar area and ulcers 
      over the roof of the mouth. Antibody tests for ANAs and dsDNA were negative. She 
      was initially treated for herpes simplex with a secondary infection, but the 
      symptoms did not improve. She ultimately died from acute myocardial infarction 
      while awaiting results of direct immunofluorescence, which revealed the 
      deposition of immunoglobulin (Ig) M, IgG, and C3 along the basement membrane, 
      thus enabling a diagnosis of SLE. Therefore, SLE can be difficult to diagnose in 
      patients with HIV, and other diagnostic criteria should be considered when 
      suspecting SLE and treating these patients. Additionally, we also present our 
      experience with ChatGPT (OpenAI LP, OpenAI Inc., San Francisco, CA, USA) in 
      academic publishing and its pros and cons.
CI  - Copyright © 2023, Manohar et al.
FAU - Manohar, Naveen
AU  - Manohar N
AD  - Dermatology, Belagavi Institute of Medical Sciences, Belagavi, IND.
FAU - Prasad, Shruthi S
AU  - Prasad SS
AD  - Dermatology, St. John's Medical College, Bangalore, IND.
LA  - eng
PT  - Case Reports
DEP - 20230204
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
CIN - Foot Ankle Surg. 2023 Jul;29(5):385-386. PMID: 37198001
PMC - PMC9988440
OTO - NOTNLM
OT  - anti-retroviral therapy
OT  - antibodies
OT  - autoimmune
OT  - chatgpt
OT  - hiv
OT  - lupus
OT  - seronegative sle
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/03/11 06:00
MHDA- 2023/03/11 06:01
PMCR- 2023/02/04
CRDT- 2023/03/10 02:29
PHST- 2023/02/04 00:00 [accepted]
PHST- 2023/03/10 02:29 [entrez]
PHST- 2023/03/11 06:00 [pubmed]
PHST- 2023/03/11 06:01 [medline]
PHST- 2023/02/04 00:00 [pmc-release]
AID - 10.7759/cureus.34616 [doi]
PST - epublish
SO  - Cureus. 2023 Feb 4;15(2):e34616. doi: 10.7759/cureus.34616. eCollection 2023 Feb.

PMID- 38270616
OWN - NLM
STAT- Publisher
LR  - 20240229
IS  - 1432-2161 (Electronic)
IS  - 0364-2348 (Linking)
DP  - 2024 Jan 25
TI  - Translating musculoskeletal radiology reports into patient-friendly summaries 
      using ChatGPT-4.
LID - 10.1007/s00256-024-04599-2 [doi]
AB  - OBJECTIVE: To assess the feasibility of using large language models (LLMs), 
      specifically ChatGPT-4, to generate concise and accurate layperson summaries of 
      musculoskeletal radiology reports. METHODS: Sixty radiology reports, comprising 
      20 MR shoulder, 20 MR knee, and 20 MR lumbar spine reports, were obtained via 
      PACS. The reports were deidentified and then submitted to ChatGPT-4, with the 
      prompt "Produce an organized and concise layperson summary of the findings of the 
      following radiology report. Target a reading level of 8-9th grade and word count 
      &lt;300 words." Three (two primary and one later added for validation) independent 
      readers evaluated the summaries for completeness and accuracy compared to the 
      original reports. Summaries were rated on a scale of 1 to 3: 1) summaries that 
      were incorrect or incomplete, potentially providing harmful or confusing 
      information; 2) summaries that were mostly correct and complete, unlikely to 
      cause confusion or harm; and 3) summaries that were entirely correct and 
      complete. RESULTS: All 60 responses met the criteria for word count and 
      readability. Mean ratings for accuracy were 2.58 for reader 1, 2.71 for reader 2, 
      and 2.77 for reader 3. Mean ratings for completeness were 2.87 for reader 1 and 
      2.73 for reader 2 and 2.87 for reader 3. For accuracy, reader 1 identified three 
      summaries as a 1, reader 2 identified one, and reader 3 identified none. For the 
      two primary readers, inter-reader agreement was low for accuracy (kappa 0.33) and 
      completeness (kappa 0.29). There were no statistically significant changes in 
      inter-reader agreement when the third reader's ratings were included in analysis. 
      CONCLUSION: Overall ratings for accuracy and completeness of the AI-generated 
      layperson report summaries were high with only a small minority likely to be 
      confusing or inaccurate. This study illustrates the potential for leveraging 
      generative AI, such as ChatGPT-4, to automate the production of patient-friendly 
      summaries for musculoskeletal MR imaging.
CI  - © 2024. The Author(s), under exclusive licence to International Skeletal Society 
      (ISS).
FAU - Kuckelman, Ian J
AU  - Kuckelman IJ
AUID- ORCID: 0009-0005-9148-3937
AD  - University of Wisconsin School of Medicine &amp; Public Health, 750 Highland Ave, 
      Madison, WI, 53705, USA. kuckelman@wisc.edu.
FAU - Wetley, Karla
AU  - Wetley K
AD  - University of Wisconsin School of Medicine &amp; Public Health, 750 Highland Ave, 
      Madison, WI, 53705, USA.
FAU - Yi, Paul Hyunsoo
AU  - Yi PH
AD  - University of Maryland School of Medicine, 655 W Baltimore St S, Baltimore, MD, 
      21201, USA.
FAU - Ross, Andrew Bailey
AU  - Ross AB
AD  - University of Wisconsin School of Medicine &amp; Public Health, 750 Highland Ave, 
      Madison, WI, 53705, USA.
LA  - eng
PT  - Journal Article
DEP - 20240125
PL  - Germany
TA  - Skeletal Radiol
JT  - Skeletal radiology
JID - 7701953
SB  - IM
CIN - Skeletal Radiol. 2024 Feb 29;:. PMID: 38421402
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Large language models
OT  - Musculoskeletal
OT  - Patient education
OT  - Report
EDAT- 2024/01/25 12:43
MHDA- 2024/01/25 12:43
CRDT- 2024/01/25 11:04
PHST- 2023/10/02 00:00 [received]
PHST- 2024/01/16 00:00 [accepted]
PHST- 2024/01/16 00:00 [revised]
PHST- 2024/01/25 12:43 [pubmed]
PHST- 2024/01/25 12:43 [medline]
PHST- 2024/01/25 11:04 [entrez]
AID - 10.1007/s00256-024-04599-2 [pii]
AID - 10.1007/s00256-024-04599-2 [doi]
PST - aheadofprint
SO  - Skeletal Radiol. 2024 Jan 25. doi: 10.1007/s00256-024-04599-2.

PMID- 38224925
OWN - NLM
STAT- Publisher
LR  - 20240225
IS  - 1558-349X (Electronic)
IS  - 1546-1440 (Linking)
DP  - 2024 Jan 13
TI  - The Application of Large Language Models for Radiologic Decision Making.
LID - S1546-1440(24)00056-5 [pii]
LID - 10.1016/j.jacr.2024.01.007 [doi]
AB  - BACKGROUND AND PURPOSE: Large language models (LLMs) have seen explosive growth, 
      but their potential role in medical applications remains underexplored. Our study 
      investigates the capability of LLMs to predict the most appropriate imaging study 
      for specific clinical presentations in various subspecialty areas in radiology. 
      METHODS AND MATERIALS: Chat Generative Pretrained Transformer (ChatGPT), by 
      OpenAI and Glass AI by Glass Health were tested on 1,075 clinical scenarios from 
      11 ACR expert panels to determine the most appropriate imaging study, benchmarked 
      against the ACR Appropriateness Criteria. Two responses per clinical presentation 
      were generated and averaged for the final clinical presentation score. Clinical 
      presentation scores for each topic area were averaged as its final score. The 
      average of the topic scores within a panel determined the final score of each 
      panel. LLM responses were on a scale of 0 to 3. Partial scores were given for 
      nonspecific answers. Pearson correlation coefficient (R-value) was calculated for 
      each panel to determine a context-specific performance. RESULTS: Glass AI scored 
      significantly higher than ChatGPT (2.32 ± 0.67 versus 2.08 ± 0.74, P&nbsp;= .002). 
      Both LLMs performed the best in the Polytrauma, Breast, and Vascular panels, and 
      performed the worst in the Neurologic, Musculoskeletal, and Cardiac panels. Glass 
      AI outperformed ChatGPT in 10 of 11 panels, except Obstetrics and Gynecology. 
      Maximum agreement was in the Pediatrics, Neurologic, and Thoracic panels, and the 
      most disagreement occurred in the Vascular, Breast, and Urologic panels. 
      CONCLUSION: LLMs can be used to predict imaging studies, with Glass AI's superior 
      performance indicating the benefits of extra medical-text training. This supports 
      the potential of LLMs in radiologic decision making.
CI  - Copyright © 2024 American College of Radiology. Published by Elsevier Inc. All 
      rights reserved.
FAU - Zaki, Hossam A
AU  - Zaki HA
AD  - Department of Diagnostic Imaging, The Warren Alpert Medical School of Brown 
      University/Rhode Island Hospital, Providence, Rhode Island. Electronic address: 
      hossam_zaki@brown.edu.
FAU - Aoun, Andrew
AU  - Aoun A
AD  - Taub Institute for Research on Alzheimer's Disease and the Aging Brain, Columbia, 
      University Medical Center, New York, New York.
FAU - Munshi, Saminah
AU  - Munshi S
AD  - Department of Diagnostic Imaging, The Warren Alpert Medical School of Brown 
      University/Rhode Island Hospital, Providence, Rhode Island.
FAU - Abdel-Megid, Hazem
AU  - Abdel-Megid H
AD  - Department of Diagnostic Imaging, The Warren Alpert Medical School of Brown 
      University/Rhode Island Hospital, Providence, Rhode Island.
FAU - Nazario-Johnson, Lleayem
AU  - Nazario-Johnson L
AD  - Department of Diagnostic Imaging, The Warren Alpert Medical School of Brown 
      University/Rhode Island Hospital, Providence, Rhode Island.
FAU - Ahn, Sun Ho
AU  - Ahn SH
AD  - Professor of Diagnostic Imaging; Interventional Radiology Integrated Residency 
      Program Director, Medical Student Radiology Education Co-Director, Department of 
      Diagnostic Imaging, The Warren Alpert Medical School of Brown University/Rhode 
      Island Hospital, Providence, Rhode Island.
LA  - eng
PT  - Journal Article
DEP - 20240113
PL  - United States
TA  - J Am Coll Radiol
JT  - Journal of the American College of Radiology : JACR
JID - 101190326
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - clinical decision making
EDAT- 2024/01/16 00:42
MHDA- 2024/01/16 00:42
CRDT- 2024/01/15 19:35
PHST- 2023/10/27 00:00 [received]
PHST- 2024/01/05 00:00 [revised]
PHST- 2024/01/05 00:00 [accepted]
PHST- 2024/01/16 00:42 [pubmed]
PHST- 2024/01/16 00:42 [medline]
PHST- 2024/01/15 19:35 [entrez]
AID - S1546-1440(24)00056-5 [pii]
AID - 10.1016/j.jacr.2024.01.007 [doi]
PST - aheadofprint
SO  - J Am Coll Radiol. 2024 Jan 13:S1546-1440(24)00056-5. doi: 
      10.1016/j.jacr.2024.01.007.

PMID- 38188855
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240109
IS  - 2055-2076 (Print)
IS  - 2055-2076 (Electronic)
IS  - 2055-2076 (Linking)
VI  - 10
DP  - 2024 Jan-Dec
TI  - Performance of ChatGPT incorporated chain-of-thought method in bilingual nuclear 
      medicine physician board examinations.
PG  - 20552076231224074
LID - 10.1177/20552076231224074 [doi]
LID - 20552076231224074
AB  - OBJECTIVE: This research explores the performance of ChatGPT, compared to human 
      doctors, in bilingual, Mandarin Chinese and English, medical specialty exam in 
      Nuclear Medicine in Taiwan. METHODS: The study employed generative pre-trained 
      transformer (GPT-4) and integrated chain-of-thoughts (COT) method to enhance 
      performance by triggering and explaining the thinking process to answer the 
      question in a coherent and logical manner. Questions from the Taiwanese Nuclear 
      Medicine Specialty Exam served as the basis for testing. The research analyzed 
      the correctness of AI responses in different sections of the exam and explored 
      the influence of question length and language proportion on accuracy. RESULTS: 
      AI, especially ChatGPT with COT, exhibited exceptional capabilities in 
      theoretical knowledge, clinical medicine, and handling integrated questions, 
      often surpassing, or matching human doctor performance. However, AI struggled 
      with questions related to medical regulations. The analysis of question length 
      showed that questions within the 109-163 words range yielded the highest 
      accuracy. Moreover, an increase in the proportion of English words in questions 
      improved both AI and human accuracy. CONCLUSIONS: This research highlights the 
      potential and challenges of AI in the medical field. ChatGPT demonstrates 
      significant competence in various aspects of medical knowledge. However, areas 
      like medical regulations require improvement. The study also suggests that AI may 
      help in evaluating exam question difficulty and maintaining fairness in 
      examinations. These findings shed light on AI role in the medical field, with 
      potential applications in healthcare education, exam preparation, and 
      multilingual environments. Ongoing AI advancements are expected to further 
      enhance AI utility in the medical domain.
CI  - © The Author(s) 2024.
FAU - Ting, Yu-Ting
AU  - Ting YT
AD  - Department of Nuclear Medicine and PET Center, China Medical University Hospital, 
      China Medical University, Taichung. RINGGOLD: 38020
FAU - Hsieh, Te-Chun
AU  - Hsieh TC
AD  - Department of Nuclear Medicine and PET Center, China Medical University Hospital, 
      China Medical University, Taichung. RINGGOLD: 38020
AD  - Department of Biomedical Imaging and Radiological Science, China Medical 
      University, Taichung.
FAU - Wang, Yuh-Feng
AU  - Wang YF
AD  - Department of Nuclear Medicine, Taipei Veterans General Hospital, Taipei. 
      RINGGOLD: 46615
AD  - Department of Biomedical Imaging and Radiological Sciences, National Yang Ming 
      Chiao Tung University, Taipei.
AD  - Department of Medical Imaging and Radiological Technology, Yuanpei University of 
      Medical Technology, Hsinchu.
FAU - Kuo, Yu-Chieh
AU  - Kuo YC
AD  - Artificial Intelligence Center, China Medical University Hospital, China Medical 
      University, Taichung. RINGGOLD: 38020
FAU - Chen, Yi-Jin
AU  - Chen YJ
AD  - Artificial Intelligence Center, China Medical University Hospital, China Medical 
      University, Taichung. RINGGOLD: 38020
FAU - Chan, Pak-Ki
AU  - Chan PK
AD  - Artificial Intelligence Center, China Medical University Hospital, China Medical 
      University, Taichung. RINGGOLD: 38020
FAU - Kao, Chia-Hung
AU  - Kao CH
AUID- ORCID: 0000-0002-6368-3676
AD  - Department of Nuclear Medicine and PET Center, China Medical University Hospital, 
      China Medical University, Taichung. RINGGOLD: 38020
AD  - Artificial Intelligence Center, China Medical University Hospital, China Medical 
      University, Taichung. RINGGOLD: 38020
AD  - Graduate Institute of Biomedical Sciences, School of Medicine, College of 
      Medicine, China Medical University, Taichung.
AD  - Department of Bioinformatics and Medical Engineering, Asia University, Taichung.
LA  - eng
PT  - Journal Article
DEP - 20240105
PL  - United States
TA  - Digit Health
JT  - Digital health
JID - 101690863
PMC - PMC10771043
OTO - NOTNLM
OT  - ChatGPT
OT  - chain-of-thoughts (COT)
OT  - multilingual environment
OT  - nuclear medicine exam
COIS- The authors declared no potential conflicts of interest with respect to the 
      research, authorship, and/or publication of this article.
EDAT- 2024/01/08 06:42
MHDA- 2024/01/08 06:43
PMCR- 2024/01/05
CRDT- 2024/01/08 05:24
PHST- 2023/08/16 00:00 [received]
PHST- 2023/12/14 00:00 [accepted]
PHST- 2024/01/08 06:43 [medline]
PHST- 2024/01/08 06:42 [pubmed]
PHST- 2024/01/08 05:24 [entrez]
PHST- 2024/01/05 00:00 [pmc-release]
AID - 10.1177_20552076231224074 [pii]
AID - 10.1177/20552076231224074 [doi]
PST - epublish
SO  - Digit Health. 2024 Jan 5;10:20552076231224074. doi: 10.1177/20552076231224074. 
      eCollection 2024 Jan-Dec.

PMID- 38341517
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 2045-2322 (Electronic)
IS  - 2045-2322 (Linking)
VI  - 14
IP  - 1
DP  - 2024 Feb 10
TI  - Evaluating responses by ChatGPT to farmers' questions on irrigated lowland rice 
      cultivation in Nigeria.
PG  - 3407
LID - 10.1038/s41598-024-53916-1 [doi]
LID - 3407
AB  - The limited number of agricultural extension agents (EAs) in sub-Saharan Africa 
      limits farmers' access to extension services. Artificial intelligence (AI) 
      assistants could potentially aid in providing answers to farmers' questions. The 
      objective of this study was to evaluate the ability of an AI chatbot assistant 
      (ChatGPT) to provide quality responses to farmers' questions. We compiled a list 
      of 32 questions related to irrigated rice cultivation from farmers in Kano State, 
      Nigeria. Six EAs from the state were randomly selected to answer these questions. 
      Their answers, along with those of ChatGPT, were assessed by four evaluators in 
      terms of quality and local relevancy. Overall, chatbot responses were rated 
      significantly higher quality than EAs' responses. Chatbot responses received the 
      best score nearly six times as often as the EAs' (40% vs. 7%). The evaluators 
      preferred chatbot responses to EAs in 78% of cases. The topics for which the 
      chatbot responses received poorer scores than those by EAs included planting 
      time, seed rate, and fertilizer application rate and timing. In conclusion, while 
      the chatbot could offer an alternative source for providing agricultural advisory 
      services to farmers, incorporating site-specific input rate-and-timing agronomic 
      practices into AI assistants is critical for their direct use by farmers.
CI  - © 2024. The Author(s).
FAU - Ibrahim, Ali
AU  - Ibrahim A
AD  - Africa Rice Center (AfricaRice), PMB 82, Abuja, 901101, Nigeria.
AD  - Faculté d'Agronomie, Université Abdou Moumouni, B.P. 10960, Niamey, Niger.
FAU - Senthilkumar, Kalimuthu
AU  - Senthilkumar K
AD  - Africa Rice Center (AfricaRice), B.P. 1690, 101, Antananarivo, Madagascar.
FAU - Saito, Kazuki
AU  - Saito K
AD  - Africa Rice Center (AfricaRice), 01 B.P. 2551, Bouaké 01, Côte d'Ivoire. 
      k.saito@irri.org.
AD  - International Rice Research Institute (IRRI), DAPO Box 7777, 1301, Metro Manila, 
      Philippines. k.saito@irri.org.
LA  - eng
GR  - Grant ID INV-005431/GATES/Bill &amp; Melinda Gates Foundation/United States
PT  - Journal Article
DEP - 20240210
PL  - England
TA  - Sci Rep
JT  - Scientific reports
JID - 101563288
SB  - IM
MH  - Humans
MH  - *Oryza
MH  - Nigeria
MH  - Artificial Intelligence
MH  - Farmers
MH  - Software
PMC - PMC10858882
COIS- The authors declare that they have no known competing financial interests or 
      personal relationships that could have appeared to influence the work reported in 
      this paper.
EDAT- 2024/02/11 07:19
MHDA- 2024/02/11 07:20
PMCR- 2024/02/10
CRDT- 2024/02/10 23:21
PHST- 2023/11/01 00:00 [received]
PHST- 2024/02/06 00:00 [accepted]
PHST- 2024/02/11 07:20 [medline]
PHST- 2024/02/11 07:19 [pubmed]
PHST- 2024/02/10 23:21 [entrez]
PHST- 2024/02/10 00:00 [pmc-release]
AID - 10.1038/s41598-024-53916-1 [pii]
AID - 53916 [pii]
AID - 10.1038/s41598-024-53916-1 [doi]
PST - epublish
SO  - Sci Rep. 2024 Feb 10;14(1):3407. doi: 10.1038/s41598-024-53916-1.

PMID- 38289662
OWN - NLM
STAT- MEDLINE
DCOM- 20240225
LR  - 20240225
IS  - 1438-8871 (Electronic)
IS  - 1439-4456 (Print)
IS  - 1438-8871 (Linking)
VI  - 26
DP  - 2024 Jan 30
TI  - Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study.
PG  - e51069
LID - 10.2196/51069 [doi]
LID - e51069
AB  - BACKGROUND: Sentiment analysis is a significant yet difficult task in natural 
      language processing. The linguistic peculiarities of Cantonese, including its 
      high similarity with Standard Chinese, its grammatical and lexical uniqueness, 
      and its colloquialism and multilingualism, make it different from other languages 
      and pose additional challenges to sentiment analysis. Recent advances in models 
      such as ChatGPT offer potential viable solutions. OBJECTIVE: This study 
      investigated the efficacy of GPT-3.5 and GPT-4 in Cantonese sentiment analysis in 
      the context of web-based counseling and compared their performance with other 
      mainstream methods, including lexicon-based methods and machine learning 
      approaches. METHODS: We analyzed transcripts from a web-based, text-based 
      counseling service in Hong Kong, including a total of 131 individual counseling 
      sessions and 6169 messages between counselors and help-seekers. First, a codebook 
      was developed for human annotation. A simple prompt ("Is the sentiment of this 
      Cantonese text positive, neutral, or negative? Respond with the sentiment label 
      only.") was then given to GPT-3.5 and GPT-4 to label each message's sentiment. 
      GPT-3.5 and GPT-4's performance was compared with a lexicon-based method and 3 
      state-of-the-art models, including linear regression, support vector machines, 
      and long short-term memory neural networks. RESULTS: Our findings revealed 
      ChatGPT's remarkable accuracy in sentiment classification, with GPT-3.5 and 
      GPT-4, respectively, achieving 92.1% (5682/6169) and 95.3% (5880/6169) accuracy 
      in identifying positive, neutral, and negative sentiment, thereby outperforming 
      the traditional lexicon-based method, which had an accuracy of 37.2% (2295/6169), 
      and the 3 machine learning models, which had accuracies ranging from 66% 
      (4072/6169) to 70.9% (4374/6169). CONCLUSIONS: Among many text analysis 
      techniques, ChatGPT demonstrates superior accuracy and emerges as a promising 
      tool for Cantonese sentiment analysis. This study also highlights ChatGPT's 
      applicability in real-world scenarios, such as monitoring the quality of 
      text-based counseling services and detecting message-level sentiments in vivo. 
      The insights derived from this study pave the way for further exploration into 
      the capabilities of ChatGPT in the context of underresourced languages and 
      specialized domains like psychotherapy and natural language processing.
CI  - ©Ziru Fu, Yu Cheng Hsu, Christian S Chan, Chaak Ming Lau, Joyce Liu, Paul Siu Fai 
      Yip. Originally published in the Journal of Medical Internet Research 
      (https://www.jmir.org), 30.01.2024.
FAU - Fu, Ziru
AU  - Fu Z
AUID- ORCID: 0009-0006-1814-2113
AD  - The Hong Kong Jockey Club Centre for Suicide Research and Prevention, Faculty of 
      Social Sciences, The University of Hong Kong, Hong Kong SAR, China (Hong Kong).
FAU - Hsu, Yu Cheng
AU  - Hsu YC
AUID- ORCID: 0000-0002-3722-664X
AD  - The Hong Kong Jockey Club Centre for Suicide Research and Prevention, Faculty of 
      Social Sciences, The University of Hong Kong, Hong Kong SAR, China (Hong Kong).
FAU - Chan, Christian S
AU  - Chan CS
AUID- ORCID: 0000-0003-2616-5199
AD  - Department of Psychology, The University of Hong Kong, Hong Kong SAR, China (Hong 
      Kong).
AD  - Department of Psychology and Linguistics, International Christian University, 
      Tokyo, Japan.
FAU - Lau, Chaak Ming
AU  - Lau CM
AUID- ORCID: 0000-0003-2112-4187
AD  - Department of Linguistics and Modern Language Studies, The Education University 
      of Hong Kong, Hong Kong SAR, China (Hong Kong).
FAU - Liu, Joyce
AU  - Liu J
AUID- ORCID: 0009-0008-0018-6219
AD  - The Hong Kong Jockey Club Centre for Suicide Research and Prevention, Faculty of 
      Social Sciences, The University of Hong Kong, Hong Kong SAR, China (Hong Kong).
FAU - Yip, Paul Siu Fai
AU  - Yip PSF
AUID- ORCID: 0000-0003-1596-4120
AD  - The Hong Kong Jockey Club Centre for Suicide Research and Prevention, Faculty of 
      Social Sciences, The University of Hong Kong, Hong Kong SAR, China (Hong Kong).
AD  - Department of Social Work and Social Administration, Faculty of Social Sciences, 
      The University of Hong Kong, Hong Kong SAR, China (Hong Kong).
LA  - eng
PT  - Journal Article
DEP - 20240130
PL  - Canada
TA  - J Med Internet Res
JT  - Journal of medical Internet research
JID - 100959882
SB  - IM
MH  - Humans
MH  - *Asian People
MH  - Counselors
MH  - Hong Kong
MH  - *Language
MH  - Linear Models
MH  - *Artificial Intelligence
MH  - *Communication
PMC - PMC10865189
OTO - NOTNLM
OT  - Cantonese
OT  - ChatGPT
OT  - NLP
OT  - counseling
OT  - natural language processing
OT  - sentiment analysis
COIS- Conflicts of Interest: None declared.
EDAT- 2024/01/30 12:43
MHDA- 2024/01/31 06:42
PMCR- 2024/01/30
CRDT- 2024/01/30 11:54
PHST- 2023/07/19 00:00 [received]
PHST- 2023/12/11 00:00 [accepted]
PHST- 2023/10/24 00:00 [revised]
PHST- 2024/01/31 06:42 [medline]
PHST- 2024/01/30 12:43 [pubmed]
PHST- 2024/01/30 11:54 [entrez]
PHST- 2024/01/30 00:00 [pmc-release]
AID - v26i1e51069 [pii]
AID - 10.2196/51069 [doi]
PST - epublish
SO  - J Med Internet Res. 2024 Jan 30;26:e51069. doi: 10.2196/51069.

PMID- 38239542
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240121
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 12
DP  - 2023 Dec
TI  - Public Awareness of Obesity as a Risk Factor for Cancer in Central Saudi Arabia: 
      Feasibility of ChatGPT as an Educational Intervention.
PG  - e50781
LID - 10.7759/cureus.50781 [doi]
LID - e50781
AB  - BACKGROUND: While the link between obesity and chronic diseases such as diabetes 
      and cardiovascular disorders is well-documented, there is a growing body of 
      evidence connecting obesity with an increased risk of cancer. However, public 
      awareness of this connection remains limited. STUDY PURPOSE: To analyze public 
      awareness of overweight/obesity as a risk factor for cancer and analyze public 
      perceptions on the feasibility of ChatGPT, an artificial intelligence-based 
      conversational agent, as an educational intervention tool. METHODS: A 
      mixed-methods approach including deductive quantitative cross-sectional approach 
      to draw precise conclusions based on empirical evidence on public awareness of 
      the link between obesity and cancer; and inductive qualitative approach to 
      interpret public perceptions on using ChatGPT for creating awareness of obesity, 
      cancer and its risk factors was used in this study. Participants included adult 
      residents in Saudi Arabia. A total of 486 individuals and 21 individuals were 
      included in the survey and semi-structured interviews respectively. RESULTS: 
      About 65% of the participants are not completely aware of cancer and its risk 
      factors. Significant differences in awareness were observed concerning age groups 
      (p &lt; .0001), socio-economic status (p = .041), and regional distribution (p = 
      .0351). A total of 10 themes were analyzed from the interview data, which 
      included four positive factors (accessibility, personalization, 
      cost-effectiveness, anonymity and privacy, multi-language support) and five 
      negative factors (information inaccuracy, lack of emotional intelligence, 
      dependency and overreliance, data privacy and security, and inability to provide 
      physical support or diagnosis). CONCLUSION: This study has underscored the 
      potential of leveraging ChatGPT as a valuable public awareness tool for cancer in 
      Saudi Arabia.
CI  - Copyright © 2023, Alanzi et al.
FAU - Alanzi, Turki M
AU  - Alanzi TM
AD  - Department of Health Information Management and Technology, College of Public 
      Health, Imam Abdulrahman Bin Faisal University, Dammam, SAU.
FAU - Alzahrani, Wala
AU  - Alzahrani W
AD  - Department of Clinical Nutrition, College of Applied Medical Sciences, King 
      Abdulaziz University, Jeddah, SAU.
FAU - Albalawi, Nouf S
AU  - Albalawi NS
AD  - College of Medicine, Tabuk University, Tabuk, SAU.
FAU - Allahyani, Taif
AU  - Allahyani T
AD  - College of Applied Medical Sciences, Umm Al-Qura University, Makkah, SAU.
FAU - Alghamdi, Atheer
AU  - Alghamdi A
AD  - ‏Collage of Pharmacy, Taif University, Taif, SAU.
FAU - Al-Zahrani, Haneen
AU  - Al-Zahrani H
AD  - ‏Department of Hematology, ‏Armed Forces Hospital at King Abdulaziz Airbase 
      Dhahran, Dhahran, SAU.
FAU - Almutairi, Awatif
AU  - Almutairi A
AD  - Department of Clinical Laboratories Sciences, College of Applied Medical 
      Sciences, Jouf University, Jouf, SAU.
FAU - Alzahrani, Hayat
AU  - Alzahrani H
AD  - College of Pharmacy, Taif University, Taif, SAU.
FAU - Almulhem, Latifah
AU  - Almulhem L
AD  - College of Medicine, King Faisal University, Al Hofuf, SAU.
FAU - Alanzi, Nouf
AU  - Alanzi N
AD  - Department of Clinical Laboratories Sciences, College of Applied Medical 
      Sciences, Jouf University, Jouf, SAU.
FAU - Al Moarfeg, Abdulrhman
AU  - Al Moarfeg A
AD  - College of Science and Arts, King Khalid University, Muhayil Asir, SAU.
FAU - Farhah, Nesren
AU  - Farhah N
AD  - Department of Health Informatics, College of Health Sciences, Saudi Electronic 
      University, Riyadh, SAU.
LA  - eng
PT  - Journal Article
DEP - 20231219
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10795720
OTO - NOTNLM
OT  - artificial intelligence
OT  - awareness
OT  - cancer
OT  - chatgpt
OT  - knowledge
OT  - obesity
OT  - overweight
OT  - public
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/01/19 06:43
MHDA- 2024/01/19 06:44
PMCR- 2023/12/19
CRDT- 2024/01/19 03:41
PHST- 2023/12/17 00:00 [accepted]
PHST- 2024/01/19 06:44 [medline]
PHST- 2024/01/19 06:43 [pubmed]
PHST- 2024/01/19 03:41 [entrez]
PHST- 2023/12/19 00:00 [pmc-release]
AID - 10.7759/cureus.50781 [doi]
PST - epublish
SO  - Cureus. 2023 Dec 19;15(12):e50781. doi: 10.7759/cureus.50781. eCollection 2023 
      Dec.

PMID- 38305909
OWN - NLM
STAT- Publisher
LR  - 20240202
IS  - 1613-7671 (Electronic)
IS  - 0043-5325 (Linking)
DP  - 2024 Feb 2
TI  - Delayed diagnosis of a&nbsp;transient ischemic attack caused by ChatGPT.
LID - 10.1007/s00508-024-02329-1 [doi]
AB  - Techniques of artificial intelligence (AI) are increasingly used in the treatment 
      of patients, such as providing a&nbsp;diagnosis in radiological imaging, improving 
      workflow by triaging patients or providing an expert opinion based on clinical 
      symptoms; however, such AI techniques also hold intrinsic risks as AI algorithms 
      may point in the wrong direction and constitute a&nbsp;black box without explaining 
      the reason for the decision-making process.This article outlines a&nbsp;case where an 
      erroneous ChatGPT diagnosis, relied upon by the patient to evaluate symptoms, led 
      to a&nbsp;significant treatment delay and a&nbsp;potentially life-threatening situation. 
      With this case, we would like to point out the typical risks posed by the 
      widespread application of AI tools not intended for medical decision-making.
CI  - © 2024. The Author(s).
FAU - Saenger, Jonathan A
AU  - Saenger JA
AUID- ORCID: 0000-0002-8667-3884
AD  - Diagnostic and interventional Radiology, University Hospital Zurich, University 
      Zurich, Zurich, Switzerland. jonathan.saenger@usz.ch.
AD  - Institute of Radiology and Nuclear Medicine, GZO Hospital Wetzikon, Wetzikon, 
      Switzerland. jonathan.saenger@usz.ch.
FAU - Hunger, Jonathan
AU  - Hunger J
AD  - Department of Internal Medicine, GZO Hospital Wetzikon, Wetzikon, Switzerland.
FAU - Boss, Andreas
AU  - Boss A
AD  - Diagnostic and interventional Radiology, University Hospital Zurich, University 
      Zurich, Zurich, Switzerland. andreas.boss@gzo.ch.
AD  - Institute of Radiology and Nuclear Medicine, GZO Hospital Wetzikon, Wetzikon, 
      Switzerland. andreas.boss@gzo.ch.
FAU - Richter, Johannes
AU  - Richter J
AD  - Institute of Radiology and Nuclear Medicine, GZO Hospital Wetzikon, Wetzikon, 
      Switzerland.
AD  - Neurology and Stroke Unit, GZO Hospital Wetzikon, Wetzikon, Switzerland.
LA  - eng
PT  - Journal Article
DEP - 20240202
PL  - Austria
TA  - Wien Klin Wochenschr
JT  - Wiener klinische Wochenschrift
JID - 21620870R
SB  - IM
OTO - NOTNLM
OT  - AI
OT  - Artificial Intelligence
OT  - ChatGPT
OT  - Chatbot
OT  - Neurology
OT  - Stroke
EDAT- 2024/02/02 12:43
MHDA- 2024/02/02 12:43
CRDT- 2024/02/02 11:16
PHST- 2024/01/11 00:00 [received]
PHST- 2024/01/14 00:00 [accepted]
PHST- 2024/02/02 12:43 [medline]
PHST- 2024/02/02 12:43 [pubmed]
PHST- 2024/02/02 11:16 [entrez]
AID - 10.1007/s00508-024-02329-1 [pii]
AID - 10.1007/s00508-024-02329-1 [doi]
PST - aheadofprint
SO  - Wien Klin Wochenschr. 2024 Feb 2. doi: 10.1007/s00508-024-02329-1.

PMID- 38161881
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240103
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 11
DP  - 2023 Nov
TI  - Utilizing ChatGPT to Streamline the Generation of Prior Authorization Letters and 
      Enhance Clerical Workflow in Orthopedic Surgery Practice: A Case Report.
PG  - e49680
LID - 10.7759/cureus.49680 [doi]
LID - e49680
AB  - Prior authorization is a cumbersome process that requires clinicians to create an 
      individualized letter that includes detailed information about the patient's 
      medical condition, proposed treatment plan, and any supplemental information 
      required to obtain approval from a patient's insurance company before any 
      services or procedures may be provided to the patient. However, drafting 
      authorization letters is time-consuming clerical work that not only places an 
      increased administrative burden on orthopedic surgeons and office staff but also 
      concurrently takes time away from patient care. Therefore, there is a need to 
      improve this process by streamlining workflows for healthcare providers in order 
      to prioritize direct patient care. In this report, we present a case utilizing 
      OpenAI's ChatGPT (OpenAI, L.L.C., San Francisco, CA, USA) to draft a prior 
      authorization request letter for the use of matrix-induced autologous chondrocyte 
      implantation to treat a cartilage injury of the knee.
CI  - Copyright © 2023, Diane et al.
FAU - Diane, Alioune
AU  - Diane A
AD  - Department of Orthopaedic Surgery, Rutgers Robert Wood Johnson Medical School, 
      New Brunswick, USA.
FAU - Gencarelli, Pasquale Jr
AU  - Gencarelli P Jr
AD  - Department of Orthopaedic Surgery, Rutgers Robert Wood Johnson Medical School, 
      New Brunswick, USA.
FAU - Lee, James M Jr
AU  - Lee JM Jr
AD  - Department of Orthopaedic Surgery, Orange Orthopaedic Associates, West Orange, 
      USA.
FAU - Mittal, Rahul
AU  - Mittal R
AD  - Department of Health Informatics, Rutgers School of Health Professions, Newark, 
      USA.
LA  - eng
PT  - Case Reports
DEP - 20231129
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10756745
OTO - NOTNLM
OT  - artificial intelligence in healthcare
OT  - authorization letter
OT  - chatgpt
OT  - clerical workflow
OT  - orthopedic surgery
COIS- The authors have declared that no competing interests exist.
EDAT- 2024/01/02 11:46
MHDA- 2024/01/02 11:47
PMCR- 2023/11/29
CRDT- 2024/01/01 04:11
PHST- 2023/11/29 00:00 [accepted]
PHST- 2024/01/02 11:47 [medline]
PHST- 2024/01/02 11:46 [pubmed]
PHST- 2024/01/01 04:11 [entrez]
PHST- 2023/11/29 00:00 [pmc-release]
AID - 10.7759/cureus.49680 [doi]
PST - epublish
SO  - Cureus. 2023 Nov 29;15(11):e49680. doi: 10.7759/cureus.49680. eCollection 2023 
      Nov.

PMID- 37638368
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230829
IS  - 2590-1133 (Electronic)
IS  - 2590-1133 (Linking)
VI  - 5
DP  - 2023 Dec
TI  - Causal reasoning about epidemiological associations in conversational AI.
PG  - 100102
LID - 10.1016/j.gloepi.2023.100102 [doi]
LID - 100102
AB  - We present a Socratic dialogue with ChatGPT, a large language model (LLM), on the 
      causal interpretation of epidemiological associations between fine particulate 
      matter (PM2.5) and human mortality risks. ChatGPT, reflecting probable patterns 
      of human reasoning and argumentation in the sources on which it has been trained, 
      initially holds that "It is well-established that exposure to ambient levels of 
      PM2.5 does increase mortality risk" and adds the unsolicited remark that 
      "Reducing exposure to PM2.5 is an important public health priority." After 
      patient questioning, however, it concludes that "It is not known with certainty 
      that current ambient levels of PM2.5 increase mortality risk. While there is 
      strong evidence of an association between PM2.5 and mortality risk, the causal 
      nature of this association remains uncertain due to the possibility of omitted 
      confounders." This revised evaluation of the evidence suggests the potential 
      value of sustained questioning in refining and improving both the types of human 
      reasoning and argumentation imitated by current LLMs and the reliability of the 
      initial conclusions expressed by current LLMs.
CI  - © 2023 The Author.
FAU - Cox, Louis Anthony Jr
AU  - Cox LA Jr
AD  - Cox Associates, MoirAI, Entanglement, and University of Colorado, 503 N. Franklin 
      Street, Denver, CO 80218, USA.
LA  - eng
PT  - Journal Article
DEP - 20230308
PL  - United States
TA  - Glob Epidemiol
JT  - Global epidemiology
JID - 101759263
PMC - PMC10445972
OTO - NOTNLM
OT  - Causal artificial intelligence
OT  - Causal reasoning
OT  - ChatGPT
OT  - Large language models
OT  - PM2.5
COIS- The authors declare that they have no known competing financial interestsor 
      personal relationships that could have appeared to influence the work reported in 
      this paper.
EDAT- 2023/08/28 06:43
MHDA- 2023/08/28 06:44
PMCR- 2023/03/08
CRDT- 2023/08/28 05:03
PHST- 2023/02/28 00:00 [received]
PHST- 2023/03/03 00:00 [revised]
PHST- 2023/03/03 00:00 [accepted]
PHST- 2023/08/28 06:44 [medline]
PHST- 2023/08/28 06:43 [pubmed]
PHST- 2023/08/28 05:03 [entrez]
PHST- 2023/03/08 00:00 [pmc-release]
AID - S2590-1133(23)00005-6 [pii]
AID - 100102 [pii]
AID - 10.1016/j.gloepi.2023.100102 [doi]
PST - epublish
SO  - Glob Epidemiol. 2023 Mar 8;5:100102. doi: 10.1016/j.gloepi.2023.100102. 
      eCollection 2023 Dec.

PMID- 37286898
OWN - NLM
STAT- MEDLINE
DCOM- 20231227
LR  - 20231227
IS  - 1741-3850 (Electronic)
IS  - 1741-3842 (Linking)
VI  - 45
IP  - 4
DP  - 2023 Nov 29
TI  - The current existence of ChatGPT in education: a double-edged sword?
PG  - e799-e800
LID - 10.1093/pubmed/fdad082 [doi]
AB  - The authors in this article would like to invite related parties to carry out 
      updates in the world of education, that the existence of ChatGPT can provide new 
      enthusiasm for all of us to improve in a better direction.
CI  - © The Author(s) 2023. Published by Oxford University Press on behalf of Faculty 
      of Public Health. All rights reserved. For permissions, please e-mail: 
      journals.permissions@oup.com.
FAU - Situmorang, Dominikus David Biondi
AU  - Situmorang DDB
AUID- ORCID: 0000-0002-6065-2022
AD  - Faculty of Education and Language, Department of Guidance and Counseling, Atma 
      Jaya Catholic University of Indonesia, DKI Jakarta, Indonesia.
FAU - Salim, Rose Mini Agoes
AU  - Salim RMA
AD  - Faculty of Psychology, Department of Educational Psychology, University of 
      Indonesia, Depok, Indonesia.
FAU - Ifdil, Ifdil
AU  - Ifdil I
AD  - Faculty of Education, Department of Guidance and Counseling, Universitas Negeri 
      Padang, Padang, Indonesia.
FAU - Liza, Ledya Oktavia
AU  - Liza LO
AUID- ORCID: 0000-0001-9820-1481
AD  - Department of Special Education, Lancang Kuning University, Riau, Indonesia.
FAU - Rusandi, M Arli
AU  - Rusandi MA
AUID- ORCID: 0000-0001-7385-104X
AD  - Department of Guidance and Counseling, Riau University, Riau, Indonesia.
FAU - Hayati, Isnaria Rizki
AU  - Hayati IR
AD  - Department of Guidance and Counseling, Universitas Islam Negeri Sultan Syarif 
      Kasim Riau, Riau, Indonesia.
FAU - Amalia, Rizki
AU  - Amalia R
AD  - Department of Early Childhood Education, Universitas Pahlawan Tuanku Tambusai, 
      Riau, Indonesia.
FAU - Muhandaz, Ramon
AU  - Muhandaz R
AD  - Department of Mathematics Education, Universitas Islam Negeri Sultan Syarif Kasim 
      Riau, Riau, Indonesia.
FAU - Fitriani, Arbania
AU  - Fitriani A
AD  - Faculty of Psychology, Universitas Esa Unggul, DKI Jakarta, Indonesia.
LA  - eng
PT  - Journal Article
PL  - England
TA  - J Public Health (Oxf)
JT  - Journal of public health (Oxford, England)
JID - 101188638
SB  - IM
MH  - Humans
MH  - *Education
MH  - *Public Health
MH  - *Artificial Intelligence
OTO - NOTNLM
OT  - GPT Zero
OT  - advantages
OT  - chatGPT
OT  - disadvantages
OT  - double-edged sword
OT  - education
EDAT- 2023/06/08 01:08
MHDA- 2023/12/04 12:43
CRDT- 2023/06/07 23:35
PHST- 2023/04/25 00:00 [received]
PHST- 2023/12/04 12:43 [medline]
PHST- 2023/06/08 01:08 [pubmed]
PHST- 2023/06/07 23:35 [entrez]
AID - 7191715 [pii]
AID - 10.1093/pubmed/fdad082 [doi]
PST - ppublish
SO  - J Public Health (Oxf). 2023 Nov 29;45(4):e799-e800. doi: 10.1093/pubmed/fdad082.

PMID- 37270457
OWN - NLM
STAT- Publisher
LR  - 20231024
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Print)
IS  - 0090-6964 (Linking)
VI  - 51
IP  - 11
DP  - 2023 Nov
TI  - Response of ChatGPT for Humanoid Robots Role in Improving Healthcare and Patient 
      Outcomes.
PG  - 2359-2361
LID - 10.1007/s10439-023-03267-1 [doi]
AB  - Humanoid robotics is characterized by constant developments, which are supported 
      by several research facilities across the world. Humanoid robots are used in many 
      different industries. In this setting, this letter, written by people, makes use 
      of ChatGPT answers to examine how humanoid robots might be used in the medical 
      industry, particularly in light of the COVID-19 pandemic and in future. Although 
      humanoid robots can help with certain jobs, it is important to recognize the 
      indispensable importance of human healthcare professionals who have knowledge, 
      empathy, and the capacity for critical judgment. Although humanoid robots can 
      complement healthcare initiatives, they shouldn't be viewed as a full-fledged 
      replacement for human care.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Janamla, Varaprasad
AU  - Janamla V
AUID- ORCID: 0000-0001-9716-5458
AD  - Department of Electrical and Electronics Engineering, School of Engineering and 
      Technology, CHRIST (Deemed to Be University), Bangalore, Karnataka, 560074, 
      India. varaprasad.janamala@christuniversity.in.
FAU - Daram, Suresh Babu
AU  - Daram SB
AD  - Department of Electrical and Electronics Engineering, Mohan Babu University (Erst 
      while Sree Vidyanikethan Engineering College), Tirupati, Andhra Pradesh, 517102, 
      India.
FAU - Rajesh, Patil
AU  - Rajesh P
AD  - Department of Electrical and Electronics Engineering, SVERIS College of 
      Engineering, Pamdharpur, Solarpur, Maharashtra, 413304, India.
FAU - Kumari, C H Nagaraja
AU  - Kumari CHN
AD  - Department of Electrical and Electronics Engineering, Guru Nanak Institute of 
      Technology, Ibrahimpatnam, Telangana, 501506, India.
LA  - eng
PT  - Letter
DEP - 20230603
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
PMC - PMC10239274
OTO - NOTNLM
OT  - ChatGPT
OT  - Humanoid robotics
OT  - Medical field
OT  - Patient outcomes
COIS- The authors declare that they have no conflict of interest.
EDAT- 2023/06/04 01:08
MHDA- 2023/06/04 01:08
PMCR- 2023/06/03
CRDT- 2023/06/03 23:04
PHST- 2023/05/28 00:00 [received]
PHST- 2023/05/31 00:00 [accepted]
PHST- 2023/06/04 01:08 [pubmed]
PHST- 2023/06/04 01:08 [medline]
PHST- 2023/06/03 23:04 [entrez]
PHST- 2023/06/03 00:00 [pmc-release]
AID - 10.1007/s10439-023-03267-1 [pii]
AID - 3267 [pii]
AID - 10.1007/s10439-023-03267-1 [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2023 Nov;51(11):2359-2361. doi: 10.1007/s10439-023-03267-1. Epub 
      2023 Jun 3.

PMID- 37889368
OWN - NLM
STAT- MEDLINE
DCOM- 20231130
LR  - 20231216
IS  - 1708-0428 (Electronic)
IS  - 0960-8923 (Linking)
VI  - 33
IP  - 12
DP  - 2023 Dec
TI  - Bariatric Evaluation Through AI: a Survey of Expert Opinions Versus ChatGPT-4 
      (BETA-SEOV).
PG  - 3971-3980
LID - 10.1007/s11695-023-06903-w [doi]
AB  - BACKGROUND: Recent advancements in artificial intelligence, such as OpenAI's 
      ChatGPT-4, are revolutionizing various sectors, including healthcare. This study 
      investigates the use of ChatGPT-4 in identifying suitable candidates for 
      bariatric surgery and providing surgical recommendations to improve 
      decision-making in obesity treatment amid the global obesity epidemic. METHODS: 
      We devised ten patient scenarios, thoughtfully encompassing a spectrum that spans 
      from uncomplicated cases to more complex ones. Our objective was to delve into 
      the decision-making process regarding the recommendation of bariatric surgery. 
      From July 29th to August 10th, 2023, we conducted a voluntary online survey 
      involving thirty prominent bariatric surgeons, ensuring that there was no 
      predetermined bias in the selection of a specific type of bariatric surgery. This 
      survey was designed to collect their insights on these scenarios and gain a 
      deeper understanding of their professional experience and background in the field 
      of bariatric surgery. Additionally, we consulted ChatGPT-4 in two separate 
      conversations to evaluate its alignment with expert opinions on bariatric surgery 
      options. RESULTS: In 40% of the scenarios, disparities were identified between 
      the two conversations with ChatGPT-4. It matched expert opinions in 30% of cases. 
      Differences were noted in cases like gastrointestinal metaplasia and gastric 
      adenocarcinoma, but there was alignment with conditions like endometriosis and 
      GERD. CONCLUSION: The evaluation of ChatGPT-4's role in determining bariatric 
      surgery suitability uncovered both potential and shortcomings. Its alignment with 
      experts was inconsistent, and it often overlooked key factors, emphasizing human 
      expertise's value. Its current use requires caution, and further refinement is 
      needed for clinical application.
CI  - © 2023. The Author(s), under exclusive licence to Springer Science+Business 
      Media, LLC, part of Springer Nature.
FAU - Jazi, Amir Hossein Davarpanah
AU  - Jazi AHD
AD  - Department of Surgery, Minimally Invasive Surgery Research Center, Division of 
      Minimally Invasive and Bariatric Surgery, School of Medicine, Rasool-E Akram 
      Hospital, Iran University of Medical Sciences, Niyaesh Avenue, Sattar Khan 
      Street, Tehran, Iran.
FAU - Mahjoubi, Mohammad
AU  - Mahjoubi M
AD  - Clinical Research Development Center, Najafabad Branch, Islamic Azad University, 
      Najafabad, Iran.
FAU - Shahabi, Shahab
AU  - Shahabi S
AUID- ORCID: 0000-0002-8415-2567
AD  - Department of Surgery, Minimally Invasive Surgery Research Center, Division of 
      Minimally Invasive and Bariatric Surgery, School of Medicine, Rasool-E Akram 
      Hospital, Iran University of Medical Sciences, Niyaesh Avenue, Sattar Khan 
      Street, Tehran, Iran. shshahabi@yahoo.com.
FAU - Alqahtani, Aayed R
AU  - Alqahtani AR
AD  - New Medical Center, Riyadh, Saudi Arabia.
FAU - Haddad, Ashraf
AU  - Haddad A
AD  - Gastrointestinal Metabolic and Bariatric Center, GBMC M, Jordan Hospital, Amman, 
      Jordan.
FAU - Pazouki, Abdolreza
AU  - Pazouki A
AD  - Department of Surgery, Minimally Invasive Surgery Research Center, Division of 
      Minimally Invasive and Bariatric Surgery, School of Medicine, Rasool-E Akram 
      Hospital, Iran University of Medical Sciences, Niyaesh Avenue, Sattar Khan 
      Street, Tehran, Iran.
FAU - Prasad, Arun
AU  - Prasad A
AD  - GI, Bariatric and Robotic Surgery Apollo Hospital, New Delhi, India.
FAU - Safadi, Bassem Y
AU  - Safadi BY
AD  - Surgical Services Aman Hospital, Doha, Qatar.
FAU - Chiappetta, Sonja
AU  - Chiappetta S
AD  - Bariatric and Metabolic Surgery Unit, Ospedale Evangelico Betania, Naples, Italy.
FAU - Taskin, Halit Eren
AU  - Taskin HE
AD  - Department of General Surgery, Cerrahpaşa Medical Faculty, Istanbul University 
      Cerrahpaşa, Istanbul, Turkey.
FAU - Billy, Helmuth Thorlakur
AU  - Billy HT
AD  - Ventura Advanced Surgical Associates, Ventura, CA, USA.
FAU - Kasama, Kazunori
AU  - Kasama K
AD  - Weight Loss and Metabolic Surgery Center, Yotsuya Medical Cube, Tokyo, Japan.
FAU - Mahawar, Kamal
AU  - Mahawar K
AD  - Sunderland Royal Hospital, Sunderland, UK.
FAU - Gawdat, Khaled
AU  - Gawdat K
AD  - Ain Shams University Faculty of Medicine Department of General Surgery, Cairo, 
      Egypt.
FAU - Rheinwalt, Karl Peter
AU  - Rheinwalt KP
AD  - Department of Bariatric, Metabolic and Plastic Surgery, St. Franziskus Hospital, 
      Cologne, Germany.
FAU - Miller, Karl A
AU  - Miller KA
AD  - Dubai London Hospital, Dubai, UAE.
FAU - Kow, Lilian
AU  - Kow L
AD  - Department GI Surgery Flinders, University South Australia, Adelaide, Australia.
FAU - Neto, Manoel Galvao
AU  - Neto MG
AD  - Mohak Bariatric and Robotic Center, Indore, India.
FAU - Yang, Wah
AU  - Yang W
AD  - The First Affiliated Hospital of Jinan University, Guangzhou, China.
FAU - Palermo, Mariano
AU  - Palermo M
AD  - Gastrointestinal and Bariatric Surgery, University of Buenos Aires, Buenos Aires, 
      Argentina.
FAU - Ghanem, Omar M
AU  - Ghanem OM
AD  - Mayo Clinic, Rochester, MN, USA.
FAU - Lainas, Panagiotis
AU  - Lainas P
AD  - Department of Digestive and Bariatric Surgery, Metropolitan Hospital, HEAL 
      Academy, Athens, Greece.
FAU - Peterli, Ralph
AU  - Peterli R
AD  - Deputy Head of Visceral Surgery and Head of Bariatric-Metabolic Surgery Clarunis, 
      Department of Visceral Surgery, University Centre for Gastrointestinal and Liver 
      Diseases St. Clara Hospital and University Hospital Basel, 4002, Basel, 
      Switzerland.
FAU - Kassir, Radwan
AU  - Kassir R
AD  - Digestive Surgery Unit, University Hospital of La Réunion -Félix Guyon Hospital, 
      Saint-Denis, La Réunion, France.
FAU - Puy, Ramon Vilallonga
AU  - Puy RV
AD  - Head Endocrine-Metabolic and Bariatric Surgery Unit, Vall Hebron Barcelona 
      Hospital Campus, Pg. De La Vall d'hebron, 119-129, 08035, Barcelona, Spain.
FAU - Da Silva Ribeiro, Rui José
AU  - Da Silva Ribeiro RJ
AD  - General Surgery Department, Multidisciplinary Center for Obesity Treatment - 
      Hospital Lusíadas Amadora, Amadora, Portugal.
FAU - Verboonen, Sergio
AU  - Verboonen S
AD  - Obesity Goodbye Center, Tijuana, Mexico.
FAU - Pintar, Tadeja
AU  - Pintar T
AD  - UMC Ljubljana, Department of Abdominal Surgery and Medical Faculty, Ljubljana, 
      Slovenia.
FAU - Shabbir, Asim
AU  - Shabbir A
AD  - National University of Singapore, Singapore, Singapore.
FAU - Musella, Mario
AU  - Musella M
AD  - Advanced Biomedical Sciences Department, "Federico II" University, Naples, Italy.
FAU - Kermansaravi, Mohammad
AU  - Kermansaravi M
AD  - Department of Surgery, Minimally Invasive Surgery Research Center, Division of 
      Minimally Invasive and Bariatric Surgery, School of Medicine, Rasool-E Akram 
      Hospital, Iran University of Medical Sciences, Niyaesh Avenue, Sattar Khan 
      Street, Tehran, Iran.
LA  - eng
PT  - Journal Article
DEP - 20231027
PL  - United States
TA  - Obes Surg
JT  - Obesity surgery
JID - 9106714
SB  - IM
MH  - Female
MH  - Humans
MH  - Expert Testimony
MH  - Artificial Intelligence
MH  - *Obesity, Morbid/surgery
MH  - *Bariatrics
MH  - Obesity
OTO - NOTNLM
OT  - Artificial Intelligence
OT  - Bariatric Evaluation
OT  - Bariatric Surgery
OT  - ChatGPT
OT  - Health Literacy
OT  - Language Learning Models
EDAT- 2023/10/27 12:42
MHDA- 2023/11/30 06:43
CRDT- 2023/10/27 11:10
PHST- 2023/08/22 00:00 [received]
PHST- 2023/10/11 00:00 [accepted]
PHST- 2023/10/08 00:00 [revised]
PHST- 2023/11/30 06:43 [medline]
PHST- 2023/10/27 12:42 [pubmed]
PHST- 2023/10/27 11:10 [entrez]
AID - 10.1007/s11695-023-06903-w [pii]
AID - 10.1007/s11695-023-06903-w [doi]
PST - ppublish
SO  - Obes Surg. 2023 Dec;33(12):3971-3980. doi: 10.1007/s11695-023-06903-w. Epub 2023 
      Oct 27.

PMID- 37428357
OWN - NLM
STAT- Publisher
LR  - 20230710
IS  - 2196-8837 (Electronic)
IS  - 2196-8837 (Linking)
DP  - 2023 Jul 10
TI  - Psychometric Properties and Assessment of Knowledge, Attitude, and Practice 
      Towards ChatGPT in Pharmacy Practice and Education: a Study Protocol.
LID - 10.1007/s40615-023-01696-1 [doi]
AB  - ChatGPT represents an advanced conversational artificial intelligence (AI), 
      providing a powerful tool for generating human-like responses that could change 
      pharmacy prospects. This protocol aims to describe the development, validation, 
      and utilization of a tool to assess the knowledge, attitude, and practice towards 
      ChatGPT (KAP-C) in pharmacy practice and education. The development and 
      validation process of the KAP-C tool will include a comprehensive literature 
      search to identify relevant constructs, content validation by a panel of experts 
      for items relevancy using content validity index (CVI) and face validation by 
      sample participants for items clarity using face validity index (FVI), 
      readability and difficulty index using the&nbsp;Flesch-Kincaid Readability Test, 
      Gunning Fog Index, or Simple Measure of Gobbledygook (SMOG), assessment of 
      reliability using internal consistency (Cronbach's alpha), and exploratory factor 
      analysis (EFA) to determine the underlying factor structures (eigenvalues, scree 
      plot analysis, factor loadings, and varimax). The second phase will utilize&nbsp;the 
      validated KAP-C tool to conduct KAP&nbsp;surveys&nbsp;among pharmacists and pharmacy 
      students in selected low- and middle-income countries (LMICs) (Nigeria, Pakistan, 
      and Yemen). The final data will be analyzed descriptively using frequencies, 
      percentages, mean (standard deviation) or median (interquartile range), and 
      inferential statistics like Chi-square or regression&nbsp;analyses using IBM SPSS 
      version 28. A&nbsp;p&lt;0.05 will be considered statistically significant.&nbsp;ChatGPT holds 
      the potential to revolutionize pharmacy practice and education. This study will 
      highlight the psychometric properties of the KAP-C tool that assesses the 
      knowledge, attitude, and practice towards ChatGPT in pharmacy practice and 
      education. The findings will contribute to the potential ethical integration of 
      ChatGPT into pharmacy practice and education&nbsp;in LMICs, serve&nbsp;as&nbsp;a reference to 
      other economies, and provide valuable evidence for leveraging AI advancements in 
      pharmacy.
CI  - © 2023. W. Montague Cobb-NMA Health Institute.
FAU - Mohammed, Mustapha
AU  - Mohammed M
AUID- ORCID: 0000-0002-5021-1610
AD  - Department of Clinical Pharmacy and Pharmacy Practice, Faculty of Pharmaceutical 
      Sciences, Ahmadu Bello University, Zaria, Kaduna, Nigeria. 
      mohammedmmrx@gmail.com.
AD  - School of Pharmaceutical Sciences, Universiti Sains Malaysia, Gelugor, Pulau 
      Pinang, Malaysia. mohammedmmrx@gmail.com.
AD  - Vice President for Medical and Health Science Office, QU Health, Qatar 
      University, Doha, Qatar. mohammedmmrx@gmail.com.
FAU - Kumar, Narendar
AU  - Kumar N
AUID- ORCID: 0000-0002-2648-3915
AD  - Department of Pharmacy Practice, Faculty of Pharmacy, University of Sindh 
      Jamshoro, Sindh, Pakistan.
FAU - Zawiah, Mohammed
AU  - Zawiah M
AUID- ORCID: 0000-0002-3325-1442
AD  - School of Pharmaceutical Sciences, Universiti Sains Malaysia, Gelugor, Pulau 
      Pinang, Malaysia.
AD  - Department of Pharmacy Practice, Faculty of Clinical Pharmacy, Hodeidah 
      University, Al Hodeidah, Yemen.
FAU - Al-Ashwal, Fahmi Y
AU  - Al-Ashwal FY
AUID- ORCID: 0000-0003-2076-0771
AD  - Department of Pharmacy, Al-Maarif University College, Anbar, 31001, Iraq.
FAU - Bala, Auwal Adam
AU  - Bala AA
AUID- ORCID: 0000-0003-0402-9639
AD  - Department of Pharmacology, College of Medicine and Health Sciences, Federal 
      University Dutse, Dutse, Jigawa, Nigeria.
FAU - Lawal, Basira Kankia
AU  - Lawal BK
AUID- ORCID: 0000-0003-1975-0648
AD  - Department of Clinical Pharmacy and Pharmacy Management, Kaduna State University, 
      Kaduna, Nigeria.
FAU - Wada, Abubakar Sadiq
AU  - Wada AS
AUID- ORCID: 0000-0001-8221-1159
AD  - Department of Pharmacology and Therapeutics, Bayero University, Kano, Nigeria.
FAU - Halboup, Abdulsalam
AU  - Halboup A
AUID- ORCID: 0000-0002-9026-5656
AD  - School of Pharmaceutical Sciences, Universiti Sains Malaysia, Gelugor, Pulau 
      Pinang, Malaysia.
AD  - Department of Clinical Pharmacy and Pharmacy Practice, University of Science and 
      Technology, Sana'a, Yemen.
FAU - Muhammad, Surajuddeen
AU  - Muhammad S
AUID- ORCID: 0000-0001-8567-0748
AD  - Faculty of Veterinary Medicine, Ahmadu Bello University, Zaria, Kaduna, Nigeria.
FAU - Ahmad, Rabbiya
AU  - Ahmad R
AUID- ORCID: 0000-0003-0648-6825
AD  - School of Pharmaceutical Sciences, Universiti Sains Malaysia, Gelugor, Pulau 
      Pinang, Malaysia.
FAU - Sha'aban, Abubakar
AU  - Sha'aban A
AUID- ORCID: 0000-0002-5491-9851
AD  - Division of Population Medicine, School of Medicine, Cardiff University, Heath 
      Park, Cardiff, CF14 4YS, Wales, UK.
LA  - eng
PT  - Journal Article
DEP - 20230710
PL  - Switzerland
TA  - J Racial Ethn Health Disparities
JT  - Journal of racial and ethnic health disparities
JID - 101628476
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Attitude
OT  - ChatGPT
OT  - Knowledge
OT  - Pharmacy
OT  - Practice
EDAT- 2023/07/10 13:05
MHDA- 2023/07/10 13:05
CRDT- 2023/07/10 11:13
PHST- 2023/04/28 00:00 [received]
PHST- 2023/06/21 00:00 [accepted]
PHST- 2023/06/17 00:00 [revised]
PHST- 2023/07/10 13:05 [medline]
PHST- 2023/07/10 13:05 [pubmed]
PHST- 2023/07/10 11:13 [entrez]
AID - 10.1007/s40615-023-01696-1 [pii]
AID - 10.1007/s40615-023-01696-1 [doi]
PST - aheadofprint
SO  - J Racial Ethn Health Disparities. 2023 Jul 10. doi: 10.1007/s40615-023-01696-1.

PMID- 38545764
OWN - NLM
STAT- Publisher
LR  - 20240328
IS  - 1557-900X (Electronic)
IS  - 0892-7790 (Linking)
DP  - 2024 Mar 28
TI  - Artificial Intelligence for Urology Research: The Holy Grail of Data Science or 
      Pandora's Box of Misinformation?
LID - 10.1089/end.2023.0703 [doi]
AB  - Introduction Artificial intelligence tools such as the large language models 
      (LLMs) Bard and ChatGPT have generated significant research interest. Utilization 
      of these LLMs to study epidemiology of a target population could benefit 
      urologists. We investigated whether Bard and ChatGPT can perform a large-scale 
      calculation of the incidence and prevalence of kidney stone disease. Materials 
      and Methods We obtained reference values from two published studies which used 
      the National Health and Nutrition Examination Survey (NHANES) database to 
      calculate the prevalence and incidence of kidney stone disease. We then tested 
      the capability of Bard and ChatGPT to perform similar calculations using two 
      different methods. First, we instructed the LLMs to access the datasets and 
      independently perform the calculation. Second, we instructed the interfaces to 
      generate customized computer code which could perform the calculation on 
      downloaded datasets. Results While ChatGPT denied the ability to access and 
      perform calculations on the NHANES database, Bard intermittently claimed the 
      ability to do so. Bard provided either accurate results or inaccurate and 
      inconsistent results. For example, Bard's "calculations" for the incidence of 
      kidney stones from 2015-2018 were 2.1% (95% CI: 1.5-2.7), 1.75% (95% CI: 
      1.6-1.9), and 0.8% (95% CI 0.7-0.9), while the published number was 2.1% (95% CI 
      1.5-2.7). Bard provided discrete mathematical details of its calculations, 
      however when prompted further, admitted to having obtained the numbers from 
      online sources, including our chosen reference papers, rather than from a de novo 
      calculation. Both LLMs were able to produce code (Python) to use on the 
      downloaded NHANES datasets, however these would not readily execute. Conclusions 
      ChatGPT and Bard are currently incapable of performing epidemiological 
      calculations and lack transparency and accountability. Caution should be used, 
      particularly with Bard, as claims of its capabilities were convincingly 
      misleading, and results were inconsistent.
FAU - Blake, Ryan Matthew
AU  - Blake RM
AD  - Icahn School of Medicine at Mount Sinai, 5925, 1 Gustav Levy Pl, New York, New 
      York, United States, 10029-6574; ryanmblake@outlook.com.
FAU - Khusid, Johnathan Alexander
AU  - Khusid JA
AD  - Icahn School of Medicine at Mount Sinai, 5925, Urology, 1 Gustave Levy Pl., New 
      York, New York, United States, 10029-6574; johnathan.khusid@mountsinai.org.
LA  - eng
PT  - Journal Article
DEP - 20240328
PL  - United States
TA  - J Endourol
JT  - Journal of endourology
JID - 8807503
SB  - IM
EDAT- 2024/03/28 06:46
MHDA- 2024/03/28 06:46
CRDT- 2024/03/28 04:53
PHST- 2024/03/28 06:46 [medline]
PHST- 2024/03/28 06:46 [pubmed]
PHST- 2024/03/28 04:53 [entrez]
AID - 10.1089/end.2023.0703 [doi]
PST - aheadofprint
SO  - J Endourol. 2024 Mar 28. doi: 10.1089/end.2023.0703.

PMID- 38353523
OWN - NLM
STAT- Publisher
LR  - 20240214
IS  - 1524-4040 (Electronic)
IS  - 0148-396X (Linking)
DP  - 2024 Feb 14
TI  - A Quantitative Assessment of ChatGPT as a Neurosurgical Triaging Tool.
LID - 10.1227/neu.0000000000002867 [doi]
AB  - BACKGROUND AND OBJECTIVES: ChatGPT is a natural language processing chatbot with 
      increasing applicability to the medical workflow. Although ChatGPT has been shown 
      to be capable of passing the American Board of Neurological Surgery board 
      examination, there has never been an evaluation of the chatbot in triaging and 
      diagnosing novel neurosurgical scenarios without defined answer choices. In this 
      study, we assess ChatGPT's capability to determine the emergent nature of 
      neurosurgical scenarios and make diagnoses based on information one would find in 
      a neurosurgical consult. METHODS: Thirty clinical scenarios were given to 3 
      attendings, 4 residents, 2 physician assistants, and 2 subinterns. Participants 
      were asked to determine if the scenario constituted an urgent neurosurgical 
      consultation and what the most likely diagnosis was. Attending responses provided 
      a consensus to use as the answer key. Generative pretraining transformer (GPT) 
      3.5 and GPT 4 were given the same questions, and their responses were compared 
      with the other participants. RESULTS: GPT 4 was 100% accurate in both diagnosis 
      and triage of the scenarios. GPT 3.5 had an accuracy of 92.59%, slightly below 
      that of a PGY1 (96.3%), an 88.24% sensitivity, 100% specificity, 100% positive 
      predictive value, and 83.3% negative predicative value in triaging each 
      situation. When making a diagnosis, GPT 3.5 had an accuracy of 92.59%, which was 
      higher than the subinterns and similar to resident responders. CONCLUSION: GPT 4 
      is able to diagnose and triage neurosurgical scenarios at the level of a senior 
      neurosurgical resident. There has been a clear improvement between GPT 3.5 and 4. 
      It is likely that the recent updates in internet access and directing the 
      functionality of ChatGPT will further improve its utility in neurosurgical 
      triage.
CI  - Copyright © Congress of Neurological Surgeons 2024. All rights reserved.
FAU - Ward, Max
AU  - Ward M
AUID- ORCID: 0000-0003-1279-4920
AD  - Department of Neurological Surgery, Zucker School of Medicine at 
      Hofstra/Northwell, Hempstead, New York, USA.
FAU - Unadkat, Prashin
AU  - Unadkat P
AD  - Department of Neurological Surgery, Zucker School of Medicine at 
      Hofstra/Northwell, Hempstead, New York, USA.
AD  - Elmezzi Graduate School of Molecular Medicine, Feinstein Institutes of Medical 
      Research, Northwell Health, Manhasset, New York, USA.
FAU - Toscano, Daniel
AU  - Toscano D
AD  - Department of Neurological Surgery, Zucker School of Medicine at 
      Hofstra/Northwell, Hempstead, New York, USA.
FAU - Kashanian, Alon
AU  - Kashanian A
AD  - Department of Neurological Surgery, Zucker School of Medicine at 
      Hofstra/Northwell, Hempstead, New York, USA.
FAU - Lynch, Daniel G
AU  - Lynch DG
AD  - Department of Neurological Surgery, Zucker School of Medicine at 
      Hofstra/Northwell, Hempstead, New York, USA.
FAU - Horn, Alexander C
AU  - Horn AC
AD  - Department of Neurological Surgery, Wake Forest School of Medicine, 
      Winston-Salem, North Carolina, USA.
FAU - D'Amico, Randy S
AU  - D'Amico RS
AD  - Department of Neurological Surgery, Zucker School of Medicine at 
      Hofstra/Northwell, Hempstead, New York, USA.
AD  - Department of Neurological Surgery, Lenox Hill Hospital, New York, New York, USA.
FAU - Mittler, Mark
AU  - Mittler M
AD  - Department of Neurological Surgery, Zucker School of Medicine at 
      Hofstra/Northwell, Hempstead, New York, USA.
AD  - Department of Pediatric Neurosurgery, Cohens Childrens Medical Center, Queens, 
      New York, USA.
FAU - Baum, Griffin R
AU  - Baum GR
AD  - Department of Neurological Surgery, Zucker School of Medicine at 
      Hofstra/Northwell, Hempstead, New York, USA.
AD  - Department of Neurological Surgery, Lenox Hill Hospital, New York, New York, USA.
LA  - eng
PT  - Journal Article
DEP - 20240214
PL  - United States
TA  - Neurosurgery
JT  - Neurosurgery
JID - 7802914
SB  - IM
EDAT- 2024/02/14 12:50
MHDA- 2024/02/14 12:50
CRDT- 2024/02/14 09:02
PHST- 2023/08/10 00:00 [received]
PHST- 2023/12/26 00:00 [accepted]
PHST- 2024/02/14 12:50 [medline]
PHST- 2024/02/14 12:50 [pubmed]
PHST- 2024/02/14 09:02 [entrez]
AID - 00006123-990000000-01055 [pii]
AID - 10.1227/neu.0000000000002867 [doi]
PST - aheadofprint
SO  - Neurosurgery. 2024 Feb 14. doi: 10.1227/neu.0000000000002867.

PMID- 37726551
OWN - NLM
STAT- MEDLINE
DCOM- 20231122
LR  - 20231122
IS  - 1473-1150 (Electronic)
IS  - 1470-269X (Linking)
VI  - 23
IP  - 6
DP  - 2023 Nov
TI  - Using ChatGPT to predict the future of personalized medicine.
PG  - 178-184
LID - 10.1038/s41397-023-00316-9 [doi]
AB  - Personalized medicine is a novel frontier in health care that is based on each 
      person's unique genetic makeup. It represents an exciting opportunity to improve 
      the future of individualized health care for all individuals. Pharmacogenomics, 
      as the main part of personalized medicine, aims to optimize and create a more 
      targeted treatment approach based on genetic variations in drug response. It is 
      predicted that future treatments will be algorithm-based instead of 
      evidence-based that will consider a patient's genetic, transcriptomic, proteomic, 
      epigenetic, and lifestyle factors resulting in individualized medication. A 
      generative pretrained transformer (GPT) is an artificial intelligence (AI) tool 
      that generates language resembling human-like writing enabling users to engage in 
      a manner that is practically identical to speaking with a human being. GPT's 
      predictive algorithms can respond to questions that have never been addressed. 
      Chat Generative Pretrained Transformer (ChatGPT) is an AI chatbot's advanced with 
      conversational capabilities. In the present study, questions were asked from 
      ChatGPT about the future of personalized medicine and pharmacogenomics. ChatGPT 
      predicted both to be a promising approach with a bright future that holds great 
      promises in improving patient outcomes and transforming the field of medicine. 
      But it still has several limitations that need to be solved.
CI  - © 2023. The Author(s), under exclusive licence to Springer Nature Limited.
FAU - Patrinos, George P
AU  - Patrinos GP
AUID- ORCID: 0000-0002-0519-7776
AD  - Department of Pharmacy, School of Health Sciences, University of Patras, 26504, 
      Patras, Greece.
AD  - Department of Genetics and Genomics, College of Medicine and Health Sciences, 
      United Arab Emirates University, Al Ain, P.O. Box 15551, Abu Dhabi, UAE.
AD  - United Arab Emirates University, Zayed Center for Health Sciences, Al Ain, P.O. 
      Box 15551, Abu Dhabi, UAE.
FAU - Sarhangi, Negar
AU  - Sarhangi N
AD  - Personalized Medicine Research Center, Endocrinology and Metabolism Clinical 
      Sciences Institute, Tehran University of Medical Sciences, 1411713119, Tehran, 
      Iran.
FAU - Sarrami, Behnaz
AU  - Sarrami B
AD  - Missouri Pharmacogenomics Consulting, St. Mo, 63011, St. Louis, MO, USA.
FAU - Khodayari, Nazli
AU  - Khodayari N
AD  - Division of Pulmonary, Critical Care and Sleep Medicine, Department of Medicine, 
      University of Florida, Gainesville, 32610-0225, FL, USA.
FAU - Larijani, Bagher
AU  - Larijani B
AD  - Endocrinology and Metabolism Research Center, Endocrinology and Metabolism 
      Clinical Sciences Institute, Tehran University of Medical Sciences, 1411713119, 
      Tehran, Iran.
FAU - Hasanzad, Mandana
AU  - Hasanzad M
AUID- ORCID: 0000-0002-0538-1135
AD  - Personalized Medicine Research Center, Endocrinology and Metabolism Clinical 
      Sciences Institute, Tehran University of Medical Sciences, 1411713119, Tehran, 
      Iran. mandanahasanzad@yahoo.com.
AD  - Medical Genomics Research Center, Tehran Medical Sciences, Islamic Azad 
      University, 193951459, Tehran, Iran. mandanahasanzad@yahoo.com.
LA  - eng
PT  - Journal Article
DEP - 20230919
PL  - United States
TA  - Pharmacogenomics J
JT  - The pharmacogenomics journal
JID - 101083949
RN  - EC 2.6.1.2 (Alanine Transaminase)
SB  - IM
MH  - Humans
MH  - *Precision Medicine
MH  - *Artificial Intelligence
MH  - Proteomics
MH  - Pharmacogenetics
MH  - Alanine Transaminase
EDAT- 2023/09/20 00:43
MHDA- 2023/11/22 06:43
CRDT- 2023/09/19 23:36
PHST- 2023/04/10 00:00 [received]
PHST- 2023/09/05 00:00 [accepted]
PHST- 2023/11/22 06:43 [medline]
PHST- 2023/09/20 00:43 [pubmed]
PHST- 2023/09/19 23:36 [entrez]
AID - 10.1038/s41397-023-00316-9 [pii]
AID - 10.1038/s41397-023-00316-9 [doi]
PST - ppublish
SO  - Pharmacogenomics J. 2023 Nov;23(6):178-184. doi: 10.1038/s41397-023-00316-9. Epub 
      2023 Sep 19.

PMID- 37851468
OWN - NLM
STAT- Publisher
LR  - 20240207
IS  - 1555-905X (Electronic)
IS  - 1555-9041 (Print)
IS  - 1555-9041 (Linking)
VI  - 19
IP  - 1
DP  - 2023 Oct 18
TI  - Performance of ChatGPT on Nephrology Test Questions.
PG  - 35-43
LID - 10.2215/CJN.0000000000000330 [doi]
AB  - BACKGROUND: ChatGPT is a novel tool that allows people to engage in conversations 
      with an advanced machine learning model. ChatGPT's performance in the US Medical 
      Licensing Examination is comparable with a successful candidate's performance. 
      However, its performance in the nephrology field remains undetermined. This study 
      assessed ChatGPT's capabilities in answering nephrology test questions. METHODS: 
      Questions sourced from Nephrology Self-Assessment Program and Kidney 
      Self-Assessment Program were used, each with multiple-choice single-answer 
      questions. Questions containing visual elements were excluded. Each question bank 
      was run twice using GPT-3.5 and GPT-4. Total accuracy rate, defined as the 
      percentage of correct answers obtained by ChatGPT in either the first or second 
      run, and the total concordance, defined as the percentage of identical answers 
      provided by ChatGPT during both runs, regardless of their correctness, were used 
      to assess its performance. RESULTS: A comprehensive assessment was conducted on a 
      set of 975 questions, comprising 508 questions from Nephrology Self-Assessment 
      Program and 467 from Kidney Self-Assessment Program. GPT-3.5 resulted in a total 
      accuracy rate of 51%. Notably, the employment of Nephrology Self-Assessment 
      Program yielded a higher accuracy rate compared with Kidney Self-Assessment 
      Program (58% versus 44%; P &lt; 0.001). The total concordance rate across all 
      questions was 78%, with correct answers exhibiting a higher concordance rate 
      (84%) compared with incorrect answers (73%) ( P &lt; 0.001). When examining various 
      nephrology subfields, the total accuracy rates were relatively lower in 
      electrolyte and acid-base disorder, glomerular disease, and kidney-related bone 
      and stone disorders. The total accuracy rate of GPT-4's response was 74%, higher 
      than GPT-3.5 ( P &lt; 0.001) but remained below the passing threshold and average 
      scores of nephrology examinees (77%). CONCLUSIONS: ChatGPT exhibited limitations 
      regarding accuracy and repeatability when addressing nephrology-related 
      questions. Variations in performance were evident across various subfields.
CI  - Copyright © 2023 by the American Society of Nephrology.
FAU - Miao, Jing
AU  - Miao J
AUID- ORCID: 0000-0003-0642-9740
AD  - Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
      Rochester, Minnesota.
FAU - Thongprayoon, Charat
AU  - Thongprayoon C
AUID- ORCID: 0000-0002-8313-3604
FAU - Garcia Valencia, Oscar A
AU  - Garcia Valencia OA
AUID- ORCID: 0000-0003-0186-9448
FAU - Krisanapan, Pajaree
AU  - Krisanapan P
AUID- ORCID: 0000-0002-2888-881
FAU - Sheikh, Mohammad S
AU  - Sheikh MS
AUID- ORCID: 0009-0006-9388-8505
FAU - Davis, Paul W
AU  - Davis PW
AUID- ORCID: 0000-0003-4637-3198
FAU - Mekraksakit, Poemlarp
AU  - Mekraksakit P
AUID- ORCID: 0000-0002-2127-2529
FAU - Suarez, Maria Gonzalez
AU  - Suarez MG
AUID- ORCID: 0000-0002-8930-4611
FAU - Craici, Iasmina M
AU  - Craici IM
FAU - Cheungpasitporn, Wisit
AU  - Cheungpasitporn W
AUID- ORCID: 0000-0001-9954-9711
LA  - eng
PT  - Journal Article
DEP - 20231018
PL  - United States
TA  - Clin J Am Soc Nephrol
JT  - Clinical journal of the American Society of Nephrology : CJASN
JID - 101271570
SB  - IM
CIN - doi: 10.2215/CJN.0000000000000378
PMC - PMC10843340
COIS- W. Cheungpasitporn, I.M. Craici, P. Mekraksakit, J. Miao, and M.S. Sheikh report 
      employment with Mayo Clinic. M. Gonzalez Suarez reports employment with Mayo 
      Clinic and research funding from Gilead. P. Krisanapan reports ownership interest 
      in Thai stocks (AURA, CHASE, COM7, DOHOME, HL, JMT, M-CHAI, MEGA, PHG, SABINA); 
      research funding from PT. Kalbe Genexine Biologics, Indonesia and Thai Otsuka 
      Pharmaceutical; honoraria from Apexcela Co., Ltd; and speakers bureau for 
      AstraZeneca, Boehringer Ingelheim, Novo Nordisk, Thai Otsuka Pharmaceutical, and 
      Zuellig Pharma. All remaining authors have nothing to disclose.
EDAT- 2023/10/18 12:43
MHDA- 2023/10/18 12:43
PMCR- 2025/01/01
CRDT- 2023/10/18 11:34
PHST- 2023/06/01 00:00 [received]
PHST- 2023/10/12 00:00 [accepted]
PHST- 2025/01/01 00:00 [pmc-release]
PHST- 2023/10/18 12:43 [pubmed]
PHST- 2023/10/18 12:43 [medline]
PHST- 2023/10/18 11:34 [entrez]
AID - 01277230-990000000-00266 [pii]
AID - CJASN-2023-000859 [pii]
AID - 10.2215/CJN.0000000000000330 [doi]
PST - aheadofprint
SO  - Clin J Am Soc Nephrol. 2023 Oct 18;19(1):35-43. doi: 
      10.2215/CJN.0000000000000330.

PMID- 36915398
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230315
IS  - 2752-4191 (Electronic)
IS  - 2752-4191 (Linking)
VI  - 3
IP  - 2
DP  - 2023 Mar
TI  - ChatGPT: the next frontier in academic writing for cardiologists or a pandora's 
      box of ethical dilemmas.
PG  - oead007
LID - 10.1093/ehjopen/oead007 [doi]
LID - oead007
FAU - Marchandot, Benjamin
AU  - Marchandot B
AUID- ORCID: 0000-0001-7034-1466
AD  - Division of Cardiovascular Medicine, Nouvel Hôpital Civil, Strasbourg University 
      Hospital, 1 place de l'Hôpital, 67000 Strasbourg, France.
FAU - Matsushita, Kensuke
AU  - Matsushita K
AUID- ORCID: 0000-0002-3722-2921
AD  - Division of Cardiovascular Medicine, Nouvel Hôpital Civil, Strasbourg University 
      Hospital, 1 place de l'Hôpital, 67000 Strasbourg, France.
AD  - UMR 1260, INSERM (French National Institute of Health and Medical Research), 
      Regenerative Nanomedicine, FMTS, 1 rue Eugene Boeckel, Strasbourg 67000, France.
FAU - Carmona, Adrien
AU  - Carmona A
AD  - Division of Cardiovascular Medicine, Nouvel Hôpital Civil, Strasbourg University 
      Hospital, 1 place de l'Hôpital, 67000 Strasbourg, France.
FAU - Trimaille, Antonin
AU  - Trimaille A
AD  - Division of Cardiovascular Medicine, Nouvel Hôpital Civil, Strasbourg University 
      Hospital, 1 place de l'Hôpital, 67000 Strasbourg, France.
AD  - UMR 1260, INSERM (French National Institute of Health and Medical Research), 
      Regenerative Nanomedicine, FMTS, 1 rue Eugene Boeckel, Strasbourg 67000, France.
FAU - Morel, Olivier
AU  - Morel O
AD  - Division of Cardiovascular Medicine, Nouvel Hôpital Civil, Strasbourg University 
      Hospital, 1 place de l'Hôpital, 67000 Strasbourg, France.
AD  - UMR 1260, INSERM (French National Institute of Health and Medical Research), 
      Regenerative Nanomedicine, FMTS, 1 rue Eugene Boeckel, Strasbourg 67000, France.
LA  - eng
PT  - Journal Article
DEP - 20230213
PL  - England
TA  - Eur Heart J Open
JT  - European heart journal open
JID - 9918282081406676
PMC - PMC10006694
OTO - NOTNLM
OT  - Academic Writing
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Ethics
COIS- Conflict of interest: The authors declare that there is no conflict of interest 
      regarding the publication of this article. Prof Olivier Morel (O.M) received 
      institutional research grants from Fondation Coeur et Vaisseaux, AstraZeneca and 
      Boehringher Ingelheim. The manuscript has been read and approved for submission 
      by all authors. The authors take responsibility for all aspects of the 
      reliability and freedom from bias of the data presented and their discussed 
      interpretation.
EDAT- 2023/03/15 06:00
MHDA- 2023/03/15 06:01
PMCR- 2023/02/13
CRDT- 2023/03/14 02:06
PHST- 2023/01/27 00:00 [received]
PHST- 2023/02/02 00:00 [revised]
PHST- 2023/03/14 02:06 [entrez]
PHST- 2023/03/15 06:00 [pubmed]
PHST- 2023/03/15 06:01 [medline]
PHST- 2023/02/13 00:00 [pmc-release]
AID - oead007 [pii]
AID - 10.1093/ehjopen/oead007 [doi]
PST - epublish
SO  - Eur Heart J Open. 2023 Feb 13;3(2):oead007. doi: 10.1093/ehjopen/oead007. 
      eCollection 2023 Mar.

PMID- 36875254
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230307
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - When Palliative Care May Be the Only Option in the Management of Severe Burns: A 
      Case Report Written With the Help of ChatGPT.
PG  - e35649
LID - 10.7759/cureus.35649 [doi]
LID - e35649
AB  - We present a case of 100% third-degree burns. The patient received full 
      resuscitative measures, but the family was prepared for a poor outcome based on 
      the severe extent of the injuries. After several days of treatment, it became 
      apparent that the patient indeed could not survive the injuries and palliative 
      care was instituted, including mechanical ventilation, fluid therapy, and 
      analgesia. Surgery was not possible without causing major disfigurement, 
      including enucleation of both eyes and amputation of all limbs.
CI  - Copyright © 2023, Nachshon et al.
FAU - Nachshon, Akiva
AU  - Nachshon A
AD  - General Intensive Care Unit, Hadassah Medical Center, Jerusalem, ISR.
FAU - Batzofin, Baruch
AU  - Batzofin B
AD  - General Intensive Care Unit, Hadassah Medical Center, Jerusalem, ISR.
FAU - Beil, Michael
AU  - Beil M
AD  - Medical Intensive Care Unit, Hadassah Medical Center, Jerusalem, ISR.
FAU - van Heerden, Peter V
AU  - van Heerden PV
AD  - General Intensive Care Unit, Hadassah Medical Center, Jerusalem, ISR.
LA  - eng
PT  - Case Reports
DEP - 20230301
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC9976839
OTO - NOTNLM
OT  - chatgpt
OT  - critical care
OT  - major trauma
OT  - palliative care
OT  - severe burns
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/03/07 06:00
MHDA- 2023/03/07 06:01
PMCR- 2023/03/01
CRDT- 2023/03/06 04:07
PHST- 2023/03/01 00:00 [accepted]
PHST- 2023/03/06 04:07 [entrez]
PHST- 2023/03/07 06:00 [pubmed]
PHST- 2023/03/07 06:01 [medline]
PHST- 2023/03/01 00:00 [pmc-release]
AID - 10.7759/cureus.35649 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 1;15(3):e35649. doi: 10.7759/cureus.35649. eCollection 2023 Mar.

PMID- 36840450
OWN - NLM
STAT- Publisher
LR  - 20240216
IS  - 1545-5815 (Electronic)
IS  - 0898-9621 (Linking)
DP  - 2023 Feb 25
TI  - Challenges for enforcing editorial policies on AI-generated papers.
PG  - 1-3
LID - 10.1080/08989621.2023.2184262 [doi]
AB  - ChatGPT, a chatbot released by OpenAI in November 2022, has rocked academia with 
      its capacity to generate papers "good enough" for academic journals. Major 
      journals such as Nature and professional societies such as the World Association 
      of Medical Editors have moved fast to issue policies to ban or curb AI-written 
      papers. Amid the flurry of policy initiatives, one important challenge seems to 
      be overlooked: AI-generated papers are not easily discernible to the human eye, 
      and we lack the right tools to implement the policies. Without such tools, the 
      well-intentioned policies are likely to remain on paper.
FAU - Hu, Guangwei
AU  - Hu G
AUID- ORCID: 0000-0002-2297-4784
AD  - Department of English and Communication, The Hong Kong Polytechnic University, 
      Hong Kong, China.
LA  - eng
PT  - Journal Article
DEP - 20230225
PL  - United States
TA  - Account Res
JT  - Accountability in research
JID - 9100813
SB  - IM
OTO - NOTNLM
OT  - ChatGPT; editorial policies; AI-generated papers; challenges
EDAT- 2023/02/26 06:00
MHDA- 2023/02/26 06:00
CRDT- 2023/02/25 04:50
PHST- 2023/02/26 06:00 [pubmed]
PHST- 2023/02/26 06:00 [medline]
PHST- 2023/02/25 04:50 [entrez]
AID - 10.1080/08989621.2023.2184262 [doi]
PST - aheadofprint
SO  - Account Res. 2023 Feb 25:1-3. doi: 10.1080/08989621.2023.2184262.

PMID- 37898341
OWN - NLM
STAT- MEDLINE
DCOM- 20240214
LR  - 20240214
IS  - 1097-6787 (Electronic)
IS  - 0190-9622 (Linking)
VI  - 90
IP  - 3
DP  - 2024 Mar
TI  - Analysis of ChatGPT generated differential diagnoses in response to physical exam 
      findings for benign and malignant cutaneous neoplasms.
PG  - 615-616
LID - S0190-9622(23)03044-X [pii]
LID - 10.1016/j.jaad.2023.10.040 [doi]
FAU - Rundle, Chandler W
AU  - Rundle CW
AD  - Department of Dermatology, Duke University Hospital, Durham, North Carolina.
FAU - Szeto, Mindy D
AU  - Szeto MD
AD  - Department of Dermatology, University of Colorado Anschutz Medical Campus, 
      Aurora, Colorado.
FAU - Presley, Colby L
AU  - Presley CL
AD  - Division of Dermatology, Lehigh Valley Network, Allentown, Pennsylvania.
FAU - Shahwan, Kathryn T
AU  - Shahwan KT
AD  - Department of Dermatology, The Ohio State University Medical Center, Columbus, 
      Ohio; Department of Internal Medicine, University of North Dakota Medical School, 
      Grand Forks, North Dakota; Department of Dermatology, Altru Health System, Grand 
      Forks, North Dakota. Electronic address: kathryn.shahwan@osumc.edu.
FAU - Carr, David R
AU  - Carr DR
AD  - Department of Dermatology, The Ohio State University Medical Center, Columbus, 
      Ohio.
LA  - eng
PT  - Journal Article
DEP - 20231028
PL  - United States
TA  - J Am Acad Dermatol
JT  - Journal of the American Academy of Dermatology
JID - 7907132
SB  - IM
MH  - Humans
MH  - Diagnosis, Differential
MH  - *Physical Examination
MH  - *Neoplasms
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - cutaneous oncology
OT  - large language models
OT  - machine learning
OT  - neoplastic dermatology
COIS- Conflicts of interest None disclosed.
EDAT- 2023/10/29 06:46
MHDA- 2024/02/12 05:43
CRDT- 2023/10/28 19:29
PHST- 2023/09/10 00:00 [received]
PHST- 2023/10/08 00:00 [revised]
PHST- 2023/10/21 00:00 [accepted]
PHST- 2024/02/12 05:43 [medline]
PHST- 2023/10/29 06:46 [pubmed]
PHST- 2023/10/28 19:29 [entrez]
AID - S0190-9622(23)03044-X [pii]
AID - 10.1016/j.jaad.2023.10.040 [doi]
PST - ppublish
SO  - J Am Acad Dermatol. 2024 Mar;90(3):615-616. doi: 10.1016/j.jaad.2023.10.040. Epub 
      2023 Oct 28.

PMID- 37396972
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231101
IS  - 2624-8212 (Electronic)
IS  - 2624-8212 (Linking)
VI  - 6
DP  - 2023
TI  - ChatGPT in society: emerging issues.
PG  - 1130913
LID - 10.3389/frai.2023.1130913 [doi]
LID - 1130913
AB  - We review and critically assess several issues arising from the potential 
      -large-scale- implementation or deployment of Large Language Models (LLMs) in 
      society. These include security, political, economic, cultural, and educational 
      issues as well as issues concerning social biases, creativity, copyright, and 
      freedom of speech. We argue, without a preconceived pessimism toward these tools, 
      that they may bring about many benefits. However, we also call for a balance 
      assessment of their downsides. While our work is only preliminary and certainly 
      partial it nevertheless holds some value as one of the first exploratory attempts 
      in the literature.
CI  - Copyright © 2023 Farina and Lavazza.
FAU - Farina, Mirko
AU  - Farina M
AD  - Faculty of Humanities and Social Science, HMI Lab, Innopolis University, 
      Innopolis, Russia.
FAU - Lavazza, Andrea
AU  - Lavazza A
AD  - CUI: Neuroethics UNIVP: Brain and Behavioral Sciences, University of Pavia, 
      Pavia, Lombardy, Italy.
AD  - Centro Universitario Internazionale, Arezzo, Italy.
LA  - eng
PT  - Journal Article
DEP - 20230615
PL  - Switzerland
TA  - Front Artif Intell
JT  - Frontiers in artificial intelligence
JID - 101770551
PMC - PMC10310434
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - OpenAI
OT  - copyright
OT  - creativity
OT  - ethics
OT  - machine learning
OT  - social biases
COIS- The authors declare that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2023/07/03 13:06
MHDA- 2023/07/03 13:07
PMCR- 2023/06/15
CRDT- 2023/07/03 11:28
PHST- 2022/12/23 00:00 [received]
PHST- 2023/05/30 00:00 [accepted]
PHST- 2023/07/03 13:07 [medline]
PHST- 2023/07/03 13:06 [pubmed]
PHST- 2023/07/03 11:28 [entrez]
PHST- 2023/06/15 00:00 [pmc-release]
AID - 10.3389/frai.2023.1130913 [doi]
PST - epublish
SO  - Front Artif Intell. 2023 Jun 15;6:1130913. doi: 10.3389/frai.2023.1130913. 
      eCollection 2023.

PMID- 37436486
OWN - NLM
STAT- MEDLINE
DCOM- 20230731
LR  - 20231002
IS  - 2731-7013 (Electronic)
IS  - 2731-7005 (Linking)
VI  - 74
IP  - 8
DP  - 2023 Aug
TI  - ChatGPT-4: dermatology aid in the emergency room.
PG  - 633-634
LID - 10.1007/s00105-023-05186-7 [doi]
FAU - Castro, Harvey
AU  - Castro H
AD  - Coppell ER, 720 north Denton tap, 75019, Coppell, TX, USA. 
      harveycastro4@gmail.com.
LA  - eng
PT  - Journal Article
TT  - ChatGPT-4: dermatologische Unterstützung in der Notfallambulanz.
DEP - 20230712
PL  - Germany
TA  - Dermatologie (Heidelb)
JT  - Dermatologie (Heidelberg, Germany)
JID - 9918384885206676
SB  - IM
MH  - *Dermatology
MH  - *Artificial Intelligence
MH  - *Emergency Service, Hospital
EDAT- 2023/07/12 13:07
MHDA- 2023/07/25 06:42
CRDT- 2023/07/12 11:05
PHST- 2023/06/07 00:00 [accepted]
PHST- 2023/07/25 06:42 [medline]
PHST- 2023/07/12 13:07 [pubmed]
PHST- 2023/07/12 11:05 [entrez]
AID - 10.1007/s00105-023-05186-7 [pii]
AID - 10.1007/s00105-023-05186-7 [doi]
PST - ppublish
SO  - Dermatologie (Heidelb). 2023 Aug;74(8):633-634. doi: 10.1007/s00105-023-05186-7. 
      Epub 2023 Jul 12.

PMID- 37046397
OWN - NLM
STAT- MEDLINE
DCOM- 20230414
LR  - 20230907
IS  - 1576-6578 (Electronic)
IS  - 0210-0010 (Print)
IS  - 0210-0010 (Linking)
VI  - 76
IP  - 8
DP  - 2023 Apr 16
TI  - [ChatGPT: a novel tool for writing scientific articles, but not an author (for 
      the time being)].
PG  - 277
LID - 10.33588/rn.7608.2023066 [doi]
FAU - Ros-Arlanzón, P
AU  - Ros-Arlanzón P
AD  - Hospital General Universitario Dr. Balmis, Alicante, España.
FAU - Pérez-Sempere, A
AU  - Pérez-Sempere A
AD  - Hospital General Universitario Dr. Balmis, Alicante, España.
LA  - spa
PT  - Letter
TT  - ChatGPT: una novedosa herramienta de escritura para artículos científicos, pero 
      no un autor (por el momento).
PL  - Spain
TA  - Rev Neurol
JT  - Revista de neurologia
JID - 7706841
SB  - IM
MH  - Humans
MH  - *Writing
MH  - *Authorship
PMC - PMC10478136
COIS- Conflicto de intereses: Los autores declaran no tener conflictos de interés.
EDAT- 2023/04/14 06:00
MHDA- 2023/04/14 06:41
PMCR- 2023/04/16
CRDT- 2023/04/13 00:42
PHST- 2023/04/14 06:41 [medline]
PHST- 2023/04/13 00:42 [entrez]
PHST- 2023/04/14 06:00 [pubmed]
PHST- 2023/04/16 00:00 [pmc-release]
AID - rn2023066 [pii]
AID - RN-76-277 [pii]
AID - 10.33588/rn.7608.2023066 [doi]
PST - ppublish
SO  - Rev Neurol. 2023 Apr 16;76(8):277. doi: 10.33588/rn.7608.2023066.

PMID- 38166323
OWN - NLM
STAT- Publisher
LR  - 20240102
IS  - 1938-808X (Electronic)
IS  - 1040-2446 (Linking)
DP  - 2023 Dec 28
TI  - Large Language Models in Medical Education: Comparing ChatGPT- to Human-Generated 
      Exam Questions.
LID - 10.1097/ACM.0000000000005626 [doi]
AB  - PROBLEM: Creating medical exam questions is time consuming, but well-written 
      questions can be used for test-enhanced learning, which has been shown to have a 
      positive effect on student learning. The automated generation of high-quality 
      questions using large language models (LLMs), such as ChatGPT, would therefore be 
      desirable. However, there are no current studies that compare students' 
      performance on LLM-generated questions to questions developed by humans. 
      APPROACH: The authors compared student performance on questions generated by 
      ChatGPT (LLM questions) with questions created by medical educators (human 
      questions). Two sets of 25 multiple-choice questions (MCQs) were created, each 
      with 5 answer options, 1 of which was correct. The first set of questions was 
      written by an experienced medical educator, and the second set was created by 
      ChatGPT after the authors identified learning objectives and extracted some 
      specifications from the human questions. Students answered all questions in 
      random order in a formative paper-and-pencil test that was offered leading up to 
      the final summative neurophysiology exam (summer 2023). For each question, 
      students also indicated whether they thought it had been written by a human or 
      created by ChatGPT. OUTCOMES: The final data set consisted of 161 participants 
      and 46 MCQs (25 human and 21 LLM questions). There was no statistically 
      significant difference in item difficulty between the 2 question sets, but 
      discriminatory power was statistically significantly higher in human than LLM 
      questions (mean = .36, standard deviation [SD] = .09 vs mean = .24, SD = .14; P = 
      .001). On average, students identified 57% of question sources (human or LLM) 
      correctly. NEXT STEPS: Future research should replicate the study procedure in 
      other contexts (e.g., other medical subjects, semesters, countries, and 
      languages). In addition, the question of whether LLMs are suitable for generating 
      different question types, such as key feature questions, should be investigated.
CI  - Copyright © 2023 the Association of American Medical Colleges.
FAU - Laupichler, Matthias Carl
AU  - Laupichler MC
AUID- ORCID: 0000-0003-3104-1123
AD  - M.C. Laupichler is a research assistant, Institute of Medical Education, 
      University Hospital Bonn, and doctoral student, Institute of Psychology, 
      University of Bonn, Bonn, Germany; ORCID: https://orcid.org/0000-0003-3104-1123.
FAU - Rother, Johanna Flora
AU  - Rother JF
AUID- ORCID: 0009-0004-3526-3211
AD  - J.F. Rother is a research assistant, Institute of Medical Education, University 
      Hospital Bonn, Bonn, Germany; ORCID: https://orcid.org/0009-0004-3526-3211.
FAU - Grunwald Kadow, Ilona C
AU  - Grunwald Kadow IC
AUID- ORCID: 0000-0002-9085-4274
AD  - I.C. Grunwald Kadow is full professor and head, Institute of Physiology II, 
      Department of Medicine, University of Bonn, Bonn, Germany; ORCID: 
      https://orcid.org/0000-0002-9085-4274.
FAU - Ahmadi, Seifollah
AU  - Ahmadi S
AD  - S. Ahmadi is a research associate, Institute of Physiology II, Department of 
      Medicine, University of Bonn, Bonn, Germany.
FAU - Raupach, Tobias
AU  - Raupach T
AUID- ORCID: 0000-0003-2555-8097
AD  - T. Raupach is consultant cardiologist and head, Institute of Medical Education, 
      University Hospital Bonn, Bonn, Germany; ORCID: 
      https://orcid.org/0000-0003-2555-8097.
LA  - eng
PT  - Journal Article
DEP - 20231228
PL  - United States
TA  - Acad Med
JT  - Academic medicine : journal of the Association of American Medical Colleges
JID - 8904605
SB  - IM
EDAT- 2024/01/03 09:43
MHDA- 2024/01/03 09:43
CRDT- 2024/01/02 16:34
PHST- 2024/01/03 09:43 [medline]
PHST- 2024/01/03 09:43 [pubmed]
PHST- 2024/01/02 16:34 [entrez]
AID - 00001888-990000000-00719 [pii]
AID - 10.1097/ACM.0000000000005626 [doi]
PST - aheadofprint
SO  - Acad Med. 2023 Dec 28. doi: 10.1097/ACM.0000000000005626.

PMID- 38495947
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240319
IS  - 1179-5514 (Print)
IS  - 1179-5514 (Electronic)
IS  - 1179-5514 (Linking)
VI  - 17
DP  - 2024
TI  - Benefit-Risk Assessment of ChatGPT Applications in the Field of Diabetes and 
      Metabolic Illnesses: A Qualitative Study.
PG  - 11795514241235514
LID - 10.1177/11795514241235514 [doi]
LID - 11795514241235514
AB  - BACKGROUND: The use of ChatGPT and artificial intelligence (AI) in the management 
      of metabolic and endocrine disorders presents both significant opportunities and 
      notable risks. OBJECTIVES: To investigate the benefits and risks associated with 
      the application of ChatGPT in managing diabetes and metabolic illnesses by 
      exploring the perspectives of endocrinologists and diabetologists. METHODS AND 
      MATERIALS: The study employed a qualitative research approach. A semi-structured 
      in-depth interview guide was developed. A convenience sample of 25 
      endocrinologists and diabetologists was enrolled and interviewed. All interviews 
      were audiotaped and verbatim transcribed; then, thematic analysis was used to 
      determine the themes in the data. RESULTS: The findings of the thematic analysis 
      resulted in 19 codes and 9 major themes regarding the benefits of implementing AI 
      and ChatGPT in managing diabetes and metabolic illnesses. Moreover, the extracted 
      risks of implementing AI and ChatGPT in managing diabetes and metabolic illnesses 
      were categorized into 7 themes and 14 codes. The benefits of heightened 
      diagnostic precision, tailored treatment, and efficient resource utilization have 
      potential to improve patient results. Concurrently, the identification of 
      potential challenges, such as data security concerns and the need for AI that can 
      be explained, enables stakeholders to proactively tackle these issues. 
      CONCLUSIONS: Regulatory frameworks must evolve to keep pace with the rapid 
      adoption of AI in healthcare. Sustained attention to ethical considerations, 
      including obtaining patient consent, safeguarding data privacy, ensuring 
      accountability, and promoting fairness, remains critical. Despite its potential 
      impact on the human aspect of healthcare, AI will remain an integral component of 
      patient-centered care. Striking a balance between AI-assisted decision-making and 
      human expertise is essential to uphold trust and provide comprehensive patient 
      care.
CI  - © The Author(s) 2024.
FAU - Jairoun, Ammar Abdulrahman
AU  - Jairoun AA
AUID- ORCID: 0000-0002-4471-0878
AD  - Discipline of Clinical Pharmacy, School of Pharmaceutical Sciences, Universiti 
      Sains Malaysia (USM), Pulau Pinang, Malaysia.
AD  - Health and Safety Department, Dubai Municipality, Dubai, United Arab Emirates.
FAU - Al-Hemyari, Sabaa Saleh
AU  - Al-Hemyari SS
AD  - Discipline of Clinical Pharmacy, School of Pharmaceutical Sciences, Universiti 
      Sains Malaysia (USM), Pulau Pinang, Malaysia.
AD  - Pharmacy Department, Emirates Health Services, Dubai, United Arab Emirates.
FAU - Shahwan, Moyad
AU  - Shahwan M
AD  - College of Pharmacy and Health Sciences, Ajman University, Ajman, United Arab 
      Emirates.
AD  - Centre of Medical and Bio-allied Health Sciences Research, Ajman University, 
      United Arab Emirates.
FAU - Al-Qirim, Tariq
AU  - Al-Qirim T
AD  - Faculty of Pharmacy, Al-Zaytoonah University of Jordan, Amman, Jordan.
FAU - Shahwan, Monzer
AU  - Shahwan M
AD  - Diabetes Clinic, AL-Swity Center for Dermatology and Chronic Diseases, Ramallah, 
      Palestine.
LA  - eng
PT  - Journal Article
DEP - 20240315
PL  - United States
TA  - Clin Med Insights Endocrinol Diabetes
JT  - Clinical medicine insights. Endocrinology and diabetes
JID - 101578235
PMC - PMC10943713
OAB - Regulatory frameworks must evolve to keep pace with the rapid adoption of AI in 
      healthcare. Sustained attention to ethical considerations, including obtaining 
      patient consent, safeguarding data privacy, ensuring accountability, and 
      promoting fairness, remains critical. Despite its potential impact on the human 
      aspect of healthcare, AI will remain an integral component of patient-centered 
      care. The use of ChatGPT in the management of metabolic and endocrine disorders 
      presents both significant opportunities and notable risks. The benefits of 
      heightened diagnostic precision, tailored treatment, and efficient resource 
      utilization have potential to improve patient results. Concurrently, the 
      identification of potential challenges, such as data security concerns and the 
      need for AI that can be explained, enables stakeholders to proactively tackle 
      these issues. Regulatory frameworks must evolve to keep pace with the rapid 
      adoption of AI in healthcare. Sustained attention to ethical considerations, 
      including obtaining patient consent, safeguarding data privacy, ensuring 
      accountability, and promoting fairness, remains critical. Despite its potential 
      impact on the human aspect of healthcare, AI will remain an integral component of 
      patient-centered care. Striking a balance between AI-assisted decision-making and 
      human expertise is essential to uphold trust and provide comprehensive patient 
      care.
OABL- eng
OTO - NOTNLM
OT  - ChatGPT
OT  - Diabetes
OT  - artificial intelligence
OT  - chronic diseases
OT  - data privacy
OT  - metabolic disease
COIS- The author(s) declared no potential conflicts of interest with respect to the 
      research, authorship, and/or publication of this article.
EDAT- 2024/03/18 06:44
MHDA- 2024/03/18 06:45
PMCR- 2024/03/15
CRDT- 2024/03/18 04:30
PHST- 2023/10/30 00:00 [received]
PHST- 2024/02/06 00:00 [accepted]
PHST- 2024/03/18 06:45 [medline]
PHST- 2024/03/18 06:44 [pubmed]
PHST- 2024/03/18 04:30 [entrez]
PHST- 2024/03/15 00:00 [pmc-release]
AID - 10.1177_11795514241235514 [pii]
AID - 10.1177/11795514241235514 [doi]
PST - epublish
SO  - Clin Med Insights Endocrinol Diabetes. 2024 Mar 15;17:11795514241235514. doi: 
      10.1177/11795514241235514. eCollection 2024.

PMID- 38227177
OWN - NLM
STAT- MEDLINE
DCOM- 20240305
LR  - 20240307
IS  - 1544-2241 (Electronic)
IS  - 1544-1873 (Print)
IS  - 1544-1873 (Linking)
VI  - 22
IP  - 1
DP  - 2024 Feb
TI  - The Use of Artificial Intelligence in Writing Scientific Review Articles.
PG  - 115-121
LID - 10.1007/s11914-023-00852-0 [doi]
AB  - PURPOSE OF REVIEW: With the recent explosion in the use of artificial 
      intelligence (AI) and specifically ChatGPT, we sought to determine whether 
      ChatGPT could be used to assist in writing credible, peer-reviewed, scientific 
      review articles. We also sought to assess, in a scientific study, the advantages 
      and limitations of using ChatGPT for this purpose. To accomplish this, 3 topics 
      of importance in musculoskeletal research were selected: (1) the intersection of 
      Alzheimer's disease and bone; (2) the neural regulation of fracture healing; and 
      (3) COVID-19 and musculoskeletal health. For each of these topics, 3 approaches 
      to write manuscript drafts were undertaken: (1) human only; (2) ChatGPT only 
      (AI-only); and (3) combination approach of #1 and #2 (AI-assisted). Articles were 
      extensively fact checked and edited to ensure scientific quality, resulting in 
      final manuscripts that were significantly different from the original drafts. 
      Numerous parameters were measured throughout the process to quantitate advantages 
      and disadvantages of approaches. RECENT FINDINGS: Overall, use of AI decreased 
      the time spent to write the review article, but required more extensive fact 
      checking. With the AI-only approach, up to 70% of the references cited were found 
      to be inaccurate. Interestingly, the AI-assisted approach resulted in the highest 
      similarity indices suggesting a higher likelihood of plagiarism. Finally, 
      although the technology is rapidly changing, at the time of study, ChatGPT 4.0 
      had a cutoff date of September 2021 rendering identification of recent articles 
      impossible. Therefore, all literature published past the cutoff date was manually 
      provided to ChatGPT, rendering approaches #2 and #3 identical for contemporary 
      citations. As a result, for the COVID-19 and musculoskeletal health topic, 
      approach #2 was abandoned midstream due to the extensive overlap with approach 
      #3. The main objective of this scientific study was to see whether AI could be 
      used in a scientifically appropriate manner to improve the scientific writing 
      process. Indeed, AI reduced the time for writing but had significant 
      inaccuracies. The latter necessitates that AI cannot currently be used alone but 
      could be used with careful oversight by humans to assist in writing scientific 
      review articles.
CI  - © 2024. This is a U.S. Government work and not under copyright protection in the 
      US; foreign copyright protection may apply.
FAU - Kacena, Melissa A
AU  - Kacena MA
AD  - Department of Orthopaedic Surgery, Indiana University School of Medicine, 
      Indianapolis, IN, 46202, USA. mkacena@iupui.edu.
AD  - Department of Anatomy, Cell Biology &amp; Physiology, Indiana University School of 
      Medicine, Indianapolis, IN, 46202, USA. mkacena@iupui.edu.
AD  - Indiana Center for Musculoskeletal Health, Indiana University School of Medicine, 
      Indianapolis, IN, 46202, USA. mkacena@iupui.edu.
AD  - Richard L. Roudebush VA Medical Center, Indianapolis, IN, 46202, USA. 
      mkacena@iupui.edu.
FAU - Plotkin, Lilian I
AU  - Plotkin LI
AD  - Department of Anatomy, Cell Biology &amp; Physiology, Indiana University School of 
      Medicine, Indianapolis, IN, 46202, USA.
AD  - Indiana Center for Musculoskeletal Health, Indiana University School of Medicine, 
      Indianapolis, IN, 46202, USA.
AD  - Richard L. Roudebush VA Medical Center, Indianapolis, IN, 46202, USA.
FAU - Fehrenbacher, Jill C
AU  - Fehrenbacher JC
AD  - Indiana Center for Musculoskeletal Health, Indiana University School of Medicine, 
      Indianapolis, IN, 46202, USA. jfehrenb@iu.edu.
AD  - Department of Pharmacology and Toxicology, Indiana University School of Medicine, 
      Indianapolis, IN, 46202, USA. jfehrenb@iu.edu.
AD  - Stark Neuroscience Research Institute, Indiana University School of Medicine, 
      Indianapolis, IN, 46202, USA. jfehrenb@iu.edu.
LA  - eng
GR  - R01 AG060621/AG/NIA NIH HHS/United States
GR  - R21 AG078861/AG/NIA NIH HHS/United States
GR  - UL1 TR002529/TR/NCATS NIH HHS/United States
GR  - I01 RX003552/RX/RRD VA/United States
GR  - I01 BX003751/BX/BLRD VA/United States
GR  - IK6 RX004809/RX/RRD VA/United States
GR  - AG060621/AG060621-05S1/AG060621-05S2/NH/NIH HHS/United States
GR  - AG078861/AG078861-S1/NH/NIH HHS/United States
GR  - I01 BX005154/BX/BLRD VA/United States
GR  - AG060621/AG060621-05S1/AG060621-05S2/NH/NIH HHS/United States
GR  - AG078861/AG078861-S1/NH/NIH HHS/United States
PT  - Journal Article
PT  - Review
DEP - 20240116
PL  - United States
TA  - Curr Osteoporos Rep
JT  - Current osteoporosis reports
JID - 101176492
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *COVID-19
MH  - Fracture Healing
MH  - Writing
PMC - PMC10912250
OTO - NOTNLM
OT  - Alzheimer's disease
OT  - Artificial intelligence (AI)
OT  - COVID-19
OT  - ChatGPT
OT  - Fracture healing
OT  - Musculoskeletal system
OT  - Neural&nbsp;regulation
OT  - Osteoporosis
OT  - SARS-CoV-2
OT  - Scientific writing
COIS- Dr. Kacena is Editor-in-Chief of Current Osteoporosis Reports. Drs. Fehrenbacher 
      and Plotkin are Section Editors for Current Osteoporosis Reports.
EDAT- 2024/01/16 13:41
MHDA- 2024/03/05 06:45
PMCR- 2024/01/16
CRDT- 2024/01/16 11:18
PHST- 2023/12/21 00:00 [accepted]
PHST- 2024/03/05 06:45 [medline]
PHST- 2024/01/16 13:41 [pubmed]
PHST- 2024/01/16 11:18 [entrez]
PHST- 2024/01/16 00:00 [pmc-release]
AID - 10.1007/s11914-023-00852-0 [pii]
AID - 852 [pii]
AID - 10.1007/s11914-023-00852-0 [doi]
PST - ppublish
SO  - Curr Osteoporos Rep. 2024 Feb;22(1):115-121. doi: 10.1007/s11914-023-00852-0. 
      Epub 2024 Jan 16.

PMID- 37995379
OWN - NLM
STAT- MEDLINE
DCOM- 20231127
LR  - 20231217
IS  - 2448-5667 (Electronic)
IS  - 0443-5117 (Print)
IS  - 0443-5117 (Linking)
VI  - 61
IP  - 6
DP  - 2023 Nov 6
TI  - [Scientific integrity faces plagiarism fabricated with the ChatGPT].
PG  - 857-862
LID - 10.5281/zenodo.10064458 [doi]
AB  - Among the malpractices that undermine research integrity, plagiarism is a major 
      threat given its frequency and evolving presentations. Plagiarism implies the 
      intentional grabbing of texts, ideas, images, or data belonging to others and 
      without crediting them. However, the different and even masked forms of 
      plagiarism often difficult a clear identification. Currently, the many kinds of 
      fraud and plagiarism account for most retractions in traditional and open access 
      journals. Further, the rate of retracted articles is higher in the Latin American 
      databases LILACS and Scielo than in PubMed and Web of Science. This difference 
      has been related to the typical laxity of our culture and the lack of English 
      writing skills of non-Anglophone researchers. These features explain the conflict 
      experienced by Latin American students in USA where they face a stricter culture 
      regarding academic and scientific plagiarism. In the internet era, the ease of 
      accessing scientific literature has increased the temptation to plagiarize but 
      this ethical breach has been countered by antiplagiarism software. Now, the 
      so-called "paraphragiarism" prompted by paraphrasing tools exceeds the infamous 
      "copy-paste". For instance, the innovative ChatGPT can be used for plagiarizing 
      and paraphragiarizing. Moreover, its inclusion as coauthor in scientific papers 
      has been banned by prestigious journals and the International Committee of 
      Medical Journal Editors because such chatbot cannot meet the required public 
      responsibility criterium. To avoid plagiarism, it is enough to always give due 
      credit in the proper way. Lastly, I question the ill-fated and now prevailing 
      conjunction of blind faith in progress and zero skepticism that prevents us from 
      foreseeing the negative consequences of technological advances.
CI  - Licencia CC 4.0 (BY-NC-ND) © 2023 Revista Médica del Instituto Mexicano del 
      Seguro Social.
FAU - Rivera, Horacio
AU  - Rivera H
AUID- ORCID: 0000-0001-6940-0668
AD  - Universidad de Guadalajara, Centro Universitario de Ciencias de la Salud, 
      Departamento de Biología Molecular y Genómica. Guadalajara, Jalisco, México.
LA  - spa
PT  - English Abstract
PT  - Journal Article
TT  - La integridad científica ante los plagios fabricados con el ChatGPT.
DEP - 20231106
PL  - Mexico
TA  - Rev Med Inst Mex Seguro Soc
JT  - Revista medica del Instituto Mexicano del Seguro Social
JID - 101243727
SB  - IM
MH  - Humans
MH  - *Plagiarism
MH  - Research Personnel
MH  - *Scientific Misconduct
PMC - PMC10723832
OTO - NOTNLM
OT  - Scientific Integrity Review
OT  - Plagiarism
OT  - Artificial Intelligence
OT  - Revisión de Integridad Científica
OT  - Plagio
OT  - Inteligencia Artificial
OT  - ChatGPT
COIS- los autores han completado y enviado la forma traducida al español de la 
      declaración de conflictos potenciales de interés del Comité Internacional de 
      Editores de Revistas Médicas, y no fue reportado alguno que tuviera relación con 
      este artículo.
EDAT- 2023/11/23 18:41
MHDA- 2023/11/27 12:42
PMCR- 2023/11/01
CRDT- 2023/11/23 17:44
PHST- 2023/04/10 00:00 [received]
PHST- 2023/06/13 00:00 [accepted]
PHST- 2023/11/27 12:42 [medline]
PHST- 2023/11/23 18:41 [pubmed]
PHST- 2023/11/23 17:44 [entrez]
PHST- 2023/11/01 00:00 [pmc-release]
AID - 10.5281/zenodo.10064458 [doi]
PST - epublish
SO  - Rev Med Inst Mex Seguro Soc. 2023 Nov 6;61(6):857-862. doi: 
      10.5281/zenodo.10064458.

PMID- 37090271
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230425
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - Personality Changes and Staring Spells in a 12-Year-Old Child: A Case Report 
      Incorporating ChatGPT, a Natural Language Processing Tool Driven by Artificial 
      Intelligence (AI).
PG  - e36408
LID - 10.7759/cureus.36408 [doi]
LID - e36408
AB  - Low grade gliomas (LGGs) are the most common type of brain tumors diagnosed in 
      children. The presentation of intracranial tumors in pediatric patients is varied 
      and diverse. The early identification and treatment of LGGs are important to 
      achieve favorable outcomes. Although personality changes can be a symptom of 
      intracranial tumors, they are rarely the only main presenting feature. In 
      addition to central nervous system (CNS) tumors, personality changes can be 
      associated with psychological and endocrine conditions, contributing to a broad 
      differential diagnosis. Because symptoms such as personality changes have the 
      potential to be missed, communication between family members and clinicians is 
      imperative to identify these symptoms early. We report the case of a 12-year-old 
      child who presented with personality changes as her main symptom and was found to 
      have an intracranial neoplasm. This case report integrates original author 
      writing with output from ChatGPT, a natural language processing tool driven by 
      artificial intelligence (AI). In addition to the case itself, this report will 
      explore the benefits and drawbacks of using natural language AI in this context.
CI  - Copyright © 2023, Puthenpura et al.
FAU - Puthenpura, Vidya
AU  - Puthenpura V
AD  - Pediatric Hematology and Oncology, Yale School of Medicine, New Haven, USA.
FAU - Nadkarni, Siddhi
AU  - Nadkarni S
AD  - Pediatrics, Yale School of Medicine, New Haven, USA.
FAU - DiLuna, Michael
AU  - DiLuna M
AD  - Neurosurgery, Yale School of Medicine, New Haven, USA.
FAU - Hieftje, Kimberly
AU  - Hieftje K
AD  - Pediatrics, Yale School of Medicine, New Haven, USA.
FAU - Marks, Asher
AU  - Marks A
AD  - Pediatric Hematology and Oncology, Yale School of Medicine, New Haven, USA.
LA  - eng
PT  - Case Reports
DEP - 20230320
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10115215
OTO - NOTNLM
OT  - artificial intelligence
OT  - chatgpt
OT  - low grade glioma
OT  - pediatric neuro-oncology
OT  - pediatric oncology
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/04/24 06:41
MHDA- 2023/04/24 06:42
PMCR- 2023/03/20
CRDT- 2023/04/24 03:40
PHST- 2023/03/20 00:00 [accepted]
PHST- 2023/04/24 06:42 [medline]
PHST- 2023/04/24 06:41 [pubmed]
PHST- 2023/04/24 03:40 [entrez]
PHST- 2023/03/20 00:00 [pmc-release]
AID - 10.7759/cureus.36408 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 20;15(3):e36408. doi: 10.7759/cureus.36408. eCollection 2023 
      Mar.

PMID- 38353783
OWN - NLM
STAT- Publisher
LR  - 20240214
IS  - 1437-7799 (Electronic)
IS  - 1342-1751 (Linking)
DP  - 2024 Feb 14
TI  - Performance of ChatGPT and Bard in self-assessment questions for nephrology board 
      renewal.
LID - 10.1007/s10157-023-02451-w [doi]
AB  - BACKGROUND: Large language models (LLMs) have impacted advances in artificial 
      intelligence. While LLMs have demonstrated high performance in general medical 
      examinations, their performance in specialized areas such as nephrology is 
      unclear. This study aimed to evaluate ChatGPT and Bard in their potential 
      nephrology applications. METHODS: Ninety-nine questions from the Self-Assessment 
      Questions for Nephrology Board Renewal from 2018 to 2022 were presented to two 
      versions of ChatGPT (GPT-3.5 and GPT-4) and Bard. We calculated the correct 
      answer rates for the five years, each year, and question categories and checked 
      whether they exceeded the pass criterion. The correct answer rates were compared 
      with those of the nephrology residents. RESULTS: The overall correct answer rates 
      for GPT-3.5, GPT-4, and Bard were 31.3% (31/99), 54.5% (54/99), and 32.3% 
      (32/99), respectively, thus GPT-4 significantly outperformed GPT-3.5 (p&nbsp;&lt;&nbsp;0.01) 
      and Bard (p&nbsp;&lt;&nbsp;0.01). GPT-4 passed in three years, barely meeting the minimum 
      threshold in two. GPT-4 demonstrated significantly higher performance in 
      problem-solving, clinical, and non-image questions than GPT-3.5 and Bard. GPT-4's 
      performance was between third- and fourth-year nephrology residents. CONCLUSIONS: 
      GPT-4 outperformed GPT-3.5 and Bard and met the Nephrology Board renewal 
      standards in specific years, albeit marginally. These results highlight LLMs' 
      potential and limitations in nephrology. As LLMs advance, nephrologists should 
      understand their performance for future applications.
CI  - © 2024. The Author(s), under exclusive licence to Japanese Society of Nephrology.
FAU - Noda, Ryunosuke
AU  - Noda R
AUID- ORCID: 0000-0002-5472-3277
AD  - Division of Nephrology and Hypertension, Department of Internal Medicine, St. 
      Marianna University School of Medicine, 2-16-1 Sugao, Miyamae-Ku, Kawasaki, 
      Kanagawa, 216-8511, Japan. nodaryu00@gmail.com.
FAU - Izaki, Yuto
AU  - Izaki Y
AD  - Division of Nephrology and Hypertension, Department of Internal Medicine, St. 
      Marianna University School of Medicine, 2-16-1 Sugao, Miyamae-Ku, Kawasaki, 
      Kanagawa, 216-8511, Japan.
FAU - Kitano, Fumiya
AU  - Kitano F
AD  - Division of Nephrology and Hypertension, Department of Internal Medicine, St. 
      Marianna University School of Medicine, 2-16-1 Sugao, Miyamae-Ku, Kawasaki, 
      Kanagawa, 216-8511, Japan.
FAU - Komatsu, Jun
AU  - Komatsu J
AD  - Division of Nephrology and Hypertension, Department of Internal Medicine, St. 
      Marianna University School of Medicine, 2-16-1 Sugao, Miyamae-Ku, Kawasaki, 
      Kanagawa, 216-8511, Japan.
FAU - Ichikawa, Daisuke
AU  - Ichikawa D
AD  - Division of Nephrology and Hypertension, Department of Internal Medicine, St. 
      Marianna University School of Medicine, 2-16-1 Sugao, Miyamae-Ku, Kawasaki, 
      Kanagawa, 216-8511, Japan.
FAU - Shibagaki, Yugo
AU  - Shibagaki Y
AD  - Division of Nephrology and Hypertension, Department of Internal Medicine, St. 
      Marianna University School of Medicine, 2-16-1 Sugao, Miyamae-Ku, Kawasaki, 
      Kanagawa, 216-8511, Japan.
LA  - eng
PT  - Journal Article
DEP - 20240214
PL  - Japan
TA  - Clin Exp Nephrol
JT  - Clinical and experimental nephrology
JID - 9709923
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - GPT-4
OT  - Large language models
OT  - Nephrology
EDAT- 2024/02/14 12:45
MHDA- 2024/02/14 12:45
CRDT- 2024/02/14 11:08
PHST- 2023/06/15 00:00 [received]
PHST- 2023/12/25 00:00 [accepted]
PHST- 2024/02/14 12:45 [medline]
PHST- 2024/02/14 12:45 [pubmed]
PHST- 2024/02/14 11:08 [entrez]
AID - 10.1007/s10157-023-02451-w [pii]
AID - 10.1007/s10157-023-02451-w [doi]
PST - aheadofprint
SO  - Clin Exp Nephrol. 2024 Feb 14. doi: 10.1007/s10157-023-02451-w.

PMID- 37523010
OWN - NLM
STAT- MEDLINE
DCOM- 20230831
LR  - 20231215
IS  - 1435-1544 (Electronic)
IS  - 0938-7412 (Print)
IS  - 0938-7412 (Linking)
VI  - 34
IP  - 3
DP  - 2023 Sep
TI  - [Big hype about ChapGPT in medicine : Is it something for rhythmologists? What 
      must be taken into consideration?].
PG  - 240-245
LID - 10.1007/s00399-023-00960-5 [doi]
AB  - ChatGPT, a&nbsp;chatbot based on a&nbsp;large language model, is currently attracting much 
      attention. Modern machine learning (ML) architectures enable the program to 
      answer almost any question, to summarize, translate, and even generate its own 
      texts, all in a&nbsp;text-based dialogue with the user. Underlying technologies, 
      summarized under the acronym NLP (natural language processing), go back to the 
      1960s. In almost all areas including medicine, ChatGPT is raising enormous hopes. 
      It can easily pass medical exams and may be useful in patient care, diagnostic 
      and therapeutic assistance, and medical research. The enthusiasm for this new 
      technology shown even by medical professionals is surprising. Although the system 
      knows much, it does not know everything; not everything it outputs is accurate 
      either. Every output has to be carefully checked by the user for correctness, 
      which is often not easily done since references to sources are lacking. Issues 
      regarding data protection and ethics also arise. Today's language models are not 
      free of bias and systematic distortion. These shortcomings have led to calls for 
      stronger regulation of the use of ChatGPT and an increasing number of similar 
      language models. However, this new technology represents an enormous progress in 
      knowledge processing and dissemination. Numerous scenarios in which ChatGPT can 
      provide assistance are conceivable, including in rhythmology. In the future, it 
      will be crucial to render the models error-free and transparent and to clearly 
      define the rules for their use. Responsible use requires systematic training to 
      improve the digital competence of users, including physicians who use such 
      programs.
CI  - © 2023. The Author(s).
FAU - Haverkamp, W
AU  - Haverkamp W
AD  - Abteilung für Kardiologie und Metabolismus, Medizinische Klinik mit Schwerpunkt 
      Kardiologie, Campus Virchow-Klinikum, Deutsches Herzzentrum der Charité, 
      Charité&nbsp;- Universitätsmedizin Berlin, Augustenburger Platz&nbsp;1, 13353, Berlin, 
      Deutschland. wilhelm.haverkamp@dhzc-charite.de.
AD  - Berlin Institute of Health Center for Regenerative Therapies (BCRT), Berlin, 
      Deutschland. wilhelm.haverkamp@dhzc-charite.de.
FAU - Strodthoff, N
AU  - Strodthoff N
AD  - Department für Versorgungsforschung, Fakultät VI&nbsp;- Medizin und 
      Gesundheitswissenschaften, Abteilung AI4Health, Universität Oldenburg, Oldenburg, 
      Deutschland.
FAU - Tennenbaum, J
AU  - Tennenbaum J
AD  - Center for the Philosophy of Science, University of Lisbon, Lisbon, Portugal.
FAU - Israel, C
AU  - Israel C
AD  - Klinik für Innere Medizin&nbsp;- Kardiologie, Diabetologie und Nephrologie, 
      Evangelisches Klinikum Bethel, Bielefeld, Deutschland.
LA  - ger
PT  - English Abstract
PT  - Journal Article
PT  - Review
TT  - Großer Hype um ChatGPT in der Medizin : Ist es etwas für den Rhythmologen? Was 
      muss man bedenken?
DEP - 20230731
PL  - Germany
TA  - Herzschrittmacherther Elektrophysiol
JT  - Herzschrittmachertherapie &amp; Elektrophysiologie
JID - 9425873
SB  - IM
MH  - Humans
MH  - *Software
MH  - *Artificial Intelligence
MH  - *Medicine
PMC - PMC10462516
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Chat-GPT
OT  - Language models
OT  - Natural language processing
EDAT- 2023/07/31 13:09
MHDA- 2023/08/29 12:43
PMCR- 2023/07/31
CRDT- 2023/07/31 11:09
PHST- 2023/06/01 00:00 [received]
PHST- 2023/07/20 00:00 [accepted]
PHST- 2023/08/29 12:43 [medline]
PHST- 2023/07/31 13:09 [pubmed]
PHST- 2023/07/31 11:09 [entrez]
PHST- 2023/07/31 00:00 [pmc-release]
AID - 10.1007/s00399-023-00960-5 [pii]
AID - 960 [pii]
AID - 10.1007/s00399-023-00960-5 [doi]
PST - ppublish
SO  - Herzschrittmacherther Elektrophysiol. 2023 Sep;34(3):240-245. doi: 
      10.1007/s00399-023-00960-5. Epub 2023 Jul 31.

PMID- 37378876
OWN - NLM
STAT- MEDLINE
DCOM- 20240125
LR  - 20240206
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 52
IP  - 2
DP  - 2024 Feb
TI  - The Role and Potential Contributions of the Artificial Intelligence Language 
      Model ChatGPT.
PG  - 130-133
LID - 10.1007/s10439-023-03296-w [doi]
AB  - This discussion paper aims to examine the potential benefits and limitations of 
      using artificial intelligence (AI) chatbots in nursing practice, with a 
      particular focus on the ChatGPT example. The study discusses how chatbots can 
      serve as a valuable tool for nurses' continuing education, consultation, and 
      information access. It is suggested that ChatGPT can contribute to enhancing 
      nurses' knowledge and skill levels, providing rapid and accurate information, and 
      improving time management. However, the potential risks and limitations of using 
      AI chatbots have also been evaluated. The study highlights the possibility of 
      negative impacts on the nurse-patient relationship due to chatbots' inadequacy in 
      emotional and empathetic communication. Additionally, concerns about chatbots 
      providing inaccurate or biased information and issues regarding data privacy are 
      addressed. The review draws attention to the limited existing literature on the 
      use of AI chatbots in nursing and emphasizes the need for expanding research in 
      this area. Future studies are suggested to focus on identifying the necessary 
      training and support resources for nurses to effectively utilize this technology. 
      This study underscores an important ethical and professional point for nurses, 
      reminding them not to overlook the significance of human touch and emotional 
      connection while evaluating the advantages offered by technology.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Berşe, Soner
AU  - Berşe S
AUID- ORCID: 0000-0001-9108-3216
AD  - Faculty of Health Sciences, Gaziantep University, Gaziantep, Turkey. 
      sonerberse@gmail.com.
FAU - Akça, Kamile
AU  - Akça K
AUID- ORCID: 0000-0002-2833-8754
AD  - Faculty of Health Sciences, Gaziantep İslam Science and Technology University, 
      Gaziantep, Turkey.
FAU - Dirgar, Ezgi
AU  - Dirgar E
AUID- ORCID: 0000-0001-8214-7441
AD  - Faculty of Health Sciences, Gaziantep University, Gaziantep, Turkey.
FAU - Kaplan Serin, Emine
AU  - Kaplan Serin E
AUID- ORCID: 0000-0002-7327-9167
AD  - Faculty of Health Sciences, Mersin University, Mersin, Turkey. 
      emine_3354@hotmail.com.
LA  - eng
PT  - Letter
DEP - 20230628
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Clinical Competence
MH  - Language
MH  - Technology
MH  - Touch
OTO - NOTNLM
OT  - Advantages
OT  - Artificial intelligence
OT  - ChatGPT
OT  - Chatbots
OT  - Disadvantages
OT  - Nurse–patient relationship
OT  - Nursing practice
EDAT- 2023/06/28 13:09
MHDA- 2024/01/25 06:43
CRDT- 2023/06/28 11:15
PHST- 2023/06/14 00:00 [received]
PHST- 2023/06/21 00:00 [accepted]
PHST- 2024/01/25 06:43 [medline]
PHST- 2023/06/28 13:09 [pubmed]
PHST- 2023/06/28 11:15 [entrez]
AID - 10.1007/s10439-023-03296-w [pii]
AID - 10.1007/s10439-023-03296-w [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2024 Feb;52(2):130-133. doi: 10.1007/s10439-023-03296-w. Epub 
      2023 Jun 28.

PMID- 38095605
OWN - NLM
STAT- Publisher
LR  - 20231214
IS  - 1437-4331 (Electronic)
IS  - 1434-6621 (Linking)
DP  - 2023 Dec 14
TI  - Comparison of three chatbots as an assistant for problem-solving in clinical 
      laboratory.
LID - 10.1515/cclm-2023-1058 [doi]
AB  - OBJECTIVES: Data generation in clinical settings is ongoing and perpetually 
      increasing. Artificial intelligence (AI) software may help detect data-related 
      errors or facilitate process management. The aim of the present study was to test 
      the extent to which the frequently encountered pre-analytical, analytical, and 
      postanalytical errors in clinical laboratories, and likely clinical diagnoses can 
      be detected through the use of a chatbot. METHODS: A total of 20 case scenarios, 
      20 multiple-choice, and 20 direct questions related to errors observed in 
      pre-analytical, analytical, and postanalytical processes were developed in 
      English. Difficulty assessment was performed for the 60 questions. Responses by 4 
      chatbots to the questions were scored in a blinded manner by 3 independent 
      laboratory experts for accuracy, usefulness, and completeness. RESULTS: According 
      to Chi-squared test, accuracy score of ChatGPT-3.5 (54.4 %) was significantly 
      lower than CopyAI (86.7 %) (p=0.0269) and ChatGPT v4.0. (88.9 %) (p=0.0168), 
      respectively in cases. In direct questions, there was no significant difference 
      between ChatGPT-3.5 (67.8 %) and WriteSonic (69.4 %), ChatGPT v4.0. (78.9 %) and 
      CopyAI (73.9 %) (p=0.914, p=0.433 and p=0.675, respectively) accuracy scores. 
      CopyAI (90.6 %) presented significantly better performance compared to 
      ChatGPT-3.5 (62.2 %) (p=0.036) in multiple choice questions. CONCLUSIONS: These 
      applications presented considerable performance to find out the cases and reply 
      to questions. In the future, the use of AI applications is likely to increase in 
      clinical settings if trained and validated by technical and medical experts 
      within a structural framework.
CI  - © 2023 Walter de Gruyter GmbH, Berlin/Boston.
FAU - Abusoglu, Sedat
AU  - Abusoglu S
AUID- ORCID: 0000-0002-2984-0527
AD  - Department of Biochemistry, Selcuk University Faculty of Medicine, Konya, 
      Türkiye.
FAU - Serdar, Muhittin
AU  - Serdar M
AUID- ORCID: 0000-0002-3014-748X
AD  - Department of Biochemistry, Acıbadem Mehmet Ali Aydinlar University Faculty of 
      Medicine, İstanbul, Türkiye.
FAU - Unlu, Ali
AU  - Unlu A
AUID- ORCID: 0000-0002-9991-3939
AD  - Department of Biochemistry, Selcuk University Faculty of Medicine, Konya, 
      Türkiye.
FAU - Abusoglu, Gulsum
AU  - Abusoglu G
AUID- ORCID: 0000-0003-1630-1257
AD  - Department of Medical Laboratory Techniques, Selcuk University Vocational School 
      of Medicine, Konya, Türkiye.
LA  - eng
PT  - Journal Article
DEP - 20231214
PL  - Germany
TA  - Clin Chem Lab Med
JT  - Clinical chemistry and laboratory medicine
JID - 9806306
SB  - IM
OTO - NOTNLM
OT  - artificial intelligence
OT  - assistant
OT  - clinical laboratory
OT  - machine learning
EDAT- 2023/12/14 12:43
MHDA- 2023/12/14 12:43
CRDT- 2023/12/14 10:24
PHST- 2023/09/22 00:00 [received]
PHST- 2023/12/05 00:00 [accepted]
PHST- 2023/12/14 12:43 [medline]
PHST- 2023/12/14 12:43 [pubmed]
PHST- 2023/12/14 10:24 [entrez]
AID - cclm-2023-1058 [pii]
AID - 10.1515/cclm-2023-1058 [doi]
PST - aheadofprint
SO  - Clin Chem Lab Med. 2023 Dec 14. doi: 10.1515/cclm-2023-1058.

PMID- 37572695
OWN - NLM
STAT- Publisher
LR  - 20230813
IS  - 1715-3360 (Electronic)
IS  - 0008-4182 (Linking)
DP  - 2023 Aug 9
TI  - Artificial intelligence chatbot performance in triage of ophthalmic conditions.
LID - S0008-4182(23)00234-X [pii]
LID - 10.1016/j.jcjo.2023.07.016 [doi]
AB  - BACKGROUND: Timely access to human expertise for affordable and efficient triage 
      of ophthalmic conditions is inconsistent. With recent advancements in publicly 
      available artificial intelligence (AI) chatbots, the lay public may turn to these 
      tools for triage of ophthalmic complaints. Validation studies are necessary to 
      evaluate the performance of AI chatbots as triage tools and inform the public 
      regarding their safety. OBJECTIVE: To evaluate the triage performance of AI 
      chatbots for ophthalmic conditions. DESIGN: Cross-sectional study. SETTING: 
      Single centre. PARTICIPANTS: Ophthalmology trainees, OpenAI ChatGPT (GPT-4), Bing 
      Chat, and WebMD Symptom Checker. METHODS: Forty-four clinical vignettes 
      representing common ophthalmic complaints were developed, and a standardized 
      pathway of prompts was presented to each tool in March 2023. Primary outcomes 
      were proportion of responses with the correct diagnosis listed in the top 3 
      possible diagnoses and proportion with correct triage urgency. Ancillary outcomes 
      included presence of grossly inaccurate statements, mean reading grade level, 
      mean response word count, proportion with attribution, and most common sources 
      cited. RESULTS: The ophthalmologists in training, ChatGPT, Bing Chat, and the 
      WebMD Symptom Checker listed the appropriate diagnosis among the top 3 
      suggestions in 42 (95%), 41 (93%), 34 (77%), and 8 (33%) cases, respectively. 
      Triage urgency was appropriate in 38 (86%), 43 (98%), and 37 (84%) cases for 
      ophthalmology trainees, ChatGPT, and Bing Chat, correspondingly. CONCLUSIONS: 
      ChatGPT using the GPT-4 model offered high diagnostic and triage accuracy that 
      was comparable with that of ophthalmology trainees with no grossly inaccurate 
      statements. Bing Chat had lower accuracy and a tendency to overestimate triage 
      urgency.
CI  - Copyright © 2023 Canadian Ophthalmological Society. Published by Elsevier Inc. 
      All rights reserved.
FAU - Lyons, Riley J
AU  - Lyons RJ
AD  - Department of Ophthalmology, Emory University School of Medicine, Atlanta, GA.
FAU - Arepalli, Sruthi R
AU  - Arepalli SR
AD  - Department of Ophthalmology, Emory University School of Medicine, Atlanta, GA.
FAU - Fromal, Ollya
AU  - Fromal O
AD  - Department of Ophthalmology, Emory University School of Medicine, Atlanta, GA.
FAU - Choi, Jinho D
AU  - Choi JD
AD  - Department of Computer Science, Emory University, Atlanta, GA.
FAU - Jain, Nieraj
AU  - Jain N
AD  - Department of Ophthalmology, Emory University School of Medicine, Atlanta, GA. 
      Electronic address: nieraj.jain@emory.edu.
LA  - eng
PT  - Journal Article
DEP - 20230809
PL  - England
TA  - Can J Ophthalmol
JT  - Canadian journal of ophthalmology. Journal canadien d'ophtalmologie
JID - 0045312
SB  - IM
EDAT- 2023/08/13 00:43
MHDA- 2023/08/13 00:43
CRDT- 2023/08/12 19:21
PHST- 2023/06/02 00:00 [received]
PHST- 2023/07/06 00:00 [revised]
PHST- 2023/07/21 00:00 [accepted]
PHST- 2023/08/13 00:43 [pubmed]
PHST- 2023/08/13 00:43 [medline]
PHST- 2023/08/12 19:21 [entrez]
AID - S0008-4182(23)00234-X [pii]
AID - 10.1016/j.jcjo.2023.07.016 [doi]
PST - aheadofprint
SO  - Can J Ophthalmol. 2023 Aug 9:S0008-4182(23)00234-X. doi: 
      10.1016/j.jcjo.2023.07.016.

PMID- 37379558
OWN - NLM
STAT- PubMed-not-MEDLINE
DCOM- 20230707
LR  - 20230717
IS  - 1669-9106 (Electronic)
IS  - 0025-7680 (Linking)
VI  - 83
IP  - 3
DP  - 2023
TI  - [ChatGPT artificial intelligence and its usefulness in research: the future is 
      already here].
PG  - 499-501
FAU - Burgos, Lucrecia M
AU  - Burgos LM
AD  - Instituto Cardiovascular de Buenos Aires, Buenos Aires, Argentina. E-mail: 
      lburgos@icba.com.ar.
FAU - Suárez, Lucas L
AU  - Suárez LL
AD  - Instituto Cardiovascular de Buenos Aires, Buenos Aires, Argentina.
FAU - Benzadón, Mariano
AU  - Benzadón M
AD  - Instituto Cardiovascular de Buenos Aires, Buenos Aires, Argentina.
LA  - spa
PT  - Letter
TT  - Inteligencia artificial ChatGPT y su utilidad en la investigación: el futuro ya 
      está aquí.
PL  - Argentina
TA  - Medicina (B Aires)
JT  - Medicina
JID - 0204271
SB  - IM
EDAT- 2023/06/28 19:14
MHDA- 2023/06/30 06:42
CRDT- 2023/06/28 17:03
PHST- 2023/06/30 06:42 [medline]
PHST- 2023/06/28 19:14 [pubmed]
PHST- 2023/06/28 17:03 [entrez]
PST - ppublish
SO  - Medicina (B Aires). 2023;83(3):499-501.

PMID- 37904927
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240210
DP  - 2023 Oct 17
TI  - Bioinformatics Illustrations Decoded by ChatGPT: The Good, The Bad, and The Ugly.
LID - 2023.10.15.562423 [pii]
LID - 10.1101/2023.10.15.562423 [doi]
AB  - Emerging studies underscore the promising capabilities of large language 
      model-based chatbots in conducting fundamental bioinformatics data analyses. The 
      recent feature of accepting image-inputs by ChatGPT motivated us to explore its 
      efficacy in deciphering bioinformatics illustrations. Our evaluation with 
      examples in cancer research, including sequencing data analysis, multimodal 
      network-based drug repositioning, and tumor clonal evolution, revealed that 
      ChatGPT can proficiently explain different plot types and apply biological 
      knowledge to enrich interpretations. However, it struggled to provide accurate 
      interpretations when quantitative analysis of visual elements was involved. 
      Furthermore, while the chatbot can draft figure legends and summarize findings 
      from the figures, stringent proofreading is imperative to ensure the accuracy and 
      reliability of the content.
FAU - Wang, Jinge
AU  - Wang J
AD  - Department of Microbiology, Immunology &amp; Cell Biology, West Virginia University, 
      Morgantown, WV 26506, USA.
FAU - Ye, Qing
AU  - Ye Q
AD  - West Virginia University Cancer Institute, West Virginia University, Morgantown, 
      WV 26506, USA.
FAU - Liu, Li
AU  - Liu L
AD  - College of Health Solutions, Arizona State University, Phoenix, AZ 85004, USA.
AD  - Biodesign Institute, Arizona State University, Tempe, AZ, 85281 USA.
FAU - Lan Guo, Nancy
AU  - Lan Guo N
AD  - West Virginia University Cancer Institute, West Virginia University, Morgantown, 
      WV 26506, USA.
AD  - Department of Occupational and Environmental Health Sciences, West Virginia 
      University, Morgantown, WV 26506, USA.
FAU - Hu, Gangqing
AU  - Hu G
AD  - Department of Microbiology, Immunology &amp; Cell Biology, West Virginia University, 
      Morgantown, WV 26506, USA.
AD  - West Virginia University Cancer Institute, West Virginia University, Morgantown, 
      WV 26506, USA.
LA  - eng
GR  - P20 GM103434/GM/NIGMS NIH HHS/United States
GR  - P20 GM121322/GM/NIGMS NIH HHS/United States
GR  - R01 LM013438/LM/NLM NIH HHS/United States
GR  - U54 GM104942/GM/NIGMS NIH HHS/United States
PT  - Preprint
DEP - 20231017
PL  - United States
TA  - bioRxiv
JT  - bioRxiv : the preprint server for biology
JID - 101680187
PMC - PMC10614796
COIS- Competing Interests The authors declared no competing interests.
EDAT- 2023/10/31 06:42
MHDA- 2023/10/31 06:43
PMCR- 2023/10/30
CRDT- 2023/10/31 03:59
PHST- 2023/10/31 06:42 [pubmed]
PHST- 2023/10/31 06:43 [medline]
PHST- 2023/10/31 03:59 [entrez]
PHST- 2023/10/30 00:00 [pmc-release]
AID - 2023.10.15.562423 [pii]
AID - 10.1101/2023.10.15.562423 [doi]
PST - epublish
SO  - bioRxiv [Preprint]. 2023 Oct 17:2023.10.15.562423. doi: 
      10.1101/2023.10.15.562423.

PMID- 37548259
OWN - NLM
STAT- MEDLINE
DCOM- 20230808
LR  - 20230828
IS  - 2001-1326 (Electronic)
IS  - 2001-1326 (Linking)
VI  - 13
IP  - 8
DP  - 2023 Aug
TI  - A regulatory challenge for natural language processing (NLP)-based tools such as 
      ChatGPT to be legally used for healthcare decisions. Where are we now?
PG  - e1362
LID - 10.1002/ctm2.1362 [doi]
LID - e1362
FAU - Baumgartner, Christian
AU  - Baumgartner C
AUID- ORCID: 0000-0002-3763-5195
AD  - Institute of Health Care Engineering with European Testing Center of Medical 
      Devices, Graz University of Technology, Graz, Austria.
FAU - Baumgartner, Daniela
AU  - Baumgartner D
AUID- ORCID: 0000-0001-8141-2426
AD  - Clinical Division of Pediatric Cardiology, Department of Pediatrics and 
      Adolescent Medicine, Medical University of Graz, Graz, Austria.
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Clin Transl Med
JT  - Clinical and translational medicine
JID - 101597971
SB  - IM
MH  - *Natural Language Processing
MH  - *Artificial Intelligence
MH  - *Clinical Decision-Making
PMC - PMC10405238
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial intelligence
OT  - healthcare decisions
OT  - natural language processing (NLP)
OT  - regulatory approval
OT  - software as medical device (SaMD)
COIS- The authors declare that they have no conflict of interest.
EDAT- 2023/08/07 13:10
MHDA- 2023/08/08 06:42
PMCR- 2023/08/07
CRDT- 2023/08/07 06:54
PHST- 2023/07/24 00:00 [received]
PHST- 2023/07/29 00:00 [accepted]
PHST- 2023/08/08 06:42 [medline]
PHST- 2023/08/07 13:10 [pubmed]
PHST- 2023/08/07 06:54 [entrez]
PHST- 2023/08/07 00:00 [pmc-release]
AID - CTM21362 [pii]
AID - 10.1002/ctm2.1362 [doi]
PST - ppublish
SO  - Clin Transl Med. 2023 Aug;13(8):e1362. doi: 10.1002/ctm2.1362.

PMID- 37831553
OWN - NLM
STAT- Publisher
LR  - 20231013
IS  - 1744-5078 (Electronic)
IS  - 0927-3948 (Linking)
DP  - 2023 Oct 13
TI  - Chatbots Vs. Human Experts: Evaluating Diagnostic Performance of Chatbots in 
      Uveitis and the Perspectives on AI Adoption in Ophthalmology.
PG  - 1-8
LID - 10.1080/09273948.2023.2266730 [doi]
AB  - PURPOSE: To assess the diagnostic performance of two chatbots, ChatGPT and Glass, 
      in uveitis diagnosis compared to renowned uveitis specialists, and evaluate 
      clinicians' perception about utilizing artificial intelligence (AI) in 
      ophthalmology practice. METHODS: Six cases were presented to uveitis experts, 
      ChatGPT (version 3.5 and 4.0) and Glass 1.0, and diagnostic accuracy was 
      analyzed. Additionally, a survey about the emotions, confidence in utilizing 
      AI-based tools, and the likelihood of incorporating such tools in clinical 
      practice was done. RESULTS: Uveitis experts accurately diagnosed all cases 
      (100%), while ChatGPT achieved a diagnostic success rate of 66% and Glass 1.0 
      achieved 33%. Most attendees felt excited or optimistic about utilizing AI in 
      ophthalmology practice. Older age and high level of education were positively 
      correlated with increased inclination to adopt AI-based tools. CONCLUSIONS: 
      ChatGPT demonstrated promising diagnostic capabilities in uveitis cases and 
      ophthalmologist showed enthusiasm for the integration of AI into clinical 
      practice.
FAU - Rojas-Carabali, William
AU  - Rojas-Carabali W
AD  - Lee Kong Chiang School of Medicine, Nanyang Technological University, Singapore, 
      Singapore.
FAU - Sen, Alok
AU  - Sen A
AD  - Retina and Uvea Services, Sadguru Netra Chikitsalaya, Chitrakoot, India.
FAU - Agarwal, Aniruddha
AU  - Agarwal A
AD  - Eye Institute, Cleveland Clinic, Abu Dhabi, UAE.
AD  - Cleveland Clinic Lerner College of Medicine, Case Western Reserve University, 
      Cleveland, Ohio, USA.
AD  - Department of Ophthalmology, Maastricht University Medical Center, Maastricht, 
      The Netherlands.
FAU - Tan, Gavin
AU  - Tan G
AD  - Singapore Eye Research Institute, Singapore National Eye Centre, Singapore, 
      Singapore.
AD  - Duke-NUS Medical School, Singapore, Singapore.
FAU - Cheung, Carol Y
AU  - Cheung CY
AD  - Department of Ophthalmology and Visual Sciences, Faculty of Medicine, The Chinese 
      University of Hong Kong, Hong Kong, China.
FAU - Rousselot, Andres
AU  - Rousselot A
AD  - Department of Ophthalmology, Universidad del Salvador, Buenos Aires, Argentina.
FAU - Agrawal, Rajdeep
AU  - Agrawal R
AD  - Lee Kong Chiang School of Medicine, Nanyang Technological University, Singapore, 
      Singapore.
FAU - Liu, Renee
AU  - Liu R
AD  - Department of Ophthalmology and Schepens Eye Research Institute,Harvard Medical 
      School, Massachusetts Eye and Ear Infirmary, Boston, Massachusetts, USA.
FAU - Cifuentes-González, Carlos
AU  - Cifuentes-González C
AUID- ORCID: 0000-0002-2703-0977
AD  - Neuroscience Research Group (NEUROS), Neurovitae Center for Neuroscience, 
      Institute of Translational Medicine (IMT), Escuela de Medicina y Ciencias de la 
      Salud, Universidad del Rosario, Bogotá, Colombia.
FAU - Elze, Tobias
AU  - Elze T
AD  - Department of Ophthalmology and Schepens Eye Research Institute,Harvard Medical 
      School, Massachusetts Eye and Ear Infirmary, Boston, Massachusetts, USA.
FAU - Kempen, John H
AU  - Kempen JH
AUID- ORCID: 0000-0002-2967-4792
AD  - Department of Ophthalmology and Schepens Eye Research Institute,Harvard Medical 
      School, Massachusetts Eye and Ear Infirmary, Boston, Massachusetts, USA.
AD  - Sight for Souls, Bellevue, Washington, USA.
AD  - Department of Ophthalmology, Addis Ababa University School of Medicine, Addis 
      Ababa, Ethiopia.
AD  - Department of Ophthalmology, Myungsung Medical College/MCM Comprehensive 
      Specialized Hospital, Addis Abeba, Ethiopia.
FAU - Sobrin, Lucia
AU  - Sobrin L
AUID- ORCID: 0000-0003-1575-0819
AD  - Department of Ophthalmology and Schepens Eye Research Institute,Harvard Medical 
      School, Massachusetts Eye and Ear Infirmary, Boston, Massachusetts, USA.
FAU - Nguyen, Quan Dong
AU  - Nguyen QD
AD  - Byers Eye Institute, Stanford University, Palo Alto, California, USA.
FAU - de-la-Torre, Alejandra
AU  - de-la-Torre A
AUID- ORCID: 0000-0003-0684-1989
AD  - Neuroscience Research Group (NEUROS), Neurovitae Center for Neuroscience, 
      Institute of Translational Medicine (IMT), Escuela de Medicina y Ciencias de la 
      Salud, Universidad del Rosario, Bogotá, Colombia.
FAU - Lee, Bernett
AU  - Lee B
AD  - Lee Kong Chiang School of Medicine, Nanyang Technological University, Singapore, 
      Singapore.
FAU - Gupta, Vishali
AU  - Gupta V
AD  - Post Graduate Institute of Medical Education and Research (PGIMER), Advance Eye 
      Centre, Chandigarh, India.
FAU - Agrawal, Rupesh
AU  - Agrawal R
AD  - Lee Kong Chiang School of Medicine, Nanyang Technological University, Singapore, 
      Singapore.
AD  - Duke-NUS Medical School, Singapore, Singapore.
AD  - Moorfields Eye Hospital, NHS Foundation Trust, London, UK.
AD  - Tan Tock Seng Hospital, National Healthcare Group, Singapore, Singapore.
LA  - eng
PT  - Journal Article
DEP - 20231013
PL  - England
TA  - Ocul Immunol Inflamm
JT  - Ocular immunology and inflammation
JID - 9312169
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - ChatGPT
OT  - diagnosis
OT  - large language model
OT  - ophthalmology
EDAT- 2023/10/13 18:43
MHDA- 2023/10/13 18:43
CRDT- 2023/10/13 12:23
PHST- 2023/10/13 18:43 [medline]
PHST- 2023/10/13 18:43 [pubmed]
PHST- 2023/10/13 12:23 [entrez]
AID - 10.1080/09273948.2023.2266730 [doi]
PST - aheadofprint
SO  - Ocul Immunol Inflamm. 2023 Oct 13:1-8. doi: 10.1080/09273948.2023.2266730.

PMID- 37743152
OWN - NLM
STAT- Publisher
LR  - 20230924
IS  - 2603-6479 (Electronic)
IS  - 2603-6479 (Linking)
DP  - 2023 Sep 22
TI  - [Considerations for using ChatGPT in medical practice].
LID - S2603-6479(23)00049-0 [pii]
LID - 10.1016/j.jhqr.2023.09.002 [doi]
FAU - Iglesias-Puzas, Á
AU  - Iglesias-Puzas Á
AD  - Servicio de Dermatología, Hospital Universitario Clínico San Carlos, Universidad 
      Complutense, Madrid, España. Electronic address: alvaroigpu@gmail.com.
FAU - Conde-Taboada, A
AU  - Conde-Taboada A
AD  - Servicio de Dermatología, Hospital Universitario Clínico San Carlos, Universidad 
      Complutense, Madrid, España.
FAU - López-Bran, E
AU  - López-Bran E
AD  - Servicio de Dermatología, Hospital Universitario Clínico San Carlos, Universidad 
      Complutense, Madrid, España.
LA  - spa
PT  - Letter
TT  - Consideraciones sobre el uso de ChatGPT en la práctica médica.
DEP - 20230922
PL  - Spain
TA  - J Healthc Qual Res
JT  - Journal of healthcare quality research
JID - 101735273
SB  - IM
EDAT- 2023/09/25 00:42
MHDA- 2023/09/25 00:42
CRDT- 2023/09/24 21:55
PHST- 2023/08/22 00:00 [received]
PHST- 2023/09/01 00:00 [accepted]
PHST- 2023/09/25 00:42 [medline]
PHST- 2023/09/25 00:42 [pubmed]
PHST- 2023/09/24 21:55 [entrez]
AID - S2603-6479(23)00049-0 [pii]
AID - 10.1016/j.jhqr.2023.09.002 [doi]
PST - aheadofprint
SO  - J Healthc Qual Res. 2023 Sep 22:S2603-6479(23)00049-0. doi: 
      10.1016/j.jhqr.2023.09.002.

PMID- 38509182
OWN - NLM
STAT- Publisher
LR  - 20240321
IS  - 1476-5454 (Electronic)
IS  - 0950-222X (Linking)
DP  - 2024 Mar 20
TI  - ChatGPT-3.5 and Bing Chat in ophthalmology: an updated evaluation of performance, 
      readability, and informative sources.
LID - 10.1038/s41433-024-03037-w [doi]
AB  - BACKGROUND/OBJECTIVES: Experimental investigation. Bing Chat (Microsoft) 
      integration with ChatGPT-4 (OpenAI) integration has conferred the capability of 
      accessing online data past 2021. We investigate its performance against 
      ChatGPT-3.5 on a multiple-choice question ophthalmology exam. SUBJECTS/METHODS: 
      In August 2023, ChatGPT-3.5 and Bing Chat were evaluated against 913 questions 
      derived from the Academy's Basic and Clinical Science Collection collection. For 
      each response, the sub-topic, performance, Simple Measure of Gobbledygook 
      readability score (measuring years of required education to understand a given 
      passage), and cited resources were collected. The primary outcomes were the 
      comparative scores between models, and qualitatively, the resources referenced by 
      Bing Chat. Secondary outcomes included performance stratified by response 
      readability, question type (explicit or situational), and BCSC sub-topic. 
      RESULTS: Across 913 questions, ChatGPT-3.5 scored 59.69% [95% CI 56.45,62.94] 
      while Bing Chat scored 73.60% [95% CI 70.69,76.52]. Both models performed 
      significantly better in explicit than clinical reasoning questions. Both models 
      performed best on general medicine questions than ophthalmology subsections. Bing 
      Chat referenced 927 online entities and provided at-least one citation to 836 of 
      the 913 questions. The use of more reliable (peer-reviewed) sources was 
      associated with higher likelihood of correct response. The most-cited resources 
      were eyewiki.aao.org, aao.org, wikipedia.org, and ncbi.nlm.nih.gov. Bing Chat 
      showed significantly better readability than ChatGPT-3.5, averaging a reading 
      level of grade 11.4 [95% CI 7.14, 15.7] versus 12.4 [95% CI 8.77, 16.1], 
      respectively (p-value &lt; 0.0001, ρ = 0.25). CONCLUSIONS: The online access, 
      improved readability, and citation feature of Bing Chat confers additional 
      utility for ophthalmology learners. We recommend critical appraisal of cited 
      sources during response interpretation.
CI  - © 2024. The Author(s), under exclusive licence to The Royal College of 
      Ophthalmologists.
FAU - Tao, Brendan Ka-Lok
AU  - Tao BK
AUID- ORCID: 0000-0001-7069-3162
AD  - Faculty of Medicine, The University of British Columbia, 317-2194 Health Sciences 
      Mall, Vancouver, BC, V6T 1Z3, Canada.
FAU - Hua, Nicholas
AU  - Hua N
AD  - Temerty Faculty of Medicine, University of Toronto, 1 King's College Circle, 
      Toronto, ON, M5S 1A8, Canada.
FAU - Milkovich, John
AU  - Milkovich J
AD  - Temerty Faculty of Medicine, University of Toronto, 1 King's College Circle, 
      Toronto, ON, M5S 1A8, Canada.
FAU - Micieli, Jonathan Andrew
AU  - Micieli JA
AUID- ORCID: 0000-0003-4911-9152
AD  - Temerty Faculty of Medicine, University of Toronto, 1 King's College Circle, 
      Toronto, ON, M5S 1A8, Canada. jonathanmicieli@gmail.com.
AD  - Department of Ophthalmology and Vision Sciences, University of Toronto, 340 
      College Street, Toronto, ON, M5T 3A9, Canada. jonathanmicieli@gmail.com.
AD  - Division of Neurology, Department of Medicine, University of Toronto, 6 Queen's 
      Park Crescent West, Toronto, ON, M5S 3H2, Canada. jonathanmicieli@gmail.com.
AD  - Kensington Vision and Research Center, 340 College Street, Toronto, ON, M5T 3A9, 
      Canada. jonathanmicieli@gmail.com.
AD  - St. Michael's Hospital, 36 Queen Street East, Toronto, ON, M5B 1W8, Canada. 
      jonathanmicieli@gmail.com.
AD  - Toronto Western Hospital, 399 Bathurst Street, Toronto, ON, M5T 2S8, Canada. 
      jonathanmicieli@gmail.com.
AD  - University Health Network, 190 Elizabeth Street, Toronto, ON, M5G 2C4, Canada. 
      jonathanmicieli@gmail.com.
LA  - eng
PT  - Journal Article
DEP - 20240320
PL  - England
TA  - Eye (Lond)
JT  - Eye (London, England)
JID - 8703986
SB  - IM
EDAT- 2024/03/21 06:43
MHDA- 2024/03/21 06:43
CRDT- 2024/03/21 00:22
PHST- 2023/08/26 00:00 [received]
PHST- 2024/03/14 00:00 [accepted]
PHST- 2024/03/04 00:00 [revised]
PHST- 2024/03/21 06:43 [medline]
PHST- 2024/03/21 06:43 [pubmed]
PHST- 2024/03/21 00:22 [entrez]
AID - 10.1038/s41433-024-03037-w [pii]
AID - 10.1038/s41433-024-03037-w [doi]
PST - aheadofprint
SO  - Eye (Lond). 2024 Mar 20. doi: 10.1038/s41433-024-03037-w.

PMID- 37949663
OWN - NLM
STAT- MEDLINE
DCOM- 20240229
LR  - 20240305
IS  - 1365-2125 (Electronic)
IS  - 0306-5251 (Linking)
VI  - 90
IP  - 3
DP  - 2024 Mar
TI  - Clinical decision-making in benzodiazepine deprescribing by healthcare providers 
      vs. AI-assisted approach.
PG  - 662-674
LID - 10.1111/bcp.15963 [doi]
AB  - AIMS: The aim of this study was to compare the clinical decision-making for 
      benzodiazepine deprescribing between a healthcare provider (HCP) and an 
      artificial intelligence (AI) chatbot GPT4 (ChatGPT-4). METHODS: We analysed 
      real-world data from a Croatian cohort of community-dwelling benzodiazepine 
      patients (n = 154) within the EuroAgeism H2020 ESR 7 project. HCPs evaluated the 
      data using pre-established deprescribing criteria to assess benzodiazepine 
      discontinuation potential. The research team devised and tested AI prompts to 
      ensure consistency with HCP judgements. An independent researcher employed 
      ChatGPT-4 with predetermined prompts to simulate clinical decisions for each 
      patient case. Data derived from human-HCP and ChatGPT-4 decisions were compared 
      for agreement rates and Cohen's kappa. RESULTS: Both HPC and ChatGPT identified 
      patients for benzodiazepine deprescribing (96.1% and 89.6%, respectively), 
      showing an agreement rate of 95% (κ = .200, P = .012). Agreement on four 
      deprescribing criteria ranged from 74.7% to 91.3% (lack of indication κ = .352, 
      P &lt; .001; prolonged use κ = .088, P = .280; safety concerns κ = .123, P = .006; 
      incorrect dosage κ = .264, P = .001). Important limitations of GPT-4 responses 
      were identified, including 22.1% ambiguous outputs, generic answers and 
      inaccuracies, posing inappropriate decision-making risks. CONCLUSIONS: While 
      AI-HCP agreement is substantial, sole AI reliance poses a risk for unsuitable 
      clinical decision-making. This study's findings reveal both strengths and areas 
      for enhancement of ChatGPT-4 in the deprescribing recommendations within a 
      real-world sample. Our study underscores the need for additional research on 
      chatbot functionality in patient therapy decision-making, further fostering the 
      advancement of AI for optimal performance.
CI  - © 2023 British Pharmacological Society.
FAU - Bužančić, Iva
AU  - Bužančić I
AD  - Center for Applied Pharmacy, Faculty of Pharmacy and Biochemistry, University of 
      Zagreb, Zagreb, Croatia.
AD  - City Pharmacy Zagreb, Zagreb, Croatia.
FAU - Belec, Dora
AU  - Belec D
AD  - Center for Applied Pharmacy, Faculty of Pharmacy and Biochemistry, University of 
      Zagreb, Zagreb, Croatia.
FAU - Držaić, Margita
AU  - Držaić M
AD  - Center for Applied Pharmacy, Faculty of Pharmacy and Biochemistry, University of 
      Zagreb, Zagreb, Croatia.
AD  - City Pharmacy Zagreb, Zagreb, Croatia.
FAU - Kummer, Ingrid
AU  - Kummer I
AD  - Department of Social and Clinical Pharmacy, Faculty of Pharmacy in Hradec 
      Králové, Charles University, Hradec Králové, Czech Republic.
FAU - Brkić, Jovana
AU  - Brkić J
AD  - Department of Social and Clinical Pharmacy, Faculty of Pharmacy in Hradec 
      Králové, Charles University, Hradec Králové, Czech Republic.
AD  - Department of Social Pharmacy and Pharmaceutical Legislation, Faculty of 
      Pharmacy, University of Belgrade, Belgrade, Serbia.
FAU - Fialová, Daniela
AU  - Fialová D
AD  - Department of Social and Clinical Pharmacy, Faculty of Pharmacy in Hradec 
      Králové, Charles University, Hradec Králové, Czech Republic.
AD  - Department of Geriatrics and Gerontology, 1st Faculty of Medicine in Prague, 
      Charles University, Prague, Czech Republic.
FAU - Ortner Hadžiabdić, Maja
AU  - Ortner Hadžiabdić M
AUID- ORCID: 0000-0003-1578-9764
AD  - Center for Applied Pharmacy, Faculty of Pharmacy and Biochemistry, University of 
      Zagreb, Zagreb, Croatia.
LA  - eng
GR  - MSCF-ITN-764632/Marie Skłodowska-Curie Foundation/
GR  - CZ.02.1.01/0.0/0.0/18_069/0010046/InoMed/
GR  - SVV 260 551/European Horizon 2020 I-CARE4OLD/
GR  - START/MED/093 EN.02.2.69/0.0/0.0/19_073/0016935/European Horizon 2020 I-CARE4OLD/
GR  - 965341/European Horizon 2020 I-CARE4OLD/
GR  - Faculty of Pharmacy, Charles University/
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20231203
PL  - England
TA  - Br J Clin Pharmacol
JT  - British journal of clinical pharmacology
JID - 7503323
RN  - 12794-10-4 (Benzodiazepines)
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - Benzodiazepines/adverse effects
MH  - *Deprescriptions
MH  - Clinical Decision-Making
MH  - Health Personnel
OTO - NOTNLM
OT  - ChatGPT-4
OT  - artificial intelligence (AI)
OT  - benzodiazepines
OT  - chatbot
OT  - deprescribing
EDAT- 2023/11/11 11:43
MHDA- 2024/02/29 06:43
CRDT- 2023/11/10 21:40
PHST- 2023/10/26 00:00 [revised]
PHST- 2023/08/05 00:00 [received]
PHST- 2023/10/29 00:00 [accepted]
PHST- 2024/02/29 06:43 [medline]
PHST- 2023/11/11 11:43 [pubmed]
PHST- 2023/11/10 21:40 [entrez]
AID - 10.1111/bcp.15963 [doi]
PST - ppublish
SO  - Br J Clin Pharmacol. 2024 Mar;90(3):662-674. doi: 10.1111/bcp.15963. Epub 2023 
      Dec 3.

PMID- 38466827
OWN - NLM
STAT- Publisher
LR  - 20240311
IS  - 1938-2367 (Electronic)
IS  - 0147-7447 (Linking)
DP  - 2024 Mar 12
TI  - Performance of Two Artificial Intelligence Generative Language Models on the 
      Orthopaedic In-Training Examination.
PG  - 1-5
LID - 10.3928/01477447-20240304-02 [doi]
AB  - BACKGROUND: Artificial intelligence (AI) generative large language models are 
      powerful and increasingly accessible tools with potential applications in health 
      care education and training. The annual Orthopaedic In-Training Examination 
      (OITE) is widely used to assess resident academic progress and preparation for 
      the American Board of Orthopaedic Surgery Part 1 Examination. MATERIALS AND 
      METHODS: Open AI's ChatGPT and Google's Bard generative language models were 
      administered the 2022 OITE. Question stems that contained images were input 
      without and then with a text-based description of the imaging findings. RESULTS: 
      ChatGPT answered 69.1% of questions correctly. When provided with text describing 
      accompanying media, this increased to 77.8% correct. In contrast, Bard answered 
      49.8% of questions correctly. This increased to 58% correct when text describing 
      imaging in question stems was provided (P&lt;.0001). ChatGPT was most accurate in 
      questions within the shoulder category, with 90.9% correct. Bard performed best 
      in the sports category, with 65.4% correct. ChatGPT performed above the published 
      mean of Accreditation Council for Graduate Medical Education orthopedic resident 
      test-takers (66%). CONCLUSION: There is significant variability in the accuracy 
      of publicly available AI models on the OITE. AI generative language software may 
      play numerous potential roles in the future in orthopedic education, including 
      simulating patient presentations and clinical scenarios, customizing individual 
      learning plans, and driving evidence-based case discussion. Further research and 
      collaboration within the orthopedic community is required to safely adopt these 
      tools and minimize risks associated with their use. [Orthopedics. 
      202x;4x(x):xx-xx.].
FAU - Lubitz, Marc
AU  - Lubitz M
FAU - Latario, Luke
AU  - Latario L
LA  - eng
PT  - Journal Article
DEP - 20240312
PL  - United States
TA  - Orthopedics
JT  - Orthopedics
JID - 7806107
SB  - IM
EDAT- 2024/03/11 18:42
MHDA- 2024/03/11 18:42
CRDT- 2024/03/11 14:42
PHST- 2024/03/11 18:42 [medline]
PHST- 2024/03/11 18:42 [pubmed]
PHST- 2024/03/11 14:42 [entrez]
AID - 10.3928/01477447-20240304-02 [doi]
PST - aheadofprint
SO  - Orthopedics. 2024 Mar 12:1-5. doi: 10.3928/01477447-20240304-02.

PMID- 38147494
OWN - NLM
STAT- Publisher
LR  - 20231226
IS  - 1938-2367 (Electronic)
IS  - 0147-7447 (Linking)
DP  - 2023 Dec 28
TI  - Utility of Artificial Intelligence in Orthopedic Surgery Literature Review: A 
      Comparative Pilot Study.
PG  - 1-6
LID - 10.3928/01477447-20231220-02 [doi]
AB  - OBJECTIVE: Literature reviews are essential to the scientific process and allow 
      clinician researchers to advance general knowledge. The purpose of this study was 
      to evaluate if the artificial intelligence (AI) programs Chat-GPT and 
      Perplexity.AI can perform an orthopedic surgery literature review. MATERIALS AND 
      METHODS: Five different search topics of varying specificity within orthopedic 
      surgery were chosen for each search arm to investigate. A consolidated list of 
      unique articles for each search topic was recorded for the experimental AI search 
      arms and compared with the results of the control arm of two independent 
      reviewers. Articles in the experimental arms were examined by the two independent 
      reviewers for relevancy and validity. RESULTS: ChatGPT was able to identify a 
      total of 61 unique articles. Four articles were not relevant to the search topic 
      and 51 articles were deemed to be fraudulent, resulting in 6 valid articles. 
      Perplexity.AI was able to identify a total of 43 unique articles. Nineteen were 
      not relevant to the search topic but all articles were able to be verified, 
      resulting in 24 valid articles. The control arm was able to identify 132 
      articles. Success rates for ChatGPT and Perplexity.AI were 4.6% (6 of 132) and 
      18.2% (24 of 132), respectively. CONCLUSION: The current iteration of ChatGPT 
      cannot perform a reliable literature review, and Perplexity.AI is only able to 
      perform a limited review of the medical literature. Any utilization of these open 
      AI programs should be done with caution and human quality assurance to promote 
      responsible use and avoid the risk of using fabricated search results. 
      [Orthopedics. 202x;4x(x):xx-xx.].
FAU - Sanii, Ryan Y
AU  - Sanii RY
FAU - Kasto, Johnny K
AU  - Kasto JK
FAU - Wines, Wade B
AU  - Wines WB
FAU - Mahylis, Jared M
AU  - Mahylis JM
FAU - Muh, Stephanie J
AU  - Muh SJ
LA  - eng
PT  - Journal Article
DEP - 20231228
PL  - United States
TA  - Orthopedics
JT  - Orthopedics
JID - 7806107
SB  - IM
EDAT- 2023/12/26 18:41
MHDA- 2023/12/26 18:41
CRDT- 2023/12/26 13:43
PHST- 2023/12/26 18:41 [medline]
PHST- 2023/12/26 18:41 [pubmed]
PHST- 2023/12/26 13:43 [entrez]
AID - 10.3928/01477447-20231220-02 [doi]
PST - aheadofprint
SO  - Orthopedics. 2023 Dec 28:1-6. doi: 10.3928/01477447-20231220-02.

PMID- 37257860
OWN - NLM
STAT- MEDLINE
DCOM- 20230605
LR  - 20230605
IS  - 2049-4408 (Electronic)
IS  - 2049-4394 (Linking)
VI  - 105-B
IP  - 6
DP  - 2023 Jun 1
TI  - What's all the chatter about?
PG  - 587-589
LID - 10.1302/0301-620X.105B6.BJJ-2023-0156 [doi]
AB  - The OpenAI chatbot ChatGPT is an artificial intelligence (AI) application that 
      uses state-of-the-art language processing AI. It can perform a vast number of 
      tasks, from writing poetry and explaining complex quantum mechanics, to 
      translating language and writing research articles with a human-like 
      understanding and legitimacy. Since its initial release to the public in November 
      2022, ChatGPT has garnered considerable attention due to its ability to mimic the 
      patterns of human language, and it has attracted billion-dollar investments from 
      Microsoft and PricewaterhouseCoopers. The scope of ChatGPT and other large 
      language models appears infinite, but there are several important limitations. 
      This editorial provides an introduction to the basic functionality of ChatGPT and 
      other large language models, their current applications and limitations, and the 
      associated implications for clinical practice and research.
CI  - © 2023 The British Editorial Society of Bone &amp; Joint Surgery.
FAU - Kunze, Kyle N
AU  - Kunze KN
AUID- ORCID: 0000-0002-0363-3482
AD  - Department of Orthopaedic Surgery, Hospital for Special Surgery, New York, New 
      York, USA.
FAU - Jang, Seong J
AU  - Jang SJ
AUID- ORCID: 0000-0002-9967-9476
AD  - Weill Cornell Medical College, New York, New York, USA.
FAU - Fullerton, Mark A
AU  - Fullerton MA
AD  - The Bone &amp; Joint Journal , London, UK.
FAU - Vigdorchik, Jonathan M
AU  - Vigdorchik JM
AUID- ORCID: 0000-0003-0308-9648
AD  - Department of Orthopaedic Surgery, Hospital for Special Surgery, New York, New 
      York, USA.
AD  - Adult Reconstruction and Joint Replacement Service, Hospital for Special Surgery, 
      New York, New York, USA.
FAU - Haddad, Fares S
AU  - Haddad FS
AD  - The Bone &amp; Joint Journal , London, UK.
AD  - University College London Hospitals, and The NIHR Biomedical Research Centre at 
      UCLH, London, UK.
AD  - Princess Grace Hospital, London, UK.
LA  - eng
PT  - Editorial
DEP - 20230601
PL  - England
TA  - Bone Joint J
JT  - The bone &amp; joint journal
JID - 101599229
SB  - IM
MH  - *Artificial Intelligence
MH  - Language
COIS- J. M. Vigdorchik reports royalties or licenses from Corin and Depuy Synthes, 
      consulting fees from Stryker, Zimmer Biomet, and Depuy Synthes, patents planned, 
      issued or pending from Ortho AI, and stock or stock options from Corin, 
      Intellijoint, Ortho AI, and T3 Medical, none of which are related to this 
      article. F. S. Haddad reports board membership of The Bone &amp; Joint Journal and 
      the Annals of the Royal College of Surgeons; consultancy for Smith &amp; Nephew, 
      Corin, MatOrtho, and Stryker; payment for lectures, including service on 
      speakers’ bureaus, for Smith &amp; Nephew and Stryker; and royalties paid by Smith &amp; 
      Nephew, MatOrtho, Corin, and Stryker, all of which are also unrelated to this 
      article.
EDAT- 2023/06/01 01:08
MHDA- 2023/06/02 06:42
CRDT- 2023/05/31 20:03
PHST- 2023/06/02 06:42 [medline]
PHST- 2023/06/01 01:08 [pubmed]
PHST- 2023/05/31 20:03 [entrez]
AID - BJJ-2023-0156 [pii]
AID - 10.1302/0301-620X.105B6.BJJ-2023-0156 [doi]
PST - epublish
SO  - Bone Joint J. 2023 Jun 1;105-B(6):587-589. doi: 
      10.1302/0301-620X.105B6.BJJ-2023-0156.

PMID- 38464946
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240312
IS  - 2673-8740 (Electronic)
IS  - 2673-8740 (Linking)
VI  - 3
DP  - 2023
TI  - Surviving ChatGPT in healthcare.
PG  - 1224682
LID - 10.3389/fradi.2023.1224682 [doi]
LID - 1224682
AB  - At the dawn of of Artificial General Intelligence (AGI), the emergence of large 
      language models such as ChatGPT show promise in revolutionizing healthcare by 
      improving patient care, expanding medical access, and optimizing clinical 
      processes. However, their integration into healthcare systems requires careful 
      consideration of potential risks, such as inaccurate medical advice, patient 
      privacy violations, the creation of falsified documents or images, overreliance 
      on AGI in medical education, and the perpetuation of biases. It is crucial to 
      implement proper oversight and regulation to address these risks, ensuring the 
      safe and effective incorporation of AGI technologies into healthcare systems. By 
      acknowledging and mitigating these challenges, AGI can be harnessed to enhance 
      patient care, medical knowledge, and healthcare processes, ultimately benefiting 
      society as a whole.
CI  - © 2024 Liu, Zhang, Wu, Yu, Cao, Dai, Liu, Liu, Liu, Li, Shen, Li, Zhu and Liu.
FAU - Liu, Zhengliang
AU  - Liu Z
AD  - School of Computing, University of Georgia, Athens, GA, United States.
FAU - Zhang, Lu
AU  - Zhang L
AD  - Department of Computer Science and Engineering, The University of Texas at 
      Arlington, Arlington, TX, United States.
FAU - Wu, Zihao
AU  - Wu Z
AD  - School of Computing, University of Georgia, Athens, GA, United States.
FAU - Yu, Xiaowei
AU  - Yu X
AD  - Department of Computer Science and Engineering, The University of Texas at 
      Arlington, Arlington, TX, United States.
FAU - Cao, Chao
AU  - Cao C
AD  - Department of Computer Science and Engineering, The University of Texas at 
      Arlington, Arlington, TX, United States.
FAU - Dai, Haixing
AU  - Dai H
AD  - School of Computing, University of Georgia, Athens, GA, United States.
FAU - Liu, Ninghao
AU  - Liu N
AD  - School of Computing, University of Georgia, Athens, GA, United States.
FAU - Liu, Jun
AU  - Liu J
AD  - Department of Radiology, Second Xiangya Hospital, Changsha, Hunan, China.
FAU - Liu, Wei
AU  - Liu W
AD  - Department of Radiation Oncology, Mayo Clinic, Scottsdale, AZ, United States.
FAU - Li, Quanzheng
AU  - Li Q
AD  - Department of Radiology, Massachusetts General Hospital and Harvard Medical 
      School, Boston, MA, United States.
FAU - Shen, Dinggang
AU  - Shen D
AD  - School of Biomedical Engineering, ShanghaiTech University, Shanghai, China.
AD  - Department of Research and Development, Shanhai United Imaging Intelligence Co., 
      Ltd., Shanghai, China.
AD  - Shanghai Clinical Research and Trial Center, Shanghai, China.
FAU - Li, Xiang
AU  - Li X
AD  - Department of Radiology, Massachusetts General Hospital and Harvard Medical 
      School, Boston, MA, United States.
FAU - Zhu, Dajiang
AU  - Zhu D
AD  - Department of Computer Science and Engineering, The University of Texas at 
      Arlington, Arlington, TX, United States.
FAU - Liu, Tianming
AU  - Liu T
AD  - School of Computing, University of Georgia, Athens, GA, United States.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20240223
PL  - Switzerland
TA  - Front Radiol
JT  - Frontiers in radiology
JID - 9918367586306676
PMC - PMC10920216
OTO - NOTNLM
OT  - Artificial General Intelligence (AGI)
OT  - Artificial Intelligence-AI
OT  - ChatGPT
OT  - GPT-4
OT  - large language models (LLM)
COIS- DS was employed by Shanghai United Imaging Intelligence Co. Ltd. The remaining 
      authors declare that the research was conducted in the absence of any commercial 
      or financial relationships that could be construed as a potential conflict of 
      interest. The authors DZ, XL, WL, JL declared that they were an editorial board 
      member of Frontiers, at the time of submission. This had no impact on the peer 
      review process and the final decision.
EDAT- 2024/03/11 06:42
MHDA- 2024/03/11 06:43
PMCR- 2024/02/23
CRDT- 2024/03/11 04:44
PHST- 2023/05/18 00:00 [received]
PHST- 2023/07/25 00:00 [accepted]
PHST- 2024/03/11 06:43 [medline]
PHST- 2024/03/11 06:42 [pubmed]
PHST- 2024/03/11 04:44 [entrez]
PHST- 2024/02/23 00:00 [pmc-release]
AID - 10.3389/fradi.2023.1224682 [doi]
PST - epublish
SO  - Front Radiol. 2024 Feb 23;3:1224682. doi: 10.3389/fradi.2023.1224682. eCollection 
      2023.

PMID- 38056135
OWN - NLM
STAT- MEDLINE
DCOM- 20240124
LR  - 20240201
IS  - 1618-0631 (Electronic)
IS  - 0344-0338 (Linking)
VI  - 253
DP  - 2024 Jan
TI  - ChatGPT as an aid for pathological diagnosis of cancer.
PG  - 154989
LID - S0344-0338(23)00690-8 [pii]
LID - 10.1016/j.prp.2023.154989 [doi]
AB  - Diagnostic workup of cancer patients is highly reliant on the science of 
      pathology using cytopathology, histopathology, and other ancillary techniques 
      like immunohistochemistry and molecular cytogenetics. Data processing and 
      learning by means of artificial intelligence (AI) has become a spearhead for the 
      advancement of medicine, with pathology and laboratory medicine being no 
      exceptions. ChatGPT, an artificial intelligence (AI)-based chatbot, that was 
      recently launched by OpenAI, is currently a talk of the town, and its role in 
      cancer diagnosis is also being explored meticulously. Pathology workflow by 
      integration of digital slides, implementation of advanced algorithms, and 
      computer-aided diagnostic techniques extend the frontiers of the pathologist's 
      view beyond a microscopic slide and enables effective integration, assimilation, 
      and utilization of knowledge that is beyond human limits and boundaries. Despite 
      of it's numerous advantages in the pathological diagnosis of cancer, it comes 
      with several challenges like integration of digital slides with input language 
      parameters, problems of bias, and legal issues which have to be addressed and 
      worked up soon so that we as a pathologists diagnosing malignancies are on the 
      same band wagon and don't miss the train.
CI  - Copyright © 2023 Elsevier GmbH. All rights reserved.
FAU - Malik, Shaivy
AU  - Malik S
AD  - Department of Pathology, Vardhman Mahavir Medical College and Safdarjung 
      Hospital, New Delhi, India.
FAU - Zaheer, Sufian
AU  - Zaheer S
AD  - Department of Pathology, Vardhman Mahavir Medical College and Safdarjung 
      Hospital, New Delhi, India. Electronic address: sufianzaheer@gmail.com.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20231129
PL  - Germany
TA  - Pathol Res Pract
JT  - Pathology, research and practice
JID - 7806109
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Neoplasms/diagnosis
MH  - Algorithms
MH  - Cytogenetics
MH  - Pathologists
OTO - NOTNLM
OT  - AI
OT  - Cancer diagnosis
OT  - ChatGPT
OT  - Image analysis and integration
OT  - Pathology
COIS- Declaration of Competing Interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2023/12/07 00:42
MHDA- 2024/01/24 06:43
CRDT- 2023/12/06 18:07
PHST- 2023/08/23 00:00 [received]
PHST- 2023/11/26 00:00 [revised]
PHST- 2023/11/27 00:00 [accepted]
PHST- 2024/01/24 06:43 [medline]
PHST- 2023/12/07 00:42 [pubmed]
PHST- 2023/12/06 18:07 [entrez]
AID - S0344-0338(23)00690-8 [pii]
AID - 10.1016/j.prp.2023.154989 [doi]
PST - ppublish
SO  - Pathol Res Pract. 2024 Jan;253:154989. doi: 10.1016/j.prp.2023.154989. Epub 2023 
      Nov 29.

PMID- 37332007
OWN - NLM
STAT- MEDLINE
DCOM- 20231109
LR  - 20231129
IS  - 1573-9686 (Electronic)
IS  - 0090-6964 (Linking)
VI  - 51
IP  - 12
DP  - 2023 Dec
TI  - Is It Feasible to Reduce Academic Stress in Net-Zero Energy Buildings? Reaction 
      from ChatGPT.
PG  - 2654-2656
LID - 10.1007/s10439-023-03286-y [doi]
AB  - We may lessen the detrimental effects of global warming on human thought 
      processes by reducing greenhouse gas emissions, encouraging sustainability, and 
      giving adaption measures top priority. The purpose of the letter is to draw 
      attention to the necessity of net-zero energy buildings (NZEB) in academic 
      institutions in order to reduce academic stress, promote well-being, and improve 
      cognitive functions. While some levels of stress might be advantageous, excessive 
      and mismanaged stress can be detrimental to students' well-being. To foster a 
      healthy academic atmosphere, it is essential to offer resources, support 
      networks, and stress-reduction techniques. As human authors, we thoroughly edited 
      ChatGPT's responses to create this letter.
CI  - © 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.
FAU - Rani, P Sobha
AU  - Rani PS
AUID- ORCID: 0000-0002-5095-5994
AD  - Department of Electrical and Electronics Engineering, Lakireddy Bali Reddy 
      College of Engineering (Autonomous), Jawaharlal Nehru Technological University, 
      Kakinada (JNTUK), East Godavari, Kakinada, Andhra Pradesh, 533003, India. 
      sobhareveru@gmail.com.
FAU - Rani, K Radha
AU  - Rani KR
AD  - Department of Electrical and Electronics Engineering, R.V.R &amp; J.C. College of 
      Engineering, Chowdavaram, Guntur, Andhra Pradesh, India.
FAU - Daram, Suresh Babu
AU  - Daram SB
AD  - Department of Electrical and Electronics Engineering, Mohan Babu University (Erst 
      while Sree Vidyanikethan Engineering College), Tirupati, Andhra Pradesh, 517102, 
      India.
FAU - Angadi, Ravi V
AU  - Angadi RV
AD  - Department of Electrical and Electronics Engineering, School of Engineering, 
      Presidency University, Bangalore, Karnataka, 560064, India.
LA  - eng
PT  - Letter
DEP - 20230618
PL  - United States
TA  - Ann Biomed Eng
JT  - Annals of biomedical engineering
JID - 0361512
RN  - 0 (Greenhouse Gases)
SB  - IM
MH  - Humans
MH  - *Greenhouse Gases
MH  - Global Warming
MH  - Universities
OTO - NOTNLM
OT  - Academic stress
OT  - ChatGPT
OT  - Global warming
OT  - Net-zero energy buildings
OT  - Sustainability
EDAT- 2023/06/19 00:42
MHDA- 2023/11/09 06:42
CRDT- 2023/06/18 23:35
PHST- 2023/06/07 00:00 [received]
PHST- 2023/06/13 00:00 [accepted]
PHST- 2023/11/09 06:42 [medline]
PHST- 2023/06/19 00:42 [pubmed]
PHST- 2023/06/18 23:35 [entrez]
AID - 10.1007/s10439-023-03286-y [pii]
AID - 10.1007/s10439-023-03286-y [doi]
PST - ppublish
SO  - Ann Biomed Eng. 2023 Dec;51(12):2654-2656. doi: 10.1007/s10439-023-03286-y. Epub 
      2023 Jun 18.

PMID- 38415145
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240229
IS  - 2223-4292 (Print)
IS  - 2223-4306 (Electronic)
IS  - 2223-4306 (Linking)
VI  - 14
IP  - 2
DP  - 2024 Feb 1
TI  - ChatGPT in radiology structured reporting: analysis of ChatGPT-3.5 Turbo and 
      GPT-4 in reducing word count and recalling findings.
PG  - 2096-2102
LID - 10.21037/qims-23-1300 [doi]
FAU - Mallio, Carlo A
AU  - Mallio CA
AD  - Fondazione Policlinico Universitario Campus Bio-Medico, Rome, Italy.
AD  - Research Unit of Radiology, Department of Medicine and Surgery, Università Campus 
      Bio-Medico di Roma, Rome, Italy.
FAU - Bernetti, Caterina
AU  - Bernetti C
AD  - Fondazione Policlinico Universitario Campus Bio-Medico, Rome, Italy.
AD  - Research Unit of Radiology, Department of Medicine and Surgery, Università Campus 
      Bio-Medico di Roma, Rome, Italy.
FAU - Sertorio, Andrea C
AU  - Sertorio AC
AD  - Fondazione Policlinico Universitario Campus Bio-Medico, Rome, Italy.
AD  - Research Unit of Radiology, Department of Medicine and Surgery, Università Campus 
      Bio-Medico di Roma, Rome, Italy.
FAU - Zobel, Bruno Beomonte
AU  - Zobel BB
AD  - Fondazione Policlinico Universitario Campus Bio-Medico, Rome, Italy.
AD  - Research Unit of Radiology, Department of Medicine and Surgery, Università Campus 
      Bio-Medico di Roma, Rome, Italy.
LA  - eng
PT  - Journal Article
DEP - 20240105
PL  - China
TA  - Quant Imaging Med Surg
JT  - Quantitative imaging in medicine and surgery
JID - 101577942
PMC - PMC10895108
COIS- Conflicts of Interest: All authors have completed the ICMJE uniform disclosure 
      form (available at 
      https://qims.amegroups.com/article/view/10.21037/qims-23-1300/coif). C.A.M. 
      serves as an unpaid editorial board member of Quantitative Imaging in Medicine 
      and Surgery. The other authors have no conflicts of interest to declare.
EDAT- 2024/02/28 06:44
MHDA- 2024/02/28 06:45
PMCR- 2024/02/01
CRDT- 2024/02/28 03:58
PHST- 2023/09/11 00:00 [received]
PHST- 2023/11/20 00:00 [accepted]
PHST- 2024/02/28 06:45 [medline]
PHST- 2024/02/28 06:44 [pubmed]
PHST- 2024/02/28 03:58 [entrez]
PHST- 2024/02/01 00:00 [pmc-release]
AID - qims-14-02-2096 [pii]
AID - 10.21037/qims-23-1300 [doi]
PST - ppublish
SO  - Quant Imaging Med Surg. 2024 Feb 1;14(2):2096-2102. doi: 10.21037/qims-23-1300. 
      Epub 2024 Jan 5.

PMID- 38337430
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240212
IS  - 2077-0383 (Print)
IS  - 2077-0383 (Electronic)
IS  - 2077-0383 (Linking)
VI  - 13
IP  - 3
DP  - 2024 Jan 27
TI  - Accuracy and Completeness of ChatGPT-Generated Information on Interceptive 
      Orthodontics: A Multicenter Collaborative Study.
LID - 10.3390/jcm13030735 [doi]
LID - 735
AB  - Background: this study aims to investigate the accuracy and completeness of 
      ChatGPT in answering questions and solving clinical scenarios of interceptive 
      orthodontics. Materials and Methods: ten specialized orthodontists from ten 
      Italian postgraduate orthodontics schools developed 21 clinical open-ended 
      questions encompassing all of the subspecialities of interceptive orthodontics 
      and 7 comprehensive clinical cases. Questions and scenarios were inputted into 
      ChatGPT4, and the resulting answers were evaluated by the researchers using 
      predefined accuracy (range 1-6) and completeness (range 1-3) Likert scales. 
      Results: For the open-ended questions, the overall median score was 4.9/6 for the 
      accuracy and 2.4/3 for completeness. In addition, the reviewers rated the 
      accuracy of open-ended answers as entirely correct (score 6 on Likert scale) in 
      40.5% of cases and completeness as entirely correct (score 3 n Likert scale) in 
      50.5% of cases. As for the clinical cases, the overall median score was 4.9/6 for 
      accuracy and 2.5/3 for completeness. Overall, the reviewers rated the accuracy of 
      clinical case answers as entirely correct in 46% of cases and the completeness of 
      clinical case answers as entirely correct in 54.3% of cases. Conclusions: The 
      results showed a high level of accuracy and completeness in AI responses and a 
      great ability to solve difficult clinical cases, but the answers were not 100% 
      accurate and complete. ChatGPT is not yet sophisticated enough to replace the 
      intellectual work of human beings.
FAU - Hatia, Arjeta
AU  - Hatia A
AD  - Orthodontics Postgraduate School, Department of Medical Biotechnologies, 
      University of Siena, 53100 Siena, Italy.
FAU - Doldo, Tiziana
AU  - Doldo T
AD  - Orthodontics Postgraduate School, Department of Medical Biotechnologies, 
      University of Siena, 53100 Siena, Italy.
FAU - Parrini, Stefano
AU  - Parrini S
AD  - Oral Surgery Postgraduate School, Department of Medical Biotechnologies, 
      University of Siena, 53100 Siena, Italy.
FAU - Chisci, Elettra
AU  - Chisci E
AD  - Orthodontics Postgraduate School, University of Ferrara, 44121 Ferrara, Italy.
FAU - Cipriani, Linda
AU  - Cipriani L
AD  - Orthodontics Postgraduate School, Department of Medical Biotechnologies, 
      University of Siena, 53100 Siena, Italy.
FAU - Montagna, Livia
AU  - Montagna L
AD  - Orthodontics Postgraduate School, University of Cagliari, 09121 Cagliari, Italy.
FAU - Lagana, Giuseppina
AU  - Lagana G
AD  - Orthodontics Postgraduate School, "Sapienza" University of Rome, 00185 Rome, 
      Italy.
FAU - Guenza, Guia
AU  - Guenza G
AD  - Orthodontics Postgraduate School, University of Milano, 20019 Milan, Italy.
FAU - Agosta, Edoardo
AU  - Agosta E
AD  - Orthodontics Postgraduate School, University of Torino, 10024 Turin, Italy.
FAU - Vinjolli, Franceska
AU  - Vinjolli F
AD  - Orthodontics Postgraduate School, University of Roma Tor Vergata, 00133 Rome, 
      Italy.
FAU - Hoxha, Meladiona
AU  - Hoxha M
AD  - Orthodontics Postgraduate School, "Cattolica" University of Rome, 00168 Rome, 
      Italy.
FAU - D'Amelio, Claudio
AU  - D'Amelio C
AD  - Orthodontics Postgraduate School, University of Chieti, 66100 Chieti, Italy.
FAU - Favaretto, Nicolò
AU  - Favaretto N
AD  - Orthodontics Postgraduate School, University of Trieste, 34100 Trieste, Italy.
FAU - Chisci, Glauco
AU  - Chisci G
AUID- ORCID: 0000-0001-6992-324X
AD  - Oral Surgery Postgraduate School, Department of Medical Biotechnologies, 
      University of Siena, 53100 Siena, Italy.
LA  - eng
PT  - Journal Article
DEP - 20240127
PL  - Switzerland
TA  - J Clin Med
JT  - Journal of clinical medicine
JID - 101606588
PMC - PMC10856539
OTO - NOTNLM
OT  - ChatGPT
OT  - artificial bot
OT  - artificial intelligence
OT  - interceptive orthodontics
OT  - methodology
OT  - orthodontics
COIS- The authors declare no conflicts of interest.
EDAT- 2024/02/10 10:42
MHDA- 2024/02/10 10:43
PMCR- 2024/01/27
CRDT- 2024/02/10 01:03
PHST- 2024/01/10 00:00 [received]
PHST- 2024/01/21 00:00 [revised]
PHST- 2024/01/25 00:00 [accepted]
PHST- 2024/02/10 10:43 [medline]
PHST- 2024/02/10 10:42 [pubmed]
PHST- 2024/02/10 01:03 [entrez]
PHST- 2024/01/27 00:00 [pmc-release]
AID - jcm13030735 [pii]
AID - jcm-13-00735 [pii]
AID - 10.3390/jcm13030735 [doi]
PST - epublish
SO  - J Clin Med. 2024 Jan 27;13(3):735. doi: 10.3390/jcm13030735.

PMID- 37609457
OWN - NLM
STAT- MEDLINE
DCOM- 20231102
LR  - 20231107
IS  - 0026-6620 (Print)
IS  - 0026-6620 (Linking)
VI  - 120
IP  - 4
DP  - 2023 Jul-Aug
TI  - The Promise &amp; Peril of Artificial Intelligence: A Conversation with ChatGPT.
PG  - 240-242
FAU - Hagan, John C 3rd
AU  - Hagan JC 3rd
AD  - Editor of Missouri Medicine since 2001 and recent AI user. ChatGPT: AI generated 
      anthropomorphized image.
LA  - eng
PT  - Editorial
PL  - United States
TA  - Mo Med
JT  - Missouri medicine
JID - 0400744
SB  - IM
MH  - Humans
MH  - *Artificial Intelligence
MH  - *Communication
PMC - PMC10441258
EDAT- 2023/08/23 06:42
MHDA- 2023/08/23 06:43
PMCR- 2023/07/01
CRDT- 2023/08/23 04:16
PHST- 2023/08/23 06:43 [medline]
PHST- 2023/08/23 06:42 [pubmed]
PHST- 2023/08/23 04:16 [entrez]
PHST- 2023/07/01 00:00 [pmc-release]
AID - ms120_p0240 [pii]
PST - ppublish
SO  - Mo Med. 2023 Jul-Aug;120(4):240-242.

PMID- 38052322
OWN - NLM
STAT- Publisher
LR  - 20231210
IS  - 2468-7855 (Electronic)
IS  - 2468-7855 (Linking)
VI  - 125
IP  - 3
DP  - 2023 Dec 3
TI  - Can natural language processing serve as a consultant in oral surgery?
PG  - 101724
LID - S2468-7855(23)00345-2 [pii]
LID - 10.1016/j.jormas.2023.101724 [doi]
AB  - OBJECTIVE: In this comprehensive evaluation, ten experienced oral surgeon experts 
      posed a total of twenty oral surgery-related questions, including dental implant 
      and tooth extractions, to three distinct Natural Language Processing (NLP)-based 
      chatbot platforms: ChatGPT, Microsoft Bing, and Google Bard. The study aimed to 
      assess the effectiveness of these chatbots in responding to specialized medical 
      questions. MATERIALS AND METHODS: Two primary evaluation metrics were employed: a 
      Likert Scale (LS) for measuring the accuracy and completeness of responses and a 
      Global Quality Scale (GQS) for evaluating the clarity of responses. Statistical 
      analyses, including one-way analysis of variance (ANOVA) and Post Hoc Tukey, were 
      conducted to assess and compare the performance of the chatbots as rated by the 
      experts. RESULTS: The results of the study revealed significant differences in 
      the performance of the chatbots. ChatGPT statistically achieved a better mean LS 
      score of 1.4000±0.15986 than Microsoft Bing (1.8750±0.18143) and Google Bards 
      (2.0500±0.12472) (P &lt; 0.001). Additionally, ChatGPT statistically achieved a 
      higher GQS score of 4.4200±0.30111 than Microsoft Bing (3.7550±0.28621) and 
      Google Bards (3.5250±0.22392) (P &lt; 0.001). CONCLUSIONS: These findings showed the 
      substantial advantage of ChatGPT in effectively addressing oral surgery-related 
      questions with superior accuracy, completeness, and clarity. The study highlights 
      the potential of advanced NLP platforms to enhance information retrieval and 
      communication within the field of oral surgery, reinforcing the utility of such 
      technologies in medical and surgical domains.
CI  - Copyright © 2023 Elsevier Masson SAS. All rights reserved.
FAU - Acar, Ahmet Hüseyin
AU  - Acar AH
AD  - Department of Oral and Maxillofacial Surgery, Faculty of Dentistry, Istanbul 
      Medeniyet University, Istanbul, Turkey. Electronic address: 
      ahmethuseyin.acar@medeniyet.edu.tr.
LA  - eng
PT  - Journal Article
DEP - 20231203
PL  - France
TA  - J Stomatol Oral Maxillofac Surg
JT  - Journal of stomatology, oral and maxillofacial surgery
JID - 101701089
SB  - IM
OTO - NOTNLM
OT  - Artificial intelligence
OT  - Chatbot
OT  - Oral surgery
COIS- Declaration of Competing Interest The authors declare that they have no known 
      competing financial interests or personal relationships that could have appeared 
      to influence the work reported in this paper.
EDAT- 2023/12/06 03:43
MHDA- 2023/12/06 03:43
CRDT- 2023/12/05 19:20
PHST- 2023/11/07 00:00 [received]
PHST- 2023/11/26 00:00 [revised]
PHST- 2023/12/01 00:00 [accepted]
PHST- 2023/12/06 03:43 [pubmed]
PHST- 2023/12/06 03:43 [medline]
PHST- 2023/12/05 19:20 [entrez]
AID - S2468-7855(23)00345-2 [pii]
AID - 10.1016/j.jormas.2023.101724 [doi]
PST - aheadofprint
SO  - J Stomatol Oral Maxillofac Surg. 2023 Dec 3;125(3):101724. doi: 
      10.1016/j.jormas.2023.101724.

PMID- 37361298
OWN - NLM
STAT- Publisher
LR  - 20230928
IS  - 2198-1833 (Print)
IS  - 1613-2238 (Electronic)
IS  - 0943-1853 (Linking)
DP  - 2023 May 26
TI  - A step-by-step researcher's guide to the use of an AI-based transformer in 
      epidemiology: an exploratory analysis of ChatGPT using the STROBE checklist for 
      observational studies.
PG  - 1-36
LID - 10.1007/s10389-023-01936-y [doi]
AB  - OBJECTIVE: This study aims at investigating how AI-based transformers can support 
      researchers in designing and conducting an epidemiological study. To accomplish 
      this, we used ChatGPT to reformulate the STROBE recommendations into a list of 
      questions to be answered by the transformer itself. We then qualitatively 
      evaluated the coherence and relevance of the transformer's outputs. STUDY DESIGN: 
      Descriptive study. METHODS: We first chose a study to be used as a basis for the 
      simulation. We then used ChatGPT to transform each STROBE checklist's item into 
      specific prompts. Each answer to the respective prompt was evaluated by 
      independent researchers in terms of coherence and relevance. RESULTS: The mean 
      scores assigned to each prompt were heterogeneous. On average, for the coherence 
      domain, the overall mean score was 3.6 out of 5.0, and for relevance it was 3.3 
      out of 5.0. The lowest scores were assigned to items belonging to the Methods 
      section of the checklist. CONCLUSIONS: ChatGPT can be considered as a valuable 
      support for researchers in conducting an epidemiological study, following 
      internationally recognized guidelines and standards. It is crucial for the users 
      to have knowledge on the subject and a critical mindset when evaluating the 
      outputs. The potential benefits of AI in scientific research and publishing are 
      undeniable, but it is crucial to address the risks, and the ethical and legal 
      consequences associated with its use.
CI  - © The Author(s) 2023.
FAU - Sanmarchi, Francesco
AU  - Sanmarchi F
AUID- ORCID: 0000-0001-8288-0563
AD  - Department of Biomedical and Neuromotor Sciences, Alma Mater Studiorum - 
      University of Bologna, Via San Giacomo 12, 40126 Bologna, Italy. GRID: 
      grid.6292.f. ISNI: 0000 0004 1757 1758
FAU - Bucci, Andrea
AU  - Bucci A
AD  - Department of Economics and Law, University of Macerata, Macerata, Italy. GRID: 
      grid.8042.e. ISNI: 0000 0001 2188 0260
FAU - Nuzzolese, Andrea Giovanni
AU  - Nuzzolese AG
AD  - STLab, Institute for Cognitive Sciences and Technologies (ISTC)-CNR, Roma, Italy. 
      GRID: grid.428479.4. ISNI: 0000 0001 2297 9633
FAU - Carullo, Gherardo
AU  - Carullo G
AD  - Department of Italian and Supranational Public Law, University of Milan, Milan, 
      Italy. GRID: grid.4708.b. ISNI: 0000 0004 1757 2822
FAU - Toscano, Fabrizio
AU  - Toscano F
AD  - Montefiore Medical Center, Bronx, NY USA. GRID: grid.240283.f. ISNI: 0000 0001 
      2152 0791
FAU - Nante, Nicola
AU  - Nante N
AD  - Present Address: Department of Molecular and Developmental Medicine, University 
      of Siena, Siena, Italy. GRID: grid.9024.f. ISNI: 0000 0004 1757 4641
FAU - Golinelli, Davide
AU  - Golinelli D
AD  - Department of Biomedical and Neuromotor Sciences, Alma Mater Studiorum - 
      University of Bologna, Via San Giacomo 12, 40126 Bologna, Italy. GRID: 
      grid.6292.f. ISNI: 0000 0004 1757 1758
AD  - Present Address: Department of Molecular and Developmental Medicine, University 
      of Siena, Siena, Italy. GRID: grid.9024.f. ISNI: 0000 0004 1757 4641
LA  - eng
PT  - Journal Article
DEP - 20230526
PL  - Germany
TA  - Z Gesundh Wiss
JT  - Zeitschrift fur Gesundheitswissenschaften = Journal of public health
JID - 9425271
PMC - PMC10215032
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - Epidemiology
OT  - Ethics
OT  - Legal
OT  - Methodology
OT  - Public Health
OT  - STROBE
OT  - Scientific research
OT  - Transformers
COIS- Competing interestsThe authors report no conflict of interest.
EDAT- 2023/06/26 19:07
MHDA- 2023/06/26 19:07
PMCR- 2023/05/26
CRDT- 2023/06/26 12:50
PHST- 2023/02/16 00:00 [received]
PHST- 2023/05/03 00:00 [accepted]
PHST- 2023/06/26 19:07 [pubmed]
PHST- 2023/06/26 19:07 [medline]
PHST- 2023/06/26 12:50 [entrez]
PHST- 2023/05/26 00:00 [pmc-release]
AID - 1936 [pii]
AID - 10.1007/s10389-023-01936-y [doi]
PST - aheadofprint
SO  - Z Gesundh Wiss. 2023 May 26:1-36. doi: 10.1007/s10389-023-01936-y.

PMID- 37713254
OWN - NLM
STAT- MEDLINE
DCOM- 20230918
LR  - 20231003
IS  - 1438-8871 (Electronic)
IS  - 1439-4456 (Print)
IS  - 1438-8871 (Linking)
VI  - 25
DP  - 2023 Sep 15
TI  - The Potential of ChatGPT as a Self-Diagnostic Tool in Common Orthopedic Diseases: 
      Exploratory Study.
PG  - e47621
LID - 10.2196/47621 [doi]
LID - e47621
AB  - BACKGROUND: Artificial intelligence (AI) has gained tremendous popularity 
      recently, especially the use of natural language processing (NLP). ChatGPT is a 
      state-of-the-art chatbot capable of creating natural conversations using NLP. The 
      use of AI in medicine can have a tremendous impact on health care delivery. 
      Although some studies have evaluated ChatGPT's accuracy in self-diagnosis, there 
      is no research regarding its precision and the degree to which it recommends 
      medical consultations. OBJECTIVE: The aim of this study was to evaluate ChatGPT's 
      ability to accurately and precisely self-diagnose common orthopedic diseases, as 
      well as the degree of recommendation it provides for medical consultations. 
      METHODS: Over a 5-day course, each of the study authors submitted the same 
      questions to ChatGPT. The conditions evaluated were carpal tunnel syndrome (CTS), 
      cervical myelopathy (CM), lumbar spinal stenosis (LSS), knee osteoarthritis 
      (KOA), and hip osteoarthritis (HOA). Answers were categorized as either correct, 
      partially correct, incorrect, or a differential diagnosis. The percentage of 
      correct answers and reproducibility were calculated. The reproducibility between 
      days and raters were calculated using the Fleiss κ coefficient. Answers that 
      recommended that the patient seek medical attention were recategorized according 
      to the strength of the recommendation as defined by the study. RESULTS: The 
      ratios of correct answers were 25/25, 1/25, 24/25, 16/25, and 17/25 for CTS, CM, 
      LSS, KOA, and HOA, respectively. The ratios of incorrect answers were 23/25 for 
      CM and 0/25 for all other conditions. The reproducibility between days was 1.0, 
      0.15, 0.7, 0.6, and 0.6 for CTS, CM, LSS, KOA, and HOA, respectively. The 
      reproducibility between raters was 1.0, 0.1, 0.64, -0.12, and 0.04 for CTS, CM, 
      LSS, KOA, and HOA, respectively. Among the answers recommending medical 
      attention, the phrases "essential," "recommended," "best," and "important" were 
      used. Specifically, "essential" occurred in 4 out of 125, "recommended" in 12 out 
      of 125, "best" in 6 out of 125, and "important" in 94 out of 125 answers. 
      Additionally, 7 out of the 125 answers did not include a recommendation to seek 
      medical attention. CONCLUSIONS: The accuracy and reproducibility of ChatGPT to 
      self-diagnose five common orthopedic conditions were inconsistent. The accuracy 
      could potentially be improved by adding symptoms that could easily identify a 
      specific location. Only a few answers were accompanied by a strong recommendation 
      to seek medical attention according to our study standards. Although ChatGPT 
      could serve as a potential first step in accessing care, we found variability in 
      accurate self-diagnosis. Given the risk of harm with self-diagnosis without 
      medical follow-up, it would be prudent for an NLP to include clear language 
      alerting patients to seek expert medical opinions. We hope to shed further light 
      on the use of AI in a future clinical study.
CI  - ©Tomoyuki Kuroiwa, Aida Sarcon, Takuya Ibara, Eriku Yamada, Akiko Yamamoto, 
      Kazuya Tsukamoto, Koji Fujita. Originally published in the Journal of Medical 
      Internet Research (https://www.jmir.org), 15.09.2023.
FAU - Kuroiwa, Tomoyuki
AU  - Kuroiwa T
AUID- ORCID: 0000-0002-9942-1811
AD  - Department of Orthopaedic and Spinal Surgery, Graduate School of Medical and 
      Dental Sciences, Tokyo Medical and Dental University, Tokyo, Japan.
AD  - Division of Orthopedic Surgery Research, Mayo Clinic, Rochester, MN, United 
      States.
FAU - Sarcon, Aida
AU  - Sarcon A
AUID- ORCID: 0000-0002-2763-878X
AD  - Department of Surgery, Mayo Clinic, Rochester, MN, United States.
FAU - Ibara, Takuya
AU  - Ibara T
AUID- ORCID: 0000-0002-0518-1918
AD  - Department of Functional Joint Anatomy, Graduate School of Medical and Dental 
      Sciences, Tokyo Medical and Dental University, Tokyo, Japan.
FAU - Yamada, Eriku
AU  - Yamada E
AUID- ORCID: 0000-0001-8777-9552
AD  - Department of Orthopaedic and Spinal Surgery, Graduate School of Medical and 
      Dental Sciences, Tokyo Medical and Dental University, Tokyo, Japan.
FAU - Yamamoto, Akiko
AU  - Yamamoto A
AUID- ORCID: 0000-0003-3639-8201
AD  - Department of Orthopaedic and Spinal Surgery, Graduate School of Medical and 
      Dental Sciences, Tokyo Medical and Dental University, Tokyo, Japan.
FAU - Tsukamoto, Kazuya
AU  - Tsukamoto K
AUID- ORCID: 0000-0003-4927-2149
AD  - Department of Orthopaedic and Spinal Surgery, Graduate School of Medical and 
      Dental Sciences, Tokyo Medical and Dental University, Tokyo, Japan.
FAU - Fujita, Koji
AU  - Fujita K
AUID- ORCID: 0000-0003-3733-0188
AD  - Department of Functional Joint Anatomy, Graduate School of Medical and Dental 
      Sciences, Tokyo Medical and Dental University, Tokyo, Japan.
AD  - Division of Medical Design Innovations, Open Innovation Center, Institute of 
      Research Innovation, Tokyo Medical and Dental University, Tokyo, Japan.
LA  - eng
PT  - Journal Article
DEP - 20230915
PL  - Canada
TA  - J Med Internet Res
JT  - Journal of medical Internet research
JID - 100959882
SB  - IM
MH  - Humans
MH  - Artificial Intelligence
MH  - Reproducibility of Results
MH  - *Spinal Cord Diseases
MH  - Natural Language Processing
MH  - Communication
MH  - *Musculoskeletal Diseases
MH  - *Osteoarthritis, Knee
PMC - PMC10541638
OTO - NOTNLM
OT  - AI model
OT  - ChatGPT
OT  - accuracy
OT  - artificial intelligence
OT  - chatbot
OT  - diagnosis
OT  - generative pretrained transformer
OT  - health information
OT  - language model
OT  - natural language processing
OT  - orthopedic disease
OT  - precision
OT  - self-diagnosis
COIS- Conflicts of Interest: None declared.
EDAT- 2023/09/15 12:42
MHDA- 2023/09/18 12:43
PMCR- 2023/09/15
CRDT- 2023/09/15 11:54
PHST- 2023/03/27 00:00 [received]
PHST- 2023/08/17 00:00 [accepted]
PHST- 2023/05/17 00:00 [revised]
PHST- 2023/09/18 12:43 [medline]
PHST- 2023/09/15 12:42 [pubmed]
PHST- 2023/09/15 11:54 [entrez]
PHST- 2023/09/15 00:00 [pmc-release]
AID - v25i1e47621 [pii]
AID - 10.2196/47621 [doi]
PST - epublish
SO  - J Med Internet Res. 2023 Sep 15;25:e47621. doi: 10.2196/47621.

PMID- 38042556
OWN - NLM
STAT- MEDLINE
DCOM- 20231205
LR  - 20231205
IS  - 1532-8481 (Electronic)
IS  - 8755-7223 (Linking)
VI  - 49
DP  - 2023 Nov-Dec
TI  - What does ChatGPT advise about predatory publishing?
PG  - 188-189
LID - S8755-7223(23)00119-9 [pii]
LID - 10.1016/j.profnurs.2023.08.002 [doi]
AB  - The debate surrounding "predatory publishing" continues to be unable to find 
      entirely effective solutions to dealing with this problem, despite fervent 
      efforts by many academics and policy makers around the world. Given this 
      situation, we were interested in appreciating whether ChatGPT would be able to 
      offer insight and solutions, to complement current human-based efforts.
CI  - Copyright © 2023 Elsevier Inc. All rights reserved.
FAU - Tsigaris, Panagiotis
AU  - Tsigaris P
AD  - 805 TRU Way, Department of Economics, Thompson Rivers University, Kamloops, 
      British Columbia V2C 0C8, Canada. Electronic address: ptsigaris@tru.ca.
FAU - Kendall, Graham
AU  - Kendall G
AD  - University of Nottingham (UK and Malaysia), University Park, Nottingham NG7 2RD, 
      UK. Electronic address: graham.kendall@nottingham.edu.my.
FAU - Teixeira da Silva, Jaime A
AU  - Teixeira da Silva JA
AD  - Ikenobe 3011-2, Kagawa-ken 761-0799, Japan. Electronic address: 
      jaimetex@yahoo.com.
LA  - eng
PT  - Letter
DEP - 20230819
PL  - United States
TA  - J Prof Nurs
JT  - Journal of professional nursing : official journal of the American Association of 
      Colleges of Nursing
JID - 8511298
SB  - IM
MH  - *Publishing
MH  - *Predatory Journals as Topic
MH  - *Artificial Intelligence
OTO - NOTNLM
OT  - Ethics
OT  - Safelists (whitelists) and watchlists (blacklists)
OT  - Transparency
OT  - Trust
COIS- Declaration of competing interest The authors, who declare no conflicts of 
      interest, contributed equally to the conceptual design, writing, editing, and 
      take responsibility for the content of the paper (i.e., the four ICMJE clauses 
      for authorship). The authors declare that the free version of ChatGPT was used 
      via the first author's account.
EDAT- 2023/12/03 00:42
MHDA- 2023/12/04 12:43
CRDT- 2023/12/02 20:59
PHST- 2023/05/30 00:00 [received]
PHST- 2023/08/02 00:00 [accepted]
PHST- 2023/12/04 12:43 [medline]
PHST- 2023/12/03 00:42 [pubmed]
PHST- 2023/12/02 20:59 [entrez]
AID - S8755-7223(23)00119-9 [pii]
AID - 10.1016/j.profnurs.2023.08.002 [doi]
PST - ppublish
SO  - J Prof Nurs. 2023 Nov-Dec;49:188-189. doi: 10.1016/j.profnurs.2023.08.002. Epub 
      2023 Aug 19.

PMID- 37073196
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20230421
IS  - 2168-8184 (Print)
IS  - 2168-8184 (Electronic)
IS  - 2168-8184 (Linking)
VI  - 15
IP  - 3
DP  - 2023 Mar
TI  - Treatment of Periorbital Edema in a Patient With Systemic Lupus Erythematosus 
      During Pregnancy: A Case Report Written With the Assistance of ChatGPT.
PG  - e36302
LID - 10.7759/cureus.36302 [doi]
LID - e36302
AB  - Systemic lupus erythematosus (SLE) is an autoimmune disease that has a wide range 
      of manifestations and can affect nearly every organ system. Skin manifestations 
      are a common finding in SLE. They are often photosensitive and can be exacerbated 
      by exposure to ultraviolet light. Here, we discuss the case of a 34-year-old 
      African American woman who presented with periorbital edema while 12 weeks 
      pregnant. This case highlights the importance of avoiding sun exposure in 
      patients with SLE and the challenge of treating SLE during pregnancy.
CI  - Copyright © 2023, Jansz et al.
FAU - Jansz, Jacqueline
AU  - Jansz J
AD  - Internal Medicine, University of Illinois at Chicago, Chicago, USA.
FAU - Manansala, Michael J
AU  - Manansala MJ
AD  - Rheumatology, Rush University Medical Center, Chicago, USA.
FAU - Sweiss, Nadera J
AU  - Sweiss NJ
AD  - Rheumatology, University of Illinois at Chicago, Chicago, USA.
LA  - eng
PT  - Case Reports
DEP - 20230317
PL  - United States
TA  - Cureus
JT  - Cureus
JID - 101596737
PMC - PMC10106120
OTO - NOTNLM
OT  - chatgpt
OT  - periorbital edema
OT  - pregnancy
OT  - sle
OT  - sun exposure
OT  - systemic lupus erythematosus
COIS- The authors have declared that no competing interests exist.
EDAT- 2023/04/19 06:41
MHDA- 2023/04/19 06:42
PMCR- 2023/03/17
CRDT- 2023/04/19 01:44
PHST- 2023/03/17 00:00 [accepted]
PHST- 2023/04/19 06:42 [medline]
PHST- 2023/04/19 06:41 [pubmed]
PHST- 2023/04/19 01:44 [entrez]
PHST- 2023/03/17 00:00 [pmc-release]
AID - 10.7759/cureus.36302 [doi]
PST - epublish
SO  - Cureus. 2023 Mar 17;15(3):e36302. doi: 10.7759/cureus.36302. eCollection 2023 
      Mar.

PMID- 37257586
OWN - NLM
STAT- MEDLINE
DCOM- 20230807
LR  - 20230810
IS  - 2589-9333 (Electronic)
IS  - 2589-9333 (Linking)
VI  - 5
IP  - 8
DP  - 2023 Aug
TI  - ChatGPT: a pioneering approach to complex prenatal differential diagnosis.
PG  - 101029
LID - S2589-9333(23)00171-4 [pii]
LID - 10.1016/j.ajogmf.2023.101029 [doi]
AB  - This commentary examines how ChatGPT can assist healthcare teams in the prenatal 
      diagnosis of rare and complex cases by creating a differential diagnoses based on 
      deidentified clinical findings, while also acknowledging its limitations.
CI  - Copyright © 2023. Published by Elsevier Inc.
FAU - Suhag, Anju
AU  - Suhag A
AD  - Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, NYU 
      Langone Health, NYU Langone Hospital-Long Island, NYU Long Island School of 
      Medicine, Mineola, NY (Drs Suhag and Kidd, Mses McGath and Cacace, and Dr 
      Chavez). Electronic address: anju.suhag@nyulangone.org.
FAU - Kidd, Jennifer
AU  - Kidd J
AD  - Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, NYU 
      Langone Health, NYU Langone Hospital-Long Island, NYU Long Island School of 
      Medicine, Mineola, NY (Drs Suhag and Kidd, Mses McGath and Cacace, and Dr 
      Chavez).
FAU - McGath, Meghan
AU  - McGath M
AD  - Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, NYU 
      Langone Health, NYU Langone Hospital-Long Island, NYU Long Island School of 
      Medicine, Mineola, NY (Drs Suhag and Kidd, Mses McGath and Cacace, and Dr 
      Chavez); Department of Clinical Genetics, NYU Langone Hospital-Long Island, 
      Mineola, NY (Mses McGath and Cacace, and Dr Monteleone).
FAU - Rajesh, Raeshmma
AU  - Rajesh R
AD  - Department of Obstetrics and Gynecology, Richmond University Medical Center, 
      Staten Island, NY (Dr Rajesh).
FAU - Gelfinbein, Joseph
AU  - Gelfinbein J
AD  - NYU Long Island School of Medicine, Mineola, NY (Mr Gelfinbein).
FAU - Cacace, Nicole
AU  - Cacace N
AD  - Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, NYU 
      Langone Health, NYU Langone Hospital-Long Island, NYU Long Island School of 
      Medicine, Mineola, NY (Drs Suhag and Kidd, Mses McGath and Cacace, and Dr 
      Chavez); Department of Clinical Genetics, NYU Langone Hospital-Long Island, 
      Mineola, NY (Mses McGath and Cacace, and Dr Monteleone).
FAU - Monteleone, Berrin
AU  - Monteleone B
AD  - Department of Clinical Genetics, NYU Langone Hospital-Long Island, Mineola, NY 
      (Mses McGath and Cacace, and Dr Monteleone).
FAU - Chavez, Martin R
AU  - Chavez MR
AD  - Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, NYU 
      Langone Health, NYU Langone Hospital-Long Island, NYU Long Island School of 
      Medicine, Mineola, NY (Drs Suhag and Kidd, Mses McGath and Cacace, and Dr 
      Chavez).
LA  - eng
PT  - Journal Article
DEP - 20230529
PL  - United States
TA  - Am J Obstet Gynecol MFM
JT  - American journal of obstetrics &amp; gynecology MFM
JID - 101746609
SB  - IM
MH  - Humans
MH  - Female
MH  - Pregnancy
MH  - Diagnosis, Differential
MH  - *Patient Care Team
MH  - *Prenatal Diagnosis
OTO - NOTNLM
OT  - ChatGPT-4
OT  - artificial intelligence
OT  - complex prenatal disorders
OT  - generative pre-trained transformer language model
OT  - human phenotype ontology
OT  - neonatal genetic disorders
OT  - online mendelian inheritance in man
OT  - prenatal diagnosis
EDAT- 2023/06/01 01:08
MHDA- 2023/08/07 06:41
CRDT- 2023/05/31 19:25
PHST- 2023/05/01 00:00 [received]
PHST- 2023/05/19 00:00 [accepted]
PHST- 2023/08/07 06:41 [medline]
PHST- 2023/06/01 01:08 [pubmed]
PHST- 2023/05/31 19:25 [entrez]
AID - S2589-9333(23)00171-4 [pii]
AID - 10.1016/j.ajogmf.2023.101029 [doi]
PST - ppublish
SO  - Am J Obstet Gynecol MFM. 2023 Aug;5(8):101029. doi: 10.1016/j.ajogmf.2023.101029. 
      Epub 2023 May 29.

PMID- 38093584
OWN - NLM
STAT- MEDLINE
DCOM- 20240125
LR  - 20240204
IS  - 1473-4877 (Electronic)
IS  - 0300-7995 (Linking)
VI  - 40
IP  - 2
DP  - 2024 Feb
TI  - The use of large language models in medicine: proceeding with caution.
PG  - 151-153
LID - 10.1080/03007995.2023.2295411 [doi]
AB  - Large language models, like ChatGPT and Bard, have potential clinical 
      applications due to their ability to generate conversational responses and encode 
      medical knowledge. However, their clinical adoption faces challenges including 
      hallucinations, lack of transparency, and lack of consistency. Ethicolegal 
      concerns surrounding patient consent, legal liability, and data privacy further 
      complicate matters. Despite their promise, an optimistic but cautious approach is 
      essential for the safe integration of large language models into clinical 
      settings.
FAU - Deng, Jiawen
AU  - Deng J
AUID- ORCID: 0000-0002-8274-6468
AD  - Temerty Faculty of Medicine, University of Toronto, Toronto, ON, Canada.
FAU - Zubair, Areeba
AU  - Zubair A
AD  - Temerty Faculty of Medicine, University of Toronto, Toronto, ON, Canada.
FAU - Park, Ye-Jean
AU  - Park YJ
AD  - Temerty Faculty of Medicine, University of Toronto, Toronto, ON, Canada.
FAU - Affan, Eesha
AU  - Affan E
AD  - Faculty of Science, Carleton University, Ottawa, ON, Canada.
FAU - Zuo, Qi Kang
AU  - Zuo QK
AD  - UBC Faculty of Medicine, University of British Columbia, Vancouver, BC, Canada.
LA  - eng
PT  - Journal Article
DEP - 20240124
PL  - England
TA  - Curr Med Res Opin
JT  - Current medical research and opinion
JID - 0351014
SB  - IM
MH  - Humans
MH  - *Language
MH  - *Medicine
OTO - NOTNLM
OT  - ChatGPT
OT  - GPT
OT  - large language model
OT  - medicine
EDAT- 2023/12/14 06:42
MHDA- 2024/01/25 06:43
CRDT- 2023/12/14 02:13
PHST- 2024/01/25 06:43 [medline]
PHST- 2023/12/14 06:42 [pubmed]
PHST- 2023/12/14 02:13 [entrez]
AID - 10.1080/03007995.2023.2295411 [doi]
PST - ppublish
SO  - Curr Med Res Opin. 2024 Feb;40(2):151-153. doi: 10.1080/03007995.2023.2295411. 
      Epub 2024 Jan 24.

PMID- 38046053
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231205
IS  - 1916-257X (Print)
IS  - 1916-257X (Electronic)
VI  - 16
IP  - 4
DP  - 2023 Dec
TI  - Navigating Generative AI: Opportunities, Limitations, and Ethical Considerations 
      in Massage Therapy and Beyond.
PG  - 1-4
LID - 10.3822/ijtmb.v16i4.949 [doi]
AB  - Generative artificial intelligence (AI) has become a hot topic, particularly 
      ChatGPT's quick adoption and popularity, prompting discussions about its 
      disruptive potential in health care, education, and creative sectors. The author, 
      an early adopter, shares personal insights on leveraging generative AI for 
      creative tasks and communication challenges, while also exploring its role as a 
      tool rather than an author. Opportunities and limitations of integrating 
      generative AI in the massage therapy field are explored, reflecting on the 
      profession's reluctance to embrace technology and the potential efficiency gains. 
      While acknowledging generative AI's creative promise, the importance of ethical 
      and regulated utilization, highlighting data biases and limitations, is 
      underscored. Overall, a balanced and responsible approach to incorporating 
      generative AI into various domains is recommended.
CI  - Copyright© The Author(s) 2023. Published by the Massage Therapy Foundation.
FAU - Baskwill, Amanda
AU  - Baskwill A
AD  - Loyalist College, Belleville, ON.
LA  - eng
PT  - Editorial
DEP - 20231201
PL  - United States
TA  - Int J Ther Massage Bodywork
JT  - International journal of therapeutic massage &amp; bodywork
JID - 101539415
PMC - PMC10665080
OTO - NOTNLM
OT  - ChatGPT
OT  - disruption
OT  - ethical utilization
OT  - generative artificial intelligence
OT  - massage therapy
EDAT- 2023/12/04 06:42
MHDA- 2023/12/04 06:43
PMCR- 2023/12/01
CRDT- 2023/12/04 04:48
PHST- 2023/12/04 06:43 [medline]
PHST- 2023/12/04 06:42 [pubmed]
PHST- 2023/12/04 04:48 [entrez]
PHST- 2023/12/01 00:00 [pmc-release]
AID - ijtmb-16-1 [pii]
AID - 10.3822/ijtmb.v16i4.949 [doi]
PST - epublish
SO  - Int J Ther Massage Bodywork. 2023 Dec 1;16(4):1-4. doi: 10.3822/ijtmb.v16i4.949. 
      eCollection 2023 Dec.

PMID- 37998447
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231126
IS  - 2227-9032 (Print)
IS  - 2227-9032 (Electronic)
IS  - 2227-9032 (Linking)
VI  - 11
IP  - 22
DP  - 2023 Nov 13
TI  - Reply to Moreno et al. Comment on "Sallam, M. ChatGPT Utility in Healthcare 
      Education, Research, and Practice: Systematic Review on the Promising 
      Perspectives and Valid Concerns. Healthcare 2023, 11, 887".
LID - 10.3390/healthcare11222955 [doi]
LID - 2955
AB  - I would like to thank the authors for their commentary on the publication 
      "ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic 
      Review on the Promising Perspectives and Valid Concerns" [...].
FAU - Sallam, Malik
AU  - Sallam M
AUID- ORCID: 0000-0002-0165-9670
AD  - Department of Pathology, Microbiology and Forensic Medicine, School of Medicine, 
      The University of Jordan, Amman 11942, Jordan.
AD  - Department of Clinical Laboratories and Forensic Medicine, Jordan University 
      Hospital, Amman 11942, Jordan.
LA  - eng
PT  - Journal Article
DEP - 20231113
PL  - Switzerland
TA  - Healthcare (Basel)
JT  - Healthcare (Basel, Switzerland)
JID - 101666525
CIN - Healthcare (Basel). 11:2819.
PMC - PMC10671560
COIS- The author declares no conflict of interest.
EDAT- 2023/11/24 12:42
MHDA- 2023/11/24 12:43
PMCR- 2023/11/13
CRDT- 2023/11/24 09:36
PHST- 2023/08/30 00:00 [received]
PHST- 2023/10/20 00:00 [revised]
PHST- 2023/10/26 00:00 [accepted]
PHST- 2023/11/24 12:43 [medline]
PHST- 2023/11/24 12:42 [pubmed]
PHST- 2023/11/24 09:36 [entrez]
PHST- 2023/11/13 00:00 [pmc-release]
AID - healthcare11222955 [pii]
AID - healthcare-11-02955 [pii]
AID - 10.3390/healthcare11222955 [doi]
PST - epublish
SO  - Healthcare (Basel). 2023 Nov 13;11(22):2955. doi: 10.3390/healthcare11222955.

PMID- 37528548
OWN - NLM
STAT- MEDLINE
DCOM- 20230905
LR  - 20230919
IS  - 1326-5377 (Electronic)
IS  - 0025-729X (Linking)
VI  - 219
IP  - 5
DP  - 2023 Sep 4
TI  - This too shall pass: the performance of ChatGPT-3.5, ChatGPT-4 and New Bing in an 
      Australian medical licensing examination.
PG  - 237
LID - 10.5694/mja2.52061 [doi]
FAU - Kleinig, Oliver
AU  - Kleinig O
AUID- ORCID: 0000-0003-3320-4424
AD  - University of Adelaide, Adelaide, SA.
FAU - Gao, Christina
AU  - Gao C
AUID- ORCID: 0009-0005-0033-3352
AD  - University of Adelaide, Adelaide, SA.
FAU - Bacchi, Stephen
AU  - Bacchi S
AUID- ORCID: 0000-0001-5130-8628
AD  - Royal Adelaide Hospital, Adelaide, SA.
LA  - eng
PT  - Letter
DEP - 20230801
PL  - Australia
TA  - Med J Aust
JT  - The Medical journal of Australia
JID - 0400714
SB  - IM
MH  - Australia
MH  - *Licensure
MH  - *Artificial Intelligence
MH  - Educational Measurement
OTO - NOTNLM
OT  - Artificial intelligence
EDAT- 2023/08/02 06:43
MHDA- 2023/09/05 06:42
CRDT- 2023/08/02 00:52
PHST- 2023/06/28 00:00 [revised]
PHST- 2023/03/21 00:00 [received]
PHST- 2023/07/03 00:00 [accepted]
PHST- 2023/09/05 06:42 [medline]
PHST- 2023/08/02 06:43 [pubmed]
PHST- 2023/08/02 00:52 [entrez]
AID - 10.5694/mja2.52061 [doi]
PST - ppublish
SO  - Med J Aust. 2023 Sep 4;219(5):237. doi: 10.5694/mja2.52061. Epub 2023 Aug 1.

PMID- 36872153
OWN - NLM
STAT- MEDLINE
DCOM- 20230417
LR  - 20230520
IS  - 1471-5007 (Electronic)
IS  - 1471-4922 (Linking)
VI  - 39
IP  - 5
DP  - 2023 May
TI  - Are ChatGPT and other pretrained language models good parasitologists?
PG  - 314-316
LID - S1471-4922(23)00039-9 [pii]
LID - 10.1016/j.pt.2023.02.006 [doi]
AB  - Large language models, such as ChatGPT, will have far-reaching impacts on 
      parasitology, including on students. Authentic experiences gained during 
      students' training are absent from these models. This is not a weakness of the 
      models but rather an opportunity benefiting parasitology at large.
CI  - Copyright © 2023 Elsevier Ltd. All rights reserved.
FAU - Šlapeta, Jan
AU  - Šlapeta J
AD  - Sydney School of Veterinary Science, Faculty of Science, The University of 
      Sydney, Sydney, New South Wales 2006, Australia; The University of Sydney 
      Institute for Infectious Diseases, University of Sydney, Sydney, New South Wales 
      2006, Australia. Electronic address: jan.slapeta@sydney.edu.au.
LA  - eng
PT  - Journal Article
DEP - 20230303
PL  - England
TA  - Trends Parasitol
JT  - Trends in parasitology
JID - 100966034
SB  - IM
MH  - Humans
MH  - *Language
OTO - NOTNLM
OT  - artificial intelligence
OT  - education
OT  - examination
OT  - generative pretrained transformer
OT  - study
COIS- Declaration of interests The author declares no competing interests.
EDAT- 2023/03/06 06:00
MHDA- 2023/04/17 06:41
CRDT- 2023/03/05 22:05
PHST- 2023/02/02 00:00 [received]
PHST- 2023/02/15 00:00 [revised]
PHST- 2023/02/16 00:00 [accepted]
PHST- 2023/04/17 06:41 [medline]
PHST- 2023/03/06 06:00 [pubmed]
PHST- 2023/03/05 22:05 [entrez]
AID - S1471-4922(23)00039-9 [pii]
AID - 10.1016/j.pt.2023.02.006 [doi]
PST - ppublish
SO  - Trends Parasitol. 2023 May;39(5):314-316. doi: 10.1016/j.pt.2023.02.006. Epub 
      2023 Mar 3.

PMID- 38130802
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20231223
IS  - 2055-2076 (Print)
IS  - 2055-2076 (Electronic)
IS  - 2055-2076 (Linking)
VI  - 9
DP  - 2023 Jan-Dec
TI  - Feasibility and acceptability of ChatGPT generated radiology report summaries for 
      cancer patients.
PG  - 20552076231221620
LID - 10.1177/20552076231221620 [doi]
LID - 20552076231221620
AB  - OBJECTIVE: Patients now have direct access to their radiology reports, which can 
      include complex terminology and be difficult to understand. We assessed ChatGPT's 
      ability to generate summarized MRI reports for patients with prostate cancer and 
      evaluated physician satisfaction with the artificial intelligence (AI)-summarized 
      report. METHODS: We used ChatGPT to summarize five full MRI reports for patients 
      with prostate cancer performed at a single institution from 2021 to 2022. Three 
      summarized reports were generated for each full MRI report. Full MRI and 
      summarized reports were assessed for readability using Flesch-Kincaid Grade Level 
      (FK) score. Radiation oncologists were asked to evaluate the AI-summarized 
      reports via an anonymous questionnaire. Qualitative responses were given on a 1-5 
      Likert-type scale. Fifty newly diagnosed prostate cancer patient MRIs performed 
      at a single institution were additionally assessed for physician online portal 
      response rates. RESULTS: Fifteen summarized reports were generated from five full 
      MRI reports using ChatGPT. The median FK score for the full MRI reports and 
      summarized reports was 9.6 vs. 5.0, (p &lt; 0.05), respectively. Twelve radiation 
      oncologists responded to our questionnaire. The mean [SD] ratings for summarized 
      reports were factual correctness (4.0 [0.6], understanding 4.0 [0.7]), 
      completeness (4.1 [0.5]), potential for harm (3.5 [0.9]), overall quality (3.4 
      [0.9]), and likelihood to send to patient (3.1 [1.1]). Current physician online 
      portal response rates were 14/50 (28%) at our institution. CONCLUSIONS: We 
      demonstrate a novel application of ChatGPT to summarize MRI reports at a reading 
      level appropriate for patients. Physicians were likely to be satisfied with the 
      summarized reports with respect to factual correctness, ease of understanding, 
      and completeness. Physicians were less likely to be satisfied with respect to 
      potential for harm, overall quality, and likelihood to send to patients. Further 
      research is needed to optimize ChatGPT's ability to summarize radiology reports 
      and understand what factors influence physician trust in AI-summarized reports.
CI  - © The Author(s) 2023.
FAU - Chung, Eric M
AU  - Chung EM
AUID- ORCID: 0000-0001-9651-3801
AD  - Department of Radiation Oncology, Cedars-Sinai Medical Center, Los Angeles, CA, 
      USA. RINGGOLD: 22494
AD  - Samuel Oschin Comprehensive Cancer Institute, Cedars-Sinai Medical Center, Los 
      Angeles, CA, USA. RINGGOLD: 22494
FAU - Zhang, Samuel C
AU  - Zhang SC
AD  - Department of Radiation Oncology, Cedars-Sinai Medical Center, Los Angeles, CA, 
      USA. RINGGOLD: 22494
AD  - Samuel Oschin Comprehensive Cancer Institute, Cedars-Sinai Medical Center, Los 
      Angeles, CA, USA. RINGGOLD: 22494
FAU - Nguyen, Anthony T
AU  - Nguyen AT
AD  - Department of Radiation Oncology, Cedars-Sinai Medical Center, Los Angeles, CA, 
      USA. RINGGOLD: 22494
AD  - Samuel Oschin Comprehensive Cancer Institute, Cedars-Sinai Medical Center, Los 
      Angeles, CA, USA. RINGGOLD: 22494
FAU - Atkins, Katelyn M
AU  - Atkins KM
AD  - Department of Radiation Oncology, Cedars-Sinai Medical Center, Los Angeles, CA, 
      USA. RINGGOLD: 22494
AD  - Samuel Oschin Comprehensive Cancer Institute, Cedars-Sinai Medical Center, Los 
      Angeles, CA, USA. RINGGOLD: 22494
FAU - Sandler, Howard M
AU  - Sandler HM
AD  - Department of Radiation Oncology, Cedars-Sinai Medical Center, Los Angeles, CA, 
      USA. RINGGOLD: 22494
AD  - Samuel Oschin Comprehensive Cancer Institute, Cedars-Sinai Medical Center, Los 
      Angeles, CA, USA. RINGGOLD: 22494
FAU - Kamrava, Mitchell
AU  - Kamrava M
AD  - Department of Radiation Oncology, Cedars-Sinai Medical Center, Los Angeles, CA, 
      USA. RINGGOLD: 22494
AD  - Samuel Oschin Comprehensive Cancer Institute, Cedars-Sinai Medical Center, Los 
      Angeles, CA, USA. RINGGOLD: 22494
LA  - eng
PT  - Journal Article
DEP - 20231219
PL  - United States
TA  - Digit Health
JT  - Digital health
JID - 101690863
PMC - PMC10734360
OTO - NOTNLM
OT  - artificial intelligence
OT  - cancer
OT  - digital health
OT  - disease
OT  - electronic
OT  - general
OT  - medicine
OT  - oncology
OT  - radiology
COIS- The author(s) declared no potential conflicts of interest with respect to the 
      research, authorship, and/or publication of this article: KA—honoraria Onclive 
      2021. HS—consulting/advisory role at Janssen, other relationship at Caribou 
      Publishing. MK—serves on the board of directors for American Brachytherapy 
      Society and Association for Directors of Radiation Oncology Programs, reports 
      advisory board fees for Theragenics, serves on the Data and Safety Monitoring 
      Board for Alessa Therapeutics and GammaTile, and receives book royalties from 
      Springer Publishing.
EDAT- 2023/12/22 06:43
MHDA- 2023/12/22 06:44
PMCR- 2023/12/19
CRDT- 2023/12/22 04:01
PHST- 2023/05/08 00:00 [received]
PHST- 2023/11/30 00:00 [accepted]
PHST- 2023/12/22 06:44 [medline]
PHST- 2023/12/22 06:43 [pubmed]
PHST- 2023/12/22 04:01 [entrez]
PHST- 2023/12/19 00:00 [pmc-release]
AID - 10.1177_20552076231221620 [pii]
AID - 10.1177/20552076231221620 [doi]
PST - epublish
SO  - Digit Health. 2023 Dec 19;9:20552076231221620. doi: 10.1177/20552076231221620. 
      eCollection 2023 Jan-Dec.

PMID- 38312403
OWN - NLM
STAT- PubMed-not-MEDLINE
LR  - 20240206
IS  - 2813-2092 (Electronic)
IS  - 2813-2092 (Linking)
VI  - 2
DP  - 2023
TI  - Transforming Abdominal Wall Surgery With Generative Artificial Intelligence.
PG  - 12419
LID - 10.3389/jaws.2023.12419 [doi]
LID - 12419
FAU - Mayol, Julio
AU  - Mayol J
AD  - Hospital Clinico San Carlos, Instituto de Investigación Sanitaria San Carlos, 
      Universidad Complutense de Madrid, Madrid, Spain.
LA  - eng
PT  - Journal Article
DEP - 20231127
PL  - Switzerland
TA  - J Abdom Wall Surg
JT  - Journal of abdominal wall surgery : JAWS
JID - 9918713888406676
PMC - PMC10831645
OTO - NOTNLM
OT  - AI
OT  - ChatGPT
OT  - abdominal wall
OT  - hernia
OT  - surgery
COIS- The author declares that the research was conducted in the absence of any 
      commercial or financial relationships that could be construed as a potential 
      conflict of interest.
EDAT- 2024/02/05 06:42
MHDA- 2024/02/05 06:43
PMCR- 2023/11/27
CRDT- 2024/02/05 04:18
PHST- 2023/11/14 00:00 [received]
PHST- 2023/11/16 00:00 [accepted]
PHST- 2024/02/05 06:43 [medline]
PHST- 2024/02/05 06:42 [pubmed]
PHST- 2024/02/05 04:18 [entrez]
PHST- 2023/11/27 00:00 [pmc-release]
AID - 12419 [pii]
AID - 10.3389/jaws.2023.12419 [doi]
PST - epublish
SO  - J Abdom Wall Surg. 2023 Nov 27;2:12419. doi: 10.3389/jaws.2023.12419. eCollection 
      2023.

PMID- 37890505
OWN - NLM
STAT- Publisher
LR  - 20231027
IS  - 1439-3999 (Electronic)
IS  - 0023-2165 (Linking)
DP  - 2023 Oct 27
TI  - [ChatGPT in Ophthalmology - A Report].
LID - 10.1055/a-2142-2910 [doi]
FAU - Herwig-Carl, Martina C
AU  - Herwig-Carl MC
AUID- ORCID: 0000-0002-5943-7675
AD  - Klinik für Augenheilkunde, Universitätsklinikum Bonn, Deutschland.
LA  - ger
PT  - Journal Article
TT  - ChatGPT in der Augenheilkunde – ein Erfahrungsbericht.
DEP - 20231027
PL  - Germany
TA  - Klin Monbl Augenheilkd
JT  - Klinische Monatsblatter fur Augenheilkunde
JID - 0014133
SB  - IM
COIS- Glaxo Smith Kline (Teilnahme an Advisory Board, Referent). In Bezug auf diese 
      Arbeit besteht kein Interessenkonflikt.
EDAT- 2023/10/28 11:42
MHDA- 2023/10/28 11:42
CRDT- 2023/10/27 19:07
PHST- 2023/10/28 11:42 [medline]
PHST- 2023/10/28 11:42 [pubmed]
PHST- 2023/10/27 19:07 [entrez]
AID - 10.1055/a-2142-2910 [doi]
PST - aheadofprint
SO  - Klin Monbl Augenheilkd. 2023 Oct 27. doi: 10.1055/a-2142-2910.
</pre>
      
    </div>
  </main>


  
  


  


</body></html>